{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tensorflow/lingvo/blob/master/codelabs/introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "od97_nvR82Qo"
   },
   "source": [
    "# Introduction to Lingvo\n",
    "\n",
    "This codelab will guide you through the implementation of a **sequence-to-sequence** model using [**Lingvo**](https://github.com/tensorflow/lingvo).\n",
    "\n",
    "**Sequence-to-sequence** models map input sequences of arbitrary length to\n",
    "output sequences of arbitrary length. Example uses of sequence-to-sequence\n",
    "models include machine translation, which maps a sequence of words from one\n",
    "language into a sequence of words in another language with the same meaning;\n",
    "speech recognition, which maps a sequence of acoustic features into a sequence\n",
    "of words; and text summarization, which\n",
    "maps a sequence of words into a shorter sequence which conveys the same meaning.\n",
    "\n",
    "In this codelab, you will create a model which restores punctuation and\n",
    "capitalization to text which has been lowercased and stripped of punctuation.\n",
    "For example, given the following text:\n",
    "\n",
    "> she asked do you know the way to san jose\n",
    "\n",
    "The model will output the following properly-punctuated-and-capitalized text:\n",
    "\n",
    "> She asked, \"Do you know the way to San Jose\"?\n",
    "\n",
    "We will train an RNMT+ model based off of [\"The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation. (Chen et al., 2018)\"](https://arxiv.org/abs/1804.09849)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4bPGJjZkLBdz"
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "In Colab, click `[View]->[Table of contents]` on the menu bar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bnpyjXz1OFAu"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "The main goal of this codelab is to teach you how to define and train sequence-to-sequence models in Lingvo. We do not aim to teach either Python or Tensorflow, and no sophisticated Python or Tensorflow programming will be required. However, the following will be helpful in understanding this codelab:\n",
    "\n",
    "-   Familiarity with high-level machine learning primitives, in particular,\n",
    "    recurrent neural networks, LSTMs, and attention.\n",
    "-   Comfort reading and writing simple Python code. In particular, you should\n",
    "    know how to define simple classes and how inheritance works.\n",
    "-   Basic knowledge of the Tensorflow training workflow.  If you have trained\n",
    "    simple Tensorflow models before (e.g., via another codelab), you should know\n",
    "    enough for this codelab.\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Introduction to RNNs and LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Official Tensorflow Tutorials](https://www.tensorflow.org/tutorials/)\n",
    "- For more advanced topics or to get a deeper understanding of Lingvo beyond this codelab, see the [paper](https://arxiv.org/abs/1902.08295)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "coa-U7N6Kunb"
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "This codelab will teach you the following:\n",
    "\n",
    "-   How to generate input data for training a sequence-to-sequence model in Lingvo.\n",
    "-   How models are specified and configured in Lingvo, by adapting a pre-existing model architecture for machine translation.\n",
    "-   How to use the trained model for inference.\n",
    "\n",
    "This codelab does not:\n",
    "\n",
    "-   Teach you how to design a model for solving specific tasks.\n",
    "-   Provide a state-of-the-art model for the punctuator task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_mtJSYor9d6d"
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "To start with, we need to connect this Colab notebook with Lingvo.\n",
    "\n",
    "```shell\n",
    "mkdir -p /tmp/lingvo_codelab && cd /tmp/lingvo_codelab\n",
    "pip3 install lingvo\n",
    "python3 -m lingvo.ipython_kernel\n",
    "```\n",
    "\n",
    "Finally, on the top right hand side of this Colab notebook, open the dropdown beside \"CONNECT\" and select \"Connect to local runtime...\", enter `http://localhost:8888` and press CONNECT.\n",
    "\n",
    "You should now see the words \"CONNECTED\" and be able to execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2JRfZTumHxpY"
   },
   "outputs": [],
   "source": [
    "import lingvo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lkN9qajvNtEs"
   },
   "source": [
    "## Input Pipeline\n",
    "\n",
    "In order to train a sequence-to-sequence model, we need a set of pairs of source\n",
    "and target sequences. For this codelab, our source sequences will be\n",
    "text which has been lowercased and had its punctuation removed, and the target\n",
    "sequences will be the original sentences, with their original casing and\n",
    "punctuation.\n",
    "\n",
    "Since neural networks require numeric inputs, we will also need a tokenization scheme mapping the sequence of characters to a sequence of numbers. In this codelab, we will use a pre-trained word-piece model.\n",
    "\n",
    "### Download Raw Input\n",
    "\n",
    "We will use the [Brown Corpus](http://www.nltk.org/nltk_data) as the source of our training data. Run the following cell to download and preprocess the dataset. The script will generate `train.txt` and `test.txt` containing the training and test data at an 80:20 split with individual sentences on each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Asfie_a8YmN3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Lingvo does not support eager execution yet. Please disable eager execution with tf.compat.v1.disable_eager_execution() or proceed at your own risk.\n",
      "Downloading data from https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/brown_tei.zip\n",
      "8740864/8737738 [==============================] - 0s 0us/step\n",
      "I0709 07:36:43.656234 140188354057600 download_brown_corpus.py:46] \n",
      "Download completed. Preprocessing...\n",
      "I0709 07:36:47.080183 140188354057600 download_brown_corpus.py:140] All done.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  145k  100  145k    0     0   592k      0 --:--:-- --:--:-- --:--:--  592k\n"
     ]
    }
   ],
   "source": [
    "!python3 -m lingvo.tasks.punctuator.tools.download_brown_corpus --outdir=/tmp/punctuator_data\n",
    "!curl -O https://raw.githubusercontent.com/tensorflow/lingvo/master/lingvo/tasks/punctuator/params/brown_corpus_wpm.16000.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BHq87NF-8lgD"
   },
   "source": [
    "### Define an Input Generator\n",
    "\n",
    "In order to train a model, we need an input generator that provides structured mini-batches of source-target pairs. The input generator handles all the processing necessary to generate numeric data that can be fed to the model. This includes:\n",
    "\n",
    "- reading examples from the data source in random order, where the data source may be split across multiple files;\n",
    "- processing the data -- for our task this involves generating a \"source\" sentence by converting all characters to lower-case and removing punctuation, and then tokenizing both the source and target sequences into integer tokens; and\n",
    "- batching together examples by padding them to the same length. Multiple buckets of different lengths may be used to avoid inefficiency from padding a short input to a very long length.\n",
    "\n",
    "Fortunately, the majority of this is handled in the background by Lingvo. We only need to specify how the data should be processed.\n",
    "\n",
    "Input generators are subclasses of *BaseInputGenerator* found in [lingvo/core/base_input_generator.py](https://github.com/tensorflow/lingvo/blob/master/lingvo/core/base_input_generator.py) and have the following structure:\n",
    "\n",
    "- a `Params` classmethod that returns a default Params object for configuring the input generator. Experiment configurations inside Lingvo are controlled using these Params objects.\n",
    "- an `_InputBatch` method that returns a [`NestedMap`](https://github.com/tensorflow/lingvo/blob/3344e201719961183d88713784ccbae447f5c52a/lingvo/core/py_utils.py#L392) containing the input batch. `NestedMap` is an arbitrarily nested map structure used throughout Lingvo.\n",
    "- an optional `_PreprocessInputBatch` method that preprocesses the batch returned by `_InputBatch`.\n",
    "\n",
    "Here is an example of the input generator for the Punctuator task, found in [lingvo/tasks/punctuator/input_generator.py](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/punctuator/input_generator.py).\n",
    "\n",
    "Run the cell below to write the file to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T2BzBCqE_yvt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting input_generator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile input_generator.py\n",
    "import string\n",
    "import lingvo.compat as tf\n",
    "from lingvo.core import base_input_generator\n",
    "from lingvo.core import base_layer\n",
    "from lingvo.core import generic_input\n",
    "from lingvo.core import py_utils\n",
    "from lingvo.core import tokenizers\n",
    "\n",
    "\n",
    "class PunctuatorInput(base_input_generator.BaseSequenceInputGenerator):\n",
    "  \"\"\"Reads text line by line and processes them for the punctuator task.\"\"\"\n",
    "\n",
    "  @classmethod\n",
    "  def Params(cls):\n",
    "    \"\"\"Defaults params for PunctuatorInput.\"\"\"\n",
    "    p = super(PunctuatorInput, cls).Params()\n",
    "    p.tokenizer = tokenizers.WpmTokenizer.Params()\n",
    "    return p\n",
    "\n",
    "  def _ProcessLine(self, line):\n",
    "    \"\"\"A single-text-line processor.\n",
    "    Gets a string tensor representing a line of text that have been read from\n",
    "    the input file, and splits it to graphemes (characters).\n",
    "    We use original characters as the target labels, and the lowercased and\n",
    "    punctuation-removed characters as the source labels.\n",
    "    Args:\n",
    "      line: a 1D string tensor.\n",
    "    Returns:\n",
    "      A list of tensors, in the expected order by __init__.\n",
    "    \"\"\"\n",
    "    # Tokenize the input into integer ids.\n",
    "    # tgt_ids has the start-of-sentence token prepended, and tgt_labels has the\n",
    "    # end-of-sentence token appended.\n",
    "    tgt_ids, tgt_labels, tgt_paddings = self.StringsToIds(\n",
    "        tf.convert_to_tensor([line]))\n",
    "\n",
    "    def Normalize(line):\n",
    "      # Lowercase and remove punctuation.\n",
    "      line = line.lower().translate(None, string.punctuation.encode('utf-8'))\n",
    "      # Convert multiple consecutive spaces to a single one.\n",
    "      line = b' '.join(line.split())\n",
    "      return line\n",
    "\n",
    "    normalized_line = tf.py_func(Normalize, [line], tf.string, stateful=False)\n",
    "    _, src_labels, src_paddings = self.StringsToIds(\n",
    "        tf.convert_to_tensor([normalized_line]), is_source=True)\n",
    "    # The model expects the source without a start-of-sentence token.\n",
    "    src_ids = src_labels\n",
    "\n",
    "    # Compute the length for bucketing.\n",
    "    bucket_key = tf.cast(\n",
    "        tf.round(\n",
    "            tf.maximum(\n",
    "                tf.reduce_sum(1.0 - src_paddings),\n",
    "                tf.reduce_sum(1.0 - tgt_paddings))), tf.int32)\n",
    "    tgt_weights = 1.0 - tgt_paddings\n",
    "\n",
    "    # Return tensors in an order consistent with __init__.\n",
    "    out_tensors = [\n",
    "        src_ids, src_paddings, tgt_ids, tgt_paddings, tgt_labels, tgt_weights\n",
    "    ]\n",
    "    return [tf.squeeze(t, axis=0) for t in out_tensors], bucket_key\n",
    "\n",
    "  def _DataSourceFromFilePattern(self, file_pattern):\n",
    "    \"\"\"Create the input processing op.\n",
    "    Args:\n",
    "      file_pattern: The file pattern to use as input.\n",
    "    Returns:\n",
    "      an operation that when executed, calls `_ProcessLine` on a line read\n",
    "    from `file_pattern`.\n",
    "    \"\"\"\n",
    "    return generic_input.GenericInput(\n",
    "        file_pattern=file_pattern,\n",
    "        processor=self._ProcessLine,\n",
    "        # Pad dimension 0 to the same length.\n",
    "        dynamic_padding_dimensions=[0] * 6,\n",
    "        # The constant values to use for padding each of the outputs.\n",
    "        dynamic_padding_constants=[0, 1, 0, 1, 0, 0],\n",
    "        **self.CommonInputOpArgs())\n",
    "\n",
    "  def __init__(self, params):\n",
    "    super(PunctuatorInput, self).__init__(params)\n",
    "\n",
    "    # Build the input processing graph.\n",
    "    (self._src_ids, self._src_paddings, self._tgt_ids, self._tgt_paddings,\n",
    "     self._tgt_labels,\n",
    "     self._tgt_weights), self._bucket_keys = self._BuildDataSource()\n",
    "\n",
    "    self._sample_ids = tf.range(0, self.InfeedBatchSize(), 1)\n",
    "\n",
    "  def InfeedBatchSize(self):\n",
    "    return tf.shape(self._src_ids)[0]\n",
    "\n",
    "  def _InputBatch(self):\n",
    "    \"\"\"Returns a single batch as a `.NestedMap` to be passed to the model.\"\"\"\n",
    "    ret = py_utils.NestedMap()\n",
    "\n",
    "    ret.bucket_keys = self._bucket_keys\n",
    "\n",
    "    ret.src = py_utils.NestedMap()\n",
    "    ret.src.ids = tf.cast(self._src_ids, dtype=tf.int32)\n",
    "    ret.src.paddings = self._src_paddings\n",
    "\n",
    "    ret.tgt = py_utils.NestedMap()\n",
    "    ret.tgt.ids = self._tgt_ids\n",
    "    ret.tgt.labels = tf.cast(self._tgt_labels, dtype=tf.int32)\n",
    "    ret.tgt.weights = self._tgt_weights\n",
    "    ret.tgt.paddings = self._tgt_paddings\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lrwT1f0IfPca"
   },
   "source": [
    "## Model Definition\n",
    "\n",
    "Next, we need to define the network structure for the task. The network is a nested structure of layers. Most classes in Lingvo are subclasses of *BaseLayer* found in [lingvo/core/base_layer.py](https://github.com/tensorflow/lingvo/blob/560f838bd576c7b911df379121eb58252b6ae326/lingvo/core/base_layer.py#L150) and inherit the following:\n",
    "\n",
    "- a Params classmethod that returns a default [Params](https://github.com/tensorflow/lingvo/blob/3344e201719961183d88713784ccbae447f5c52a/lingvo/core/hyperparams.py#L151) object for configuring the class. In addition to hyperparameters, the Params object can also contain Params objects for configuring child layers. Some of the properties present in all Params objects include:\n",
    "  - `cls`: the python class that the Params object is associated with. This can be used to construct an instance of the class;\n",
    "  - `name`: the name of this layer;\n",
    "  - `dtype`: the default dtype to use when creating variables.\n",
    "- The `__init__` constructor. All child layers and variables should be created here.\n",
    "- A `CreateVariable` method that is called to create variables.\n",
    "- A `CreateChild` method that is called to create child layers.\n",
    "- A `FProp` method that implements forward propagation through the layer.\n",
    "\n",
    "As a reference, many examples of layers can be found in [lingvo/core/layers.py](https://github.com/tensorflow/lingvo/blob/master/lingvo/core/layers.py), [lingvo/core/attention.py](https://github.com/tensorflow/lingvo/blob/master/lingvo/core/attention.py), and [lingvo/core/rnn_layers.py](https://github.com/tensorflow/lingvo/blob/master/lingvo/core/rnn_layers.py).\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "The root layer for the network should be a subclass of `BaseTask` found in [lingvo/core/base_model.py](https://github.com/tensorflow/lingvo/blob/918c584f057481717eff6e1e29ae028aeab3d165/lingvo/core/base_model.py#L79), and implements the following:\n",
    "\n",
    "- A `ComputePredictions` method that takes the current variable values (`theta`) and `input_batch` and returns the network predictions.\n",
    "- A `ComputeLoss` method that takes `theta`, `input_batch`, and the `predictions` returned from `ComputePredictions` and returns a dictionary of scalar metrics, one of which should be `loss`. These scalar metrics are exported to TensorBoard as summaries.\n",
    "- An optional `Decode` method for creating a separate graph for decoding. For example, training and evaluation might use teacher forcing while decoding might not.\n",
    "- An optional `Inference` method that returns a graph with feeds and fetches that can be used together with a saved checkpoint for inference. This differs from `Decode` in that it can be fed data directly instead of using data from the input generator.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "This codelab uses the existing networks from [lingvo/tasks/punctuator/model.py](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/punctuator/model.py), which is derived from the networks in [lingvo/tasks/mt/model.py](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/mt/model.py) with an added `Inference` method for the punctuator task. The actual logic lies mostly in [lingvo/tasks/mt/encoder.py](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/mt/encoder.py) and [lingvo/tasks/mt/decoder.py](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/mt/decoder.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ItaOgXNpql0"
   },
   "source": [
    "## Model Configuration\n",
    "\n",
    "After defining the input generator and the network, we need to create an model configuration with the specific hyperparameters to use for our model.\n",
    "\n",
    "Since there is only a single task, we create a subclass of `SingleTaskModelParams` found in [lingvo/core/base_model_params.py](https://github.com/tensorflow/lingvo/blob/4747cf80a7e6cf58211aa899bae854820a3b42f6/lingvo/core/base_model_params.py#L47). It has the following structure:\n",
    "\n",
    "- The `Train`/`Dev`/`Test` methods configure the input generator for the respective datasets.\n",
    "- The `Task` method configures the network.\n",
    "\n",
    "The following cell contains the configuration that will be used in this codelab. It can also be found in [lingvo/tasks/punctuator/params/codelab.py](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/punctuator/params/codelab.py). The network configuration in the `Task` classmethod is delegated to [lingvo/tasks/mt/params/base_config.py](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/mt/params/base_config.py).\n",
    "\n",
    "Run the cell below to write the file to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3CjruUvXY5we"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codelab.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codelab.py\n",
    "import input_generator\n",
    "import os\n",
    "from lingvo import model_registry\n",
    "import lingvo.compat as tf\n",
    "from lingvo.core import base_model_params\n",
    "from lingvo.tasks.mt import base_config\n",
    "from lingvo.tasks.punctuator import model\n",
    "\n",
    "\n",
    "# This base class defines parameters for the input generator for a specific\n",
    "# dataset. Specific network architectures will be implemented in subclasses.\n",
    "class BrownCorpusWPM(base_model_params.SingleTaskModelParams):\n",
    "  \"\"\"Brown Corpus data with a Word-Piece Model tokenizer.\"\"\"\n",
    "\n",
    "  # Generated using\n",
    "  # lingvo/tasks/punctuator/tools:download_brown_corpus.\n",
    "  _DATADIR = '/tmp/punctuator_data'\n",
    "  _VOCAB_FILE = 'brown_corpus_wpm.16000.vocab'\n",
    "  # _VOCAB_SIZE needs to be a multiple of 16 because we use a sharded softmax\n",
    "  # with 16 shards.\n",
    "  _VOCAB_SIZE = 16000\n",
    "\n",
    "  def Train(self):\n",
    "    p = input_generator.PunctuatorInput.Params()\n",
    "    p.file_pattern = 'text:' + os.path.join(self._DATADIR, 'train.txt')\n",
    "    p.file_random_seed = 0  # Do not use a fixed seed.\n",
    "    p.file_parallelism = 1  # We only have a single input file.\n",
    "\n",
    "    # The bucket upper bound specifies how to split the input into buckets. We\n",
    "    # train on sequences up to maximum bucket size and discard longer examples.\n",
    "    p.bucket_upper_bound = [10, 20, 30, 60, 120]\n",
    "\n",
    "    # The bucket batch limit determines how many examples are there in each\n",
    "    # batch during training. We reduce the batch size for the buckets that\n",
    "    # have higher upper bound (batches that consist of longer sequences)\n",
    "    # in order to prevent out of memory issues.\n",
    "    # Note that this hyperparameter varies widely based on the model and\n",
    "    # language. Larger models may warrant smaller batches in order to fit in\n",
    "    # memory, for example; and ideographical languages like Chinese may benefit\n",
    "    # from more buckets.\n",
    "    p.bucket_batch_limit = [512, 256, 160, 80, 40]\n",
    "\n",
    "    p.tokenizer.vocab_filepath = self._VOCAB_FILE\n",
    "    p.tokenizer.vocab_size = self._VOCAB_SIZE\n",
    "    p.tokenizer.pad_to_max_length = False\n",
    "\n",
    "    # Set the tokenizer max length slightly longer than the largest bucket to\n",
    "    # discard examples that are longer than we allow.\n",
    "    p.source_max_length = p.bucket_upper_bound[-1] + 2\n",
    "    p.target_max_length = p.bucket_upper_bound[-1] + 2\n",
    "    return p\n",
    "\n",
    "  # There is also a Dev method for dev set params, but we don't have a dev set.\n",
    "  def Test(self):\n",
    "    p = input_generator.PunctuatorInput.Params()\n",
    "    p.file_pattern = 'text:' + os.path.join(self._DATADIR, 'test.txt')\n",
    "    p.file_random_seed = 27182818  # Fix random seed for testing.\n",
    "    # The following two parameters are important if there's more than one input\n",
    "    # file. For this codelab it doesn't actually matter.\n",
    "    p.file_parallelism = 1  # Avoid randomness in testing.\n",
    "    # In order to make exactly one pass over the dev/test sets, we set buffer\n",
    "    # size to 1. Greater numbers may cause inaccurate dev/test scores.\n",
    "    p.file_buffer_size = 1\n",
    "\n",
    "    p.bucket_upper_bound = [10, 20, 30, 60, 120, 200]\n",
    "    p.bucket_batch_limit = [16] * 4 + [4] * 2\n",
    "\n",
    "    p.tokenizer.vocab_filepath = self._VOCAB_FILE\n",
    "    p.tokenizer.vocab_size = self._VOCAB_SIZE\n",
    "    p.tokenizer.pad_to_max_length = False\n",
    "\n",
    "    p.source_max_length = p.bucket_upper_bound[-1] + 2\n",
    "    p.target_max_length = p.bucket_upper_bound[-1] + 2\n",
    "    return p\n",
    "\n",
    "\n",
    "# This decorator registers the model in the Lingvo model registry.\n",
    "# This file is lingvo/tasks/punctuator/params/codelab.py,\n",
    "# so the model will be registered as punctuator.codelab.RNMTModel.\n",
    "@model_registry.RegisterSingleTaskModel\n",
    "class RNMTModel(BrownCorpusWPM):\n",
    "  \"\"\"RNMT+ Model.\"\"\"\n",
    "\n",
    "  def Task(self):\n",
    "    p = base_config.SetupRNMTParams(\n",
    "        model.RNMTModel.Params(),\n",
    "        name='punctuator_rnmt',\n",
    "        vocab_size=self._VOCAB_SIZE,\n",
    "        embedding_dim=1024,\n",
    "        hidden_dim=1024,\n",
    "        num_heads=4,\n",
    "        num_encoder_layers=6,\n",
    "        num_decoder_layers=8,\n",
    "        learning_rate=1e-4,\n",
    "        l2_regularizer_weight=1e-5,\n",
    "        lr_warmup_steps=500,\n",
    "        lr_decay_start=400000,\n",
    "        lr_decay_end=1200000,\n",
    "        lr_min=0.5,\n",
    "        ls_uncertainty=0.1,\n",
    "        atten_dropout_prob=0.3,\n",
    "        residual_dropout_prob=0.3,\n",
    "        adam_beta2=0.98,\n",
    "        adam_epsilon=1e-6,\n",
    "    )\n",
    "    p.eval.samples_per_summary = 2466\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ooJYcOA4tcXW"
   },
   "source": [
    "## Model Training\n",
    "\n",
    "The following cell trains the model. Note that this will require approximately 2.5GB of space in `logdir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "91eOF9Sqy1iG",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Lingvo does not support eager execution yet. Please disable eager execution with tf.compat.v1.disable_eager_execution() or proceed at your own risk.\n",
      "model_imports.py: Importing codelab\n",
      "model_imports.py: Imported codelab\n",
      "2020-07-10 07:18:44.327387: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-07-10 07:18:44.336734: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n",
      "2020-07-10 07:18:44.337717: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558583bd7910 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-07-10 07:18:44.337748: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-07-10 07:18:44.340790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-07-10 07:18:44.768869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.802775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.812015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.825100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.826371: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558583c9cc20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-07-10 07:18:44.826405: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2020-07-10 07:18:44.826419: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2020-07-10 07:18:44.826432: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2020-07-10 07:18:44.826438: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2020-07-10 07:18:44.829812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.830908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2020-07-10 07:18:44.831007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.831994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \n",
      "pciBusID: 0000:00:05.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2020-07-10 07:18:44.832064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.833042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 2 with properties: \n",
      "pciBusID: 0000:00:06.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2020-07-10 07:18:44.833102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.834063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 3 with properties: \n",
      "pciBusID: 0000:00:07.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2020-07-10 07:18:44.834315: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-07-10 07:18:44.836281: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-07-10 07:18:44.838033: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-07-10 07:18:44.838386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-07-10 07:18:44.840292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-07-10 07:18:44.841444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-07-10 07:18:44.845700: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-07-10 07:18:44.845819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.846958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.847993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.849014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.850122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.851234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.852299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.853347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.854393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2020-07-10 07:18:44.854459: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-07-10 07:18:44.861342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-07-10 07:18:44.861370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 2 3 \n",
      "2020-07-10 07:18:44.861378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y Y Y \n",
      "2020-07-10 07:18:44.861384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N Y Y \n",
      "2020-07-10 07:18:44.861389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 2:   Y Y N Y \n",
      "2020-07-10 07:18:44.861396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 3:   Y Y Y N \n",
      "2020-07-10 07:18:44.861742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.862875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.863958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.864983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.866016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.867056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14576 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n",
      "2020-07-10 07:18:44.867759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.868796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 14576 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:05.0, compute capability: 7.0)\n",
      "2020-07-10 07:18:44.869399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.870517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 14576 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:06.0, compute capability: 7.0)\n",
      "2020-07-10 07:18:44.871104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:44.872239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 14576 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:07.0, compute capability: 7.0)\n",
      "2020-07-10 07:18:44.879739: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45289}\n",
      "2020-07-10 07:18:44.880714: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:45289\n",
      "I0710 07:18:44.881058 140310643746176 trainer.py:1539] Job controller start\n",
      "model_imports.py: Importing codelab\n",
      "model_imports.py: Imported codelab\n",
      "I0710 07:18:44.901668 140310643746176 base_runner.py:57] ============================================================\n",
      "I0710 07:18:44.911722 140310643746176 base_runner.py:59] allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.911852 140310643746176 base_runner.py:59] cls : type/lingvo.core.base_model/SingleTaskModel\n",
      "I0710 07:18:44.911909 140310643746176 base_runner.py:59] cluster.add_summary : NoneType\n",
      "I0710 07:18:44.911955 140310643746176 base_runner.py:59] cluster.cls : type/lingvo.core.cluster/_Cluster\n",
      "I0710 07:18:44.911998 140310643746176 base_runner.py:59] cluster.controller.cpus_per_replica : 1\n",
      "I0710 07:18:44.912046 140310643746176 base_runner.py:59] cluster.controller.devices_per_split : 1\n",
      "I0710 07:18:44.912088 140310643746176 base_runner.py:59] cluster.controller.gpus_per_replica : 0\n",
      "I0710 07:18:44.912130 140310643746176 base_runner.py:59] cluster.controller.name : '/job:localhost'\n",
      "I0710 07:18:44.912168 140310643746176 base_runner.py:59] cluster.controller.num_tpu_hosts : 0\n",
      "I0710 07:18:44.912206 140310643746176 base_runner.py:59] cluster.controller.replicas : 1\n",
      "I0710 07:18:44.912246 140310643746176 base_runner.py:59] cluster.controller.targets : ''\n",
      "I0710 07:18:44.912285 140310643746176 base_runner.py:59] cluster.controller.tpus_per_replica : 0\n",
      "I0710 07:18:44.912324 140310643746176 base_runner.py:59] cluster.decoder.cpus_per_replica : 1\n",
      "I0710 07:18:44.912364 140310643746176 base_runner.py:59] cluster.decoder.devices_per_split : 1\n",
      "I0710 07:18:44.912403 140310643746176 base_runner.py:59] cluster.decoder.gpus_per_replica : 1\n",
      "I0710 07:18:44.912441 140310643746176 base_runner.py:59] cluster.decoder.name : '/job:localhost'\n",
      "I0710 07:18:44.912480 140310643746176 base_runner.py:59] cluster.decoder.num_tpu_hosts : 0\n",
      "I0710 07:18:44.912519 140310643746176 base_runner.py:59] cluster.decoder.replicas : 1\n",
      "I0710 07:18:44.912558 140310643746176 base_runner.py:59] cluster.decoder.targets : ''\n",
      "I0710 07:18:44.912597 140310643746176 base_runner.py:59] cluster.decoder.tpus_per_replica : 0\n",
      "I0710 07:18:44.912636 140310643746176 base_runner.py:59] cluster.do_eval : False\n",
      "I0710 07:18:44.912674 140310643746176 base_runner.py:59] cluster.evaler.cpus_per_replica : 1\n",
      "I0710 07:18:44.912713 140310643746176 base_runner.py:59] cluster.evaler.devices_per_split : 1\n",
      "I0710 07:18:44.912752 140310643746176 base_runner.py:59] cluster.evaler.gpus_per_replica : 1\n",
      "I0710 07:18:44.912790 140310643746176 base_runner.py:59] cluster.evaler.name : '/job:localhost'\n",
      "I0710 07:18:44.912828 140310643746176 base_runner.py:59] cluster.evaler.num_tpu_hosts : 0\n",
      "I0710 07:18:44.912867 140310643746176 base_runner.py:59] cluster.evaler.replicas : 1\n",
      "I0710 07:18:44.912906 140310643746176 base_runner.py:59] cluster.evaler.targets : ''\n",
      "I0710 07:18:44.912945 140310643746176 base_runner.py:59] cluster.evaler.tpus_per_replica : 0\n",
      "I0710 07:18:44.912983 140310643746176 base_runner.py:59] cluster.input.cpus_per_replica : 1\n",
      "I0710 07:18:44.913027 140310643746176 base_runner.py:59] cluster.input.devices_per_split : 1\n",
      "I0710 07:18:44.913067 140310643746176 base_runner.py:59] cluster.input.gpus_per_replica : 0\n",
      "I0710 07:18:44.913106 140310643746176 base_runner.py:59] cluster.input.name : '/job:localhost'\n",
      "I0710 07:18:44.913145 140310643746176 base_runner.py:59] cluster.input.num_tpu_hosts : 0\n",
      "I0710 07:18:44.913184 140310643746176 base_runner.py:59] cluster.input.replicas : 0\n",
      "I0710 07:18:44.913223 140310643746176 base_runner.py:59] cluster.input.targets : ''\n",
      "I0710 07:18:44.913261 140310643746176 base_runner.py:59] cluster.input.tpus_per_replica : 0\n",
      "I0710 07:18:44.913300 140310643746176 base_runner.py:59] cluster.job : 'controller'\n",
      "I0710 07:18:44.913339 140310643746176 base_runner.py:59] cluster.logdir : ''\n",
      "I0710 07:18:44.913376 140310643746176 base_runner.py:59] cluster.mode : 'sync'\n",
      "I0710 07:18:44.913414 140310643746176 base_runner.py:59] cluster.ps.cpus_per_replica : 1\n",
      "I0710 07:18:44.913453 140310643746176 base_runner.py:59] cluster.ps.devices_per_split : 1\n",
      "I0710 07:18:44.913492 140310643746176 base_runner.py:59] cluster.ps.gpus_per_replica : 0\n",
      "I0710 07:18:44.913531 140310643746176 base_runner.py:59] cluster.ps.name : '/job:localhost'\n",
      "I0710 07:18:44.913570 140310643746176 base_runner.py:59] cluster.ps.num_tpu_hosts : 0\n",
      "I0710 07:18:44.913609 140310643746176 base_runner.py:59] cluster.ps.replicas : 1\n",
      "I0710 07:18:44.913648 140310643746176 base_runner.py:59] cluster.ps.targets : ''\n",
      "I0710 07:18:44.913686 140310643746176 base_runner.py:59] cluster.ps.tpus_per_replica : 0\n",
      "I0710 07:18:44.913726 140310643746176 base_runner.py:59] cluster.split_id : 0\n",
      "I0710 07:18:44.913765 140310643746176 base_runner.py:59] cluster.task : 0\n",
      "I0710 07:18:44.913804 140310643746176 base_runner.py:59] cluster.worker.cpus_per_replica : 1\n",
      "I0710 07:18:44.913843 140310643746176 base_runner.py:59] cluster.worker.devices_per_split : 1\n",
      "I0710 07:18:44.913882 140310643746176 base_runner.py:59] cluster.worker.gpus_per_replica : 1\n",
      "I0710 07:18:44.913941 140310643746176 base_runner.py:59] cluster.worker.name : '/job:localhost'\n",
      "I0710 07:18:44.913981 140310643746176 base_runner.py:59] cluster.worker.num_tpu_hosts : 0\n",
      "I0710 07:18:44.914026 140310643746176 base_runner.py:59] cluster.worker.replicas : 1\n",
      "I0710 07:18:44.914066 140310643746176 base_runner.py:59] cluster.worker.targets : ''\n",
      "I0710 07:18:44.914105 140310643746176 base_runner.py:59] cluster.worker.tpus_per_replica : 0\n",
      "I0710 07:18:44.914144 140310643746176 base_runner.py:59] dtype : float32\n",
      "I0710 07:18:44.914183 140310643746176 base_runner.py:59] fprop_dtype : NoneType\n",
      "I0710 07:18:44.914221 140310643746176 base_runner.py:59] inference_driver_name : NoneType\n",
      "I0710 07:18:44.914260 140310643746176 base_runner.py:59] input.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.914299 140310643746176 base_runner.py:59] input.bucket_adjust_every_n : 0\n",
      "I0710 07:18:44.914338 140310643746176 base_runner.py:59] input.bucket_batch_limit : [512, 256, 160, 80, 40]\n",
      "I0710 07:18:44.914377 140310643746176 base_runner.py:59] input.bucket_upper_bound : [10, 20, 30, 60, 120]\n",
      "I0710 07:18:44.914416 140310643746176 base_runner.py:59] input.cls : type/input_generator/PunctuatorInput\n",
      "I0710 07:18:44.914454 140310643746176 base_runner.py:59] input.dtype : float32\n",
      "I0710 07:18:44.914494 140310643746176 base_runner.py:59] input.file_buffer_size : 10000\n",
      "I0710 07:18:44.914533 140310643746176 base_runner.py:59] input.file_buffer_size_in_seconds : 0\n",
      "I0710 07:18:44.914571 140310643746176 base_runner.py:59] input.file_datasource : NoneType\n",
      "I0710 07:18:44.914610 140310643746176 base_runner.py:59] input.file_parallelism : 1\n",
      "I0710 07:18:44.914649 140310643746176 base_runner.py:59] input.file_pattern : 'text:/tmp/punctuator_data/train.txt'\n",
      "I0710 07:18:44.914687 140310643746176 base_runner.py:59] input.file_random_seed : 0\n",
      "I0710 07:18:44.914726 140310643746176 base_runner.py:59] input.flush_every_n : 0\n",
      "I0710 07:18:44.914764 140310643746176 base_runner.py:59] input.fprop_dtype : NoneType\n",
      "I0710 07:18:44.914802 140310643746176 base_runner.py:59] input.inference_driver_name : NoneType\n",
      "I0710 07:18:44.914841 140310643746176 base_runner.py:59] input.is_inference : NoneType\n",
      "I0710 07:18:44.914879 140310643746176 base_runner.py:59] input.name : 'input'\n",
      "I0710 07:18:44.914919 140310643746176 base_runner.py:59] input.num_batcher_threads : 1\n",
      "I0710 07:18:44.914957 140310643746176 base_runner.py:59] input.num_partitions : NoneType\n",
      "I0710 07:18:44.914996 140310643746176 base_runner.py:59] input.num_samples : 0\n",
      "I0710 07:18:44.915040 140310643746176 base_runner.py:59] input.pad_to_max_seq_length : False\n",
      "I0710 07:18:44.915087 140310643746176 base_runner.py:59] input.params_init.method : 'xavier'\n",
      "I0710 07:18:44.915170 140310643746176 base_runner.py:59] input.params_init.scale : 1.000001\n",
      "I0710 07:18:44.915230 140310643746176 base_runner.py:59] input.params_init.seed : NoneType\n",
      "I0710 07:18:44.915272 140310643746176 base_runner.py:59] input.random_seed : NoneType\n",
      "I0710 07:18:44.915313 140310643746176 base_runner.py:59] input.remote.max_inflights_per_target : 32\n",
      "I0710 07:18:44.915354 140310643746176 base_runner.py:59] input.remote.shardable_batch : False\n",
      "I0710 07:18:44.915394 140310643746176 base_runner.py:59] input.repeat_count : -1\n",
      "I0710 07:18:44.915432 140310643746176 base_runner.py:59] input.require_sequential_order : False\n",
      "I0710 07:18:44.915472 140310643746176 base_runner.py:59] input.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.915511 140310643746176 base_runner.py:59] input.source_max_length : 122\n",
      "I0710 07:18:44.915550 140310643746176 base_runner.py:59] input.target_max_length : 122\n",
      "I0710 07:18:44.915591 140310643746176 base_runner.py:59] input.tokenizer.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.915632 140310643746176 base_runner.py:59] input.tokenizer.append_eos : True\n",
      "I0710 07:18:44.915673 140310643746176 base_runner.py:59] input.tokenizer.cls : type/lingvo.core.tokenizers/WpmTokenizer\n",
      "I0710 07:18:44.915712 140310643746176 base_runner.py:59] input.tokenizer.dtype : float32\n",
      "I0710 07:18:44.915751 140310643746176 base_runner.py:59] input.tokenizer.fprop_dtype : NoneType\n",
      "I0710 07:18:44.915790 140310643746176 base_runner.py:59] input.tokenizer.inference_driver_name : NoneType\n",
      "I0710 07:18:44.915829 140310643746176 base_runner.py:59] input.tokenizer.is_inference : NoneType\n",
      "I0710 07:18:44.915868 140310643746176 base_runner.py:59] input.tokenizer.merge_prob : 1.0\n",
      "I0710 07:18:44.915907 140310643746176 base_runner.py:59] input.tokenizer.name : 'tokenizer'\n",
      "I0710 07:18:44.915946 140310643746176 base_runner.py:59] input.tokenizer.pad_to_max_length : False\n",
      "I0710 07:18:44.915984 140310643746176 base_runner.py:59] input.tokenizer.params_init.method : 'xavier'\n",
      "I0710 07:18:44.916029 140310643746176 base_runner.py:59] input.tokenizer.params_init.scale : 1.000001\n",
      "I0710 07:18:44.916069 140310643746176 base_runner.py:59] input.tokenizer.params_init.seed : NoneType\n",
      "I0710 07:18:44.916142 140310643746176 base_runner.py:59] input.tokenizer.random_seed : NoneType\n",
      "I0710 07:18:44.916182 140310643746176 base_runner.py:59] input.tokenizer.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.916221 140310643746176 base_runner.py:59] input.tokenizer.target_eos_id : 2\n",
      "I0710 07:18:44.916261 140310643746176 base_runner.py:59] input.tokenizer.target_sos_id : 1\n",
      "I0710 07:18:44.916300 140310643746176 base_runner.py:59] input.tokenizer.target_unk_id : 0\n",
      "I0710 07:18:44.916339 140310643746176 base_runner.py:59] input.tokenizer.target_wb_id : -1\n",
      "I0710 07:18:44.916378 140310643746176 base_runner.py:59] input.tokenizer.vn.global_vn : False\n",
      "I0710 07:18:44.916417 140310643746176 base_runner.py:59] input.tokenizer.vn.per_step_vn : False\n",
      "I0710 07:18:44.916455 140310643746176 base_runner.py:59] input.tokenizer.vn.scale : NoneType\n",
      "I0710 07:18:44.916494 140310643746176 base_runner.py:59] input.tokenizer.vn.seed : NoneType\n",
      "I0710 07:18:44.916532 140310643746176 base_runner.py:59] input.tokenizer.vocab_filepath : 'brown_corpus_wpm.16000.vocab'\n",
      "I0710 07:18:44.916572 140310643746176 base_runner.py:59] input.tokenizer.vocab_size : 16000\n",
      "I0710 07:18:44.916611 140310643746176 base_runner.py:59] input.tokenizer_dict : {}\n",
      "I0710 07:18:44.916650 140310643746176 base_runner.py:59] input.tpu_infeed_parallelism : 1\n",
      "I0710 07:18:44.916688 140310643746176 base_runner.py:59] input.use_chaining : False\n",
      "I0710 07:18:44.916727 140310643746176 base_runner.py:59] input.use_partitioned_infeed_queue : False\n",
      "I0710 07:18:44.916766 140310643746176 base_runner.py:59] input.use_per_host_infeed : False\n",
      "I0710 07:18:44.916805 140310643746176 base_runner.py:59] input.use_within_batch_mixing : False\n",
      "I0710 07:18:44.916844 140310643746176 base_runner.py:59] input.vn.global_vn : False\n",
      "I0710 07:18:44.916883 140310643746176 base_runner.py:59] input.vn.per_step_vn : False\n",
      "I0710 07:18:44.916922 140310643746176 base_runner.py:59] input.vn.scale : NoneType\n",
      "I0710 07:18:44.916960 140310643746176 base_runner.py:59] input.vn.seed : NoneType\n",
      "I0710 07:18:44.916999 140310643746176 base_runner.py:59] is_inference : NoneType\n",
      "I0710 07:18:44.917043 140310643746176 base_runner.py:59] model : 'codelab.RNMTModel@/home/jupyter/lingvo/codelabs/codelab.py:81'\n",
      "I0710 07:18:44.917083 140310643746176 base_runner.py:59] name : ''\n",
      "I0710 07:18:44.917122 140310643746176 base_runner.py:59] params_init.method : 'xavier'\n",
      "I0710 07:18:44.917161 140310643746176 base_runner.py:59] params_init.scale : 1.000001\n",
      "I0710 07:18:44.917199 140310643746176 base_runner.py:59] params_init.seed : NoneType\n",
      "I0710 07:18:44.917238 140310643746176 base_runner.py:59] random_seed : NoneType\n",
      "I0710 07:18:44.917278 140310643746176 base_runner.py:59] skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.917316 140310643746176 base_runner.py:59] task.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.917355 140310643746176 base_runner.py:59] task.cls : type/lingvo.tasks.punctuator.model/RNMTModel\n",
      "I0710 07:18:44.917397 140310643746176 base_runner.py:59] task.decoder.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.917436 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.917475 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.apply_pruning : False\n",
      "I0710 07:18:44.917515 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.apply_pruning_to_projection : False\n",
      "I0710 07:18:44.917554 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.bias_init.method : 'constant'\n",
      "I0710 07:18:44.917593 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.bias_init.scale : 0.0\n",
      "I0710 07:18:44.917632 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.bias_init.seed : 0\n",
      "I0710 07:18:44.917671 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.cell_value_cap : 10.0\n",
      "I0710 07:18:44.917714 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.cls : type/lingvo.core.rnn_cell/LayerNormalizedLSTMCellSimple\n",
      "I0710 07:18:44.917761 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.couple_input_forget_gates : False\n",
      "I0710 07:18:44.917800 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.dtype : float32\n",
      "I0710 07:18:44.917839 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.enable_lstm_bias : True\n",
      "I0710 07:18:44.917878 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.forget_gate_bias : 0.0\n",
      "I0710 07:18:44.917941 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.fprop_dtype : NoneType\n",
      "I0710 07:18:44.918004 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.gradient_pruning : False\n",
      "I0710 07:18:44.918069 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.inference_driver_name : NoneType\n",
      "I0710 07:18:44.918131 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.inputs_arity : 1\n",
      "I0710 07:18:44.918193 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.is_inference : NoneType\n",
      "I0710 07:18:44.918256 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.layer_norm_epsilon : 1e-08\n",
      "I0710 07:18:44.918323 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.name : ''\n",
      "I0710 07:18:44.918385 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.num_hidden_nodes : 0\n",
      "I0710 07:18:44.918443 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.num_input_nodes : 0\n",
      "I0710 07:18:44.918503 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.num_output_nodes : 1024\n",
      "I0710 07:18:44.918565 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.output_nonlinearity : False\n",
      "I0710 07:18:44.918624 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.params_init.method : 'uniform'\n",
      "I0710 07:18:44.918684 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.params_init.scale : 0.04\n",
      "I0710 07:18:44.918742 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.params_init.seed : NoneType\n",
      "I0710 07:18:44.918801 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.qdomain.c_state : NoneType\n",
      "I0710 07:18:44.918860 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.qdomain.default : NoneType\n",
      "I0710 07:18:44.918918 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.qdomain.fullyconnected : NoneType\n",
      "I0710 07:18:44.918977 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.qdomain.m_state : NoneType\n",
      "I0710 07:18:44.919040 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.qdomain.weight : NoneType\n",
      "I0710 07:18:44.919099 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.random_seed : NoneType\n",
      "I0710 07:18:44.919157 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.reset_cell_state : False\n",
      "I0710 07:18:44.919215 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.919274 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.vn.global_vn : False\n",
      "I0710 07:18:44.919332 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.vn.per_step_vn : False\n",
      "I0710 07:18:44.919390 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.vn.scale : NoneType\n",
      "I0710 07:18:44.919449 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.vn.seed : NoneType\n",
      "I0710 07:18:44.919507 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.zero_state_init_params.method : 'zeros'\n",
      "I0710 07:18:44.919567 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.zero_state_init_params.seed : NoneType\n",
      "I0710 07:18:44.919625 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.zo_prob : 0.0\n",
      "I0710 07:18:44.919684 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cls : type/lingvo.core.rnn_layers/FRNNWithAttention\n",
      "I0710 07:18:44.919742 140310643746176 base_runner.py:59] task.decoder.attention.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.919800 140310643746176 base_runner.py:59] task.decoder.attention.atten_dropout_deterministic : True\n",
      "I0710 07:18:44.919858 140310643746176 base_runner.py:59] task.decoder.attention.atten_dropout_prob : 0.3\n",
      "I0710 07:18:44.919916 140310643746176 base_runner.py:59] task.decoder.attention.attention_head_prob_index : -1\n",
      "I0710 07:18:44.919973 140310643746176 base_runner.py:59] task.decoder.attention.cls : type/lingvo.core.attention/MultiHeadedAttention\n",
      "I0710 07:18:44.920037 140310643746176 base_runner.py:59] task.decoder.attention.context_dim : 1024\n",
      "I0710 07:18:44.920101 140310643746176 base_runner.py:59] task.decoder.attention.ctx_post_proj_dim : 0\n",
      "I0710 07:18:44.920161 140310643746176 base_runner.py:59] task.decoder.attention.dtype : float32\n",
      "I0710 07:18:44.920220 140310643746176 base_runner.py:59] task.decoder.attention.enable_ctx_post_proj : False\n",
      "I0710 07:18:44.920278 140310643746176 base_runner.py:59] task.decoder.attention.enable_ctx_pre_proj : False\n",
      "I0710 07:18:44.920336 140310643746176 base_runner.py:59] task.decoder.attention.enable_query_proj : True\n",
      "I0710 07:18:44.920394 140310643746176 base_runner.py:59] task.decoder.attention.enable_source_proj : True\n",
      "I0710 07:18:44.920453 140310643746176 base_runner.py:59] task.decoder.attention.fprop_dtype : NoneType\n",
      "I0710 07:18:44.920511 140310643746176 base_runner.py:59] task.decoder.attention.hidden_dim : 1024\n",
      "I0710 07:18:44.920569 140310643746176 base_runner.py:59] task.decoder.attention.inference_driver_name : NoneType\n",
      "I0710 07:18:44.920628 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.920686 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.atten_dropout_deterministic : False\n",
      "I0710 07:18:44.920745 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.atten_dropout_prob : 0.0\n",
      "I0710 07:18:44.920803 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.cls : type/lingvo.core.attention/AdditiveAttention\n",
      "I0710 07:18:44.920862 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.dtype : float32\n",
      "I0710 07:18:44.920920 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.fprop_dtype : NoneType\n",
      "I0710 07:18:44.920977 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.hidden_dim : 0\n",
      "I0710 07:18:44.921039 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.inference_driver_name : NoneType\n",
      "I0710 07:18:44.921098 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.is_inference : NoneType\n",
      "I0710 07:18:44.921156 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.name : ''\n",
      "I0710 07:18:44.921216 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.packed_input : False\n",
      "I0710 07:18:44.921274 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.params_init.method : 'gaussian_sqrt_dim'\n",
      "I0710 07:18:44.921332 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.params_init.scale : 1.0\n",
      "I0710 07:18:44.921390 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.params_init.seed : NoneType\n",
      "I0710 07:18:44.921464 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.qdomain.default : NoneType\n",
      "I0710 07:18:44.921522 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.qdomain.fullyconnected : NoneType\n",
      "I0710 07:18:44.921580 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.qdomain.softmax : NoneType\n",
      "I0710 07:18:44.921637 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.query_dim : 0\n",
      "I0710 07:18:44.921693 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.random_seed : NoneType\n",
      "I0710 07:18:44.921749 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.same_batch_size : False\n",
      "I0710 07:18:44.921805 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.921861 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.source_dim : 0\n",
      "I0710 07:18:44.921934 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.vn.global_vn : False\n",
      "I0710 07:18:44.921994 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.vn.per_step_vn : False\n",
      "I0710 07:18:44.922075 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.vn.scale : NoneType\n",
      "I0710 07:18:44.922136 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.vn.seed : NoneType\n",
      "I0710 07:18:44.922193 140310643746176 base_runner.py:59] task.decoder.attention.is_inference : NoneType\n",
      "I0710 07:18:44.922250 140310643746176 base_runner.py:59] task.decoder.attention.name : ''\n",
      "I0710 07:18:44.922307 140310643746176 base_runner.py:59] task.decoder.attention.num_attention_heads : 4\n",
      "I0710 07:18:44.922366 140310643746176 base_runner.py:59] task.decoder.attention.num_post_proj : 1\n",
      "I0710 07:18:44.922424 140310643746176 base_runner.py:59] task.decoder.attention.packed_input : False\n",
      "I0710 07:18:44.922502 140310643746176 base_runner.py:59] task.decoder.attention.params_init.method : 'xavier'\n",
      "I0710 07:18:44.922564 140310643746176 base_runner.py:59] task.decoder.attention.params_init.scale : 1.0\n",
      "I0710 07:18:44.922624 140310643746176 base_runner.py:59] task.decoder.attention.params_init.seed : NoneType\n",
      "I0710 07:18:44.922682 140310643746176 base_runner.py:59] task.decoder.attention.proj_init : 'default'\n",
      "I0710 07:18:44.922740 140310643746176 base_runner.py:59] task.decoder.attention.qdomain.atten_context : NoneType\n",
      "I0710 07:18:44.922798 140310643746176 base_runner.py:59] task.decoder.attention.qdomain.default : NoneType\n",
      "I0710 07:18:44.922857 140310643746176 base_runner.py:59] task.decoder.attention.qdomain.fullyconnected : NoneType\n",
      "I0710 07:18:44.922915 140310643746176 base_runner.py:59] task.decoder.attention.qdomain.softmax : NoneType\n",
      "I0710 07:18:44.922974 140310643746176 base_runner.py:59] task.decoder.attention.query_dim : 1024\n",
      "I0710 07:18:44.923038 140310643746176 base_runner.py:59] task.decoder.attention.random_seed : NoneType\n",
      "I0710 07:18:44.923100 140310643746176 base_runner.py:59] task.decoder.attention.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.923161 140310643746176 base_runner.py:59] task.decoder.attention.source_dim : 1024\n",
      "I0710 07:18:44.923219 140310643746176 base_runner.py:59] task.decoder.attention.use_source_vec_as_attention_value : True\n",
      "I0710 07:18:44.923278 140310643746176 base_runner.py:59] task.decoder.attention.vn.global_vn : False\n",
      "I0710 07:18:44.923336 140310643746176 base_runner.py:59] task.decoder.attention.vn.per_step_vn : False\n",
      "I0710 07:18:44.923394 140310643746176 base_runner.py:59] task.decoder.attention.vn.scale : NoneType\n",
      "I0710 07:18:44.923452 140310643746176 base_runner.py:59] task.decoder.attention.vn.seed : NoneType\n",
      "I0710 07:18:44.923511 140310643746176 base_runner.py:59] task.decoder.beam_search.allow_empty_terminated_hyp : True\n",
      "I0710 07:18:44.923569 140310643746176 base_runner.py:59] task.decoder.beam_search.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.923627 140310643746176 base_runner.py:59] task.decoder.beam_search.batch_major_compute : False\n",
      "I0710 07:18:44.923685 140310643746176 base_runner.py:59] task.decoder.beam_search.batch_major_state : True\n",
      "I0710 07:18:44.923743 140310643746176 base_runner.py:59] task.decoder.beam_search.beam_size : 3.0\n",
      "I0710 07:18:44.923801 140310643746176 base_runner.py:59] task.decoder.beam_search.cls : type/lingvo.core.beam_search_helper/BeamSearchHelper\n",
      "I0710 07:18:44.923858 140310643746176 base_runner.py:59] task.decoder.beam_search.coverage_penalty : 0.2\n",
      "I0710 07:18:44.923916 140310643746176 base_runner.py:59] task.decoder.beam_search.dtype : float32\n",
      "I0710 07:18:44.923974 140310643746176 base_runner.py:59] task.decoder.beam_search.ensure_full_beam : False\n",
      "I0710 07:18:44.924037 140310643746176 base_runner.py:59] task.decoder.beam_search.force_eos_in_last_step : False\n",
      "I0710 07:18:44.924096 140310643746176 base_runner.py:59] task.decoder.beam_search.fprop_dtype : NoneType\n",
      "I0710 07:18:44.924154 140310643746176 base_runner.py:59] task.decoder.beam_search.inference_driver_name : NoneType\n",
      "I0710 07:18:44.924212 140310643746176 base_runner.py:59] task.decoder.beam_search.is_inference : NoneType\n",
      "I0710 07:18:44.924270 140310643746176 base_runner.py:59] task.decoder.beam_search.length_normalization : 0.2\n",
      "I0710 07:18:44.924327 140310643746176 base_runner.py:59] task.decoder.beam_search.local_eos_threshold : -100.0\n",
      "I0710 07:18:44.924385 140310643746176 base_runner.py:59] task.decoder.beam_search.merge_paths : False\n",
      "I0710 07:18:44.924443 140310643746176 base_runner.py:59] task.decoder.beam_search.name : 'beam_search'\n",
      "I0710 07:18:44.924501 140310643746176 base_runner.py:59] task.decoder.beam_search.num_hyps_per_beam : 16\n",
      "I0710 07:18:44.924559 140310643746176 base_runner.py:59] task.decoder.beam_search.params_init.method : 'xavier'\n",
      "I0710 07:18:44.924636 140310643746176 base_runner.py:59] task.decoder.beam_search.params_init.scale : 1.000001\n",
      "I0710 07:18:44.924722 140310643746176 base_runner.py:59] task.decoder.beam_search.params_init.seed : NoneType\n",
      "I0710 07:18:44.924787 140310643746176 base_runner.py:59] task.decoder.beam_search.random_seed : NoneType\n",
      "I0710 07:18:44.924832 140310643746176 base_runner.py:59] task.decoder.beam_search.short_seq_limit : 0\n",
      "I0710 07:18:44.924899 140310643746176 base_runner.py:59] task.decoder.beam_search.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.924966 140310643746176 base_runner.py:59] task.decoder.beam_search.target_eoc_id : -1\n",
      "I0710 07:18:44.925038 140310643746176 base_runner.py:59] task.decoder.beam_search.target_eos_id : 2\n",
      "I0710 07:18:44.925101 140310643746176 base_runner.py:59] task.decoder.beam_search.target_seq_len : 0\n",
      "I0710 07:18:44.925143 140310643746176 base_runner.py:59] task.decoder.beam_search.target_seq_length_ratio : 1.0\n",
      "I0710 07:18:44.925183 140310643746176 base_runner.py:59] task.decoder.beam_search.target_sos_id : 1\n",
      "I0710 07:18:44.925223 140310643746176 base_runner.py:59] task.decoder.beam_search.valid_eos_max_logit_delta : 5.0\n",
      "I0710 07:18:44.925285 140310643746176 base_runner.py:59] task.decoder.beam_search.vn.global_vn : False\n",
      "I0710 07:18:44.925335 140310643746176 base_runner.py:59] task.decoder.beam_search.vn.per_step_vn : False\n",
      "I0710 07:18:44.925375 140310643746176 base_runner.py:59] task.decoder.beam_search.vn.scale : NoneType\n",
      "I0710 07:18:44.925414 140310643746176 base_runner.py:59] task.decoder.beam_search.vn.seed : NoneType\n",
      "I0710 07:18:44.925453 140310643746176 base_runner.py:59] task.decoder.bias_only_if_consistent : True\n",
      "I0710 07:18:44.925493 140310643746176 base_runner.py:59] task.decoder.cc_schedule : NoneType\n",
      "I0710 07:18:44.925532 140310643746176 base_runner.py:59] task.decoder.cls : type/lingvo.tasks.mt.decoder/MTDecoderV1\n",
      "I0710 07:18:44.925571 140310643746176 base_runner.py:59] task.decoder.dropout_prob : 0.3\n",
      "I0710 07:18:44.925610 140310643746176 base_runner.py:59] task.decoder.dtype : float32\n",
      "I0710 07:18:44.925649 140310643746176 base_runner.py:59] task.decoder.emb.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.925688 140310643746176 base_runner.py:59] task.decoder.emb.cls : type/lingvo.core.layers/EmbeddingLayer\n",
      "I0710 07:18:44.925728 140310643746176 base_runner.py:59] task.decoder.emb.dtype : float32\n",
      "I0710 07:18:44.925767 140310643746176 base_runner.py:59] task.decoder.emb.embedding_dim : 1024\n",
      "I0710 07:18:44.925805 140310643746176 base_runner.py:59] task.decoder.emb.fprop_dtype : NoneType\n",
      "I0710 07:18:44.925844 140310643746176 base_runner.py:59] task.decoder.emb.inference_driver_name : NoneType\n",
      "I0710 07:18:44.925883 140310643746176 base_runner.py:59] task.decoder.emb.is_inference : NoneType\n",
      "I0710 07:18:44.925936 140310643746176 base_runner.py:59] task.decoder.emb.max_num_shards : 16\n",
      "I0710 07:18:44.925975 140310643746176 base_runner.py:59] task.decoder.emb.name : ''\n",
      "I0710 07:18:44.926013 140310643746176 base_runner.py:59] task.decoder.emb.on_ps : True\n",
      "I0710 07:18:44.926059 140310643746176 base_runner.py:59] task.decoder.emb.params_init.method : 'uniform'\n",
      "I0710 07:18:44.926155 140310643746176 base_runner.py:59] task.decoder.emb.params_init.scale : 0.04\n",
      "I0710 07:18:44.926195 140310643746176 base_runner.py:59] task.decoder.emb.params_init.seed : NoneType\n",
      "I0710 07:18:44.926234 140310643746176 base_runner.py:59] task.decoder.emb.random_seed : NoneType\n",
      "I0710 07:18:44.926273 140310643746176 base_runner.py:59] task.decoder.emb.scale_sqrt_depth : False\n",
      "I0710 07:18:44.926311 140310643746176 base_runner.py:59] task.decoder.emb.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.926350 140310643746176 base_runner.py:59] task.decoder.emb.vn.global_vn : False\n",
      "I0710 07:18:44.926389 140310643746176 base_runner.py:59] task.decoder.emb.vn.per_step_vn : False\n",
      "I0710 07:18:44.926429 140310643746176 base_runner.py:59] task.decoder.emb.vn.scale : 1.0\n",
      "I0710 07:18:44.926467 140310643746176 base_runner.py:59] task.decoder.emb.vn.seed : NoneType\n",
      "I0710 07:18:44.926506 140310643746176 base_runner.py:59] task.decoder.emb.vocab_size : 16000\n",
      "I0710 07:18:44.926544 140310643746176 base_runner.py:59] task.decoder.feed_attention_context_vec_to_softmax : True\n",
      "I0710 07:18:44.926583 140310643746176 base_runner.py:59] task.decoder.fprop_dtype : NoneType\n",
      "I0710 07:18:44.926621 140310643746176 base_runner.py:59] task.decoder.greedy_search.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.926659 140310643746176 base_runner.py:59] task.decoder.greedy_search.cls : type/lingvo.core.beam_search_helper/GreedySearchHelper\n",
      "I0710 07:18:44.926698 140310643746176 base_runner.py:59] task.decoder.greedy_search.dtype : float32\n",
      "I0710 07:18:44.926737 140310643746176 base_runner.py:59] task.decoder.greedy_search.fprop_dtype : NoneType\n",
      "I0710 07:18:44.926776 140310643746176 base_runner.py:59] task.decoder.greedy_search.inference_driver_name : NoneType\n",
      "I0710 07:18:44.926814 140310643746176 base_runner.py:59] task.decoder.greedy_search.is_inference : NoneType\n",
      "I0710 07:18:44.926853 140310643746176 base_runner.py:59] task.decoder.greedy_search.name : 'greedy_search'\n",
      "I0710 07:18:44.926891 140310643746176 base_runner.py:59] task.decoder.greedy_search.params_init.method : 'xavier'\n",
      "I0710 07:18:44.926930 140310643746176 base_runner.py:59] task.decoder.greedy_search.params_init.scale : 1.000001\n",
      "I0710 07:18:44.926968 140310643746176 base_runner.py:59] task.decoder.greedy_search.params_init.seed : NoneType\n",
      "I0710 07:18:44.927006 140310643746176 base_runner.py:59] task.decoder.greedy_search.random_seed : NoneType\n",
      "I0710 07:18:44.927050 140310643746176 base_runner.py:59] task.decoder.greedy_search.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.927088 140310643746176 base_runner.py:59] task.decoder.greedy_search.target_eos_id : 2\n",
      "I0710 07:18:44.927126 140310643746176 base_runner.py:59] task.decoder.greedy_search.target_seq_len : 0\n",
      "I0710 07:18:44.927165 140310643746176 base_runner.py:59] task.decoder.greedy_search.target_sos_id : 1\n",
      "I0710 07:18:44.927203 140310643746176 base_runner.py:59] task.decoder.greedy_search.vn.global_vn : False\n",
      "I0710 07:18:44.927241 140310643746176 base_runner.py:59] task.decoder.greedy_search.vn.per_step_vn : False\n",
      "I0710 07:18:44.927278 140310643746176 base_runner.py:59] task.decoder.greedy_search.vn.scale : NoneType\n",
      "I0710 07:18:44.927317 140310643746176 base_runner.py:59] task.decoder.greedy_search.vn.seed : NoneType\n",
      "I0710 07:18:44.927356 140310643746176 base_runner.py:59] task.decoder.inference_driver_name : NoneType\n",
      "I0710 07:18:44.927394 140310643746176 base_runner.py:59] task.decoder.init_step_ids : False\n",
      "I0710 07:18:44.927432 140310643746176 base_runner.py:59] task.decoder.is_inference : NoneType\n",
      "I0710 07:18:44.927470 140310643746176 base_runner.py:59] task.decoder.label_smoothing.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.927509 140310643746176 base_runner.py:59] task.decoder.label_smoothing.cls : type/lingvo.core.layers/UniformLabelSmoother\n",
      "I0710 07:18:44.927548 140310643746176 base_runner.py:59] task.decoder.label_smoothing.dtype : float32\n",
      "I0710 07:18:44.927587 140310643746176 base_runner.py:59] task.decoder.label_smoothing.fprop_dtype : NoneType\n",
      "I0710 07:18:44.927626 140310643746176 base_runner.py:59] task.decoder.label_smoothing.inference_driver_name : NoneType\n",
      "I0710 07:18:44.927664 140310643746176 base_runner.py:59] task.decoder.label_smoothing.is_inference : NoneType\n",
      "I0710 07:18:44.927711 140310643746176 base_runner.py:59] task.decoder.label_smoothing.name : ''\n",
      "I0710 07:18:44.927751 140310643746176 base_runner.py:59] task.decoder.label_smoothing.num_classes : 16000\n",
      "I0710 07:18:44.927789 140310643746176 base_runner.py:59] task.decoder.label_smoothing.params_init.method : 'xavier'\n",
      "I0710 07:18:44.927828 140310643746176 base_runner.py:59] task.decoder.label_smoothing.params_init.scale : 1.000001\n",
      "I0710 07:18:44.927866 140310643746176 base_runner.py:59] task.decoder.label_smoothing.params_init.seed : NoneType\n",
      "I0710 07:18:44.927904 140310643746176 base_runner.py:59] task.decoder.label_smoothing.random_seed : NoneType\n",
      "I0710 07:18:44.927943 140310643746176 base_runner.py:59] task.decoder.label_smoothing.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.927982 140310643746176 base_runner.py:59] task.decoder.label_smoothing.token_id_uncertainty_larger : NoneType\n",
      "I0710 07:18:44.928024 140310643746176 base_runner.py:59] task.decoder.label_smoothing.uncertainty : 0.1\n",
      "I0710 07:18:44.928062 140310643746176 base_runner.py:59] task.decoder.label_smoothing.uncertainty_larger : 0.1\n",
      "I0710 07:18:44.928100 140310643746176 base_runner.py:59] task.decoder.label_smoothing.vn.global_vn : False\n",
      "I0710 07:18:44.928139 140310643746176 base_runner.py:59] task.decoder.label_smoothing.vn.per_step_vn : False\n",
      "I0710 07:18:44.928177 140310643746176 base_runner.py:59] task.decoder.label_smoothing.vn.scale : NoneType\n",
      "I0710 07:18:44.928215 140310643746176 base_runner.py:59] task.decoder.label_smoothing.vn.seed : NoneType\n",
      "I0710 07:18:44.928254 140310643746176 base_runner.py:59] task.decoder.name : ''\n",
      "I0710 07:18:44.928292 140310643746176 base_runner.py:59] task.decoder.packed_input : False\n",
      "I0710 07:18:44.928330 140310643746176 base_runner.py:59] task.decoder.params_init.method : 'xavier'\n",
      "I0710 07:18:44.928369 140310643746176 base_runner.py:59] task.decoder.params_init.scale : 1.000001\n",
      "I0710 07:18:44.928407 140310643746176 base_runner.py:59] task.decoder.params_init.seed : NoneType\n",
      "I0710 07:18:44.928446 140310643746176 base_runner.py:59] task.decoder.per_example_tensors : False\n",
      "I0710 07:18:44.928484 140310643746176 base_runner.py:59] task.decoder.per_word_avg_loss : False\n",
      "I0710 07:18:44.928522 140310643746176 base_runner.py:59] task.decoder.qdomain.default : NoneType\n",
      "I0710 07:18:44.928560 140310643746176 base_runner.py:59] task.decoder.qlogsoftmax_range_min : -10.0\n",
      "I0710 07:18:44.928599 140310643746176 base_runner.py:59] task.decoder.random_seed : NoneType\n",
      "I0710 07:18:44.928637 140310643746176 base_runner.py:59] task.decoder.residual_start : 2\n",
      "I0710 07:18:44.928675 140310643746176 base_runner.py:59] task.decoder.rnn_cell_dim : 1024\n",
      "I0710 07:18:44.928714 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.928752 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.apply_pruning : False\n",
      "I0710 07:18:44.928791 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.apply_pruning_to_projection : False\n",
      "I0710 07:18:44.928829 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.bias_init.method : 'constant'\n",
      "I0710 07:18:44.928867 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.bias_init.scale : 0.0\n",
      "I0710 07:18:44.928905 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.bias_init.seed : 0\n",
      "I0710 07:18:44.928943 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.cell_value_cap : 10.0\n",
      "I0710 07:18:44.928982 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.cls : type/lingvo.core.rnn_cell/LayerNormalizedLSTMCellSimple\n",
      "I0710 07:18:44.929026 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.couple_input_forget_gates : False\n",
      "I0710 07:18:44.929065 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.dtype : float32\n",
      "I0710 07:18:44.929103 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.enable_lstm_bias : True\n",
      "I0710 07:18:44.929142 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.forget_gate_bias : 0.0\n",
      "I0710 07:18:44.929180 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.fprop_dtype : NoneType\n",
      "I0710 07:18:44.929219 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.gradient_pruning : False\n",
      "I0710 07:18:44.929257 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.inference_driver_name : NoneType\n",
      "I0710 07:18:44.929296 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.inputs_arity : 1\n",
      "I0710 07:18:44.929334 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.is_inference : NoneType\n",
      "I0710 07:18:44.929373 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.layer_norm_epsilon : 1e-08\n",
      "I0710 07:18:44.929412 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.name : ''\n",
      "I0710 07:18:44.929450 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.num_hidden_nodes : 0\n",
      "I0710 07:18:44.929489 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.num_input_nodes : 0\n",
      "I0710 07:18:44.929527 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.num_output_nodes : 1024\n",
      "I0710 07:18:44.929565 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.output_nonlinearity : False\n",
      "I0710 07:18:44.929603 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.params_init.method : 'uniform'\n",
      "I0710 07:18:44.929641 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.params_init.scale : 0.04\n",
      "I0710 07:18:44.929680 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.params_init.seed : NoneType\n",
      "I0710 07:18:44.929724 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.qdomain.c_state : NoneType\n",
      "I0710 07:18:44.929763 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.qdomain.default : NoneType\n",
      "I0710 07:18:44.929802 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.qdomain.fullyconnected : NoneType\n",
      "I0710 07:18:44.929840 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.qdomain.m_state : NoneType\n",
      "I0710 07:18:44.929879 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.qdomain.weight : NoneType\n",
      "I0710 07:18:44.929931 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.random_seed : NoneType\n",
      "I0710 07:18:44.929970 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.reset_cell_state : False\n",
      "I0710 07:18:44.930009 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.930054 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.vn.global_vn : False\n",
      "I0710 07:18:44.930092 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.vn.per_step_vn : False\n",
      "I0710 07:18:44.930131 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.vn.scale : NoneType\n",
      "I0710 07:18:44.930169 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.vn.seed : NoneType\n",
      "I0710 07:18:44.930207 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.zero_state_init_params.method : 'zeros'\n",
      "I0710 07:18:44.930246 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.zero_state_init_params.seed : NoneType\n",
      "I0710 07:18:44.930285 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.zo_prob : 0.0\n",
      "I0710 07:18:44.930323 140310643746176 base_runner.py:59] task.decoder.rnn_layers : 8\n",
      "I0710 07:18:44.930361 140310643746176 base_runner.py:59] task.decoder.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.930400 140310643746176 base_runner.py:59] task.decoder.softmax.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.930438 140310643746176 base_runner.py:59] task.decoder.softmax.apply_pruning : False\n",
      "I0710 07:18:44.930476 140310643746176 base_runner.py:59] task.decoder.softmax.chunk_size : 0\n",
      "I0710 07:18:44.930514 140310643746176 base_runner.py:59] task.decoder.softmax.cls : type/lingvo.core.layers/SimpleFullSoftmax\n",
      "I0710 07:18:44.930553 140310643746176 base_runner.py:59] task.decoder.softmax.dtype : float32\n",
      "I0710 07:18:44.930591 140310643746176 base_runner.py:59] task.decoder.softmax.fprop_dtype : NoneType\n",
      "I0710 07:18:44.930629 140310643746176 base_runner.py:59] task.decoder.softmax.inference_driver_name : NoneType\n",
      "I0710 07:18:44.930666 140310643746176 base_runner.py:59] task.decoder.softmax.input_dim : 0\n",
      "I0710 07:18:44.930705 140310643746176 base_runner.py:59] task.decoder.softmax.is_inference : NoneType\n",
      "I0710 07:18:44.930744 140310643746176 base_runner.py:59] task.decoder.softmax.logits_abs_max : NoneType\n",
      "I0710 07:18:44.930782 140310643746176 base_runner.py:59] task.decoder.softmax.name : ''\n",
      "I0710 07:18:44.930821 140310643746176 base_runner.py:59] task.decoder.softmax.num_classes : 16000\n",
      "I0710 07:18:44.930859 140310643746176 base_runner.py:59] task.decoder.softmax.num_sampled : 0\n",
      "I0710 07:18:44.930897 140310643746176 base_runner.py:59] task.decoder.softmax.num_shards : 16\n",
      "I0710 07:18:44.930935 140310643746176 base_runner.py:59] task.decoder.softmax.params_init.method : 'uniform'\n",
      "I0710 07:18:44.930973 140310643746176 base_runner.py:59] task.decoder.softmax.params_init.scale : 0.04\n",
      "I0710 07:18:44.931010 140310643746176 base_runner.py:59] task.decoder.softmax.params_init.seed : NoneType\n",
      "I0710 07:18:44.931054 140310643746176 base_runner.py:59] task.decoder.softmax.qdomain.default : NoneType\n",
      "I0710 07:18:44.931092 140310643746176 base_runner.py:59] task.decoder.softmax.random_seed : NoneType\n",
      "I0710 07:18:44.931131 140310643746176 base_runner.py:59] task.decoder.softmax.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.931169 140310643746176 base_runner.py:59] task.decoder.softmax.use_num_classes_major_weight : False\n",
      "I0710 07:18:44.931207 140310643746176 base_runner.py:59] task.decoder.softmax.vn.global_vn : False\n",
      "I0710 07:18:44.931245 140310643746176 base_runner.py:59] task.decoder.softmax.vn.per_step_vn : False\n",
      "I0710 07:18:44.931284 140310643746176 base_runner.py:59] task.decoder.softmax.vn.scale : 1.0\n",
      "I0710 07:18:44.931322 140310643746176 base_runner.py:59] task.decoder.softmax.vn.seed : NoneType\n",
      "I0710 07:18:44.931361 140310643746176 base_runner.py:59] task.decoder.source_dim : 1024\n",
      "I0710 07:18:44.931400 140310643746176 base_runner.py:59] task.decoder.target_eos_id : 2\n",
      "I0710 07:18:44.931438 140310643746176 base_runner.py:59] task.decoder.target_seq_len : 300\n",
      "I0710 07:18:44.931476 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.931514 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.cls : type/lingvo.core.target_sequence_sampler/TargetSequenceSampler\n",
      "I0710 07:18:44.931553 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.dtype : float32\n",
      "I0710 07:18:44.931592 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.fprop_dtype : NoneType\n",
      "I0710 07:18:44.931630 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.inference_driver_name : NoneType\n",
      "I0710 07:18:44.931669 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.is_inference : NoneType\n",
      "I0710 07:18:44.931707 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.name : 'target_sequence_sampler'\n",
      "I0710 07:18:44.931746 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.params_init.method : 'xavier'\n",
      "I0710 07:18:44.931784 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.params_init.scale : 1.000001\n",
      "I0710 07:18:44.931823 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.params_init.seed : NoneType\n",
      "I0710 07:18:44.931862 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.random_seed : NoneType\n",
      "I0710 07:18:44.931900 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.931939 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.target_eoc_id : -1\n",
      "I0710 07:18:44.931977 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.target_eos_id : 2\n",
      "I0710 07:18:44.932011 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.target_seq_len : 0\n",
      "I0710 07:18:44.932055 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.target_sos_id : 1\n",
      "I0710 07:18:44.932093 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.temperature : 1.0\n",
      "I0710 07:18:44.932132 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.vn.global_vn : False\n",
      "I0710 07:18:44.932170 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.vn.per_step_vn : False\n",
      "I0710 07:18:44.932208 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.vn.scale : NoneType\n",
      "I0710 07:18:44.932246 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.vn.seed : NoneType\n",
      "I0710 07:18:44.932284 140310643746176 base_runner.py:59] task.decoder.target_sos_id : 1\n",
      "I0710 07:18:44.932321 140310643746176 base_runner.py:59] task.decoder.unidi_rnn_type : 'func'\n",
      "I0710 07:18:44.932360 140310643746176 base_runner.py:59] task.decoder.use_prev_atten_ctx : False\n",
      "I0710 07:18:44.932399 140310643746176 base_runner.py:59] task.decoder.use_zero_atten_state : False\n",
      "I0710 07:18:44.932436 140310643746176 base_runner.py:59] task.decoder.vn.global_vn : False\n",
      "I0710 07:18:44.932474 140310643746176 base_runner.py:59] task.decoder.vn.per_step_vn : False\n",
      "I0710 07:18:44.932512 140310643746176 base_runner.py:59] task.decoder.vn.scale : NoneType\n",
      "I0710 07:18:44.932550 140310643746176 base_runner.py:59] task.decoder.vn.seed : NoneType\n",
      "I0710 07:18:44.932587 140310643746176 base_runner.py:59] task.dtype : float32\n",
      "I0710 07:18:44.932625 140310643746176 base_runner.py:59] task.encoder.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.932663 140310643746176 base_runner.py:59] task.encoder.bidi_rnn_type : 'func'\n",
      "I0710 07:18:44.932710 140310643746176 base_runner.py:59] task.encoder.cc_schedule : NoneType\n",
      "I0710 07:18:44.932787 140310643746176 base_runner.py:59] task.encoder.cls : type/lingvo.tasks.mt.encoder/MTEncoderBiRNN\n",
      "I0710 07:18:44.932878 140310643746176 base_runner.py:59] task.encoder.dropout_prob : 0.3\n",
      "I0710 07:18:44.932945 140310643746176 base_runner.py:59] task.encoder.dtype : float32\n",
      "I0710 07:18:44.932988 140310643746176 base_runner.py:59] task.encoder.emb.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.933035 140310643746176 base_runner.py:59] task.encoder.emb.cls : type/lingvo.core.layers/EmbeddingLayer\n",
      "I0710 07:18:44.933076 140310643746176 base_runner.py:59] task.encoder.emb.dtype : float32\n",
      "I0710 07:18:44.933116 140310643746176 base_runner.py:59] task.encoder.emb.embedding_dim : 1024\n",
      "I0710 07:18:44.933155 140310643746176 base_runner.py:59] task.encoder.emb.fprop_dtype : NoneType\n",
      "I0710 07:18:44.933195 140310643746176 base_runner.py:59] task.encoder.emb.inference_driver_name : NoneType\n",
      "I0710 07:18:44.933235 140310643746176 base_runner.py:59] task.encoder.emb.is_inference : NoneType\n",
      "I0710 07:18:44.933275 140310643746176 base_runner.py:59] task.encoder.emb.max_num_shards : 16\n",
      "I0710 07:18:44.933314 140310643746176 base_runner.py:59] task.encoder.emb.name : ''\n",
      "I0710 07:18:44.933353 140310643746176 base_runner.py:59] task.encoder.emb.on_ps : True\n",
      "I0710 07:18:44.933392 140310643746176 base_runner.py:59] task.encoder.emb.params_init.method : 'uniform'\n",
      "I0710 07:18:44.933431 140310643746176 base_runner.py:59] task.encoder.emb.params_init.scale : 0.04\n",
      "I0710 07:18:44.933470 140310643746176 base_runner.py:59] task.encoder.emb.params_init.seed : NoneType\n",
      "I0710 07:18:44.933509 140310643746176 base_runner.py:59] task.encoder.emb.random_seed : NoneType\n",
      "I0710 07:18:44.933547 140310643746176 base_runner.py:59] task.encoder.emb.scale_sqrt_depth : False\n",
      "I0710 07:18:44.933586 140310643746176 base_runner.py:59] task.encoder.emb.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.933624 140310643746176 base_runner.py:59] task.encoder.emb.vn.global_vn : False\n",
      "I0710 07:18:44.933663 140310643746176 base_runner.py:59] task.encoder.emb.vn.per_step_vn : False\n",
      "I0710 07:18:44.933701 140310643746176 base_runner.py:59] task.encoder.emb.vn.scale : 1.0\n",
      "I0710 07:18:44.933740 140310643746176 base_runner.py:59] task.encoder.emb.vn.seed : NoneType\n",
      "I0710 07:18:44.933779 140310643746176 base_runner.py:59] task.encoder.emb.vocab_size : 16000\n",
      "I0710 07:18:44.933817 140310643746176 base_runner.py:59] task.encoder.encoder_out_dim : 1024\n",
      "I0710 07:18:44.933856 140310643746176 base_runner.py:59] task.encoder.fprop_dtype : NoneType\n",
      "I0710 07:18:44.933908 140310643746176 base_runner.py:59] task.encoder.inference_driver_name : NoneType\n",
      "I0710 07:18:44.933949 140310643746176 base_runner.py:59] task.encoder.is_inference : NoneType\n",
      "I0710 07:18:44.934001 140310643746176 base_runner.py:59] task.encoder.is_transparent : False\n",
      "I0710 07:18:44.934051 140310643746176 base_runner.py:59] task.encoder.lstm_cell_size : 1024\n",
      "I0710 07:18:44.934091 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.934137 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.apply_pruning : False\n",
      "I0710 07:18:44.934213 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.apply_pruning_to_projection : False\n",
      "I0710 07:18:44.934302 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.bias_init.method : 'constant'\n",
      "I0710 07:18:44.934388 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.bias_init.scale : 0.0\n",
      "I0710 07:18:44.934468 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.bias_init.seed : 0\n",
      "I0710 07:18:44.934536 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.cell_value_cap : 10.0\n",
      "I0710 07:18:44.934593 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.cls : type/lingvo.core.rnn_cell/LayerNormalizedLSTMCellSimple\n",
      "I0710 07:18:44.934635 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.couple_input_forget_gates : False\n",
      "I0710 07:18:44.934677 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.dtype : float32\n",
      "I0710 07:18:44.934717 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.enable_lstm_bias : True\n",
      "I0710 07:18:44.934758 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.forget_gate_bias : 0.0\n",
      "I0710 07:18:44.934798 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.fprop_dtype : NoneType\n",
      "I0710 07:18:44.934837 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.gradient_pruning : False\n",
      "I0710 07:18:44.934875 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.inference_driver_name : NoneType\n",
      "I0710 07:18:44.934915 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.inputs_arity : 1\n",
      "I0710 07:18:44.934954 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.is_inference : NoneType\n",
      "I0710 07:18:44.934993 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.layer_norm_epsilon : 1e-08\n",
      "I0710 07:18:44.935038 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.name : ''\n",
      "I0710 07:18:44.935077 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.num_hidden_nodes : 0\n",
      "I0710 07:18:44.935116 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.num_input_nodes : 0\n",
      "I0710 07:18:44.935155 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.num_output_nodes : 1024\n",
      "I0710 07:18:44.935194 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.output_nonlinearity : False\n",
      "I0710 07:18:44.935232 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.params_init.method : 'uniform'\n",
      "I0710 07:18:44.935271 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.params_init.scale : 0.04\n",
      "I0710 07:18:44.935310 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.params_init.seed : NoneType\n",
      "I0710 07:18:44.935348 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.qdomain.c_state : NoneType\n",
      "I0710 07:18:44.935387 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.qdomain.default : NoneType\n",
      "I0710 07:18:44.935438 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.qdomain.fullyconnected : NoneType\n",
      "I0710 07:18:44.935475 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.qdomain.m_state : NoneType\n",
      "I0710 07:18:44.935537 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.qdomain.weight : NoneType\n",
      "I0710 07:18:44.935584 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.random_seed : NoneType\n",
      "I0710 07:18:44.935622 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.reset_cell_state : False\n",
      "I0710 07:18:44.935660 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.935697 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.vn.global_vn : False\n",
      "I0710 07:18:44.935734 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.vn.per_step_vn : False\n",
      "I0710 07:18:44.935772 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.vn.scale : NoneType\n",
      "I0710 07:18:44.935810 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.vn.seed : NoneType\n",
      "I0710 07:18:44.935847 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.zero_state_init_params.method : 'zeros'\n",
      "I0710 07:18:44.935885 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.zero_state_init_params.seed : NoneType\n",
      "I0710 07:18:44.935924 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.zo_prob : 0.0\n",
      "I0710 07:18:44.935961 140310643746176 base_runner.py:59] task.encoder.name : ''\n",
      "I0710 07:18:44.935998 140310643746176 base_runner.py:59] task.encoder.num_lstm_layers : 6\n",
      "I0710 07:18:44.936045 140310643746176 base_runner.py:59] task.encoder.packed_input : False\n",
      "I0710 07:18:44.936087 140310643746176 base_runner.py:59] task.encoder.params_init.method : 'xavier'\n",
      "I0710 07:18:44.936147 140310643746176 base_runner.py:59] task.encoder.params_init.scale : 1.000001\n",
      "I0710 07:18:44.936186 140310643746176 base_runner.py:59] task.encoder.params_init.seed : NoneType\n",
      "I0710 07:18:44.936225 140310643746176 base_runner.py:59] task.encoder.proj_tpl.activation : 'RELU'\n",
      "I0710 07:18:44.936264 140310643746176 base_runner.py:59] task.encoder.proj_tpl.affine_last : False\n",
      "I0710 07:18:44.936302 140310643746176 base_runner.py:59] task.encoder.proj_tpl.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.936347 140310643746176 base_runner.py:59] task.encoder.proj_tpl.apply_pruning : False\n",
      "I0710 07:18:44.936385 140310643746176 base_runner.py:59] task.encoder.proj_tpl.batch_norm : NoneType\n",
      "I0710 07:18:44.936426 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bias_init : 0.0\n",
      "I0710 07:18:44.936464 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_fold_weights : NoneType\n",
      "I0710 07:18:44.936503 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.add_stats_to_moving_average_variables : NoneType\n",
      "I0710 07:18:44.936542 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.936581 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.cls : type/lingvo.core.bn_layers/BatchNormLayer\n",
      "I0710 07:18:44.936619 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.decay : 0.999\n",
      "I0710 07:18:44.936658 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.dim : 0\n",
      "I0710 07:18:44.936696 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.dtype : float32\n",
      "I0710 07:18:44.936735 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.enable_cross_replica_sum_on_tpu : True\n",
      "I0710 07:18:44.936773 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.fprop_dtype : NoneType\n",
      "I0710 07:18:44.936811 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.gamma_zero_init : False\n",
      "I0710 07:18:44.936851 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.inference_driver_name : NoneType\n",
      "I0710 07:18:44.936889 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.is_inference : NoneType\n",
      "I0710 07:18:44.936928 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.name : ''\n",
      "I0710 07:18:44.936966 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.params_init.method : 'xavier'\n",
      "I0710 07:18:44.937004 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.params_init.scale : 1.000001\n",
      "I0710 07:18:44.937049 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.params_init.seed : NoneType\n",
      "I0710 07:18:44.937087 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.random_seed : NoneType\n",
      "I0710 07:18:44.937126 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.set_padded_output_to_zero : True\n",
      "I0710 07:18:44.937177 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.937214 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.use_fused_batch_norm_for_eval : False\n",
      "I0710 07:18:44.937271 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.use_moving_avg_in_training : False\n",
      "I0710 07:18:44.937309 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.vn.global_vn : False\n",
      "I0710 07:18:44.937347 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.vn.per_step_vn : False\n",
      "I0710 07:18:44.937386 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.vn.scale : NoneType\n",
      "I0710 07:18:44.937424 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.vn.seed : NoneType\n",
      "I0710 07:18:44.937462 140310643746176 base_runner.py:59] task.encoder.proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer\n",
      "I0710 07:18:44.937500 140310643746176 base_runner.py:59] task.encoder.proj_tpl.dtype : float32\n",
      "I0710 07:18:44.937550 140310643746176 base_runner.py:59] task.encoder.proj_tpl.fprop_dtype : NoneType\n",
      "I0710 07:18:44.937587 140310643746176 base_runner.py:59] task.encoder.proj_tpl.has_bias : False\n",
      "I0710 07:18:44.937642 140310643746176 base_runner.py:59] task.encoder.proj_tpl.inference_driver_name : NoneType\n",
      "I0710 07:18:44.937680 140310643746176 base_runner.py:59] task.encoder.proj_tpl.input_dim : 0\n",
      "I0710 07:18:44.937721 140310643746176 base_runner.py:59] task.encoder.proj_tpl.is_inference : NoneType\n",
      "I0710 07:18:44.937759 140310643746176 base_runner.py:59] task.encoder.proj_tpl.name : ''\n",
      "I0710 07:18:44.937797 140310643746176 base_runner.py:59] task.encoder.proj_tpl.output_dim : 0\n",
      "I0710 07:18:44.937835 140310643746176 base_runner.py:59] task.encoder.proj_tpl.params_init.method : 'xavier'\n",
      "I0710 07:18:44.937878 140310643746176 base_runner.py:59] task.encoder.proj_tpl.params_init.scale : 1.000001\n",
      "I0710 07:18:44.937935 140310643746176 base_runner.py:59] task.encoder.proj_tpl.params_init.seed : NoneType\n",
      "I0710 07:18:44.937974 140310643746176 base_runner.py:59] task.encoder.proj_tpl.qdomain.default : NoneType\n",
      "I0710 07:18:44.938013 140310643746176 base_runner.py:59] task.encoder.proj_tpl.random_seed : NoneType\n",
      "I0710 07:18:44.938058 140310643746176 base_runner.py:59] task.encoder.proj_tpl.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.938097 140310643746176 base_runner.py:59] task.encoder.proj_tpl.use_einsum : False\n",
      "I0710 07:18:44.938135 140310643746176 base_runner.py:59] task.encoder.proj_tpl.vn.global_vn : False\n",
      "I0710 07:18:44.938174 140310643746176 base_runner.py:59] task.encoder.proj_tpl.vn.per_step_vn : False\n",
      "I0710 07:18:44.938212 140310643746176 base_runner.py:59] task.encoder.proj_tpl.vn.scale : NoneType\n",
      "I0710 07:18:44.938250 140310643746176 base_runner.py:59] task.encoder.proj_tpl.vn.seed : NoneType\n",
      "I0710 07:18:44.938289 140310643746176 base_runner.py:59] task.encoder.proj_tpl.weight_norm : False\n",
      "I0710 07:18:44.938327 140310643746176 base_runner.py:59] task.encoder.random_seed : NoneType\n",
      "I0710 07:18:44.938365 140310643746176 base_runner.py:59] task.encoder.residual_start : 2\n",
      "I0710 07:18:44.938404 140310643746176 base_runner.py:59] task.encoder.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.938441 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.add_weight_summaries : True\n",
      "I0710 07:18:44.938479 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.938517 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.cls : type/lingvo.core.layers/WeightedSumLayer\n",
      "I0710 07:18:44.938556 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.dtype : float32\n",
      "I0710 07:18:44.938593 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.fprop_dtype : NoneType\n",
      "I0710 07:18:44.938631 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.global_weight_scale : 1.0\n",
      "I0710 07:18:44.938670 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.inference_driver_name : NoneType\n",
      "I0710 07:18:44.938708 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.is_inference : NoneType\n",
      "I0710 07:18:44.938746 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.minimal_prob : 0.0\n",
      "I0710 07:18:44.938784 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.name : ''\n",
      "I0710 07:18:44.938822 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.num_sources : 0\n",
      "I0710 07:18:44.938858 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.params_init.method : 'xavier'\n",
      "I0710 07:18:44.938891 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.params_init.scale : 1.000001\n",
      "I0710 07:18:44.938930 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.params_init.seed : NoneType\n",
      "I0710 07:18:44.938968 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.random_seed : NoneType\n",
      "I0710 07:18:44.939006 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.939061 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.vn.global_vn : False\n",
      "I0710 07:18:44.939097 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.vn.per_step_vn : False\n",
      "I0710 07:18:44.939135 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.vn.scale : NoneType\n",
      "I0710 07:18:44.939171 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.vn.seed : NoneType\n",
      "I0710 07:18:44.939208 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.weighted_merger_dropout_prob : 0.1\n",
      "I0710 07:18:44.939244 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.weighted_merger_softmax : True\n",
      "I0710 07:18:44.939281 140310643746176 base_runner.py:59] task.encoder.vn.global_vn : False\n",
      "I0710 07:18:44.939318 140310643746176 base_runner.py:59] task.encoder.vn.per_step_vn : False\n",
      "I0710 07:18:44.939355 140310643746176 base_runner.py:59] task.encoder.vn.scale : NoneType\n",
      "I0710 07:18:44.939392 140310643746176 base_runner.py:59] task.encoder.vn.seed : NoneType\n",
      "I0710 07:18:44.939429 140310643746176 base_runner.py:59] task.eval.decoder_samples_per_summary : 0\n",
      "I0710 07:18:44.939466 140310643746176 base_runner.py:59] task.eval.load_checkpoint_from : NoneType\n",
      "I0710 07:18:44.939502 140310643746176 base_runner.py:59] task.eval.samples_per_summary : 2466\n",
      "I0710 07:18:44.939539 140310643746176 base_runner.py:59] task.eval.start_decoder_after : 0\n",
      "I0710 07:18:44.939576 140310643746176 base_runner.py:59] task.eval.start_eval_after : 0\n",
      "I0710 07:18:44.939612 140310643746176 base_runner.py:59] task.fprop_dtype : NoneType\n",
      "I0710 07:18:44.939649 140310643746176 base_runner.py:59] task.inference_driver_name : NoneType\n",
      "I0710 07:18:44.939686 140310643746176 base_runner.py:59] task.input : NoneType\n",
      "I0710 07:18:44.939723 140310643746176 base_runner.py:59] task.is_inference : NoneType\n",
      "I0710 07:18:44.939759 140310643746176 base_runner.py:59] task.name : 'punctuator_rnmt'\n",
      "I0710 07:18:44.939795 140310643746176 base_runner.py:59] task.online_encoder : NoneType\n",
      "I0710 07:18:44.939851 140310643746176 base_runner.py:59] task.params_init.method : 'xavier'\n",
      "I0710 07:18:44.939889 140310643746176 base_runner.py:59] task.params_init.scale : 1.000001\n",
      "I0710 07:18:44.939928 140310643746176 base_runner.py:59] task.params_init.seed : NoneType\n",
      "I0710 07:18:44.939966 140310643746176 base_runner.py:59] task.random_seed : NoneType\n",
      "I0710 07:18:44.940004 140310643746176 base_runner.py:59] task.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.940047 140310643746176 base_runner.py:59] task.train.bprop_variable_exclusion : NoneType\n",
      "I0710 07:18:44.940087 140310643746176 base_runner.py:59] task.train.bprop_variable_filter : NoneType\n",
      "I0710 07:18:44.940125 140310643746176 base_runner.py:59] task.train.clip_gradient_norm_to_value : 0.0\n",
      "I0710 07:18:44.940163 140310643746176 base_runner.py:59] task.train.clip_gradient_single_norm_to_value : 0.0\n",
      "I0710 07:18:44.940201 140310643746176 base_runner.py:59] task.train.colocate_gradients_with_ops : True\n",
      "I0710 07:18:44.940240 140310643746176 base_runner.py:59] task.train.early_stop.metric_history.jobname : 'eval_dev'\n",
      "I0710 07:18:44.940278 140310643746176 base_runner.py:59] task.train.early_stop.metric_history.local_filesystem : False\n",
      "I0710 07:18:44.940319 140310643746176 base_runner.py:59] task.train.early_stop.metric_history.logdir : ''\n",
      "I0710 07:18:44.940377 140310643746176 base_runner.py:59] task.train.early_stop.metric_history.metric : 'log_pplx'\n",
      "I0710 07:18:44.940418 140310643746176 base_runner.py:59] task.train.early_stop.metric_history.minimize : True\n",
      "I0710 07:18:44.940461 140310643746176 base_runner.py:59] task.train.early_stop.metric_history.name : 'MetricHistory'\n",
      "I0710 07:18:44.940500 140310643746176 base_runner.py:59] task.train.early_stop.metric_history.tfevent_file : False\n",
      "I0710 07:18:44.940539 140310643746176 base_runner.py:59] task.train.early_stop.min_steps : 0\n",
      "I0710 07:18:44.940577 140310643746176 base_runner.py:59] task.train.early_stop.name : 'EarlyStop'\n",
      "I0710 07:18:44.940615 140310643746176 base_runner.py:59] task.train.early_stop.tolerance : 0.0\n",
      "I0710 07:18:44.940654 140310643746176 base_runner.py:59] task.train.early_stop.verbose : True\n",
      "I0710 07:18:44.940692 140310643746176 base_runner.py:59] task.train.early_stop.window : 0\n",
      "I0710 07:18:44.940730 140310643746176 base_runner.py:59] task.train.ema_decay : 0.0\n",
      "I0710 07:18:44.940769 140310643746176 base_runner.py:59] task.train.ema_decay_moving_vars : NoneType\n",
      "I0710 07:18:44.940807 140310643746176 base_runner.py:59] task.train.enqueue_max_steps : -1\n",
      "I0710 07:18:44.940845 140310643746176 base_runner.py:59] task.train.gate_gradients : False\n",
      "I0710 07:18:44.940903 140310643746176 base_runner.py:59] task.train.grad_aggregation_method : 1\n",
      "I0710 07:18:44.940942 140310643746176 base_runner.py:59] task.train.grad_norm_to_clip_to_zero : 100000.0\n",
      "I0710 07:18:44.940981 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.941025 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.clip_threshold : 4.0\n",
      "I0710 07:18:44.941064 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.cls : type/lingvo.core.layers/GradNormTracker\n",
      "I0710 07:18:44.941104 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.decay : 0.995\n",
      "I0710 07:18:44.941142 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.dtype : float32\n",
      "I0710 07:18:44.941182 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.fprop_dtype : NoneType\n",
      "I0710 07:18:44.941220 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.grad_norm_clip_cap_min : 0.0\n",
      "I0710 07:18:44.941258 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.grad_norm_lower_cap : 0.01\n",
      "I0710 07:18:44.941296 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.inference_driver_name : NoneType\n",
      "I0710 07:18:44.941334 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.is_inference : NoneType\n",
      "I0710 07:18:44.941372 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.name : 'gradient_norm_tracker'\n",
      "I0710 07:18:44.941411 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.params_init.method : 'xavier'\n",
      "I0710 07:18:44.941449 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.params_init.scale : 1.000001\n",
      "I0710 07:18:44.941488 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.params_init.seed : NoneType\n",
      "I0710 07:18:44.941527 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.random_seed : NoneType\n",
      "I0710 07:18:44.941565 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.941604 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.vn.global_vn : False\n",
      "I0710 07:18:44.941642 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.vn.per_step_vn : False\n",
      "I0710 07:18:44.941680 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.vn.scale : NoneType\n",
      "I0710 07:18:44.941724 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.vn.seed : NoneType\n",
      "I0710 07:18:44.941762 140310643746176 base_runner.py:59] task.train.init_from_checkpoint_rules : {}\n",
      "I0710 07:18:44.941801 140310643746176 base_runner.py:59] task.train.l1_regularizer_weight : NoneType\n",
      "I0710 07:18:44.941839 140310643746176 base_runner.py:59] task.train.l2_regularizer_weight : 1e-05\n",
      "I0710 07:18:44.941878 140310643746176 base_runner.py:59] task.train.learner : NoneType\n",
      "I0710 07:18:44.941929 140310643746176 base_runner.py:59] task.train.learning_rate : 0.0001\n",
      "I0710 07:18:44.941970 140310643746176 base_runner.py:59] task.train.lr_schedule.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.942027 140310643746176 base_runner.py:59] task.train.lr_schedule.cls : type/lingvo.core.schedule/LinearRampupExponentialDecayScaledByNumSplitSchedule\n",
      "I0710 07:18:44.942070 140310643746176 base_runner.py:59] task.train.lr_schedule.decay_end : 1200000\n",
      "I0710 07:18:44.942109 140310643746176 base_runner.py:59] task.train.lr_schedule.decay_start : 400000\n",
      "I0710 07:18:44.942148 140310643746176 base_runner.py:59] task.train.lr_schedule.dtype : float32\n",
      "I0710 07:18:44.942186 140310643746176 base_runner.py:59] task.train.lr_schedule.fprop_dtype : NoneType\n",
      "I0710 07:18:44.942225 140310643746176 base_runner.py:59] task.train.lr_schedule.inference_driver_name : NoneType\n",
      "I0710 07:18:44.942264 140310643746176 base_runner.py:59] task.train.lr_schedule.is_inference : NoneType\n",
      "I0710 07:18:44.942302 140310643746176 base_runner.py:59] task.train.lr_schedule.max : 100000000.0\n",
      "I0710 07:18:44.942341 140310643746176 base_runner.py:59] task.train.lr_schedule.min : 0.5\n",
      "I0710 07:18:44.942379 140310643746176 base_runner.py:59] task.train.lr_schedule.name : 'LRSched'\n",
      "I0710 07:18:44.942418 140310643746176 base_runner.py:59] task.train.lr_schedule.num_splits : 0\n",
      "I0710 07:18:44.942456 140310643746176 base_runner.py:59] task.train.lr_schedule.params_init.method : 'xavier'\n",
      "I0710 07:18:44.942501 140310643746176 base_runner.py:59] task.train.lr_schedule.params_init.scale : 1.000001\n",
      "I0710 07:18:44.942574 140310643746176 base_runner.py:59] task.train.lr_schedule.params_init.seed : NoneType\n",
      "I0710 07:18:44.942664 140310643746176 base_runner.py:59] task.train.lr_schedule.random_seed : NoneType\n",
      "I0710 07:18:44.942751 140310643746176 base_runner.py:59] task.train.lr_schedule.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.942829 140310643746176 base_runner.py:59] task.train.lr_schedule.vn.global_vn : False\n",
      "I0710 07:18:44.942878 140310643746176 base_runner.py:59] task.train.lr_schedule.vn.per_step_vn : False\n",
      "I0710 07:18:44.942919 140310643746176 base_runner.py:59] task.train.lr_schedule.vn.scale : NoneType\n",
      "I0710 07:18:44.942959 140310643746176 base_runner.py:59] task.train.lr_schedule.vn.seed : NoneType\n",
      "I0710 07:18:44.943012 140310643746176 base_runner.py:59] task.train.lr_schedule.warmup : 500\n",
      "I0710 07:18:44.943058 140310643746176 base_runner.py:59] task.train.lr_schedule.warmup_init : 1.0\n",
      "I0710 07:18:44.943096 140310643746176 base_runner.py:59] task.train.max_steps : 4000000\n",
      "I0710 07:18:44.943135 140310643746176 base_runner.py:59] task.train.optimizer.allow_implicit_capture : NoneType\n",
      "I0710 07:18:44.943173 140310643746176 base_runner.py:59] task.train.optimizer.beta1 : 0.9\n",
      "I0710 07:18:44.943210 140310643746176 base_runner.py:59] task.train.optimizer.beta2 : 0.98\n",
      "I0710 07:18:44.943247 140310643746176 base_runner.py:59] task.train.optimizer.cls : type/lingvo.core.optimizer/Adam\n",
      "I0710 07:18:44.943284 140310643746176 base_runner.py:59] task.train.optimizer.dtype : float32\n",
      "I0710 07:18:44.943322 140310643746176 base_runner.py:59] task.train.optimizer.epsilon : 1e-06\n",
      "I0710 07:18:44.943361 140310643746176 base_runner.py:59] task.train.optimizer.fprop_dtype : NoneType\n",
      "I0710 07:18:44.943400 140310643746176 base_runner.py:59] task.train.optimizer.inference_driver_name : NoneType\n",
      "I0710 07:18:44.943439 140310643746176 base_runner.py:59] task.train.optimizer.is_inference : NoneType\n",
      "I0710 07:18:44.943476 140310643746176 base_runner.py:59] task.train.optimizer.name : 'Adam'\n",
      "I0710 07:18:44.943515 140310643746176 base_runner.py:59] task.train.optimizer.params_init.method : 'xavier'\n",
      "I0710 07:18:44.943552 140310643746176 base_runner.py:59] task.train.optimizer.params_init.scale : 1.000001\n",
      "I0710 07:18:44.943591 140310643746176 base_runner.py:59] task.train.optimizer.params_init.seed : NoneType\n",
      "I0710 07:18:44.943630 140310643746176 base_runner.py:59] task.train.optimizer.random_seed : NoneType\n",
      "I0710 07:18:44.943669 140310643746176 base_runner.py:59] task.train.optimizer.skip_lp_regularization : NoneType\n",
      "I0710 07:18:44.943706 140310643746176 base_runner.py:59] task.train.optimizer.use_bf16_gradients_ar : False\n",
      "I0710 07:18:44.943753 140310643746176 base_runner.py:59] task.train.optimizer.vn.global_vn : False\n",
      "I0710 07:18:44.943801 140310643746176 base_runner.py:59] task.train.optimizer.vn.per_step_vn : False\n",
      "I0710 07:18:44.943839 140310643746176 base_runner.py:59] task.train.optimizer.vn.scale : NoneType\n",
      "I0710 07:18:44.943877 140310643746176 base_runner.py:59] task.train.optimizer.vn.seed : NoneType\n",
      "I0710 07:18:44.943915 140310643746176 base_runner.py:59] task.train.pruning_hparams_dict : NoneType\n",
      "I0710 07:18:44.943953 140310643746176 base_runner.py:59] task.train.save_interval_seconds : 600\n",
      "I0710 07:18:44.943991 140310643746176 base_runner.py:59] task.train.save_keep_checkpoint_every_n_hours : 0.5\n",
      "I0710 07:18:44.944033 140310643746176 base_runner.py:59] task.train.save_max_to_keep : 100\n",
      "I0710 07:18:44.944068 140310643746176 base_runner.py:59] task.train.scale_gradients : True\n",
      "I0710 07:18:44.944106 140310643746176 base_runner.py:59] task.train.start_up_delay_steps : 200\n",
      "I0710 07:18:44.944162 140310643746176 base_runner.py:59] task.train.summary_interval_steps : 100\n",
      "I0710 07:18:44.944201 140310643746176 base_runner.py:59] task.train.tpu_steps_per_loop : 100\n",
      "I0710 07:18:44.944239 140310643746176 base_runner.py:59] task.train.vn_start_step : 200000000\n",
      "I0710 07:18:44.944278 140310643746176 base_runner.py:59] task.train.vn_std : 0.0\n",
      "I0710 07:18:44.944329 140310643746176 base_runner.py:59] task.vn.global_vn : False\n",
      "I0710 07:18:44.944365 140310643746176 base_runner.py:59] task.vn.per_step_vn : False\n",
      "I0710 07:18:44.944403 140310643746176 base_runner.py:59] task.vn.scale : NoneType\n",
      "I0710 07:18:44.944440 140310643746176 base_runner.py:59] task.vn.seed : NoneType\n",
      "I0710 07:18:44.944478 140310643746176 base_runner.py:59] train.early_stop.metric_history.jobname : 'eval_dev'\n",
      "I0710 07:18:44.944514 140310643746176 base_runner.py:59] train.early_stop.metric_history.local_filesystem : False\n",
      "I0710 07:18:44.944576 140310643746176 base_runner.py:59] train.early_stop.metric_history.logdir : ''\n",
      "I0710 07:18:44.944633 140310643746176 base_runner.py:59] train.early_stop.metric_history.metric : 'log_pplx'\n",
      "I0710 07:18:44.944672 140310643746176 base_runner.py:59] train.early_stop.metric_history.minimize : True\n",
      "I0710 07:18:44.944710 140310643746176 base_runner.py:59] train.early_stop.metric_history.name : 'MetricHistory'\n",
      "I0710 07:18:44.944749 140310643746176 base_runner.py:59] train.early_stop.metric_history.tfevent_file : False\n",
      "I0710 07:18:44.944787 140310643746176 base_runner.py:59] train.early_stop.min_steps : 0\n",
      "I0710 07:18:44.944826 140310643746176 base_runner.py:59] train.early_stop.name : 'EarlyStop'\n",
      "I0710 07:18:44.944865 140310643746176 base_runner.py:59] train.early_stop.tolerance : 0.0\n",
      "I0710 07:18:44.944904 140310643746176 base_runner.py:59] train.early_stop.verbose : True\n",
      "I0710 07:18:44.944943 140310643746176 base_runner.py:59] train.early_stop.window : 0\n",
      "I0710 07:18:44.944982 140310643746176 base_runner.py:59] train.ema_decay : 0.0\n",
      "I0710 07:18:44.945025 140310643746176 base_runner.py:59] train.ema_decay_moving_vars : NoneType\n",
      "I0710 07:18:44.945076 140310643746176 base_runner.py:59] train.enqueue_max_steps : -1\n",
      "I0710 07:18:44.945115 140310643746176 base_runner.py:59] train.init_from_checkpoint_rules : {}\n",
      "I0710 07:18:44.945152 140310643746176 base_runner.py:59] train.max_steps : 4000000\n",
      "I0710 07:18:44.945190 140310643746176 base_runner.py:59] train.save_interval_seconds : 600\n",
      "I0710 07:18:44.945228 140310643746176 base_runner.py:59] train.save_keep_checkpoint_every_n_hours : 1.0\n",
      "I0710 07:18:44.945265 140310643746176 base_runner.py:59] train.save_max_to_keep : 3\n",
      "I0710 07:18:44.945303 140310643746176 base_runner.py:59] train.start_up_delay_steps : 200\n",
      "I0710 07:18:44.945340 140310643746176 base_runner.py:59] train.summary_interval_steps : 100\n",
      "I0710 07:18:44.945379 140310643746176 base_runner.py:59] train.tpu_steps_per_loop : 100\n",
      "I0710 07:18:44.945417 140310643746176 base_runner.py:59] vn.global_vn : False\n",
      "I0710 07:18:44.945454 140310643746176 base_runner.py:59] vn.per_step_vn : False\n",
      "I0710 07:18:44.945492 140310643746176 base_runner.py:59] vn.scale : NoneType\n",
      "I0710 07:18:44.945528 140310643746176 base_runner.py:59] vn.seed : NoneType\n",
      "I0710 07:18:44.945565 140310643746176 base_runner.py:59] \n",
      "I0710 07:18:44.945615 140310643746176 base_runner.py:60] ============================================================\n",
      "I0710 07:18:44.947917 140310643746176 base_runner.py:111] Starting ...\n",
      "I0710 07:18:44.950212 140310643746176 cluster.py:507] _LeastLoadedPlacer : ['/job:localhost/replica:0/task:0/device:CPU:0']\n",
      "I0710 07:18:44.962561 140310643746176 cluster.py:525] Place variable global_step on /job:localhost/replica:0/task:0/device:CPU:0 8\n",
      "I0710 07:18:44.982988 140310643746176 base_model.py:1068] Training parameters for <class 'lingvo.core.base_model.SingleTaskModel'>: {\n",
      "  early_stop: {\n",
      "    metric_history: {\n",
      "      jobname: \"eval_dev\"\n",
      "      local_filesystem: False\n",
      "      logdir: \"/tmp/punctuator\"\n",
      "      metric: \"log_pplx\"\n",
      "      minimize: True\n",
      "      name: \"MetricHistory\"\n",
      "      tfevent_file: False\n",
      "    }\n",
      "    min_steps: 0\n",
      "    name: \"EarlyStop\"\n",
      "    tolerance: 0.0\n",
      "    verbose: True\n",
      "    window: 0\n",
      "  }\n",
      "  ema_decay: 0.0\n",
      "  ema_decay_moving_vars: None\n",
      "  enqueue_max_steps: -1\n",
      "  init_from_checkpoint_rules: {}\n",
      "  max_steps: 4000000\n",
      "  save_interval_seconds: 600\n",
      "  save_keep_checkpoint_every_n_hours: 1.0\n",
      "  save_max_to_keep: 3\n",
      "  start_up_delay_steps: 200\n",
      "  summary_interval_steps: 100\n",
      "  tpu_steps_per_loop: 100\n",
      "}\n",
      "I0710 07:18:45.001188 140310643746176 base_model.py:280] input_params: {\n",
      "  allow_implicit_capture: None\n",
      "  bucket_adjust_every_n: 0\n",
      "  bucket_batch_limit: [512, 256, 160, 80, 40]\n",
      "  bucket_upper_bound: [10, 20, 30, 60, 120]\n",
      "  cls: <class 'input_generator.PunctuatorInput'>\n",
      "  dtype: <dtype: 'float32'>\n",
      "  file_buffer_size: 10000\n",
      "  file_buffer_size_in_seconds: 0\n",
      "  file_datasource: None\n",
      "  file_parallelism: 1\n",
      "  file_pattern: \"text:/tmp/punctuator_data/train.txt\"\n",
      "  file_random_seed: 0\n",
      "  flush_every_n: 0\n",
      "  fprop_dtype: None\n",
      "  inference_driver_name: None\n",
      "  is_inference: None\n",
      "  name: \"input\"\n",
      "  num_batcher_threads: 1\n",
      "  num_partitions: None\n",
      "  num_samples: 0\n",
      "  pad_to_max_seq_length: False\n",
      "  params_init: {\n",
      "    method: \"xavier\"\n",
      "    scale: 1.000001\n",
      "    seed: None\n",
      "  }\n",
      "  random_seed: None\n",
      "  remote: {\n",
      "    max_inflights_per_target: 32\n",
      "    shardable_batch: False\n",
      "  }\n",
      "  repeat_count: -1\n",
      "  require_sequential_order: False\n",
      "  skip_lp_regularization: None\n",
      "  source_max_length: 122\n",
      "  target_max_length: 122\n",
      "  tokenizer: {\n",
      "    allow_implicit_capture: None\n",
      "    append_eos: True\n",
      "    cls: <class 'lingvo.core.tokenizers.WpmTokenizer'>\n",
      "    dtype: <dtype: 'float32'>\n",
      "    fprop_dtype: None\n",
      "    inference_driver_name: None\n",
      "    is_inference: None\n",
      "    merge_prob: 1.0\n",
      "    name: \"tokenizer\"\n",
      "    pad_to_max_length: False\n",
      "    params_init: {\n",
      "      method: \"xavier\"\n",
      "      scale: 1.000001\n",
      "      seed: None\n",
      "    }\n",
      "    random_seed: None\n",
      "    skip_lp_regularization: None\n",
      "    target_eos_id: 2\n",
      "    target_sos_id: 1\n",
      "    target_unk_id: 0\n",
      "    target_wb_id: -1\n",
      "    vn: {\n",
      "      global_vn: False\n",
      "      per_step_vn: False\n",
      "      scale: None\n",
      "      seed: None\n",
      "    }\n",
      "    vocab_filepath: \"brown_corpus_wpm.16000.vocab\"\n",
      "    vocab_size: 16000\n",
      "  }\n",
      "  tokenizer_dict: {}\n",
      "  tpu_infeed_parallelism: 1\n",
      "  use_chaining: False\n",
      "  use_partitioned_infeed_queue: False\n",
      "  use_per_host_infeed: False\n",
      "  use_within_batch_mixing: False\n",
      "  vn: {\n",
      "    global_vn: False\n",
      "    per_step_vn: False\n",
      "    scale: None\n",
      "    seed: None\n",
      "  }\n",
      "}\n",
      "I0710 07:18:45.050369 140310643746176 base_input_generator.py:673] Building data source <lingvo.core.datasource.SimpleDataSource object at 0x7f9c1c0c5190> with params {\n",
      "  allow_implicit_capture: None\n",
      "  cls: <class 'lingvo.core.datasource.SimpleDataSource'>\n",
      "  dtype: <dtype: 'float32'>\n",
      "  file_pattern: \"text:/tmp/punctuator_data/train.txt\"\n",
      "  file_type: \"\"\n",
      "  fprop_dtype: None\n",
      "  inference_driver_name: None\n",
      "  is_inference: None\n",
      "  name: \"datasource\"\n",
      "  params_init: {\n",
      "    method: \"xavier\"\n",
      "    scale: 1.000001\n",
      "    seed: None\n",
      "  }\n",
      "  random_seed: None\n",
      "  skip_lp_regularization: None\n",
      "  vn: {\n",
      "    global_vn: False\n",
      "    per_step_vn: False\n",
      "    scale: None\n",
      "    seed: None\n",
      "  }\n",
      "} and file_pattern text:/tmp/punctuator_data/train.txt\n",
      "I0710 07:18:45.050691 140310643746176 base_input_generator.py:775] infeed_bucket_batch_limit=[512, 256, 160, 80, 40] num_splits_per_client=1 bucket_batch_limit=[512, 256, 160, 80, 40]\n",
      "I0710 07:18:45.050764 140310643746176 base_input_generator.py:798] infeed_bucket_batch_limit [512, 256, 160, 80, 40]\n",
      "2020-07-10 07:18:45.345445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:45.346584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2020-07-10 07:18:45.346722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:45.347759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \n",
      "pciBusID: 0000:00:05.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2020-07-10 07:18:45.347837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:45.348809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 2 with properties: \n",
      "pciBusID: 0000:00:06.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2020-07-10 07:18:45.348878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:45.349887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 3 with properties: \n",
      "pciBusID: 0000:00:07.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2020-07-10 07:18:45.349965: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-07-10 07:18:45.349985: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-07-10 07:18:45.350002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-07-10 07:18:45.350019: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-07-10 07:18:45.350036: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-07-10 07:18:45.350052: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-07-10 07:18:45.350070: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-07-10 07:18:45.350132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:45.351115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:45.352153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:45.353391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:45.354474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:45.355575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:45.356707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:45.357768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-10 07:18:45.358796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1, 2, 3\n",
      "I0710 07:18:47.026587 140310643746176 learner.py:383] Ignoring legacy param start_up_delay_steps=200 for optimization program\n",
      "I0710 07:18:47.026752 140310643746176 learner.py:383] Ignoring legacy param max_steps=4000000 for optimization program\n",
      "I0710 07:18:47.026810 140310643746176 learner.py:383] Ignoring legacy param tpu_steps_per_loop=100 for optimization program\n",
      "I0710 07:18:47.026858 140310643746176 learner.py:383] Ignoring legacy param vn_start_step=200000000 for optimization program\n",
      "I0710 07:18:47.026901 140310643746176 learner.py:383] Ignoring legacy param vn_std=0.0 for optimization program\n",
      "I0710 07:18:47.026944 140310643746176 learner.py:383] Ignoring legacy param early_stop={\n",
      "  metric_history: {\n",
      "    jobname: \"eval_dev\"\n",
      "    local_filesystem: False\n",
      "    logdir: \"/tmp/punctuator\"\n",
      "    metric: \"log_pplx\"\n",
      "    minimize: True\n",
      "    name: \"MetricHistory\"\n",
      "    tfevent_file: False\n",
      "  }\n",
      "  min_steps: 0\n",
      "  name: \"EarlyStop\"\n",
      "  tolerance: 0.0\n",
      "  verbose: True\n",
      "  window: 0\n",
      "} for optimization program\n",
      "I0710 07:18:47.027046 140310643746176 learner.py:383] Ignoring legacy param ema_decay=0.0 for optimization program\n",
      "I0710 07:18:47.027091 140310643746176 learner.py:383] Ignoring legacy param ema_decay_moving_vars=None for optimization program\n",
      "I0710 07:18:47.027132 140310643746176 learner.py:383] Ignoring legacy param init_from_checkpoint_rules={} for optimization program\n",
      "I0710 07:18:47.027174 140310643746176 learner.py:383] Ignoring legacy param pruning_hparams_dict=None for optimization program\n",
      "I0710 07:18:47.027213 140310643746176 learner.py:383] Ignoring legacy param enqueue_max_steps=-1 for optimization program\n",
      "I0710 07:18:47.027252 140310643746176 learner.py:383] Ignoring legacy param save_interval_seconds=600 for optimization program\n",
      "I0710 07:18:47.027290 140310643746176 learner.py:383] Ignoring legacy param save_max_to_keep=100 for optimization program\n",
      "I0710 07:18:47.027328 140310643746176 learner.py:383] Ignoring legacy param save_keep_checkpoint_every_n_hours=0.5 for optimization program\n",
      "I0710 07:18:47.027370 140310643746176 learner.py:383] Ignoring legacy param summary_interval_steps=100 for optimization program\n",
      "I0710 07:18:47.027408 140310643746176 learner.py:383] Ignoring legacy param learner=None for optimization program\n",
      "I0710 07:18:47.027962 140310643746176 learner.py:388] Learner params: allow_implicit_capture : NoneType\n",
      "I0710 07:18:47.028039 140310643746176 learner.py:388] Learner params: bprop_variable_exclusion : NoneType\n",
      "I0710 07:18:47.028085 140310643746176 learner.py:388] Learner params: bprop_variable_filter : NoneType\n",
      "I0710 07:18:47.028129 140310643746176 learner.py:388] Learner params: clip_gradient_norm_to_value : 0.0\n",
      "I0710 07:18:47.028170 140310643746176 learner.py:388] Learner params: clip_gradient_single_norm_to_value : 0.0\n",
      "I0710 07:18:47.028210 140310643746176 learner.py:388] Learner params: cls : type/lingvo.core.learner/Learner\n",
      "I0710 07:18:47.028250 140310643746176 learner.py:388] Learner params: colocate_gradients_with_ops : True\n",
      "I0710 07:18:47.028288 140310643746176 learner.py:388] Learner params: dtype : float32\n",
      "I0710 07:18:47.028326 140310643746176 learner.py:388] Learner params: fprop_dtype : NoneType\n",
      "I0710 07:18:47.028363 140310643746176 learner.py:388] Learner params: gate_gradients : False\n",
      "I0710 07:18:47.028401 140310643746176 learner.py:388] Learner params: grad_aggregation_method : 1\n",
      "I0710 07:18:47.028439 140310643746176 learner.py:388] Learner params: grad_norm_to_clip_to_zero : 100000.0\n",
      "I0710 07:18:47.028476 140310643746176 learner.py:388] Learner params: grad_norm_tracker.allow_implicit_capture : NoneType\n",
      "I0710 07:18:47.028514 140310643746176 learner.py:388] Learner params: grad_norm_tracker.clip_threshold : 4.0\n",
      "I0710 07:18:47.028558 140310643746176 learner.py:388] Learner params: grad_norm_tracker.cls : type/lingvo.core.layers/GradNormTracker\n",
      "I0710 07:18:47.028596 140310643746176 learner.py:388] Learner params: grad_norm_tracker.decay : 0.995\n",
      "I0710 07:18:47.028634 140310643746176 learner.py:388] Learner params: grad_norm_tracker.dtype : float32\n",
      "I0710 07:18:47.028672 140310643746176 learner.py:388] Learner params: grad_norm_tracker.fprop_dtype : NoneType\n",
      "I0710 07:18:47.028710 140310643746176 learner.py:388] Learner params: grad_norm_tracker.grad_norm_clip_cap_min : 0.0\n",
      "I0710 07:18:47.028748 140310643746176 learner.py:388] Learner params: grad_norm_tracker.grad_norm_lower_cap : 0.01\n",
      "I0710 07:18:47.028801 140310643746176 learner.py:388] Learner params: grad_norm_tracker.inference_driver_name : NoneType\n",
      "I0710 07:18:47.028837 140310643746176 learner.py:388] Learner params: grad_norm_tracker.is_inference : NoneType\n",
      "I0710 07:18:47.028872 140310643746176 learner.py:388] Learner params: grad_norm_tracker.name : 'gradient_norm_tracker'\n",
      "I0710 07:18:47.028907 140310643746176 learner.py:388] Learner params: grad_norm_tracker.params_init.method : 'xavier'\n",
      "I0710 07:18:47.028942 140310643746176 learner.py:388] Learner params: grad_norm_tracker.params_init.scale : 1.000001\n",
      "I0710 07:18:47.028977 140310643746176 learner.py:388] Learner params: grad_norm_tracker.params_init.seed : NoneType\n",
      "I0710 07:18:47.029012 140310643746176 learner.py:388] Learner params: grad_norm_tracker.random_seed : NoneType\n",
      "I0710 07:18:47.029048 140310643746176 learner.py:388] Learner params: grad_norm_tracker.skip_lp_regularization : NoneType\n",
      "I0710 07:18:47.029083 140310643746176 learner.py:388] Learner params: grad_norm_tracker.vn.global_vn : False\n",
      "I0710 07:18:47.029118 140310643746176 learner.py:388] Learner params: grad_norm_tracker.vn.per_step_vn : False\n",
      "I0710 07:18:47.029153 140310643746176 learner.py:388] Learner params: grad_norm_tracker.vn.scale : NoneType\n",
      "I0710 07:18:47.029188 140310643746176 learner.py:388] Learner params: grad_norm_tracker.vn.seed : NoneType\n",
      "I0710 07:18:47.029222 140310643746176 learner.py:388] Learner params: inference_driver_name : NoneType\n",
      "I0710 07:18:47.029258 140310643746176 learner.py:388] Learner params: is_inference : NoneType\n",
      "I0710 07:18:47.029292 140310643746176 learner.py:388] Learner params: l1_regularizer_weight : NoneType\n",
      "I0710 07:18:47.029327 140310643746176 learner.py:388] Learner params: l2_regularizer_weight : 1e-05\n",
      "I0710 07:18:47.029362 140310643746176 learner.py:388] Learner params: learning_rate : 0.0001\n",
      "I0710 07:18:47.029397 140310643746176 learner.py:388] Learner params: lr_schedule.allow_implicit_capture : NoneType\n",
      "I0710 07:18:47.029432 140310643746176 learner.py:388] Learner params: lr_schedule.cls : type/lingvo.core.schedule/LinearRampupExponentialDecayScaledByNumSplitSchedule\n",
      "I0710 07:18:47.029468 140310643746176 learner.py:388] Learner params: lr_schedule.decay_end : 1200000\n",
      "I0710 07:18:47.029503 140310643746176 learner.py:388] Learner params: lr_schedule.decay_start : 400000\n",
      "I0710 07:18:47.029538 140310643746176 learner.py:388] Learner params: lr_schedule.dtype : float32\n",
      "I0710 07:18:47.029577 140310643746176 learner.py:388] Learner params: lr_schedule.fprop_dtype : NoneType\n",
      "I0710 07:18:47.029613 140310643746176 learner.py:388] Learner params: lr_schedule.inference_driver_name : NoneType\n",
      "I0710 07:18:47.029648 140310643746176 learner.py:388] Learner params: lr_schedule.is_inference : NoneType\n",
      "I0710 07:18:47.029683 140310643746176 learner.py:388] Learner params: lr_schedule.max : 100000000.0\n",
      "I0710 07:18:47.029718 140310643746176 learner.py:388] Learner params: lr_schedule.min : 0.5\n",
      "I0710 07:18:47.029753 140310643746176 learner.py:388] Learner params: lr_schedule.name : 'LRSched'\n",
      "I0710 07:18:47.029788 140310643746176 learner.py:388] Learner params: lr_schedule.num_splits : 0\n",
      "I0710 07:18:47.029823 140310643746176 learner.py:388] Learner params: lr_schedule.params_init.method : 'xavier'\n",
      "I0710 07:18:47.029858 140310643746176 learner.py:388] Learner params: lr_schedule.params_init.scale : 1.000001\n",
      "I0710 07:18:47.029906 140310643746176 learner.py:388] Learner params: lr_schedule.params_init.seed : NoneType\n",
      "I0710 07:18:47.029944 140310643746176 learner.py:388] Learner params: lr_schedule.random_seed : NoneType\n",
      "I0710 07:18:47.029979 140310643746176 learner.py:388] Learner params: lr_schedule.skip_lp_regularization : NoneType\n",
      "I0710 07:18:47.030014 140310643746176 learner.py:388] Learner params: lr_schedule.vn.global_vn : False\n",
      "I0710 07:18:47.030050 140310643746176 learner.py:388] Learner params: lr_schedule.vn.per_step_vn : False\n",
      "I0710 07:18:47.030084 140310643746176 learner.py:388] Learner params: lr_schedule.vn.scale : NoneType\n",
      "I0710 07:18:47.030119 140310643746176 learner.py:388] Learner params: lr_schedule.vn.seed : NoneType\n",
      "I0710 07:18:47.030153 140310643746176 learner.py:388] Learner params: lr_schedule.warmup : 500\n",
      "I0710 07:18:47.030188 140310643746176 learner.py:388] Learner params: lr_schedule.warmup_init : 1.0\n",
      "I0710 07:18:47.030223 140310643746176 learner.py:388] Learner params: name : 'loss'\n",
      "I0710 07:18:47.030259 140310643746176 learner.py:388] Learner params: optimizer.allow_implicit_capture : NoneType\n",
      "I0710 07:18:47.030294 140310643746176 learner.py:388] Learner params: optimizer.beta1 : 0.9\n",
      "I0710 07:18:47.030328 140310643746176 learner.py:388] Learner params: optimizer.beta2 : 0.98\n",
      "I0710 07:18:47.030363 140310643746176 learner.py:388] Learner params: optimizer.cls : type/lingvo.core.optimizer/Adam\n",
      "I0710 07:18:47.030398 140310643746176 learner.py:388] Learner params: optimizer.dtype : float32\n",
      "I0710 07:18:47.030432 140310643746176 learner.py:388] Learner params: optimizer.epsilon : 1e-06\n",
      "I0710 07:18:47.030467 140310643746176 learner.py:388] Learner params: optimizer.fprop_dtype : NoneType\n",
      "I0710 07:18:47.030502 140310643746176 learner.py:388] Learner params: optimizer.inference_driver_name : NoneType\n",
      "I0710 07:18:47.030538 140310643746176 learner.py:388] Learner params: optimizer.is_inference : NoneType\n",
      "I0710 07:18:47.030577 140310643746176 learner.py:388] Learner params: optimizer.name : 'Adam'\n",
      "I0710 07:18:47.030612 140310643746176 learner.py:388] Learner params: optimizer.params_init.method : 'xavier'\n",
      "I0710 07:18:47.030647 140310643746176 learner.py:388] Learner params: optimizer.params_init.scale : 1.000001\n",
      "I0710 07:18:47.030682 140310643746176 learner.py:388] Learner params: optimizer.params_init.seed : NoneType\n",
      "I0710 07:18:47.030716 140310643746176 learner.py:388] Learner params: optimizer.random_seed : NoneType\n",
      "I0710 07:18:47.030751 140310643746176 learner.py:388] Learner params: optimizer.skip_lp_regularization : NoneType\n",
      "I0710 07:18:47.030786 140310643746176 learner.py:388] Learner params: optimizer.use_bf16_gradients_ar : False\n",
      "I0710 07:18:47.030840 140310643746176 learner.py:388] Learner params: optimizer.vn.global_vn : False\n",
      "I0710 07:18:47.030877 140310643746176 learner.py:388] Learner params: optimizer.vn.per_step_vn : False\n",
      "I0710 07:18:47.030915 140310643746176 learner.py:388] Learner params: optimizer.vn.scale : NoneType\n",
      "I0710 07:18:47.030952 140310643746176 learner.py:388] Learner params: optimizer.vn.seed : NoneType\n",
      "I0710 07:18:47.030990 140310643746176 learner.py:388] Learner params: params_init.method : 'xavier'\n",
      "I0710 07:18:47.031027 140310643746176 learner.py:388] Learner params: params_init.scale : 1.000001\n",
      "I0710 07:18:47.031064 140310643746176 learner.py:388] Learner params: params_init.seed : NoneType\n",
      "I0710 07:18:47.031101 140310643746176 learner.py:388] Learner params: random_seed : NoneType\n",
      "I0710 07:18:47.031138 140310643746176 learner.py:388] Learner params: scale_gradients : True\n",
      "I0710 07:18:47.031176 140310643746176 learner.py:388] Learner params: skip_lp_regularization : NoneType\n",
      "I0710 07:18:47.031226 140310643746176 learner.py:388] Learner params: skip_zero_gradients : NoneType\n",
      "I0710 07:18:47.031279 140310643746176 learner.py:388] Learner params: vn.global_vn : False\n",
      "I0710 07:18:47.031316 140310643746176 learner.py:388] Learner params: vn.per_step_vn : False\n",
      "I0710 07:18:47.031354 140310643746176 learner.py:388] Learner params: vn.scale : NoneType\n",
      "I0710 07:18:47.031391 140310643746176 learner.py:388] Learner params: vn.seed : NoneType\n",
      "I0710 07:18:47.031428 140310643746176 learner.py:388] Learner params: \n",
      "I0710 07:18:47.035455 140310643746176 cluster.py:525] Place variable punctuator_rnmt/gradient_norm_tracker/log_mean/var on /job:localhost/replica:0/task:0/device:CPU:0 12\n",
      "I0710 07:18:47.037125 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/gradient_norm_tracker/log_mean/var:0 shape=() on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.039637 140310643746176 cluster.py:525] Place variable punctuator_rnmt/gradient_norm_tracker/log_mean_squared/var on /job:localhost/replica:0/task:0/device:CPU:0 16\n",
      "I0710 07:18:47.041207 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/gradient_norm_tracker/log_mean_squared/var:0 shape=() on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.043910 140310643746176 cluster.py:525] Place variable punctuator_rnmt/gradient_norm_tracker/total_weight/var on /job:localhost/replica:0/task:0/device:CPU:0 20\n",
      "I0710 07:18:47.045425 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/gradient_norm_tracker/total_weight/var:0 shape=() on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.048156 140310643746176 cluster.py:525] Place variable punctuator_rnmt/gradient_norm_tracker/total_rejections/var on /job:localhost/replica:0/task:0/device:CPU:0 24\n",
      "I0710 07:18:47.049672 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/gradient_norm_tracker/total_rejections/var:0 shape=() on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.051955 140310643746176 schedule.py:432] Peak lr: 1.000000\n",
      "I0710 07:18:47.067564 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_0/var on /job:localhost/replica:0/task:0/device:CPU:0 4096024\n",
      "I0710 07:18:47.069612 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_0/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.075098 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_1/var on /job:localhost/replica:0/task:0/device:CPU:0 8192024\n",
      "I0710 07:18:47.077009 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_1/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.082559 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_2/var on /job:localhost/replica:0/task:0/device:CPU:0 12288024\n",
      "I0710 07:18:47.084466 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_2/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.089802 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_3/var on /job:localhost/replica:0/task:0/device:CPU:0 16384024\n",
      "I0710 07:18:47.091686 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_3/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.096824 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_4/var on /job:localhost/replica:0/task:0/device:CPU:0 20480024\n",
      "I0710 07:18:47.098791 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_4/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.103954 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_5/var on /job:localhost/replica:0/task:0/device:CPU:0 24576024\n",
      "I0710 07:18:47.105892 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_5/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.111165 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_6/var on /job:localhost/replica:0/task:0/device:CPU:0 28672024\n",
      "I0710 07:18:47.112993 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_6/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.118422 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_7/var on /job:localhost/replica:0/task:0/device:CPU:0 32768024\n",
      "I0710 07:18:47.120433 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_7/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.125790 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_8/var on /job:localhost/replica:0/task:0/device:CPU:0 36864024\n",
      "I0710 07:18:47.128140 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_8/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.133493 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_9/var on /job:localhost/replica:0/task:0/device:CPU:0 40960024\n",
      "I0710 07:18:47.135437 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_9/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.140541 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_10/var on /job:localhost/replica:0/task:0/device:CPU:0 45056024\n",
      "I0710 07:18:47.142554 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_10/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.147901 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_11/var on /job:localhost/replica:0/task:0/device:CPU:0 49152024\n",
      "I0710 07:18:47.150021 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_11/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.155477 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_12/var on /job:localhost/replica:0/task:0/device:CPU:0 53248024\n",
      "I0710 07:18:47.157588 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_12/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.163122 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_13/var on /job:localhost/replica:0/task:0/device:CPU:0 57344024\n",
      "I0710 07:18:47.165111 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_13/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.170587 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_14/var on /job:localhost/replica:0/task:0/device:CPU:0 61440024\n",
      "I0710 07:18:47.172683 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_14/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.177993 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_15/var on /job:localhost/replica:0/task:0/device:CPU:0 65536024\n",
      "I0710 07:18:47.179890 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_15/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.195528 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_fwd/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 99090456\n",
      "I0710 07:18:47.197503 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L0_rnn_fwd/wm/var:0 shape=(2048, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.200150 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_fwd/b/var on /job:localhost/replica:0/task:0/device:CPU:0 99106840\n",
      "I0710 07:18:47.201756 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L0_rnn_fwd/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.209091 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_fwd/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 99123224\n",
      "I0710 07:18:47.210713 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L0_rnn_fwd/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.219183 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_bak/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 132677656\n",
      "I0710 07:18:47.221235 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L0_rnn_bak/wm/var:0 shape=(2048, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.224239 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_bak/b/var on /job:localhost/replica:0/task:0/device:CPU:0 132694040\n",
      "I0710 07:18:47.225865 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L0_rnn_bak/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.233342 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_bak/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 132710424\n",
      "I0710 07:18:47.235019 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L0_rnn_bak/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.245667 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_fwd/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 183042072\n",
      "I0710 07:18:47.247826 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L1_rnn_fwd/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.250550 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_fwd/b/var on /job:localhost/replica:0/task:0/device:CPU:0 183058456\n",
      "I0710 07:18:47.252222 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L1_rnn_fwd/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.259472 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_fwd/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 183074840\n",
      "I0710 07:18:47.261040 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L1_rnn_fwd/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.269322 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_bak/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 233406488\n",
      "I0710 07:18:47.271306 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L1_rnn_bak/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.273948 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_bak/b/var on /job:localhost/replica:0/task:0/device:CPU:0 233422872\n",
      "I0710 07:18:47.275526 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L1_rnn_bak/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.283015 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_bak/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 233439256\n",
      "I0710 07:18:47.285036 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L1_rnn_bak/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.295686 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_fwd/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 283770904\n",
      "I0710 07:18:47.297733 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L2_rnn_fwd/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.300581 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_fwd/b/var on /job:localhost/replica:0/task:0/device:CPU:0 283787288\n",
      "I0710 07:18:47.302870 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L2_rnn_fwd/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.310332 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_fwd/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 283803672\n",
      "I0710 07:18:47.311895 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L2_rnn_fwd/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.320371 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_bak/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 334135320\n",
      "I0710 07:18:47.322532 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L2_rnn_bak/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.325307 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_bak/b/var on /job:localhost/replica:0/task:0/device:CPU:0 334151704\n",
      "I0710 07:18:47.327037 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L2_rnn_bak/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.334551 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_bak/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 334168088\n",
      "I0710 07:18:47.336388 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L2_rnn_bak/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.347424 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_fwd/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 384499736\n",
      "I0710 07:18:47.349419 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L3_rnn_fwd/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.352388 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_fwd/b/var on /job:localhost/replica:0/task:0/device:CPU:0 384516120\n",
      "I0710 07:18:47.354241 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L3_rnn_fwd/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.362036 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_fwd/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 384532504\n",
      "I0710 07:18:47.363709 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L3_rnn_fwd/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.372682 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_bak/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 434864152\n",
      "I0710 07:18:47.374818 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L3_rnn_bak/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.377658 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_bak/b/var on /job:localhost/replica:0/task:0/device:CPU:0 434880536\n",
      "I0710 07:18:47.379340 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L3_rnn_bak/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.387015 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_bak/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 434896920\n",
      "I0710 07:18:47.388804 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L3_rnn_bak/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.399770 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_fwd/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 485228568\n",
      "I0710 07:18:47.401850 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L4_rnn_fwd/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.405200 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_fwd/b/var on /job:localhost/replica:0/task:0/device:CPU:0 485244952\n",
      "I0710 07:18:47.407167 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L4_rnn_fwd/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.414954 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_fwd/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 485261336\n",
      "I0710 07:18:47.416704 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L4_rnn_fwd/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.425608 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_bak/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 535592984\n",
      "I0710 07:18:47.428310 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L4_rnn_bak/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.431203 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_bak/b/var on /job:localhost/replica:0/task:0/device:CPU:0 535609368\n",
      "I0710 07:18:47.432871 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L4_rnn_bak/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.440471 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_bak/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 535625752\n",
      "I0710 07:18:47.442287 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L4_rnn_bak/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.453654 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_fwd/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 585957400\n",
      "I0710 07:18:47.455713 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L5_rnn_fwd/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.458504 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_fwd/b/var on /job:localhost/replica:0/task:0/device:CPU:0 585973784\n",
      "I0710 07:18:47.460213 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L5_rnn_fwd/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.468609 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_fwd/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 585990168\n",
      "I0710 07:18:47.470245 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L5_rnn_fwd/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.478680 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_bak/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 636321816\n",
      "I0710 07:18:47.480665 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L5_rnn_bak/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.483488 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_bak/b/var on /job:localhost/replica:0/task:0/device:CPU:0 636338200\n",
      "I0710 07:18:47.485104 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L5_rnn_bak/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.492771 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_bak/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 636354584\n",
      "I0710 07:18:47.494548 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L5_rnn_bak/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "W0710 07:18:47.497564 140310643746176 py_utils.py:1534] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.\n",
      "I0710 07:18:47.502869 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/proj/w/var on /job:localhost/replica:0/task:0/device:CPU:0 644743192\n",
      "I0710 07:18:47.504964 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/proj/w/var:0 shape=(2048, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.507642 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/proj/b/var on /job:localhost/replica:0/task:0/device:CPU:0 644747288\n",
      "I0710 07:18:47.509193 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/proj/b/var:0 shape=(1024,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.525591 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_0/var on /job:localhost/replica:0/task:0/device:CPU:0 648843288\n",
      "I0710 07:18:47.527655 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_0/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.532961 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_1/var on /job:localhost/replica:0/task:0/device:CPU:0 652939288\n",
      "I0710 07:18:47.535000 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_1/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.540372 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_2/var on /job:localhost/replica:0/task:0/device:CPU:0 657035288\n",
      "I0710 07:18:47.542360 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_2/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.547861 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_3/var on /job:localhost/replica:0/task:0/device:CPU:0 661131288\n",
      "I0710 07:18:47.549867 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_3/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.555193 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_4/var on /job:localhost/replica:0/task:0/device:CPU:0 665227288\n",
      "I0710 07:18:47.557106 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_4/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.562474 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_5/var on /job:localhost/replica:0/task:0/device:CPU:0 669323288\n",
      "I0710 07:18:47.564504 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_5/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.570056 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_6/var on /job:localhost/replica:0/task:0/device:CPU:0 673419288\n",
      "I0710 07:18:47.572039 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_6/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.577545 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_7/var on /job:localhost/replica:0/task:0/device:CPU:0 677515288\n",
      "I0710 07:18:47.579570 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_7/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.584863 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_8/var on /job:localhost/replica:0/task:0/device:CPU:0 681611288\n",
      "I0710 07:18:47.586926 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_8/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.592183 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_9/var on /job:localhost/replica:0/task:0/device:CPU:0 685707288\n",
      "I0710 07:18:47.594035 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_9/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.599459 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_10/var on /job:localhost/replica:0/task:0/device:CPU:0 689803288\n",
      "I0710 07:18:47.601404 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_10/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.606621 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_11/var on /job:localhost/replica:0/task:0/device:CPU:0 693899288\n",
      "I0710 07:18:47.608775 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_11/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.614193 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_12/var on /job:localhost/replica:0/task:0/device:CPU:0 697995288\n",
      "I0710 07:18:47.616150 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_12/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.621774 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_13/var on /job:localhost/replica:0/task:0/device:CPU:0 702091288\n",
      "I0710 07:18:47.623812 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_13/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.629243 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_14/var on /job:localhost/replica:0/task:0/device:CPU:0 706187288\n",
      "I0710 07:18:47.631230 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_14/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.636716 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_15/var on /job:localhost/replica:0/task:0/device:CPU:0 710283288\n",
      "I0710 07:18:47.638811 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_15/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.648210 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten_rnn/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 760614936\n",
      "I0710 07:18:47.650268 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten_rnn/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.652934 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten_rnn/b/var on /job:localhost/replica:0/task:0/device:CPU:0 760631320\n",
      "I0710 07:18:47.654570 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten_rnn/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.662103 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten_rnn/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 760647704\n",
      "I0710 07:18:47.663685 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten_rnn/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.671601 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/source_proj/var on /job:localhost/replica:0/task:0/device:CPU:0 764842008\n",
      "I0710 07:18:47.674306 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten/source_proj/var:0 shape=(1024, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.677009 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/source_proj_b/var on /job:localhost/replica:0/task:0/device:CPU:0 764846104\n",
      "I0710 07:18:47.678752 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten/source_proj_b/var:0 shape=(1024,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.685046 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/query_proj/var on /job:localhost/replica:0/task:0/device:CPU:0 769040408\n",
      "I0710 07:18:47.687206 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten/query_proj/var:0 shape=(1024, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.690127 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/query_proj_b/var on /job:localhost/replica:0/task:0/device:CPU:0 769044504\n",
      "I0710 07:18:47.691852 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten/query_proj_b/var:0 shape=(1024,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.698089 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/inner_att/source_var/var on /job:localhost/replica:0/task:0/device:CPU:0 769306648\n",
      "I0710 07:18:47.700070 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten/inner_att/source_var/var:0 shape=(256, 256) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.705341 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/inner_att/query_var/var on /job:localhost/replica:0/task:0/device:CPU:0 769568792\n",
      "I0710 07:18:47.707361 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten/inner_att/query_var/var:0 shape=(256, 256) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.712363 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/inner_att/hidden_var/var on /job:localhost/replica:0/task:0/device:CPU:0 769569816\n",
      "I0710 07:18:47.714341 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten/inner_att/hidden_var/var:0 shape=(256,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.725471 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn1/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 819901464\n",
      "I0710 07:18:47.727550 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn1/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.730363 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn1/b/var on /job:localhost/replica:0/task:0/device:CPU:0 819917848\n",
      "I0710 07:18:47.731965 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn1/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.739743 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn1/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 819934232\n",
      "I0710 07:18:47.741486 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn1/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.749489 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn2/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 870265880\n",
      "I0710 07:18:47.751725 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn2/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.754662 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn2/b/var on /job:localhost/replica:0/task:0/device:CPU:0 870282264\n",
      "I0710 07:18:47.756443 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn2/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.764080 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn2/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 870298648\n",
      "I0710 07:18:47.765739 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn2/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.773886 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn3/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 920630296\n",
      "I0710 07:18:47.775975 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn3/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.778857 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn3/b/var on /job:localhost/replica:0/task:0/device:CPU:0 920646680\n",
      "I0710 07:18:47.780611 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn3/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.788272 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn3/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 920663064\n",
      "I0710 07:18:47.789964 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn3/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.798088 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn4/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 970994712\n",
      "I0710 07:18:47.799992 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn4/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.802691 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn4/b/var on /job:localhost/replica:0/task:0/device:CPU:0 971011096\n",
      "I0710 07:18:47.804261 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn4/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.811683 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn4/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 971027480\n",
      "I0710 07:18:47.813427 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn4/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.821492 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn5/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 1021359128\n",
      "I0710 07:18:47.823844 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn5/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.826722 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn5/b/var on /job:localhost/replica:0/task:0/device:CPU:0 1021375512\n",
      "I0710 07:18:47.828465 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn5/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.836052 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn5/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 1021391896\n",
      "I0710 07:18:47.837750 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn5/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.845463 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn6/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 1071723544\n",
      "I0710 07:18:47.847469 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn6/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.850204 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn6/b/var on /job:localhost/replica:0/task:0/device:CPU:0 1071739928\n",
      "I0710 07:18:47.851781 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn6/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.859313 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn6/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 1071756312\n",
      "I0710 07:18:47.860870 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn6/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.868822 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn7/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 1122087960\n",
      "I0710 07:18:47.870847 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn7/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.873571 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn7/b/var on /job:localhost/replica:0/task:0/device:CPU:0 1122104344\n",
      "I0710 07:18:47.875244 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn7/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.882925 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn7/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 1122120728\n",
      "I0710 07:18:47.884792 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn7/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.891487 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_0/var on /job:localhost/replica:0/task:0/device:CPU:0 1130312728\n",
      "I0710 07:18:47.893539 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_0/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.899248 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_1/var on /job:localhost/replica:0/task:0/device:CPU:0 1138504728\n",
      "I0710 07:18:47.901241 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_1/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.906632 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_2/var on /job:localhost/replica:0/task:0/device:CPU:0 1146696728\n",
      "I0710 07:18:47.908621 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_2/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.913997 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_3/var on /job:localhost/replica:0/task:0/device:CPU:0 1154888728\n",
      "I0710 07:18:47.916012 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_3/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.921547 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_4/var on /job:localhost/replica:0/task:0/device:CPU:0 1163080728\n",
      "I0710 07:18:47.923605 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_4/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.929238 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_5/var on /job:localhost/replica:0/task:0/device:CPU:0 1171272728\n",
      "I0710 07:18:47.931333 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_5/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.936862 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_6/var on /job:localhost/replica:0/task:0/device:CPU:0 1179464728\n",
      "I0710 07:18:47.938944 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_6/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.944454 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_7/var on /job:localhost/replica:0/task:0/device:CPU:0 1187656728\n",
      "I0710 07:18:47.946619 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_7/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.952087 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_8/var on /job:localhost/replica:0/task:0/device:CPU:0 1195848728\n",
      "I0710 07:18:47.954084 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_8/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.959554 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_9/var on /job:localhost/replica:0/task:0/device:CPU:0 1204040728\n",
      "I0710 07:18:47.961570 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_9/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.967075 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_10/var on /job:localhost/replica:0/task:0/device:CPU:0 1212232728\n",
      "I0710 07:18:47.969820 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_10/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.975624 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_11/var on /job:localhost/replica:0/task:0/device:CPU:0 1220424728\n",
      "I0710 07:18:47.977622 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_11/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.983065 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_12/var on /job:localhost/replica:0/task:0/device:CPU:0 1228616728\n",
      "I0710 07:18:47.985222 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_12/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.990968 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_13/var on /job:localhost/replica:0/task:0/device:CPU:0 1236808728\n",
      "I0710 07:18:47.993127 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_13/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:47.998615 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_14/var on /job:localhost/replica:0/task:0/device:CPU:0 1245000728\n",
      "I0710 07:18:48.000593 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_14/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.006129 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_15/var on /job:localhost/replica:0/task:0/device:CPU:0 1253192728\n",
      "I0710 07:18:48.008193 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_15/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.011049 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_0/var on /job:localhost/replica:0/task:0/device:CPU:0 1253196728\n",
      "I0710 07:18:48.012658 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_0/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.015340 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_1/var on /job:localhost/replica:0/task:0/device:CPU:0 1253200728\n",
      "I0710 07:18:48.017170 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_1/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.020015 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_2/var on /job:localhost/replica:0/task:0/device:CPU:0 1253204728\n",
      "I0710 07:18:48.021655 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_2/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.024531 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_3/var on /job:localhost/replica:0/task:0/device:CPU:0 1253208728\n",
      "I0710 07:18:48.026235 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_3/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.029085 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_4/var on /job:localhost/replica:0/task:0/device:CPU:0 1253212728\n",
      "I0710 07:18:48.030713 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_4/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.033609 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_5/var on /job:localhost/replica:0/task:0/device:CPU:0 1253216728\n",
      "I0710 07:18:48.035337 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_5/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.038153 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_6/var on /job:localhost/replica:0/task:0/device:CPU:0 1253220728\n",
      "I0710 07:18:48.039795 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_6/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.042645 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_7/var on /job:localhost/replica:0/task:0/device:CPU:0 1253224728\n",
      "I0710 07:18:48.044318 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_7/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.047139 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_8/var on /job:localhost/replica:0/task:0/device:CPU:0 1253228728\n",
      "I0710 07:18:48.048841 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_8/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.051672 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_9/var on /job:localhost/replica:0/task:0/device:CPU:0 1253232728\n",
      "I0710 07:18:48.053343 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_9/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.056076 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_10/var on /job:localhost/replica:0/task:0/device:CPU:0 1253236728\n",
      "I0710 07:18:48.057837 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_10/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.060600 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_11/var on /job:localhost/replica:0/task:0/device:CPU:0 1253240728\n",
      "I0710 07:18:48.062300 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_11/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.065032 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_12/var on /job:localhost/replica:0/task:0/device:CPU:0 1253244728\n",
      "I0710 07:18:48.066762 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_12/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.069698 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_13/var on /job:localhost/replica:0/task:0/device:CPU:0 1253248728\n",
      "I0710 07:18:48.071330 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_13/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.074017 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_14/var on /job:localhost/replica:0/task:0/device:CPU:0 1253252728\n",
      "I0710 07:18:48.075670 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_14/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.078507 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_15/var on /job:localhost/replica:0/task:0/device:CPU:0 1253256728\n",
      "I0710 07:18:48.080142 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_15/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.161945 140310643746176 py_utils.py:1783] === worker 0 ===\n",
      "I0710 07:18:48.164933 140310643746176 py_utils.py:1773] worker 0: dec.beam_search.global_step                                        /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165027 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[0]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165095 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[10]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165143 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[11]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165190 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[12]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165234 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[13]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165277 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[14]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165318 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[15]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165358 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[1]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165398 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[2]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165437 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[3]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165478 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[4]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165515 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[5]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165568 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[6]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165608 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[7]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165647 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[8]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165689 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[9]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165727 140310643746176 py_utils.py:1773] worker 0: dec.frnn[0].cell.b                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165766 140310643746176 py_utils.py:1773] worker 0: dec.frnn[0].cell.global_step                                       /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165806 140310643746176 py_utils.py:1773] worker 0: dec.frnn[0].cell.ln_scale                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165846 140310643746176 py_utils.py:1773] worker 0: dec.frnn[0].cell.wm                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165887 140310643746176 py_utils.py:1773] worker 0: dec.frnn[0].global_step                                            /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165944 140310643746176 py_utils.py:1773] worker 0: dec.frnn[1].cell.b                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.165982 140310643746176 py_utils.py:1773] worker 0: dec.frnn[1].cell.global_step                                       /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166022 140310643746176 py_utils.py:1773] worker 0: dec.frnn[1].cell.ln_scale                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166062 140310643746176 py_utils.py:1773] worker 0: dec.frnn[1].cell.wm                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166101 140310643746176 py_utils.py:1773] worker 0: dec.frnn[1].global_step                                            /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166139 140310643746176 py_utils.py:1773] worker 0: dec.frnn[2].cell.b                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166178 140310643746176 py_utils.py:1773] worker 0: dec.frnn[2].cell.global_step                                       /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166217 140310643746176 py_utils.py:1773] worker 0: dec.frnn[2].cell.ln_scale                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166257 140310643746176 py_utils.py:1773] worker 0: dec.frnn[2].cell.wm                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166296 140310643746176 py_utils.py:1773] worker 0: dec.frnn[2].global_step                                            /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166338 140310643746176 py_utils.py:1773] worker 0: dec.frnn[3].cell.b                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166419 140310643746176 py_utils.py:1773] worker 0: dec.frnn[3].cell.global_step                                       /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166460 140310643746176 py_utils.py:1773] worker 0: dec.frnn[3].cell.ln_scale                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166501 140310643746176 py_utils.py:1773] worker 0: dec.frnn[3].cell.wm                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166547 140310643746176 py_utils.py:1773] worker 0: dec.frnn[3].global_step                                            /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166588 140310643746176 py_utils.py:1773] worker 0: dec.frnn[4].cell.b                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166629 140310643746176 py_utils.py:1773] worker 0: dec.frnn[4].cell.global_step                                       /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166670 140310643746176 py_utils.py:1773] worker 0: dec.frnn[4].cell.ln_scale                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166710 140310643746176 py_utils.py:1773] worker 0: dec.frnn[4].cell.wm                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166751 140310643746176 py_utils.py:1773] worker 0: dec.frnn[4].global_step                                            /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166792 140310643746176 py_utils.py:1773] worker 0: dec.frnn[5].cell.b                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166832 140310643746176 py_utils.py:1773] worker 0: dec.frnn[5].cell.global_step                                       /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166873 140310643746176 py_utils.py:1773] worker 0: dec.frnn[5].cell.ln_scale                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166917 140310643746176 py_utils.py:1773] worker 0: dec.frnn[5].cell.wm                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.166964 140310643746176 py_utils.py:1773] worker 0: dec.frnn[5].global_step                                            /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167006 140310643746176 py_utils.py:1773] worker 0: dec.frnn[6].cell.b                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167045 140310643746176 py_utils.py:1773] worker 0: dec.frnn[6].cell.global_step                                       /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167089 140310643746176 py_utils.py:1773] worker 0: dec.frnn[6].cell.ln_scale                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167130 140310643746176 py_utils.py:1773] worker 0: dec.frnn[6].cell.wm                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167174 140310643746176 py_utils.py:1773] worker 0: dec.frnn[6].global_step                                            /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167212 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.atten.atten.global_step                        /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167255 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.atten.atten.hidden_var                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167293 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.atten.atten.query_var                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167336 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.atten.atten.source_var                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167377 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.atten.global_step                              /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167418 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.atten.query_proj                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167461 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.atten.query_proj_b                             /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167502 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.atten.source_proj                              /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167546 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.atten.source_proj_b                            /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167588 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.cell.b                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167629 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.cell.global_step                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167671 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.cell.ln_scale                                  /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167713 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.cell.wm                                        /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167755 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.global_step                                    /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167796 140310643746176 py_utils.py:1773] worker 0: dec.global_step                                                    /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167838 140310643746176 py_utils.py:1773] worker 0: dec.greedy_search.global_step                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167882 140310643746176 py_utils.py:1773] worker 0: dec.smoother.global_step                                           /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167923 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_0                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.167975 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_1                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168013 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_10                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168052 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_11                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168091 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_12                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168129 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_13                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168169 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_14                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168206 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_15                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168242 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_2                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168279 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_3                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168315 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_4                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168352 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_5                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168388 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_6                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168424 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_7                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168461 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_8                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168497 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_9                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168537 140310643746176 py_utils.py:1773] worker 0: dec.softmax.global_step                                            /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168575 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_0                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168615 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_1                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168653 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_10                                              /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168693 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_11                                              /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168731 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_12                                              /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168769 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_13                                              /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168806 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_14                                              /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168843 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_15                                              /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168879 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_2                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168916 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_3                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168953 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_4                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.168989 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_5                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169026 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_6                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169062 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_7                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169110 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_8                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169143 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_9                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169177 140310643746176 py_utils.py:1773] worker 0: dec.target_sequence_sampler.global_step                            /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169211 140310643746176 py_utils.py:1773] worker 0: enc.dropout.global_step                                            /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169248 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[0]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169286 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[10]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169320 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[11]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169358 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[12]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169392 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[13]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169427 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[14]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169466 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[15]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169502 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[1]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169544 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[2]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169580 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[3]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169615 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[4]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169652 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[5]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169687 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[6]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169744 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[7]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169782 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[8]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169821 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[9]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169860 140310643746176 py_utils.py:1773] worker 0: enc.final_proj.b                                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169911 140310643746176 py_utils.py:1773] worker 0: enc.final_proj.global_step                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.169983 140310643746176 py_utils.py:1773] worker 0: enc.final_proj.w                                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170021 140310643746176 py_utils.py:1773] worker 0: enc.global_step                                                    /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170060 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].bak_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170097 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].bak_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170135 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].bak_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170173 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].bak_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170212 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].bak_rnn.global_step                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170249 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].fwd_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170287 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].fwd_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170327 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].fwd_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170366 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].fwd_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170404 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].fwd_rnn.global_step                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170443 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].global_step                                             /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170481 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].bak_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170524 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].bak_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170562 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].bak_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170600 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].bak_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170639 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].bak_rnn.global_step                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170675 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].fwd_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170720 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].fwd_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170758 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].fwd_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170794 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].fwd_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170832 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].fwd_rnn.global_step                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170872 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].global_step                                             /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170909 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].bak_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170947 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].bak_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.170984 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].bak_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171023 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].bak_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171061 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].bak_rnn.global_step                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171100 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].fwd_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171138 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].fwd_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171178 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].fwd_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171217 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].fwd_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171253 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].fwd_rnn.global_step                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171290 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].global_step                                             /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171328 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].bak_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171366 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].bak_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171405 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].bak_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171443 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].bak_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171479 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].bak_rnn.global_step                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171522 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].fwd_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171561 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].fwd_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171601 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].fwd_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171640 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].fwd_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171680 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].fwd_rnn.global_step                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171718 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].global_step                                             /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171756 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].bak_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171796 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].bak_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171833 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].bak_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171871 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].bak_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171911 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].bak_rnn.global_step                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171949 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].fwd_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.171987 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].fwd_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172024 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].fwd_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172060 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].fwd_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172100 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].fwd_rnn.global_step                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172138 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].global_step                                             /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172177 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].bak_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172215 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].bak_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172254 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].bak_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172292 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].bak_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172330 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].bak_rnn.global_step                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172367 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].fwd_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172405 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].fwd_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172443 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].fwd_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172479 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].fwd_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172522 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].fwd_rnn.global_step                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172559 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].global_step                                             /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172596 140310643746176 py_utils.py:1773] worker 0: global_step                                                        /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172632 140310643746176 py_utils.py:1773] worker 0: input._tokenizer_default.global_step                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172672 140310643746176 py_utils.py:1773] worker 0: input.datasource.global_step                                       /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172710 140310643746176 py_utils.py:1773] worker 0: input.global_step                                                  /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172748 140310643746176 py_utils.py:1773] worker 0: learners[0].global_step                                            /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172786 140310643746176 py_utils.py:1773] worker 0: learners[0].grad_norm_tracker.global_step                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172825 140310643746176 py_utils.py:1773] worker 0: learners[0].lr_schedule.combine.global_step                        /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172863 140310643746176 py_utils.py:1773] worker 0: learners[0].lr_schedule.combine.schedules[0].global_step           /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172901 140310643746176 py_utils.py:1773] worker 0: learners[0].lr_schedule.combine.schedules[1].global_step           /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172939 140310643746176 py_utils.py:1773] worker 0: learners[0].lr_schedule.combine.schedules[2].global_step           /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.172977 140310643746176 py_utils.py:1773] worker 0: learners[0].lr_schedule.combine.schedules[2].linear.global_step    /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.173019 140310643746176 py_utils.py:1773] worker 0: learners[0].lr_schedule.combine.schedules[3].global_step           /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.173057 140310643746176 py_utils.py:1773] worker 0: learners[0].lr_schedule.global_step                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.173095 140310643746176 py_utils.py:1773] worker 0: learners[0].optimizer.global_step                                  /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:18:48.173137 140310643746176 py_utils.py:1789] ==========\n",
      "I0710 07:18:53.275621 140310643746176 base_input_generator.py:138] GlobalBatchSize Tensor(\"fprop/punctuator_rnmt/strided_slice:0\", shape=(), dtype=int32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
      "I0710 07:18:53.279203 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_0/var:0\n",
      "I0710 07:18:53.279330 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_1/var:0\n",
      "I0710 07:18:53.279393 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_2/var:0\n",
      "I0710 07:18:53.279454 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_3/var:0\n",
      "I0710 07:18:53.279521 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_4/var:0\n",
      "I0710 07:18:53.279559 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_5/var:0\n",
      "I0710 07:18:53.279594 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_6/var:0\n",
      "I0710 07:18:53.279630 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_7/var:0\n",
      "I0710 07:18:53.279665 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_8/var:0\n",
      "I0710 07:18:53.279700 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_9/var:0\n",
      "I0710 07:18:53.279735 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_10/var:0\n",
      "I0710 07:18:53.279770 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_11/var:0\n",
      "I0710 07:18:53.279805 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_12/var:0\n",
      "I0710 07:18:53.279839 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_13/var:0\n",
      "I0710 07:18:53.279874 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_14/var:0\n",
      "I0710 07:18:53.279910 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_15/var:0\n",
      "I0710 07:18:53.279944 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn1/b/var:0\n",
      "I0710 07:18:53.279979 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn1/ln_scale/var:0\n",
      "I0710 07:18:53.280021 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn1/wm/var:0\n",
      "I0710 07:18:53.280057 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn2/b/var:0\n",
      "I0710 07:18:53.280092 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn2/ln_scale/var:0\n",
      "I0710 07:18:53.280127 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn2/wm/var:0\n",
      "I0710 07:18:53.280162 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn3/b/var:0\n",
      "I0710 07:18:53.280197 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn3/ln_scale/var:0\n",
      "I0710 07:18:53.280231 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn3/wm/var:0\n",
      "I0710 07:18:53.280266 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn4/b/var:0\n",
      "I0710 07:18:53.280343 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn4/ln_scale/var:0\n",
      "I0710 07:18:53.280380 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn4/wm/var:0\n",
      "I0710 07:18:53.280417 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn5/b/var:0\n",
      "I0710 07:18:53.280454 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn5/ln_scale/var:0\n",
      "I0710 07:18:53.280491 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn5/wm/var:0\n",
      "I0710 07:18:53.280527 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn6/b/var:0\n",
      "I0710 07:18:53.280564 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn6/ln_scale/var:0\n",
      "I0710 07:18:53.280600 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn6/wm/var:0\n",
      "I0710 07:18:53.280637 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn7/b/var:0\n",
      "I0710 07:18:53.280693 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn7/ln_scale/var:0\n",
      "I0710 07:18:53.280732 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn7/wm/var:0\n",
      "I0710 07:18:53.280770 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten/inner_att/hidden_var/var:0\n",
      "I0710 07:18:53.280809 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten/inner_att/query_var/var:0\n",
      "I0710 07:18:53.280849 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten/inner_att/source_var/var:0\n",
      "I0710 07:18:53.280888 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten/query_proj/var:0\n",
      "I0710 07:18:53.280927 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten/query_proj_b/var:0\n",
      "I0710 07:18:53.280967 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten/source_proj/var:0\n",
      "I0710 07:18:53.281011 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten/source_proj_b/var:0\n",
      "I0710 07:18:53.281052 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten_rnn/b/var:0\n",
      "I0710 07:18:53.281090 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten_rnn/ln_scale/var:0\n",
      "I0710 07:18:53.281130 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten_rnn/wm/var:0\n",
      "I0710 07:18:53.281169 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_0/var:0\n",
      "I0710 07:18:53.281208 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_1/var:0\n",
      "I0710 07:18:53.281248 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_10/var:0\n",
      "I0710 07:18:53.281286 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_11/var:0\n",
      "I0710 07:18:53.281326 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_12/var:0\n",
      "I0710 07:18:53.281365 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_13/var:0\n",
      "I0710 07:18:53.281404 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_14/var:0\n",
      "I0710 07:18:53.281442 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_15/var:0\n",
      "I0710 07:18:53.281481 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_2/var:0\n",
      "I0710 07:18:53.281520 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_3/var:0\n",
      "I0710 07:18:53.281560 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_4/var:0\n",
      "I0710 07:18:53.281599 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_5/var:0\n",
      "I0710 07:18:53.281638 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_6/var:0\n",
      "I0710 07:18:53.281677 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_7/var:0\n",
      "I0710 07:18:53.281715 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_8/var:0\n",
      "I0710 07:18:53.281755 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_9/var:0\n",
      "I0710 07:18:53.281794 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_0/var:0\n",
      "I0710 07:18:53.281833 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_1/var:0\n",
      "I0710 07:18:53.281872 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_10/var:0\n",
      "I0710 07:18:53.281929 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_11/var:0\n",
      "I0710 07:18:53.281970 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_12/var:0\n",
      "I0710 07:18:53.282031 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_13/var:0\n",
      "I0710 07:18:53.282104 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_14/var:0\n",
      "I0710 07:18:53.282145 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_15/var:0\n",
      "I0710 07:18:53.282186 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_2/var:0\n",
      "I0710 07:18:53.282227 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_3/var:0\n",
      "I0710 07:18:53.282268 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_4/var:0\n",
      "I0710 07:18:53.282308 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_5/var:0\n",
      "I0710 07:18:53.282349 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_6/var:0\n",
      "I0710 07:18:53.282390 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_7/var:0\n",
      "I0710 07:18:53.282431 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_8/var:0\n",
      "I0710 07:18:53.282471 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_9/var:0\n",
      "I0710 07:18:53.282511 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_0/var:0\n",
      "I0710 07:18:53.282552 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_1/var:0\n",
      "I0710 07:18:53.282593 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_2/var:0\n",
      "I0710 07:18:53.282634 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_3/var:0\n",
      "I0710 07:18:53.282674 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_4/var:0\n",
      "I0710 07:18:53.282715 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_5/var:0\n",
      "I0710 07:18:53.282756 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_6/var:0\n",
      "I0710 07:18:53.282797 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_7/var:0\n",
      "I0710 07:18:53.282837 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_8/var:0\n",
      "I0710 07:18:53.282878 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_9/var:0\n",
      "I0710 07:18:53.282918 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_10/var:0\n",
      "I0710 07:18:53.282959 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_11/var:0\n",
      "I0710 07:18:53.283005 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_12/var:0\n",
      "I0710 07:18:53.283047 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_13/var:0\n",
      "I0710 07:18:53.283088 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_14/var:0\n",
      "I0710 07:18:53.283130 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_15/var:0\n",
      "I0710 07:18:53.283185 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/proj/b/var:0\n",
      "I0710 07:18:53.283224 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/proj/w/var:0\n",
      "I0710 07:18:53.283263 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L0_rnn_bak/b/var:0\n",
      "I0710 07:18:53.283303 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L0_rnn_bak/ln_scale/var:0\n",
      "I0710 07:18:53.283343 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L0_rnn_bak/wm/var:0\n",
      "I0710 07:18:53.283382 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L0_rnn_fwd/b/var:0\n",
      "I0710 07:18:53.283421 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L0_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:18:53.283461 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L0_rnn_fwd/wm/var:0\n",
      "I0710 07:18:53.283500 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L1_rnn_bak/b/var:0\n",
      "I0710 07:18:53.283539 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L1_rnn_bak/ln_scale/var:0\n",
      "I0710 07:18:53.283578 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L1_rnn_bak/wm/var:0\n",
      "I0710 07:18:53.283618 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L1_rnn_fwd/b/var:0\n",
      "I0710 07:18:53.283658 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L1_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:18:53.283696 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L1_rnn_fwd/wm/var:0\n",
      "I0710 07:18:53.283736 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L2_rnn_bak/b/var:0\n",
      "I0710 07:18:53.283775 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L2_rnn_bak/ln_scale/var:0\n",
      "I0710 07:18:53.283814 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L2_rnn_bak/wm/var:0\n",
      "I0710 07:18:53.283853 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L2_rnn_fwd/b/var:0\n",
      "I0710 07:18:53.283892 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L2_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:18:53.283931 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L2_rnn_fwd/wm/var:0\n",
      "I0710 07:18:53.283971 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L3_rnn_bak/b/var:0\n",
      "I0710 07:18:53.284034 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L3_rnn_bak/ln_scale/var:0\n",
      "I0710 07:18:53.284081 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L3_rnn_bak/wm/var:0\n",
      "I0710 07:18:53.284127 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L3_rnn_fwd/b/var:0\n",
      "I0710 07:18:53.284186 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L3_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:18:53.284227 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L3_rnn_fwd/wm/var:0\n",
      "I0710 07:18:53.284268 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L4_rnn_bak/b/var:0\n",
      "I0710 07:18:53.284308 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L4_rnn_bak/ln_scale/var:0\n",
      "I0710 07:18:53.284349 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L4_rnn_bak/wm/var:0\n",
      "I0710 07:18:53.284389 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L4_rnn_fwd/b/var:0\n",
      "I0710 07:18:53.284429 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L4_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:18:53.284470 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L4_rnn_fwd/wm/var:0\n",
      "I0710 07:18:53.284510 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L5_rnn_bak/b/var:0\n",
      "I0710 07:18:53.284550 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L5_rnn_bak/ln_scale/var:0\n",
      "I0710 07:18:53.284590 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L5_rnn_bak/wm/var:0\n",
      "I0710 07:18:53.284630 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L5_rnn_fwd/b/var:0\n",
      "I0710 07:18:53.284681 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L5_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:18:53.284739 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L5_rnn_fwd/wm/var:0\n",
      "I0710 07:19:01.349668 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity:0\n",
      "I0710 07:19:01.349851 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_1:0\n",
      "I0710 07:19:01.349931 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_2:0\n",
      "I0710 07:19:01.349988 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_3:0\n",
      "I0710 07:19:01.350039 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_4:0\n",
      "I0710 07:19:01.350094 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_5:0\n",
      "I0710 07:19:01.350164 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_6:0\n",
      "I0710 07:19:01.350212 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_7:0\n",
      "I0710 07:19:01.350258 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_8:0\n",
      "I0710 07:19:01.350303 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_9:0\n",
      "I0710 07:19:01.350348 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_10:0\n",
      "I0710 07:19:01.350401 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_11:0\n",
      "I0710 07:19:01.350447 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_12:0\n",
      "I0710 07:19:01.350492 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_13:0\n",
      "I0710 07:19:01.350538 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_14:0\n",
      "I0710 07:19:01.350583 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_15:0\n",
      "I0710 07:19:01.350632 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn1/b/var:0\n",
      "I0710 07:19:01.350673 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn1/ln_scale/var:0\n",
      "I0710 07:19:01.350714 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn1/wm/var:0\n",
      "I0710 07:19:01.350755 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn2/b/var:0\n",
      "I0710 07:19:01.350798 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn2/ln_scale/var:0\n",
      "I0710 07:19:01.350839 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn2/wm/var:0\n",
      "I0710 07:19:01.350879 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn3/b/var:0\n",
      "I0710 07:19:01.350920 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn3/ln_scale/var:0\n",
      "I0710 07:19:01.350963 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn3/wm/var:0\n",
      "I0710 07:19:01.351004 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn4/b/var:0\n",
      "I0710 07:19:01.351046 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn4/ln_scale/var:0\n",
      "I0710 07:19:01.351087 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn4/wm/var:0\n",
      "I0710 07:19:01.351133 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn5/b/var:0\n",
      "I0710 07:19:01.351174 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn5/ln_scale/var:0\n",
      "I0710 07:19:01.351215 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn5/wm/var:0\n",
      "I0710 07:19:01.351256 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn6/b/var:0\n",
      "I0710 07:19:01.351298 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn6/ln_scale/var:0\n",
      "I0710 07:19:01.351338 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn6/wm/var:0\n",
      "I0710 07:19:01.351379 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn7/b/var:0\n",
      "I0710 07:19:01.351420 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn7/ln_scale/var:0\n",
      "I0710 07:19:01.351460 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn7/wm/var:0\n",
      "I0710 07:19:01.351501 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten/inner_att/hidden_var/var:0\n",
      "I0710 07:19:01.351542 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten/inner_att/query_var/var:0\n",
      "I0710 07:19:01.351584 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten/inner_att/source_var/var:0\n",
      "I0710 07:19:01.351625 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten/query_proj/var:0\n",
      "I0710 07:19:01.351665 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten/query_proj_b/var:0\n",
      "I0710 07:19:01.351706 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten/source_proj/var:0\n",
      "I0710 07:19:01.351747 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten/source_proj_b/var:0\n",
      "I0710 07:19:01.351787 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten_rnn/b/var:0\n",
      "I0710 07:19:01.351827 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten_rnn/ln_scale/var:0\n",
      "I0710 07:19:01.351868 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten_rnn/wm/var:0\n",
      "I0710 07:19:01.351909 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_0/var:0\n",
      "I0710 07:19:01.351949 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_1/var:0\n",
      "I0710 07:19:01.351990 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_10/var:0\n",
      "I0710 07:19:01.352031 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_11/var:0\n",
      "I0710 07:19:01.352072 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_12/var:0\n",
      "I0710 07:19:01.352117 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_13/var:0\n",
      "I0710 07:19:01.352158 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_14/var:0\n",
      "I0710 07:19:01.352199 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_15/var:0\n",
      "I0710 07:19:01.352239 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_2/var:0\n",
      "I0710 07:19:01.352279 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_3/var:0\n",
      "I0710 07:19:01.352319 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_4/var:0\n",
      "I0710 07:19:01.352360 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_5/var:0\n",
      "I0710 07:19:01.352400 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_6/var:0\n",
      "I0710 07:19:01.352440 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_7/var:0\n",
      "I0710 07:19:01.352481 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_8/var:0\n",
      "I0710 07:19:01.352520 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_9/var:0\n",
      "I0710 07:19:01.352561 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_0/var:0\n",
      "I0710 07:19:01.352603 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_1/var:0\n",
      "I0710 07:19:01.352644 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_10/var:0\n",
      "I0710 07:19:01.352684 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_11/var:0\n",
      "I0710 07:19:01.352725 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_12/var:0\n",
      "I0710 07:19:01.352765 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_13/var:0\n",
      "I0710 07:19:01.352805 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_14/var:0\n",
      "I0710 07:19:01.352845 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_15/var:0\n",
      "I0710 07:19:01.352885 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_2/var:0\n",
      "I0710 07:19:01.352926 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_3/var:0\n",
      "I0710 07:19:01.352966 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_4/var:0\n",
      "I0710 07:19:01.353006 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_5/var:0\n",
      "I0710 07:19:01.353046 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_6/var:0\n",
      "I0710 07:19:01.353086 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_7/var:0\n",
      "I0710 07:19:01.353131 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_8/var:0\n",
      "I0710 07:19:01.353170 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_9/var:0\n",
      "I0710 07:19:01.353217 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_16:0\n",
      "I0710 07:19:01.353262 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_17:0\n",
      "I0710 07:19:01.353307 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_18:0\n",
      "I0710 07:19:01.353351 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_19:0\n",
      "I0710 07:19:01.353396 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_20:0\n",
      "I0710 07:19:01.353441 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_21:0\n",
      "I0710 07:19:01.353486 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_22:0\n",
      "I0710 07:19:01.353530 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_23:0\n",
      "I0710 07:19:01.353574 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_24:0\n",
      "I0710 07:19:01.353621 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_25:0\n",
      "I0710 07:19:01.353665 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_26:0\n",
      "I0710 07:19:01.353713 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_27:0\n",
      "I0710 07:19:01.353757 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_28:0\n",
      "I0710 07:19:01.353802 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_29:0\n",
      "I0710 07:19:01.353846 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_30:0\n",
      "I0710 07:19:01.353904 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_31:0\n",
      "I0710 07:19:01.353949 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/proj/b/var:0\n",
      "I0710 07:19:01.353990 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/proj/w/var:0\n",
      "I0710 07:19:01.354031 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L0_rnn_bak/b/var:0\n",
      "I0710 07:19:01.354073 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L0_rnn_bak/ln_scale/var:0\n",
      "I0710 07:19:01.354118 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L0_rnn_bak/wm/var:0\n",
      "I0710 07:19:01.354160 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L0_rnn_fwd/b/var:0\n",
      "I0710 07:19:01.354207 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L0_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:19:01.354247 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L0_rnn_fwd/wm/var:0\n",
      "I0710 07:19:01.354287 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L1_rnn_bak/b/var:0\n",
      "I0710 07:19:01.354328 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L1_rnn_bak/ln_scale/var:0\n",
      "I0710 07:19:01.354368 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L1_rnn_bak/wm/var:0\n",
      "I0710 07:19:01.354408 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L1_rnn_fwd/b/var:0\n",
      "I0710 07:19:01.354448 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L1_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:19:01.354489 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L1_rnn_fwd/wm/var:0\n",
      "I0710 07:19:01.354530 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L2_rnn_bak/b/var:0\n",
      "I0710 07:19:01.354570 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L2_rnn_bak/ln_scale/var:0\n",
      "I0710 07:19:01.354614 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L2_rnn_bak/wm/var:0\n",
      "I0710 07:19:01.354654 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L2_rnn_fwd/b/var:0\n",
      "I0710 07:19:01.354694 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L2_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:19:01.354735 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L2_rnn_fwd/wm/var:0\n",
      "I0710 07:19:01.354775 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L3_rnn_bak/b/var:0\n",
      "I0710 07:19:01.354815 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L3_rnn_bak/ln_scale/var:0\n",
      "I0710 07:19:01.354856 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L3_rnn_bak/wm/var:0\n",
      "I0710 07:19:01.354896 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L3_rnn_fwd/b/var:0\n",
      "I0710 07:19:01.354936 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L3_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:19:01.354977 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L3_rnn_fwd/wm/var:0\n",
      "I0710 07:19:01.355017 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L4_rnn_bak/b/var:0\n",
      "I0710 07:19:01.355057 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L4_rnn_bak/ln_scale/var:0\n",
      "I0710 07:19:01.355103 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L4_rnn_bak/wm/var:0\n",
      "I0710 07:19:01.355143 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L4_rnn_fwd/b/var:0\n",
      "I0710 07:19:01.355184 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L4_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:19:01.355223 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L4_rnn_fwd/wm/var:0\n",
      "I0710 07:19:01.355264 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L5_rnn_bak/b/var:0\n",
      "I0710 07:19:01.355304 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L5_rnn_bak/ln_scale/var:0\n",
      "I0710 07:19:01.355345 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L5_rnn_bak/wm/var:0\n",
      "I0710 07:19:01.355384 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L5_rnn_fwd/b/var:0\n",
      "I0710 07:19:01.355425 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L5_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:19:01.355465 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L5_rnn_fwd/wm/var:0\n",
      "I0710 07:19:03.785998 140310643746176 learner.py:314] gradient_adjuster=<bound method BaseTask.AdjustGradients of <lingvo.tasks.punctuator.model.RNMTModel object at 0x7f9c1c125fd0>>\n",
      "I0710 07:19:03.786310 140310643746176 base_model.py:534] BaseTask.AdjustGradients\n",
      "I0710 07:19:04.736996 140310643746176 cluster.py:525] Place variable beta1_power on /job:localhost/replica:0/task:0/device:CPU:0 1253256732\n",
      "I0710 07:19:04.740024 140310643746176 cluster.py:525] Place variable beta2_power on /job:localhost/replica:0/task:0/device:CPU:0 1253256736\n",
      "I0710 07:19:04.744574 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_0/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1257352736\n",
      "I0710 07:19:04.749192 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_0/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1261448736\n",
      "I0710 07:19:04.753783 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_1/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1265544736\n",
      "I0710 07:19:04.758403 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_1/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1269640736\n",
      "I0710 07:19:04.762990 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_2/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1273736736\n",
      "I0710 07:19:04.767374 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_2/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1277832736\n",
      "I0710 07:19:04.771914 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_3/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1281928736\n",
      "I0710 07:19:04.776360 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_3/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1286024736\n",
      "I0710 07:19:04.780885 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_4/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1290120736\n",
      "I0710 07:19:04.785500 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_4/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1294216736\n",
      "I0710 07:19:04.790165 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_5/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1298312736\n",
      "I0710 07:19:04.796115 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_5/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1302408736\n",
      "I0710 07:19:04.800838 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_6/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1306504736\n",
      "I0710 07:19:04.805335 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_6/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1310600736\n",
      "I0710 07:19:04.809890 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_7/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1314696736\n",
      "I0710 07:19:04.814485 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_7/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1318792736\n",
      "I0710 07:19:04.819116 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_8/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1322888736\n",
      "I0710 07:19:04.823723 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_8/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1326984736\n",
      "I0710 07:19:04.828388 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_9/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1331080736\n",
      "I0710 07:19:04.833095 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_9/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1335176736\n",
      "I0710 07:19:04.837465 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_10/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1339272736\n",
      "I0710 07:19:04.842113 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_10/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1343368736\n",
      "I0710 07:19:04.846801 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_11/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1347464736\n",
      "I0710 07:19:04.851473 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_11/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1351560736\n",
      "I0710 07:19:04.856059 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_12/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1355656736\n",
      "I0710 07:19:04.860688 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_12/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1359752736\n",
      "I0710 07:19:04.865266 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_13/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1363848736\n",
      "I0710 07:19:04.869839 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_13/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1367944736\n",
      "I0710 07:19:04.874546 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_14/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1372040736\n",
      "I0710 07:19:04.879112 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_14/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1376136736\n",
      "I0710 07:19:04.883753 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_15/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1380232736\n",
      "I0710 07:19:04.888234 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_15/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1384328736\n",
      "I0710 07:19:04.892960 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn1/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1384345120\n",
      "I0710 07:19:04.897507 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn1/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1384361504\n",
      "I0710 07:19:04.902172 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn1/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1384377888\n",
      "I0710 07:19:04.906656 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn1/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1384394272\n",
      "I0710 07:19:04.911282 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn1/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1434725920\n",
      "I0710 07:19:04.916092 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn1/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1485057568\n",
      "I0710 07:19:04.920845 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn2/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1485073952\n",
      "I0710 07:19:04.925605 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn2/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1485090336\n",
      "I0710 07:19:04.930263 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn2/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1485106720\n",
      "I0710 07:19:04.934930 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn2/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1485123104\n",
      "I0710 07:19:04.939581 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn2/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1535454752\n",
      "I0710 07:19:04.944253 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn2/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1585786400\n",
      "I0710 07:19:04.948828 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn3/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1585802784\n",
      "I0710 07:19:04.953428 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn3/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1585819168\n",
      "I0710 07:19:04.958194 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn3/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1585835552\n",
      "I0710 07:19:04.962934 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn3/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1585851936\n",
      "I0710 07:19:04.967501 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn3/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1636183584\n",
      "I0710 07:19:04.971995 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn3/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1686515232\n",
      "I0710 07:19:04.976923 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn4/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1686531616\n",
      "I0710 07:19:04.981476 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn4/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1686548000\n",
      "I0710 07:19:04.985924 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn4/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1686564384\n",
      "I0710 07:19:04.990689 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn4/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1686580768\n",
      "I0710 07:19:04.995220 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn4/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1736912416\n",
      "I0710 07:19:05.000018 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn4/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1787244064\n",
      "I0710 07:19:05.004852 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn5/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1787260448\n",
      "I0710 07:19:05.009705 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn5/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1787276832\n",
      "I0710 07:19:05.014198 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn5/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1787293216\n",
      "I0710 07:19:05.018750 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn5/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1787309600\n",
      "I0710 07:19:05.023470 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn5/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1837641248\n",
      "I0710 07:19:05.027991 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn5/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1887972896\n",
      "I0710 07:19:05.032860 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn6/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1887989280\n",
      "I0710 07:19:05.037310 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn6/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1888005664\n",
      "I0710 07:19:05.041941 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn6/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1888022048\n",
      "I0710 07:19:05.046469 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn6/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1888038432\n",
      "I0710 07:19:05.051198 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn6/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1938370080\n",
      "I0710 07:19:05.056135 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn6/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1988701728\n",
      "I0710 07:19:05.061032 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn7/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1988718112\n",
      "I0710 07:19:05.066077 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn7/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1988734496\n",
      "I0710 07:19:05.071061 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn7/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1988750880\n",
      "I0710 07:19:05.075673 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn7/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1988767264\n",
      "I0710 07:19:05.080264 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn7/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2039098912\n",
      "I0710 07:19:05.084658 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn7/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2089430560\n",
      "I0710 07:19:05.088350 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/inner_att/hidden_var/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2089431584\n",
      "I0710 07:19:05.091845 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/inner_att/hidden_var/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2089432608\n",
      "I0710 07:19:05.096501 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/inner_att/query_var/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2089694752\n",
      "I0710 07:19:05.101427 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/inner_att/query_var/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2089956896\n",
      "I0710 07:19:05.106242 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/inner_att/source_var/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2090219040\n",
      "I0710 07:19:05.110842 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/inner_att/source_var/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2090481184\n",
      "I0710 07:19:05.115386 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/query_proj/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2094675488\n",
      "I0710 07:19:05.119932 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/query_proj/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2098869792\n",
      "I0710 07:19:05.124566 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/query_proj_b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2098873888\n",
      "I0710 07:19:05.129441 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/query_proj_b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2098877984\n",
      "I0710 07:19:05.134157 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/source_proj/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2103072288\n",
      "I0710 07:19:05.138814 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/source_proj/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2107266592\n",
      "I0710 07:19:05.143533 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/source_proj_b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2107270688\n",
      "I0710 07:19:05.148157 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/source_proj_b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2107274784\n",
      "I0710 07:19:05.153079 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten_rnn/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2107291168\n",
      "I0710 07:19:05.158202 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten_rnn/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2107307552\n",
      "I0710 07:19:05.163163 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten_rnn/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2107323936\n",
      "I0710 07:19:05.167927 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten_rnn/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2107340320\n",
      "I0710 07:19:05.172627 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten_rnn/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2157671968\n",
      "I0710 07:19:05.177272 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten_rnn/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208003616\n",
      "I0710 07:19:05.181868 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_0/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208007616\n",
      "I0710 07:19:05.187841 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_0/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208011616\n",
      "I0710 07:19:05.192659 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_1/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208015616\n",
      "I0710 07:19:05.197465 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_1/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208019616\n",
      "I0710 07:19:05.202211 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_10/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208023616\n",
      "I0710 07:19:05.206844 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_10/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208027616\n",
      "I0710 07:19:05.211740 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_11/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208031616\n",
      "I0710 07:19:05.216463 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_11/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208035616\n",
      "I0710 07:19:05.221556 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_12/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208039616\n",
      "I0710 07:19:05.226590 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_12/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208043616\n",
      "I0710 07:19:05.231498 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_13/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208047616\n",
      "I0710 07:19:05.236123 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_13/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208051616\n",
      "I0710 07:19:05.241027 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_14/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208055616\n",
      "I0710 07:19:05.245592 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_14/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208059616\n",
      "I0710 07:19:05.250092 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_15/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208063616\n",
      "I0710 07:19:05.254955 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_15/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208067616\n",
      "I0710 07:19:05.259709 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_2/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208071616\n",
      "I0710 07:19:05.264384 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_2/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208075616\n",
      "I0710 07:19:05.268789 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_3/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208079616\n",
      "I0710 07:19:05.273551 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_3/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208083616\n",
      "I0710 07:19:05.278176 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_4/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208087616\n",
      "I0710 07:19:05.282826 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_4/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208091616\n",
      "I0710 07:19:05.288285 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_5/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208095616\n",
      "I0710 07:19:05.293456 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_5/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208099616\n",
      "I0710 07:19:05.298400 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_6/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208103616\n",
      "I0710 07:19:05.303237 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_6/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208107616\n",
      "I0710 07:19:05.308407 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_7/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208111616\n",
      "I0710 07:19:05.313191 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_7/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208115616\n",
      "I0710 07:19:05.318026 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_8/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208119616\n",
      "I0710 07:19:05.322986 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_8/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208123616\n",
      "I0710 07:19:05.327743 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_9/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208127616\n",
      "I0710 07:19:05.332785 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_9/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208131616\n",
      "I0710 07:19:05.337426 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_0/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2216323616\n",
      "I0710 07:19:05.342137 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_0/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2224515616\n",
      "I0710 07:19:05.346666 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_1/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2232707616\n",
      "I0710 07:19:05.351451 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_1/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2240899616\n",
      "I0710 07:19:05.356558 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_10/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2249091616\n",
      "I0710 07:19:05.361636 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_10/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2257283616\n",
      "I0710 07:19:05.366524 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_11/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2265475616\n",
      "I0710 07:19:05.371328 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_11/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2273667616\n",
      "I0710 07:19:05.376031 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_12/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2281859616\n",
      "I0710 07:19:05.380770 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_12/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2290051616\n",
      "I0710 07:19:05.385570 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_13/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2298243616\n",
      "I0710 07:19:05.390675 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_13/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2306435616\n",
      "I0710 07:19:05.395563 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_14/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2314627616\n",
      "I0710 07:19:05.400227 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_14/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2322819616\n",
      "I0710 07:19:05.404922 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_15/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2331011616\n",
      "I0710 07:19:05.409991 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_15/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2339203616\n",
      "I0710 07:19:05.414674 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_2/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2347395616\n",
      "I0710 07:19:05.419566 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_2/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2355587616\n",
      "I0710 07:19:05.424476 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_3/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2363779616\n",
      "I0710 07:19:05.429042 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_3/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2371971616\n",
      "I0710 07:19:05.433814 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_4/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2380163616\n",
      "I0710 07:19:05.438349 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_4/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2388355616\n",
      "I0710 07:19:05.442979 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_5/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2396547616\n",
      "I0710 07:19:05.447482 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_5/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2404739616\n",
      "I0710 07:19:05.452181 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_6/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2412931616\n",
      "I0710 07:19:05.456783 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_6/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2421123616\n",
      "I0710 07:19:05.461494 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_7/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2429315616\n",
      "I0710 07:19:05.466267 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_7/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2437507616\n",
      "I0710 07:19:05.471054 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_8/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2445699616\n",
      "I0710 07:19:05.475849 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_8/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2453891616\n",
      "I0710 07:19:05.480411 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_9/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2462083616\n",
      "I0710 07:19:05.484852 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_9/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2470275616\n",
      "I0710 07:19:05.489567 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_0/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2474371616\n",
      "I0710 07:19:05.494313 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_0/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2478467616\n",
      "I0710 07:19:05.498874 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_1/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2482563616\n",
      "I0710 07:19:05.503629 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_1/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2486659616\n",
      "I0710 07:19:05.508330 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_2/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2490755616\n",
      "I0710 07:19:05.512944 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_2/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2494851616\n",
      "I0710 07:19:05.517627 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_3/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2498947616\n",
      "I0710 07:19:05.522579 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_3/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2503043616\n",
      "I0710 07:19:05.527254 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_4/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2507139616\n",
      "I0710 07:19:05.531952 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_4/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2511235616\n",
      "I0710 07:19:05.536720 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_5/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2515331616\n",
      "I0710 07:19:05.541563 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_5/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2519427616\n",
      "I0710 07:19:05.546410 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_6/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2523523616\n",
      "I0710 07:19:05.551244 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_6/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2527619616\n",
      "I0710 07:19:05.555984 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_7/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2531715616\n",
      "I0710 07:19:05.560971 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_7/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2535811616\n",
      "I0710 07:19:05.565773 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_8/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2539907616\n",
      "I0710 07:19:05.570489 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_8/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2544003616\n",
      "I0710 07:19:05.575241 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_9/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2548099616\n",
      "I0710 07:19:05.579901 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_9/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2552195616\n",
      "I0710 07:19:05.585337 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_10/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2556291616\n",
      "I0710 07:19:05.590225 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_10/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2560387616\n",
      "I0710 07:19:05.594754 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_11/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2564483616\n",
      "I0710 07:19:05.599452 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_11/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2568579616\n",
      "I0710 07:19:05.604049 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_12/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2572675616\n",
      "I0710 07:19:05.609530 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_12/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2576771616\n",
      "I0710 07:19:05.614984 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_13/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2580867616\n",
      "I0710 07:19:05.620365 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_13/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2584963616\n",
      "I0710 07:19:05.625680 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_14/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2589059616\n",
      "I0710 07:19:05.630410 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_14/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2593155616\n",
      "I0710 07:19:05.635082 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_15/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2597251616\n",
      "I0710 07:19:05.639711 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_15/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2601347616\n",
      "I0710 07:19:05.644250 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/proj/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2601351712\n",
      "I0710 07:19:05.648708 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/proj/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2601355808\n",
      "I0710 07:19:05.653356 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/proj/w/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2609744416\n",
      "I0710 07:19:05.658130 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/proj/w/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2618133024\n",
      "I0710 07:19:05.662860 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_bak/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2618149408\n",
      "I0710 07:19:05.667520 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_bak/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2618165792\n",
      "I0710 07:19:05.672319 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_bak/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2618182176\n",
      "I0710 07:19:05.677743 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_bak/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2618198560\n",
      "I0710 07:19:05.682632 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_bak/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2651752992\n",
      "I0710 07:19:05.687367 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_bak/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2685307424\n",
      "I0710 07:19:05.692191 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_fwd/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2685323808\n",
      "I0710 07:19:05.696989 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_fwd/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2685340192\n",
      "I0710 07:19:05.701808 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_fwd/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2685356576\n",
      "I0710 07:19:05.706583 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_fwd/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2685372960\n",
      "I0710 07:19:05.711227 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_fwd/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2718927392\n",
      "I0710 07:19:05.715824 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_fwd/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2752481824\n",
      "I0710 07:19:05.720627 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_bak/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2752498208\n",
      "I0710 07:19:05.725444 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_bak/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2752514592\n",
      "I0710 07:19:05.730313 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_bak/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2752530976\n",
      "I0710 07:19:05.735010 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_bak/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2752547360\n",
      "I0710 07:19:05.739606 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_bak/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2802879008\n",
      "I0710 07:19:05.744353 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_bak/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2853210656\n",
      "I0710 07:19:05.749099 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_fwd/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2853227040\n",
      "I0710 07:19:05.754123 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_fwd/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2853243424\n",
      "I0710 07:19:05.759572 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_fwd/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2853259808\n",
      "I0710 07:19:05.764931 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_fwd/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2853276192\n",
      "I0710 07:19:05.769521 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_fwd/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2903607840\n",
      "I0710 07:19:05.774004 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_fwd/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2953939488\n",
      "I0710 07:19:05.778587 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_bak/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2953955872\n",
      "I0710 07:19:05.783245 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_bak/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2953972256\n",
      "I0710 07:19:05.787776 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_bak/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2953988640\n",
      "I0710 07:19:05.792648 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_bak/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2954005024\n",
      "I0710 07:19:05.797332 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_bak/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3004336672\n",
      "I0710 07:19:05.802139 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_bak/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3054668320\n",
      "I0710 07:19:05.807043 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_fwd/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3054684704\n",
      "I0710 07:19:05.811845 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_fwd/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3054701088\n",
      "I0710 07:19:05.816466 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_fwd/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3054717472\n",
      "I0710 07:19:05.821381 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_fwd/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3054733856\n",
      "I0710 07:19:05.826118 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_fwd/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3105065504\n",
      "I0710 07:19:05.830929 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_fwd/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3155397152\n",
      "I0710 07:19:05.835645 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_bak/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3155413536\n",
      "I0710 07:19:05.840454 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_bak/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3155429920\n",
      "I0710 07:19:05.845289 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_bak/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3155446304\n",
      "I0710 07:19:05.849947 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_bak/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3155462688\n",
      "I0710 07:19:05.854702 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_bak/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3205794336\n",
      "I0710 07:19:05.859422 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_bak/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3256125984\n",
      "I0710 07:19:05.864126 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_fwd/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3256142368\n",
      "I0710 07:19:05.868910 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_fwd/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3256158752\n",
      "I0710 07:19:05.873669 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_fwd/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3256175136\n",
      "I0710 07:19:05.878645 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_fwd/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3256191520\n",
      "I0710 07:19:05.883361 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_fwd/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3306523168\n",
      "I0710 07:19:05.888195 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_fwd/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3356854816\n",
      "I0710 07:19:05.893054 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_bak/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3356871200\n",
      "I0710 07:19:05.897690 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_bak/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3356887584\n",
      "I0710 07:19:05.902480 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_bak/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3356903968\n",
      "I0710 07:19:05.907136 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_bak/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3356920352\n",
      "I0710 07:19:05.912015 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_bak/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3407252000\n",
      "I0710 07:19:05.916887 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_bak/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3457583648\n",
      "I0710 07:19:05.922046 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_fwd/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3457600032\n",
      "I0710 07:19:05.926822 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_fwd/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3457616416\n",
      "I0710 07:19:05.931673 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_fwd/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3457632800\n",
      "I0710 07:19:05.936220 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_fwd/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3457649184\n",
      "I0710 07:19:05.940614 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_fwd/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3507980832\n",
      "I0710 07:19:05.945235 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_fwd/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3558312480\n",
      "I0710 07:19:05.950100 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_bak/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3558328864\n",
      "I0710 07:19:05.954888 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_bak/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3558345248\n",
      "I0710 07:19:05.959816 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_bak/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3558361632\n",
      "I0710 07:19:05.964539 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_bak/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3558378016\n",
      "I0710 07:19:05.969372 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_bak/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3608709664\n",
      "I0710 07:19:05.974027 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_bak/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3659041312\n",
      "I0710 07:19:05.978713 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_fwd/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3659057696\n",
      "I0710 07:19:05.983405 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_fwd/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3659074080\n",
      "I0710 07:19:05.988895 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_fwd/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3659090464\n",
      "I0710 07:19:05.993513 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_fwd/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3659106848\n",
      "I0710 07:19:05.998028 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_fwd/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3709438496\n",
      "I0710 07:19:06.003279 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_fwd/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3759770144\n",
      "I0710 07:19:07.297407 140310643746176 py_utils.py:1773] MODEL ANALYSIS: \n",
      "I0710 07:19:07.297569 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.emb.wm[0]                                 (1000, 1024)            1024000 punctuator_rnmt/dec/emb/var_0/var\n",
      "I0710 07:19:07.297630 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.emb.wm[10]                                (1000, 1024)            1024000 punctuator_rnmt/dec/emb/var_10/var\n",
      "I0710 07:19:07.297677 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.emb.wm[11]                                (1000, 1024)            1024000 punctuator_rnmt/dec/emb/var_11/var\n",
      "I0710 07:19:07.297745 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.emb.wm[12]                                (1000, 1024)            1024000 punctuator_rnmt/dec/emb/var_12/var\n",
      "I0710 07:19:07.297785 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.emb.wm[13]                                (1000, 1024)            1024000 punctuator_rnmt/dec/emb/var_13/var\n",
      "I0710 07:19:07.297824 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.emb.wm[14]                                (1000, 1024)            1024000 punctuator_rnmt/dec/emb/var_14/var\n",
      "I0710 07:19:07.297862 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.emb.wm[15]                                (1000, 1024)            1024000 punctuator_rnmt/dec/emb/var_15/var\n",
      "I0710 07:19:07.297916 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.emb.wm[1]                                 (1000, 1024)            1024000 punctuator_rnmt/dec/emb/var_1/var\n",
      "I0710 07:19:07.297953 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.emb.wm[2]                                 (1000, 1024)            1024000 punctuator_rnmt/dec/emb/var_2/var\n",
      "I0710 07:19:07.297996 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.emb.wm[3]                                 (1000, 1024)            1024000 punctuator_rnmt/dec/emb/var_3/var\n",
      "I0710 07:19:07.298041 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.emb.wm[4]                                 (1000, 1024)            1024000 punctuator_rnmt/dec/emb/var_4/var\n",
      "I0710 07:19:07.298092 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.emb.wm[5]                                 (1000, 1024)            1024000 punctuator_rnmt/dec/emb/var_5/var\n",
      "I0710 07:19:07.298129 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.emb.wm[6]                                 (1000, 1024)            1024000 punctuator_rnmt/dec/emb/var_6/var\n",
      "I0710 07:19:07.298166 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.emb.wm[7]                                 (1000, 1024)            1024000 punctuator_rnmt/dec/emb/var_7/var\n",
      "I0710 07:19:07.298202 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.emb.wm[8]                                 (1000, 1024)            1024000 punctuator_rnmt/dec/emb/var_8/var\n",
      "I0710 07:19:07.298238 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.emb.wm[9]                                 (1000, 1024)            1024000 punctuator_rnmt/dec/emb/var_9/var\n",
      "I0710 07:19:07.298348 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[0].cell.b                            (4096,)                    4096 punctuator_rnmt/dec/rnn1/b/var\n",
      "I0710 07:19:07.298386 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[0].cell.ln_scale                     (4096,)                    4096 punctuator_rnmt/dec/rnn1/ln_scale/var\n",
      "I0710 07:19:07.298425 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[0].cell.wm                           (3072, 4096)           12582912 punctuator_rnmt/dec/rnn1/wm/var\n",
      "I0710 07:19:07.298465 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[1].cell.b                            (4096,)                    4096 punctuator_rnmt/dec/rnn2/b/var\n",
      "I0710 07:19:07.298506 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[1].cell.ln_scale                     (4096,)                    4096 punctuator_rnmt/dec/rnn2/ln_scale/var\n",
      "I0710 07:19:07.298546 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[1].cell.wm                           (3072, 4096)           12582912 punctuator_rnmt/dec/rnn2/wm/var\n",
      "I0710 07:19:07.298586 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[2].cell.b                            (4096,)                    4096 punctuator_rnmt/dec/rnn3/b/var\n",
      "I0710 07:19:07.298627 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[2].cell.ln_scale                     (4096,)                    4096 punctuator_rnmt/dec/rnn3/ln_scale/var\n",
      "I0710 07:19:07.298666 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[2].cell.wm                           (3072, 4096)           12582912 punctuator_rnmt/dec/rnn3/wm/var\n",
      "I0710 07:19:07.298703 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[3].cell.b                            (4096,)                    4096 punctuator_rnmt/dec/rnn4/b/var\n",
      "I0710 07:19:07.298742 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[3].cell.ln_scale                     (4096,)                    4096 punctuator_rnmt/dec/rnn4/ln_scale/var\n",
      "I0710 07:19:07.298780 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[3].cell.wm                           (3072, 4096)           12582912 punctuator_rnmt/dec/rnn4/wm/var\n",
      "I0710 07:19:07.298818 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[4].cell.b                            (4096,)                    4096 punctuator_rnmt/dec/rnn5/b/var\n",
      "I0710 07:19:07.298856 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[4].cell.ln_scale                     (4096,)                    4096 punctuator_rnmt/dec/rnn5/ln_scale/var\n",
      "I0710 07:19:07.298895 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[4].cell.wm                           (3072, 4096)           12582912 punctuator_rnmt/dec/rnn5/wm/var\n",
      "I0710 07:19:07.298933 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[5].cell.b                            (4096,)                    4096 punctuator_rnmt/dec/rnn6/b/var\n",
      "I0710 07:19:07.298971 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[5].cell.ln_scale                     (4096,)                    4096 punctuator_rnmt/dec/rnn6/ln_scale/var\n",
      "I0710 07:19:07.299009 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[5].cell.wm                           (3072, 4096)           12582912 punctuator_rnmt/dec/rnn6/wm/var\n",
      "I0710 07:19:07.299052 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[6].cell.b                            (4096,)                    4096 punctuator_rnmt/dec/rnn7/b/var\n",
      "I0710 07:19:07.299090 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[6].cell.ln_scale                     (4096,)                    4096 punctuator_rnmt/dec/rnn7/ln_scale/var\n",
      "I0710 07:19:07.299128 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn[6].cell.wm                           (3072, 4096)           12582912 punctuator_rnmt/dec/rnn7/wm/var\n",
      "I0710 07:19:07.299165 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn_with_atten.atten.atten.hidden_var    (256,)                      256 punctuator_rnmt/dec/atten/inner_att/hidden_var/var\n",
      "I0710 07:19:07.299203 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn_with_atten.atten.atten.query_var     (256, 256)                65536 punctuator_rnmt/dec/atten/inner_att/query_var/var\n",
      "I0710 07:19:07.299241 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn_with_atten.atten.atten.source_var    (256, 256)                65536 punctuator_rnmt/dec/atten/inner_att/source_var/var\n",
      "I0710 07:19:07.299279 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn_with_atten.atten.query_proj          (1024, 1024)            1048576 punctuator_rnmt/dec/atten/query_proj/var\n",
      "I0710 07:19:07.299317 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn_with_atten.atten.query_proj_b        (1024,)                    1024 punctuator_rnmt/dec/atten/query_proj_b/var\n",
      "I0710 07:19:07.299350 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn_with_atten.atten.source_proj         (1024, 1024)            1048576 punctuator_rnmt/dec/atten/source_proj/var\n",
      "I0710 07:19:07.299389 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn_with_atten.atten.source_proj_b       (1024,)                    1024 punctuator_rnmt/dec/atten/source_proj_b/var\n",
      "I0710 07:19:07.299439 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn_with_atten.cell.b                    (4096,)                    4096 punctuator_rnmt/dec/atten_rnn/b/var\n",
      "I0710 07:19:07.299482 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn_with_atten.cell.ln_scale             (4096,)                    4096 punctuator_rnmt/dec/atten_rnn/ln_scale/var\n",
      "I0710 07:19:07.299519 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.frnn_with_atten.cell.wm                   (3072, 4096)           12582912 punctuator_rnmt/dec/atten_rnn/wm/var\n",
      "I0710 07:19:07.299562 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.bias_0                            (1000,)                    1000 punctuator_rnmt/dec/softmax/bias_0/var\n",
      "I0710 07:19:07.299600 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.bias_1                            (1000,)                    1000 punctuator_rnmt/dec/softmax/bias_1/var\n",
      "I0710 07:19:07.299638 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.bias_10                           (1000,)                    1000 punctuator_rnmt/dec/softmax/bias_10/var\n",
      "I0710 07:19:07.299675 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.bias_11                           (1000,)                    1000 punctuator_rnmt/dec/softmax/bias_11/var\n",
      "I0710 07:19:07.299713 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.bias_12                           (1000,)                    1000 punctuator_rnmt/dec/softmax/bias_12/var\n",
      "I0710 07:19:07.299750 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.bias_13                           (1000,)                    1000 punctuator_rnmt/dec/softmax/bias_13/var\n",
      "I0710 07:19:07.299788 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.bias_14                           (1000,)                    1000 punctuator_rnmt/dec/softmax/bias_14/var\n",
      "I0710 07:19:07.299826 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.bias_15                           (1000,)                    1000 punctuator_rnmt/dec/softmax/bias_15/var\n",
      "I0710 07:19:07.299864 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.bias_2                            (1000,)                    1000 punctuator_rnmt/dec/softmax/bias_2/var\n",
      "I0710 07:19:07.299901 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.bias_3                            (1000,)                    1000 punctuator_rnmt/dec/softmax/bias_3/var\n",
      "I0710 07:19:07.299939 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.bias_4                            (1000,)                    1000 punctuator_rnmt/dec/softmax/bias_4/var\n",
      "I0710 07:19:07.299977 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.bias_5                            (1000,)                    1000 punctuator_rnmt/dec/softmax/bias_5/var\n",
      "I0710 07:19:07.300014 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.bias_6                            (1000,)                    1000 punctuator_rnmt/dec/softmax/bias_6/var\n",
      "I0710 07:19:07.300057 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.bias_7                            (1000,)                    1000 punctuator_rnmt/dec/softmax/bias_7/var\n",
      "I0710 07:19:07.300095 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.bias_8                            (1000,)                    1000 punctuator_rnmt/dec/softmax/bias_8/var\n",
      "I0710 07:19:07.300132 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.bias_9                            (1000,)                    1000 punctuator_rnmt/dec/softmax/bias_9/var\n",
      "I0710 07:19:07.300170 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.weight_0                          (2048, 1000)            2048000 punctuator_rnmt/dec/softmax/weight_0/var\n",
      "I0710 07:19:07.300208 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.weight_1                          (2048, 1000)            2048000 punctuator_rnmt/dec/softmax/weight_1/var\n",
      "I0710 07:19:07.300252 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.weight_10                         (2048, 1000)            2048000 punctuator_rnmt/dec/softmax/weight_10/var\n",
      "I0710 07:19:07.300290 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.weight_11                         (2048, 1000)            2048000 punctuator_rnmt/dec/softmax/weight_11/var\n",
      "I0710 07:19:07.300328 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.weight_12                         (2048, 1000)            2048000 punctuator_rnmt/dec/softmax/weight_12/var\n",
      "I0710 07:19:07.300368 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.weight_13                         (2048, 1000)            2048000 punctuator_rnmt/dec/softmax/weight_13/var\n",
      "I0710 07:19:07.300405 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.weight_14                         (2048, 1000)            2048000 punctuator_rnmt/dec/softmax/weight_14/var\n",
      "I0710 07:19:07.300462 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.weight_15                         (2048, 1000)            2048000 punctuator_rnmt/dec/softmax/weight_15/var\n",
      "I0710 07:19:07.300500 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.weight_2                          (2048, 1000)            2048000 punctuator_rnmt/dec/softmax/weight_2/var\n",
      "I0710 07:19:07.300537 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.weight_3                          (2048, 1000)            2048000 punctuator_rnmt/dec/softmax/weight_3/var\n",
      "I0710 07:19:07.300575 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.weight_4                          (2048, 1000)            2048000 punctuator_rnmt/dec/softmax/weight_4/var\n",
      "I0710 07:19:07.300612 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.weight_5                          (2048, 1000)            2048000 punctuator_rnmt/dec/softmax/weight_5/var\n",
      "I0710 07:19:07.300649 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.weight_6                          (2048, 1000)            2048000 punctuator_rnmt/dec/softmax/weight_6/var\n",
      "I0710 07:19:07.300687 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.weight_7                          (2048, 1000)            2048000 punctuator_rnmt/dec/softmax/weight_7/var\n",
      "I0710 07:19:07.300724 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.weight_8                          (2048, 1000)            2048000 punctuator_rnmt/dec/softmax/weight_8/var\n",
      "I0710 07:19:07.300761 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.dec.softmax.weight_9                          (2048, 1000)            2048000 punctuator_rnmt/dec/softmax/weight_9/var\n",
      "I0710 07:19:07.300799 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.emb.wm[0]                                 (1000, 1024)            1024000 punctuator_rnmt/enc/emb/var_0/var\n",
      "I0710 07:19:07.300836 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.emb.wm[10]                                (1000, 1024)            1024000 punctuator_rnmt/enc/emb/var_10/var\n",
      "I0710 07:19:07.300874 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.emb.wm[11]                                (1000, 1024)            1024000 punctuator_rnmt/enc/emb/var_11/var\n",
      "I0710 07:19:07.300911 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.emb.wm[12]                                (1000, 1024)            1024000 punctuator_rnmt/enc/emb/var_12/var\n",
      "I0710 07:19:07.300949 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.emb.wm[13]                                (1000, 1024)            1024000 punctuator_rnmt/enc/emb/var_13/var\n",
      "I0710 07:19:07.300986 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.emb.wm[14]                                (1000, 1024)            1024000 punctuator_rnmt/enc/emb/var_14/var\n",
      "I0710 07:19:07.301033 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.emb.wm[15]                                (1000, 1024)            1024000 punctuator_rnmt/enc/emb/var_15/var\n",
      "I0710 07:19:07.301072 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.emb.wm[1]                                 (1000, 1024)            1024000 punctuator_rnmt/enc/emb/var_1/var\n",
      "I0710 07:19:07.301110 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.emb.wm[2]                                 (1000, 1024)            1024000 punctuator_rnmt/enc/emb/var_2/var\n",
      "I0710 07:19:07.301147 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.emb.wm[3]                                 (1000, 1024)            1024000 punctuator_rnmt/enc/emb/var_3/var\n",
      "I0710 07:19:07.301184 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.emb.wm[4]                                 (1000, 1024)            1024000 punctuator_rnmt/enc/emb/var_4/var\n",
      "I0710 07:19:07.301222 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.emb.wm[5]                                 (1000, 1024)            1024000 punctuator_rnmt/enc/emb/var_5/var\n",
      "I0710 07:19:07.301259 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.emb.wm[6]                                 (1000, 1024)            1024000 punctuator_rnmt/enc/emb/var_6/var\n",
      "I0710 07:19:07.301296 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.emb.wm[7]                                 (1000, 1024)            1024000 punctuator_rnmt/enc/emb/var_7/var\n",
      "I0710 07:19:07.301333 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.emb.wm[8]                                 (1000, 1024)            1024000 punctuator_rnmt/enc/emb/var_8/var\n",
      "I0710 07:19:07.301371 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.emb.wm[9]                                 (1000, 1024)            1024000 punctuator_rnmt/enc/emb/var_9/var\n",
      "I0710 07:19:07.301408 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.final_proj.b                              (1024,)                    1024 punctuator_rnmt/enc/proj/b/var\n",
      "I0710 07:19:07.301446 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.final_proj.w                              (2048, 1024)            2097152 punctuator_rnmt/enc/proj/w/var\n",
      "I0710 07:19:07.301483 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[0].bak_rnn.cell.b                     (4096,)                    4096 punctuator_rnmt/enc/L0_rnn_bak/b/var\n",
      "I0710 07:19:07.301521 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[0].bak_rnn.cell.ln_scale              (4096,)                    4096 punctuator_rnmt/enc/L0_rnn_bak/ln_scale/var\n",
      "I0710 07:19:07.301559 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[0].bak_rnn.cell.wm                    (2048, 4096)            8388608 punctuator_rnmt/enc/L0_rnn_bak/wm/var\n",
      "I0710 07:19:07.301596 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[0].fwd_rnn.cell.b                     (4096,)                    4096 punctuator_rnmt/enc/L0_rnn_fwd/b/var\n",
      "I0710 07:19:07.301634 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[0].fwd_rnn.cell.ln_scale              (4096,)                    4096 punctuator_rnmt/enc/L0_rnn_fwd/ln_scale/var\n",
      "I0710 07:19:07.301671 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[0].fwd_rnn.cell.wm                    (2048, 4096)            8388608 punctuator_rnmt/enc/L0_rnn_fwd/wm/var\n",
      "I0710 07:19:07.301709 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[1].bak_rnn.cell.b                     (4096,)                    4096 punctuator_rnmt/enc/L1_rnn_bak/b/var\n",
      "I0710 07:19:07.301746 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[1].bak_rnn.cell.ln_scale              (4096,)                    4096 punctuator_rnmt/enc/L1_rnn_bak/ln_scale/var\n",
      "I0710 07:19:07.301784 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[1].bak_rnn.cell.wm                    (3072, 4096)           12582912 punctuator_rnmt/enc/L1_rnn_bak/wm/var\n",
      "I0710 07:19:07.301822 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[1].fwd_rnn.cell.b                     (4096,)                    4096 punctuator_rnmt/enc/L1_rnn_fwd/b/var\n",
      "I0710 07:19:07.301860 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[1].fwd_rnn.cell.ln_scale              (4096,)                    4096 punctuator_rnmt/enc/L1_rnn_fwd/ln_scale/var\n",
      "I0710 07:19:07.301911 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[1].fwd_rnn.cell.wm                    (3072, 4096)           12582912 punctuator_rnmt/enc/L1_rnn_fwd/wm/var\n",
      "I0710 07:19:07.301953 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[2].bak_rnn.cell.b                     (4096,)                    4096 punctuator_rnmt/enc/L2_rnn_bak/b/var\n",
      "I0710 07:19:07.302032 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[2].bak_rnn.cell.ln_scale              (4096,)                    4096 punctuator_rnmt/enc/L2_rnn_bak/ln_scale/var\n",
      "I0710 07:19:07.302081 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[2].bak_rnn.cell.wm                    (3072, 4096)           12582912 punctuator_rnmt/enc/L2_rnn_bak/wm/var\n",
      "I0710 07:19:07.302123 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[2].fwd_rnn.cell.b                     (4096,)                    4096 punctuator_rnmt/enc/L2_rnn_fwd/b/var\n",
      "I0710 07:19:07.302161 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[2].fwd_rnn.cell.ln_scale              (4096,)                    4096 punctuator_rnmt/enc/L2_rnn_fwd/ln_scale/var\n",
      "I0710 07:19:07.302199 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[2].fwd_rnn.cell.wm                    (3072, 4096)           12582912 punctuator_rnmt/enc/L2_rnn_fwd/wm/var\n",
      "I0710 07:19:07.302238 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[3].bak_rnn.cell.b                     (4096,)                    4096 punctuator_rnmt/enc/L3_rnn_bak/b/var\n",
      "I0710 07:19:07.302276 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[3].bak_rnn.cell.ln_scale              (4096,)                    4096 punctuator_rnmt/enc/L3_rnn_bak/ln_scale/var\n",
      "I0710 07:19:07.302314 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[3].bak_rnn.cell.wm                    (3072, 4096)           12582912 punctuator_rnmt/enc/L3_rnn_bak/wm/var\n",
      "I0710 07:19:07.302352 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[3].fwd_rnn.cell.b                     (4096,)                    4096 punctuator_rnmt/enc/L3_rnn_fwd/b/var\n",
      "I0710 07:19:07.302389 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[3].fwd_rnn.cell.ln_scale              (4096,)                    4096 punctuator_rnmt/enc/L3_rnn_fwd/ln_scale/var\n",
      "I0710 07:19:07.302427 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[3].fwd_rnn.cell.wm                    (3072, 4096)           12582912 punctuator_rnmt/enc/L3_rnn_fwd/wm/var\n",
      "I0710 07:19:07.302465 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[4].bak_rnn.cell.b                     (4096,)                    4096 punctuator_rnmt/enc/L4_rnn_bak/b/var\n",
      "I0710 07:19:07.302503 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[4].bak_rnn.cell.ln_scale              (4096,)                    4096 punctuator_rnmt/enc/L4_rnn_bak/ln_scale/var\n",
      "I0710 07:19:07.302541 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[4].bak_rnn.cell.wm                    (3072, 4096)           12582912 punctuator_rnmt/enc/L4_rnn_bak/wm/var\n",
      "I0710 07:19:07.302578 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[4].fwd_rnn.cell.b                     (4096,)                    4096 punctuator_rnmt/enc/L4_rnn_fwd/b/var\n",
      "I0710 07:19:07.302617 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[4].fwd_rnn.cell.ln_scale              (4096,)                    4096 punctuator_rnmt/enc/L4_rnn_fwd/ln_scale/var\n",
      "I0710 07:19:07.302655 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[4].fwd_rnn.cell.wm                    (3072, 4096)           12582912 punctuator_rnmt/enc/L4_rnn_fwd/wm/var\n",
      "I0710 07:19:07.302693 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[5].bak_rnn.cell.b                     (4096,)                    4096 punctuator_rnmt/enc/L5_rnn_bak/b/var\n",
      "I0710 07:19:07.302731 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[5].bak_rnn.cell.ln_scale              (4096,)                    4096 punctuator_rnmt/enc/L5_rnn_bak/ln_scale/var\n",
      "I0710 07:19:07.302768 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[5].bak_rnn.cell.wm                    (3072, 4096)           12582912 punctuator_rnmt/enc/L5_rnn_bak/wm/var\n",
      "I0710 07:19:07.302806 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[5].fwd_rnn.cell.b                     (4096,)                    4096 punctuator_rnmt/enc/L5_rnn_fwd/b/var\n",
      "I0710 07:19:07.302845 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[5].fwd_rnn.cell.ln_scale              (4096,)                    4096 punctuator_rnmt/enc/L5_rnn_fwd/ln_scale/var\n",
      "I0710 07:19:07.302882 140310643746176 py_utils.py:1773] MODEL ANALYSIS: _task.enc.rnn[5].fwd_rnn.cell.wm                    (3072, 4096)           12582912 punctuator_rnmt/enc/L5_rnn_fwd/wm/var\n",
      "I0710 07:19:07.302920 140310643746176 py_utils.py:1773] MODEL ANALYSIS: ====================================================================================================\n",
      "I0710 07:19:07.302959 140310643746176 py_utils.py:1773] MODEL ANALYSIS: total #params:  313314176\n",
      "I0710 07:19:07.302996 140310643746176 py_utils.py:1773] MODEL ANALYSIS: \n",
      "I0710 07:19:13.587086 140310643746176 trainer.py:1539] Job trainer_client start\n",
      "model_imports.py: Importing codelab\n",
      "model_imports.py: Imported codelab\n",
      "I0710 07:19:13.597213 140310643746176 base_runner.py:57] ============================================================\n",
      "I0710 07:19:13.600806 140310643746176 base_runner.py:59] allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.600945 140310643746176 base_runner.py:59] cls : type/lingvo.core.base_model/SingleTaskModel\n",
      "I0710 07:19:13.601012 140310643746176 base_runner.py:59] cluster.add_summary : NoneType\n",
      "I0710 07:19:13.601060 140310643746176 base_runner.py:59] cluster.cls : type/lingvo.core.cluster/_Cluster\n",
      "I0710 07:19:13.601102 140310643746176 base_runner.py:59] cluster.controller.cpus_per_replica : 1\n",
      "I0710 07:19:13.601143 140310643746176 base_runner.py:59] cluster.controller.devices_per_split : 1\n",
      "I0710 07:19:13.601183 140310643746176 base_runner.py:59] cluster.controller.gpus_per_replica : 0\n",
      "I0710 07:19:13.601221 140310643746176 base_runner.py:59] cluster.controller.name : '/job:localhost'\n",
      "I0710 07:19:13.601259 140310643746176 base_runner.py:59] cluster.controller.num_tpu_hosts : 0\n",
      "I0710 07:19:13.601300 140310643746176 base_runner.py:59] cluster.controller.replicas : 1\n",
      "I0710 07:19:13.601339 140310643746176 base_runner.py:59] cluster.controller.targets : ''\n",
      "I0710 07:19:13.601377 140310643746176 base_runner.py:59] cluster.controller.tpus_per_replica : 0\n",
      "I0710 07:19:13.601415 140310643746176 base_runner.py:59] cluster.decoder.cpus_per_replica : 1\n",
      "I0710 07:19:13.601453 140310643746176 base_runner.py:59] cluster.decoder.devices_per_split : 1\n",
      "I0710 07:19:13.601491 140310643746176 base_runner.py:59] cluster.decoder.gpus_per_replica : 1\n",
      "I0710 07:19:13.601528 140310643746176 base_runner.py:59] cluster.decoder.name : '/job:localhost'\n",
      "I0710 07:19:13.601566 140310643746176 base_runner.py:59] cluster.decoder.num_tpu_hosts : 0\n",
      "I0710 07:19:13.601603 140310643746176 base_runner.py:59] cluster.decoder.replicas : 1\n",
      "I0710 07:19:13.601641 140310643746176 base_runner.py:59] cluster.decoder.targets : ''\n",
      "I0710 07:19:13.601679 140310643746176 base_runner.py:59] cluster.decoder.tpus_per_replica : 0\n",
      "I0710 07:19:13.601716 140310643746176 base_runner.py:59] cluster.do_eval : False\n",
      "I0710 07:19:13.601754 140310643746176 base_runner.py:59] cluster.evaler.cpus_per_replica : 1\n",
      "I0710 07:19:13.601791 140310643746176 base_runner.py:59] cluster.evaler.devices_per_split : 1\n",
      "I0710 07:19:13.601828 140310643746176 base_runner.py:59] cluster.evaler.gpus_per_replica : 1\n",
      "I0710 07:19:13.601866 140310643746176 base_runner.py:59] cluster.evaler.name : '/job:localhost'\n",
      "I0710 07:19:13.601917 140310643746176 base_runner.py:59] cluster.evaler.num_tpu_hosts : 0\n",
      "I0710 07:19:13.601962 140310643746176 base_runner.py:59] cluster.evaler.replicas : 1\n",
      "I0710 07:19:13.602015 140310643746176 base_runner.py:59] cluster.evaler.targets : ''\n",
      "I0710 07:19:13.602055 140310643746176 base_runner.py:59] cluster.evaler.tpus_per_replica : 0\n",
      "I0710 07:19:13.602093 140310643746176 base_runner.py:59] cluster.input.cpus_per_replica : 1\n",
      "I0710 07:19:13.602131 140310643746176 base_runner.py:59] cluster.input.devices_per_split : 1\n",
      "I0710 07:19:13.602168 140310643746176 base_runner.py:59] cluster.input.gpus_per_replica : 0\n",
      "I0710 07:19:13.602205 140310643746176 base_runner.py:59] cluster.input.name : '/job:localhost'\n",
      "I0710 07:19:13.602242 140310643746176 base_runner.py:59] cluster.input.num_tpu_hosts : 0\n",
      "I0710 07:19:13.602279 140310643746176 base_runner.py:59] cluster.input.replicas : 0\n",
      "I0710 07:19:13.602316 140310643746176 base_runner.py:59] cluster.input.targets : ''\n",
      "I0710 07:19:13.602353 140310643746176 base_runner.py:59] cluster.input.tpus_per_replica : 0\n",
      "I0710 07:19:13.602390 140310643746176 base_runner.py:59] cluster.job : 'trainer_client'\n",
      "I0710 07:19:13.602427 140310643746176 base_runner.py:59] cluster.logdir : ''\n",
      "I0710 07:19:13.602464 140310643746176 base_runner.py:59] cluster.mode : 'sync'\n",
      "I0710 07:19:13.602501 140310643746176 base_runner.py:59] cluster.ps.cpus_per_replica : 1\n",
      "I0710 07:19:13.602538 140310643746176 base_runner.py:59] cluster.ps.devices_per_split : 1\n",
      "I0710 07:19:13.602575 140310643746176 base_runner.py:59] cluster.ps.gpus_per_replica : 0\n",
      "I0710 07:19:13.602611 140310643746176 base_runner.py:59] cluster.ps.name : '/job:localhost'\n",
      "I0710 07:19:13.602648 140310643746176 base_runner.py:59] cluster.ps.num_tpu_hosts : 0\n",
      "I0710 07:19:13.602685 140310643746176 base_runner.py:59] cluster.ps.replicas : 1\n",
      "I0710 07:19:13.602722 140310643746176 base_runner.py:59] cluster.ps.targets : ''\n",
      "I0710 07:19:13.602759 140310643746176 base_runner.py:59] cluster.ps.tpus_per_replica : 0\n",
      "I0710 07:19:13.602796 140310643746176 base_runner.py:59] cluster.split_id : 0\n",
      "I0710 07:19:13.602833 140310643746176 base_runner.py:59] cluster.task : 0\n",
      "I0710 07:19:13.602869 140310643746176 base_runner.py:59] cluster.worker.cpus_per_replica : 1\n",
      "I0710 07:19:13.602907 140310643746176 base_runner.py:59] cluster.worker.devices_per_split : 1\n",
      "I0710 07:19:13.602943 140310643746176 base_runner.py:59] cluster.worker.gpus_per_replica : 1\n",
      "I0710 07:19:13.602980 140310643746176 base_runner.py:59] cluster.worker.name : '/job:localhost'\n",
      "I0710 07:19:13.603032 140310643746176 base_runner.py:59] cluster.worker.num_tpu_hosts : 0\n",
      "I0710 07:19:13.603069 140310643746176 base_runner.py:59] cluster.worker.replicas : 1\n",
      "I0710 07:19:13.603106 140310643746176 base_runner.py:59] cluster.worker.targets : ''\n",
      "I0710 07:19:13.603143 140310643746176 base_runner.py:59] cluster.worker.tpus_per_replica : 0\n",
      "I0710 07:19:13.603180 140310643746176 base_runner.py:59] dtype : float32\n",
      "I0710 07:19:13.603217 140310643746176 base_runner.py:59] fprop_dtype : NoneType\n",
      "I0710 07:19:13.603254 140310643746176 base_runner.py:59] inference_driver_name : NoneType\n",
      "I0710 07:19:13.603291 140310643746176 base_runner.py:59] input.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.603327 140310643746176 base_runner.py:59] input.bucket_adjust_every_n : 0\n",
      "I0710 07:19:13.603364 140310643746176 base_runner.py:59] input.bucket_batch_limit : [512, 256, 160, 80, 40]\n",
      "I0710 07:19:13.603401 140310643746176 base_runner.py:59] input.bucket_upper_bound : [10, 20, 30, 60, 120]\n",
      "I0710 07:19:13.603439 140310643746176 base_runner.py:59] input.cls : type/input_generator/PunctuatorInput\n",
      "I0710 07:19:13.603476 140310643746176 base_runner.py:59] input.dtype : float32\n",
      "I0710 07:19:13.603513 140310643746176 base_runner.py:59] input.file_buffer_size : 10000\n",
      "I0710 07:19:13.603551 140310643746176 base_runner.py:59] input.file_buffer_size_in_seconds : 0\n",
      "I0710 07:19:13.603588 140310643746176 base_runner.py:59] input.file_datasource : NoneType\n",
      "I0710 07:19:13.603625 140310643746176 base_runner.py:59] input.file_parallelism : 1\n",
      "I0710 07:19:13.603662 140310643746176 base_runner.py:59] input.file_pattern : 'text:/tmp/punctuator_data/train.txt'\n",
      "I0710 07:19:13.603699 140310643746176 base_runner.py:59] input.file_random_seed : 0\n",
      "I0710 07:19:13.603736 140310643746176 base_runner.py:59] input.flush_every_n : 0\n",
      "I0710 07:19:13.603773 140310643746176 base_runner.py:59] input.fprop_dtype : NoneType\n",
      "I0710 07:19:13.603810 140310643746176 base_runner.py:59] input.inference_driver_name : NoneType\n",
      "I0710 07:19:13.603847 140310643746176 base_runner.py:59] input.is_inference : NoneType\n",
      "I0710 07:19:13.603883 140310643746176 base_runner.py:59] input.name : 'input'\n",
      "I0710 07:19:13.603920 140310643746176 base_runner.py:59] input.num_batcher_threads : 1\n",
      "I0710 07:19:13.603957 140310643746176 base_runner.py:59] input.num_partitions : NoneType\n",
      "I0710 07:19:13.603994 140310643746176 base_runner.py:59] input.num_samples : 0\n",
      "I0710 07:19:13.604038 140310643746176 base_runner.py:59] input.pad_to_max_seq_length : False\n",
      "I0710 07:19:13.604075 140310643746176 base_runner.py:59] input.params_init.method : 'xavier'\n",
      "I0710 07:19:13.604113 140310643746176 base_runner.py:59] input.params_init.scale : 1.000001\n",
      "I0710 07:19:13.604149 140310643746176 base_runner.py:59] input.params_init.seed : NoneType\n",
      "I0710 07:19:13.604186 140310643746176 base_runner.py:59] input.random_seed : NoneType\n",
      "I0710 07:19:13.604223 140310643746176 base_runner.py:59] input.remote.max_inflights_per_target : 32\n",
      "I0710 07:19:13.604260 140310643746176 base_runner.py:59] input.remote.shardable_batch : False\n",
      "I0710 07:19:13.604296 140310643746176 base_runner.py:59] input.repeat_count : -1\n",
      "I0710 07:19:13.604351 140310643746176 base_runner.py:59] input.require_sequential_order : False\n",
      "I0710 07:19:13.604389 140310643746176 base_runner.py:59] input.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.604427 140310643746176 base_runner.py:59] input.source_max_length : 122\n",
      "I0710 07:19:13.604465 140310643746176 base_runner.py:59] input.target_max_length : 122\n",
      "I0710 07:19:13.604504 140310643746176 base_runner.py:59] input.tokenizer.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.604542 140310643746176 base_runner.py:59] input.tokenizer.append_eos : True\n",
      "I0710 07:19:13.604580 140310643746176 base_runner.py:59] input.tokenizer.cls : type/lingvo.core.tokenizers/WpmTokenizer\n",
      "I0710 07:19:13.604618 140310643746176 base_runner.py:59] input.tokenizer.dtype : float32\n",
      "I0710 07:19:13.604657 140310643746176 base_runner.py:59] input.tokenizer.fprop_dtype : NoneType\n",
      "I0710 07:19:13.604695 140310643746176 base_runner.py:59] input.tokenizer.inference_driver_name : NoneType\n",
      "I0710 07:19:13.604744 140310643746176 base_runner.py:59] input.tokenizer.is_inference : NoneType\n",
      "I0710 07:19:13.604781 140310643746176 base_runner.py:59] input.tokenizer.merge_prob : 1.0\n",
      "I0710 07:19:13.604817 140310643746176 base_runner.py:59] input.tokenizer.name : 'tokenizer'\n",
      "I0710 07:19:13.604855 140310643746176 base_runner.py:59] input.tokenizer.pad_to_max_length : False\n",
      "I0710 07:19:13.604892 140310643746176 base_runner.py:59] input.tokenizer.params_init.method : 'xavier'\n",
      "I0710 07:19:13.604929 140310643746176 base_runner.py:59] input.tokenizer.params_init.scale : 1.000001\n",
      "I0710 07:19:13.604966 140310643746176 base_runner.py:59] input.tokenizer.params_init.seed : NoneType\n",
      "I0710 07:19:13.605007 140310643746176 base_runner.py:59] input.tokenizer.random_seed : NoneType\n",
      "I0710 07:19:13.605046 140310643746176 base_runner.py:59] input.tokenizer.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.605083 140310643746176 base_runner.py:59] input.tokenizer.target_eos_id : 2\n",
      "I0710 07:19:13.605121 140310643746176 base_runner.py:59] input.tokenizer.target_sos_id : 1\n",
      "I0710 07:19:13.605161 140310643746176 base_runner.py:59] input.tokenizer.target_unk_id : 0\n",
      "I0710 07:19:13.605198 140310643746176 base_runner.py:59] input.tokenizer.target_wb_id : -1\n",
      "I0710 07:19:13.605235 140310643746176 base_runner.py:59] input.tokenizer.vn.global_vn : False\n",
      "I0710 07:19:13.605271 140310643746176 base_runner.py:59] input.tokenizer.vn.per_step_vn : False\n",
      "I0710 07:19:13.605309 140310643746176 base_runner.py:59] input.tokenizer.vn.scale : NoneType\n",
      "I0710 07:19:13.605345 140310643746176 base_runner.py:59] input.tokenizer.vn.seed : NoneType\n",
      "I0710 07:19:13.605418 140310643746176 base_runner.py:59] input.tokenizer.vocab_filepath : 'brown_corpus_wpm.16000.vocab'\n",
      "I0710 07:19:13.605452 140310643746176 base_runner.py:59] input.tokenizer.vocab_size : 16000\n",
      "I0710 07:19:13.605489 140310643746176 base_runner.py:59] input.tokenizer_dict : {}\n",
      "I0710 07:19:13.605525 140310643746176 base_runner.py:59] input.tpu_infeed_parallelism : 1\n",
      "I0710 07:19:13.605563 140310643746176 base_runner.py:59] input.use_chaining : False\n",
      "I0710 07:19:13.605600 140310643746176 base_runner.py:59] input.use_partitioned_infeed_queue : False\n",
      "I0710 07:19:13.605636 140310643746176 base_runner.py:59] input.use_per_host_infeed : False\n",
      "I0710 07:19:13.605673 140310643746176 base_runner.py:59] input.use_within_batch_mixing : False\n",
      "I0710 07:19:13.605709 140310643746176 base_runner.py:59] input.vn.global_vn : False\n",
      "I0710 07:19:13.605746 140310643746176 base_runner.py:59] input.vn.per_step_vn : False\n",
      "I0710 07:19:13.605782 140310643746176 base_runner.py:59] input.vn.scale : NoneType\n",
      "I0710 07:19:13.605819 140310643746176 base_runner.py:59] input.vn.seed : NoneType\n",
      "I0710 07:19:13.605857 140310643746176 base_runner.py:59] is_inference : NoneType\n",
      "I0710 07:19:13.605905 140310643746176 base_runner.py:59] model : 'codelab.RNMTModel@/home/jupyter/lingvo/codelabs/codelab.py:81'\n",
      "I0710 07:19:13.605945 140310643746176 base_runner.py:59] name : ''\n",
      "I0710 07:19:13.605982 140310643746176 base_runner.py:59] params_init.method : 'xavier'\n",
      "I0710 07:19:13.606024 140310643746176 base_runner.py:59] params_init.scale : 1.000001\n",
      "I0710 07:19:13.606061 140310643746176 base_runner.py:59] params_init.seed : NoneType\n",
      "I0710 07:19:13.606097 140310643746176 base_runner.py:59] random_seed : NoneType\n",
      "I0710 07:19:13.606135 140310643746176 base_runner.py:59] skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.606171 140310643746176 base_runner.py:59] task.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.606208 140310643746176 base_runner.py:59] task.cls : type/lingvo.tasks.punctuator.model/RNMTModel\n",
      "I0710 07:19:13.606246 140310643746176 base_runner.py:59] task.decoder.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.606282 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.606320 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.apply_pruning : False\n",
      "I0710 07:19:13.606367 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.apply_pruning_to_projection : False\n",
      "I0710 07:19:13.606402 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.bias_init.method : 'constant'\n",
      "I0710 07:19:13.606437 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.bias_init.scale : 0.0\n",
      "I0710 07:19:13.606471 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.bias_init.seed : 0\n",
      "I0710 07:19:13.606524 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.cell_value_cap : 10.0\n",
      "I0710 07:19:13.606564 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.cls : type/lingvo.core.rnn_cell/LayerNormalizedLSTMCellSimple\n",
      "I0710 07:19:13.606602 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.couple_input_forget_gates : False\n",
      "I0710 07:19:13.606639 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.dtype : float32\n",
      "I0710 07:19:13.606678 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.enable_lstm_bias : True\n",
      "I0710 07:19:13.606715 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.forget_gate_bias : 0.0\n",
      "I0710 07:19:13.606753 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.fprop_dtype : NoneType\n",
      "I0710 07:19:13.606790 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.gradient_pruning : False\n",
      "I0710 07:19:13.606827 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.inference_driver_name : NoneType\n",
      "I0710 07:19:13.606864 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.inputs_arity : 1\n",
      "I0710 07:19:13.606901 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.is_inference : NoneType\n",
      "I0710 07:19:13.606938 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.layer_norm_epsilon : 1e-08\n",
      "I0710 07:19:13.606976 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.name : ''\n",
      "I0710 07:19:13.607019 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.num_hidden_nodes : 0\n",
      "I0710 07:19:13.607056 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.num_input_nodes : 0\n",
      "I0710 07:19:13.607093 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.num_output_nodes : 1024\n",
      "I0710 07:19:13.607131 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.output_nonlinearity : False\n",
      "I0710 07:19:13.607168 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.params_init.method : 'uniform'\n",
      "I0710 07:19:13.607205 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.params_init.scale : 0.04\n",
      "I0710 07:19:13.607242 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.params_init.seed : NoneType\n",
      "I0710 07:19:13.607279 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.qdomain.c_state : NoneType\n",
      "I0710 07:19:13.607317 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.qdomain.default : NoneType\n",
      "I0710 07:19:13.607354 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.qdomain.fullyconnected : NoneType\n",
      "I0710 07:19:13.607392 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.qdomain.m_state : NoneType\n",
      "I0710 07:19:13.607429 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.qdomain.weight : NoneType\n",
      "I0710 07:19:13.607466 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.random_seed : NoneType\n",
      "I0710 07:19:13.607503 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.reset_cell_state : False\n",
      "I0710 07:19:13.607540 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.607577 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.vn.global_vn : False\n",
      "I0710 07:19:13.607614 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.vn.per_step_vn : False\n",
      "I0710 07:19:13.607652 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.vn.scale : NoneType\n",
      "I0710 07:19:13.607689 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.vn.seed : NoneType\n",
      "I0710 07:19:13.607726 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.zero_state_init_params.method : 'zeros'\n",
      "I0710 07:19:13.607764 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.zero_state_init_params.seed : NoneType\n",
      "I0710 07:19:13.607802 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.zo_prob : 0.0\n",
      "I0710 07:19:13.607839 140310643746176 base_runner.py:59] task.decoder.atten_rnn_cls : type/lingvo.core.rnn_layers/FRNNWithAttention\n",
      "I0710 07:19:13.607876 140310643746176 base_runner.py:59] task.decoder.attention.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.607913 140310643746176 base_runner.py:59] task.decoder.attention.atten_dropout_deterministic : True\n",
      "I0710 07:19:13.607951 140310643746176 base_runner.py:59] task.decoder.attention.atten_dropout_prob : 0.3\n",
      "I0710 07:19:13.607988 140310643746176 base_runner.py:59] task.decoder.attention.attention_head_prob_index : -1\n",
      "I0710 07:19:13.608031 140310643746176 base_runner.py:59] task.decoder.attention.cls : type/lingvo.core.attention/MultiHeadedAttention\n",
      "I0710 07:19:13.608069 140310643746176 base_runner.py:59] task.decoder.attention.context_dim : 1024\n",
      "I0710 07:19:13.608106 140310643746176 base_runner.py:59] task.decoder.attention.ctx_post_proj_dim : 0\n",
      "I0710 07:19:13.608143 140310643746176 base_runner.py:59] task.decoder.attention.dtype : float32\n",
      "I0710 07:19:13.608181 140310643746176 base_runner.py:59] task.decoder.attention.enable_ctx_post_proj : False\n",
      "I0710 07:19:13.608218 140310643746176 base_runner.py:59] task.decoder.attention.enable_ctx_pre_proj : False\n",
      "I0710 07:19:13.608256 140310643746176 base_runner.py:59] task.decoder.attention.enable_query_proj : True\n",
      "I0710 07:19:13.608293 140310643746176 base_runner.py:59] task.decoder.attention.enable_source_proj : True\n",
      "I0710 07:19:13.608330 140310643746176 base_runner.py:59] task.decoder.attention.fprop_dtype : NoneType\n",
      "I0710 07:19:13.608368 140310643746176 base_runner.py:59] task.decoder.attention.hidden_dim : 1024\n",
      "I0710 07:19:13.608404 140310643746176 base_runner.py:59] task.decoder.attention.inference_driver_name : NoneType\n",
      "I0710 07:19:13.608442 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.608479 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.atten_dropout_deterministic : False\n",
      "I0710 07:19:13.608524 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.atten_dropout_prob : 0.0\n",
      "I0710 07:19:13.608563 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.cls : type/lingvo.core.attention/AdditiveAttention\n",
      "I0710 07:19:13.608601 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.dtype : float32\n",
      "I0710 07:19:13.608639 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.fprop_dtype : NoneType\n",
      "I0710 07:19:13.608676 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.hidden_dim : 0\n",
      "I0710 07:19:13.608713 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.inference_driver_name : NoneType\n",
      "I0710 07:19:13.608762 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.is_inference : NoneType\n",
      "I0710 07:19:13.608798 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.name : ''\n",
      "I0710 07:19:13.608833 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.packed_input : False\n",
      "I0710 07:19:13.608867 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.params_init.method : 'gaussian_sqrt_dim'\n",
      "I0710 07:19:13.608902 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.params_init.scale : 1.0\n",
      "I0710 07:19:13.608957 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.params_init.seed : NoneType\n",
      "I0710 07:19:13.608994 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.qdomain.default : NoneType\n",
      "I0710 07:19:13.609039 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.qdomain.fullyconnected : NoneType\n",
      "I0710 07:19:13.609077 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.qdomain.softmax : NoneType\n",
      "I0710 07:19:13.609113 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.query_dim : 0\n",
      "I0710 07:19:13.609152 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.random_seed : NoneType\n",
      "I0710 07:19:13.609191 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.same_batch_size : False\n",
      "I0710 07:19:13.609232 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.609270 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.source_dim : 0\n",
      "I0710 07:19:13.609308 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.vn.global_vn : False\n",
      "I0710 07:19:13.609345 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.vn.per_step_vn : False\n",
      "I0710 07:19:13.609384 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.vn.scale : NoneType\n",
      "I0710 07:19:13.609418 140310643746176 base_runner.py:59] task.decoder.attention.inner_atten_params.vn.seed : NoneType\n",
      "I0710 07:19:13.609462 140310643746176 base_runner.py:59] task.decoder.attention.is_inference : NoneType\n",
      "I0710 07:19:13.609496 140310643746176 base_runner.py:59] task.decoder.attention.name : ''\n",
      "I0710 07:19:13.609533 140310643746176 base_runner.py:59] task.decoder.attention.num_attention_heads : 4\n",
      "I0710 07:19:13.609570 140310643746176 base_runner.py:59] task.decoder.attention.num_post_proj : 1\n",
      "I0710 07:19:13.609607 140310643746176 base_runner.py:59] task.decoder.attention.packed_input : False\n",
      "I0710 07:19:13.609644 140310643746176 base_runner.py:59] task.decoder.attention.params_init.method : 'xavier'\n",
      "I0710 07:19:13.609682 140310643746176 base_runner.py:59] task.decoder.attention.params_init.scale : 1.0\n",
      "I0710 07:19:13.609720 140310643746176 base_runner.py:59] task.decoder.attention.params_init.seed : NoneType\n",
      "I0710 07:19:13.609757 140310643746176 base_runner.py:59] task.decoder.attention.proj_init : 'default'\n",
      "I0710 07:19:13.609794 140310643746176 base_runner.py:59] task.decoder.attention.qdomain.atten_context : NoneType\n",
      "I0710 07:19:13.609832 140310643746176 base_runner.py:59] task.decoder.attention.qdomain.default : NoneType\n",
      "I0710 07:19:13.609870 140310643746176 base_runner.py:59] task.decoder.attention.qdomain.fullyconnected : NoneType\n",
      "I0710 07:19:13.609918 140310643746176 base_runner.py:59] task.decoder.attention.qdomain.softmax : NoneType\n",
      "I0710 07:19:13.609957 140310643746176 base_runner.py:59] task.decoder.attention.query_dim : 1024\n",
      "I0710 07:19:13.609994 140310643746176 base_runner.py:59] task.decoder.attention.random_seed : NoneType\n",
      "I0710 07:19:13.610037 140310643746176 base_runner.py:59] task.decoder.attention.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.610075 140310643746176 base_runner.py:59] task.decoder.attention.source_dim : 1024\n",
      "I0710 07:19:13.610113 140310643746176 base_runner.py:59] task.decoder.attention.use_source_vec_as_attention_value : True\n",
      "I0710 07:19:13.610150 140310643746176 base_runner.py:59] task.decoder.attention.vn.global_vn : False\n",
      "I0710 07:19:13.610188 140310643746176 base_runner.py:59] task.decoder.attention.vn.per_step_vn : False\n",
      "I0710 07:19:13.610226 140310643746176 base_runner.py:59] task.decoder.attention.vn.scale : NoneType\n",
      "I0710 07:19:13.610263 140310643746176 base_runner.py:59] task.decoder.attention.vn.seed : NoneType\n",
      "I0710 07:19:13.610300 140310643746176 base_runner.py:59] task.decoder.beam_search.allow_empty_terminated_hyp : True\n",
      "I0710 07:19:13.610337 140310643746176 base_runner.py:59] task.decoder.beam_search.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.610375 140310643746176 base_runner.py:59] task.decoder.beam_search.batch_major_compute : False\n",
      "I0710 07:19:13.610413 140310643746176 base_runner.py:59] task.decoder.beam_search.batch_major_state : True\n",
      "I0710 07:19:13.610450 140310643746176 base_runner.py:59] task.decoder.beam_search.beam_size : 3.0\n",
      "I0710 07:19:13.610487 140310643746176 base_runner.py:59] task.decoder.beam_search.cls : type/lingvo.core.beam_search_helper/BeamSearchHelper\n",
      "I0710 07:19:13.610525 140310643746176 base_runner.py:59] task.decoder.beam_search.coverage_penalty : 0.2\n",
      "I0710 07:19:13.610562 140310643746176 base_runner.py:59] task.decoder.beam_search.dtype : float32\n",
      "I0710 07:19:13.610599 140310643746176 base_runner.py:59] task.decoder.beam_search.ensure_full_beam : False\n",
      "I0710 07:19:13.610637 140310643746176 base_runner.py:59] task.decoder.beam_search.force_eos_in_last_step : False\n",
      "I0710 07:19:13.610674 140310643746176 base_runner.py:59] task.decoder.beam_search.fprop_dtype : NoneType\n",
      "I0710 07:19:13.610712 140310643746176 base_runner.py:59] task.decoder.beam_search.inference_driver_name : NoneType\n",
      "I0710 07:19:13.610750 140310643746176 base_runner.py:59] task.decoder.beam_search.is_inference : NoneType\n",
      "I0710 07:19:13.610787 140310643746176 base_runner.py:59] task.decoder.beam_search.length_normalization : 0.2\n",
      "I0710 07:19:13.610825 140310643746176 base_runner.py:59] task.decoder.beam_search.local_eos_threshold : -100.0\n",
      "I0710 07:19:13.610861 140310643746176 base_runner.py:59] task.decoder.beam_search.merge_paths : False\n",
      "I0710 07:19:13.610898 140310643746176 base_runner.py:59] task.decoder.beam_search.name : 'beam_search'\n",
      "I0710 07:19:13.610936 140310643746176 base_runner.py:59] task.decoder.beam_search.num_hyps_per_beam : 16\n",
      "I0710 07:19:13.610973 140310643746176 base_runner.py:59] task.decoder.beam_search.params_init.method : 'xavier'\n",
      "I0710 07:19:13.611040 140310643746176 base_runner.py:59] task.decoder.beam_search.params_init.scale : 1.000001\n",
      "I0710 07:19:13.611079 140310643746176 base_runner.py:59] task.decoder.beam_search.params_init.seed : NoneType\n",
      "I0710 07:19:13.611117 140310643746176 base_runner.py:59] task.decoder.beam_search.random_seed : NoneType\n",
      "I0710 07:19:13.611155 140310643746176 base_runner.py:59] task.decoder.beam_search.short_seq_limit : 0\n",
      "I0710 07:19:13.611192 140310643746176 base_runner.py:59] task.decoder.beam_search.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.611230 140310643746176 base_runner.py:59] task.decoder.beam_search.target_eoc_id : -1\n",
      "I0710 07:19:13.611267 140310643746176 base_runner.py:59] task.decoder.beam_search.target_eos_id : 2\n",
      "I0710 07:19:13.611305 140310643746176 base_runner.py:59] task.decoder.beam_search.target_seq_len : 0\n",
      "I0710 07:19:13.611342 140310643746176 base_runner.py:59] task.decoder.beam_search.target_seq_length_ratio : 1.0\n",
      "I0710 07:19:13.611379 140310643746176 base_runner.py:59] task.decoder.beam_search.target_sos_id : 1\n",
      "I0710 07:19:13.611416 140310643746176 base_runner.py:59] task.decoder.beam_search.valid_eos_max_logit_delta : 5.0\n",
      "I0710 07:19:13.611454 140310643746176 base_runner.py:59] task.decoder.beam_search.vn.global_vn : False\n",
      "I0710 07:19:13.611492 140310643746176 base_runner.py:59] task.decoder.beam_search.vn.per_step_vn : False\n",
      "I0710 07:19:13.611530 140310643746176 base_runner.py:59] task.decoder.beam_search.vn.scale : NoneType\n",
      "I0710 07:19:13.611567 140310643746176 base_runner.py:59] task.decoder.beam_search.vn.seed : NoneType\n",
      "I0710 07:19:13.611605 140310643746176 base_runner.py:59] task.decoder.bias_only_if_consistent : True\n",
      "I0710 07:19:13.611642 140310643746176 base_runner.py:59] task.decoder.cc_schedule : NoneType\n",
      "I0710 07:19:13.611680 140310643746176 base_runner.py:59] task.decoder.cls : type/lingvo.tasks.mt.decoder/MTDecoderV1\n",
      "I0710 07:19:13.611718 140310643746176 base_runner.py:59] task.decoder.dropout_prob : 0.3\n",
      "I0710 07:19:13.611755 140310643746176 base_runner.py:59] task.decoder.dtype : float32\n",
      "I0710 07:19:13.611802 140310643746176 base_runner.py:59] task.decoder.emb.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.611837 140310643746176 base_runner.py:59] task.decoder.emb.cls : type/lingvo.core.layers/EmbeddingLayer\n",
      "I0710 07:19:13.611872 140310643746176 base_runner.py:59] task.decoder.emb.dtype : float32\n",
      "I0710 07:19:13.611908 140310643746176 base_runner.py:59] task.decoder.emb.embedding_dim : 1024\n",
      "I0710 07:19:13.611943 140310643746176 base_runner.py:59] task.decoder.emb.fprop_dtype : NoneType\n",
      "I0710 07:19:13.611997 140310643746176 base_runner.py:59] task.decoder.emb.inference_driver_name : NoneType\n",
      "I0710 07:19:13.612049 140310643746176 base_runner.py:59] task.decoder.emb.is_inference : NoneType\n",
      "I0710 07:19:13.612087 140310643746176 base_runner.py:59] task.decoder.emb.max_num_shards : 16\n",
      "I0710 07:19:13.612124 140310643746176 base_runner.py:59] task.decoder.emb.name : ''\n",
      "I0710 07:19:13.612162 140310643746176 base_runner.py:59] task.decoder.emb.on_ps : True\n",
      "I0710 07:19:13.612199 140310643746176 base_runner.py:59] task.decoder.emb.params_init.method : 'uniform'\n",
      "I0710 07:19:13.612236 140310643746176 base_runner.py:59] task.decoder.emb.params_init.scale : 0.04\n",
      "I0710 07:19:13.612274 140310643746176 base_runner.py:59] task.decoder.emb.params_init.seed : NoneType\n",
      "I0710 07:19:13.612338 140310643746176 base_runner.py:59] task.decoder.emb.random_seed : NoneType\n",
      "I0710 07:19:13.612375 140310643746176 base_runner.py:59] task.decoder.emb.scale_sqrt_depth : False\n",
      "I0710 07:19:13.612412 140310643746176 base_runner.py:59] task.decoder.emb.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.612449 140310643746176 base_runner.py:59] task.decoder.emb.vn.global_vn : False\n",
      "I0710 07:19:13.612487 140310643746176 base_runner.py:59] task.decoder.emb.vn.per_step_vn : False\n",
      "I0710 07:19:13.612524 140310643746176 base_runner.py:59] task.decoder.emb.vn.scale : 1.0\n",
      "I0710 07:19:13.612561 140310643746176 base_runner.py:59] task.decoder.emb.vn.seed : NoneType\n",
      "I0710 07:19:13.612598 140310643746176 base_runner.py:59] task.decoder.emb.vocab_size : 16000\n",
      "I0710 07:19:13.612635 140310643746176 base_runner.py:59] task.decoder.feed_attention_context_vec_to_softmax : True\n",
      "I0710 07:19:13.612672 140310643746176 base_runner.py:59] task.decoder.fprop_dtype : NoneType\n",
      "I0710 07:19:13.612710 140310643746176 base_runner.py:59] task.decoder.greedy_search.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.612747 140310643746176 base_runner.py:59] task.decoder.greedy_search.cls : type/lingvo.core.beam_search_helper/GreedySearchHelper\n",
      "I0710 07:19:13.612785 140310643746176 base_runner.py:59] task.decoder.greedy_search.dtype : float32\n",
      "I0710 07:19:13.612823 140310643746176 base_runner.py:59] task.decoder.greedy_search.fprop_dtype : NoneType\n",
      "I0710 07:19:13.612860 140310643746176 base_runner.py:59] task.decoder.greedy_search.inference_driver_name : NoneType\n",
      "I0710 07:19:13.612900 140310643746176 base_runner.py:59] task.decoder.greedy_search.is_inference : NoneType\n",
      "I0710 07:19:13.612937 140310643746176 base_runner.py:59] task.decoder.greedy_search.name : 'greedy_search'\n",
      "I0710 07:19:13.612975 140310643746176 base_runner.py:59] task.decoder.greedy_search.params_init.method : 'xavier'\n",
      "I0710 07:19:13.613016 140310643746176 base_runner.py:59] task.decoder.greedy_search.params_init.scale : 1.000001\n",
      "I0710 07:19:13.613054 140310643746176 base_runner.py:59] task.decoder.greedy_search.params_init.seed : NoneType\n",
      "I0710 07:19:13.613090 140310643746176 base_runner.py:59] task.decoder.greedy_search.random_seed : NoneType\n",
      "I0710 07:19:13.613128 140310643746176 base_runner.py:59] task.decoder.greedy_search.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.613165 140310643746176 base_runner.py:59] task.decoder.greedy_search.target_eos_id : 2\n",
      "I0710 07:19:13.613202 140310643746176 base_runner.py:59] task.decoder.greedy_search.target_seq_len : 0\n",
      "I0710 07:19:13.613240 140310643746176 base_runner.py:59] task.decoder.greedy_search.target_sos_id : 1\n",
      "I0710 07:19:13.613277 140310643746176 base_runner.py:59] task.decoder.greedy_search.vn.global_vn : False\n",
      "I0710 07:19:13.613314 140310643746176 base_runner.py:59] task.decoder.greedy_search.vn.per_step_vn : False\n",
      "I0710 07:19:13.613351 140310643746176 base_runner.py:59] task.decoder.greedy_search.vn.scale : NoneType\n",
      "I0710 07:19:13.613388 140310643746176 base_runner.py:59] task.decoder.greedy_search.vn.seed : NoneType\n",
      "I0710 07:19:13.613426 140310643746176 base_runner.py:59] task.decoder.inference_driver_name : NoneType\n",
      "I0710 07:19:13.613463 140310643746176 base_runner.py:59] task.decoder.init_step_ids : False\n",
      "I0710 07:19:13.613501 140310643746176 base_runner.py:59] task.decoder.is_inference : NoneType\n",
      "I0710 07:19:13.613538 140310643746176 base_runner.py:59] task.decoder.label_smoothing.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.613575 140310643746176 base_runner.py:59] task.decoder.label_smoothing.cls : type/lingvo.core.layers/UniformLabelSmoother\n",
      "I0710 07:19:13.613613 140310643746176 base_runner.py:59] task.decoder.label_smoothing.dtype : float32\n",
      "I0710 07:19:13.613668 140310643746176 base_runner.py:59] task.decoder.label_smoothing.fprop_dtype : NoneType\n",
      "I0710 07:19:13.613707 140310643746176 base_runner.py:59] task.decoder.label_smoothing.inference_driver_name : NoneType\n",
      "I0710 07:19:13.613745 140310643746176 base_runner.py:59] task.decoder.label_smoothing.is_inference : NoneType\n",
      "I0710 07:19:13.613784 140310643746176 base_runner.py:59] task.decoder.label_smoothing.name : ''\n",
      "I0710 07:19:13.613822 140310643746176 base_runner.py:59] task.decoder.label_smoothing.num_classes : 16000\n",
      "I0710 07:19:13.613861 140310643746176 base_runner.py:59] task.decoder.label_smoothing.params_init.method : 'xavier'\n",
      "I0710 07:19:13.613911 140310643746176 base_runner.py:59] task.decoder.label_smoothing.params_init.scale : 1.000001\n",
      "I0710 07:19:13.613951 140310643746176 base_runner.py:59] task.decoder.label_smoothing.params_init.seed : NoneType\n",
      "I0710 07:19:13.614007 140310643746176 base_runner.py:59] task.decoder.label_smoothing.random_seed : NoneType\n",
      "I0710 07:19:13.614045 140310643746176 base_runner.py:59] task.decoder.label_smoothing.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.614082 140310643746176 base_runner.py:59] task.decoder.label_smoothing.token_id_uncertainty_larger : NoneType\n",
      "I0710 07:19:13.614119 140310643746176 base_runner.py:59] task.decoder.label_smoothing.uncertainty : 0.1\n",
      "I0710 07:19:13.614156 140310643746176 base_runner.py:59] task.decoder.label_smoothing.uncertainty_larger : 0.1\n",
      "I0710 07:19:13.614193 140310643746176 base_runner.py:59] task.decoder.label_smoothing.vn.global_vn : False\n",
      "I0710 07:19:13.614249 140310643746176 base_runner.py:59] task.decoder.label_smoothing.vn.per_step_vn : False\n",
      "I0710 07:19:13.614288 140310643746176 base_runner.py:59] task.decoder.label_smoothing.vn.scale : NoneType\n",
      "I0710 07:19:13.614326 140310643746176 base_runner.py:59] task.decoder.label_smoothing.vn.seed : NoneType\n",
      "I0710 07:19:13.614377 140310643746176 base_runner.py:59] task.decoder.name : ''\n",
      "I0710 07:19:13.614413 140310643746176 base_runner.py:59] task.decoder.packed_input : False\n",
      "I0710 07:19:13.614451 140310643746176 base_runner.py:59] task.decoder.params_init.method : 'xavier'\n",
      "I0710 07:19:13.614488 140310643746176 base_runner.py:59] task.decoder.params_init.scale : 1.000001\n",
      "I0710 07:19:13.614526 140310643746176 base_runner.py:59] task.decoder.params_init.seed : NoneType\n",
      "I0710 07:19:13.614565 140310643746176 base_runner.py:59] task.decoder.per_example_tensors : False\n",
      "I0710 07:19:13.614602 140310643746176 base_runner.py:59] task.decoder.per_word_avg_loss : False\n",
      "I0710 07:19:13.614639 140310643746176 base_runner.py:59] task.decoder.qdomain.default : NoneType\n",
      "I0710 07:19:13.614676 140310643746176 base_runner.py:59] task.decoder.qlogsoftmax_range_min : -10.0\n",
      "I0710 07:19:13.614713 140310643746176 base_runner.py:59] task.decoder.random_seed : NoneType\n",
      "I0710 07:19:13.614751 140310643746176 base_runner.py:59] task.decoder.residual_start : 2\n",
      "I0710 07:19:13.614788 140310643746176 base_runner.py:59] task.decoder.rnn_cell_dim : 1024\n",
      "I0710 07:19:13.614825 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.614862 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.apply_pruning : False\n",
      "I0710 07:19:13.614899 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.apply_pruning_to_projection : False\n",
      "I0710 07:19:13.614936 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.bias_init.method : 'constant'\n",
      "I0710 07:19:13.614974 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.bias_init.scale : 0.0\n",
      "I0710 07:19:13.615015 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.bias_init.seed : 0\n",
      "I0710 07:19:13.615053 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.cell_value_cap : 10.0\n",
      "I0710 07:19:13.615090 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.cls : type/lingvo.core.rnn_cell/LayerNormalizedLSTMCellSimple\n",
      "I0710 07:19:13.615128 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.couple_input_forget_gates : False\n",
      "I0710 07:19:13.615165 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.dtype : float32\n",
      "I0710 07:19:13.615203 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.enable_lstm_bias : True\n",
      "I0710 07:19:13.615240 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.forget_gate_bias : 0.0\n",
      "I0710 07:19:13.615278 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.fprop_dtype : NoneType\n",
      "I0710 07:19:13.615315 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.gradient_pruning : False\n",
      "I0710 07:19:13.615363 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.inference_driver_name : NoneType\n",
      "I0710 07:19:13.615415 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.inputs_arity : 1\n",
      "I0710 07:19:13.615453 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.is_inference : NoneType\n",
      "I0710 07:19:13.615489 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.layer_norm_epsilon : 1e-08\n",
      "I0710 07:19:13.615527 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.name : ''\n",
      "I0710 07:19:13.615564 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.num_hidden_nodes : 0\n",
      "I0710 07:19:13.615602 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.num_input_nodes : 0\n",
      "I0710 07:19:13.615639 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.num_output_nodes : 1024\n",
      "I0710 07:19:13.615676 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.output_nonlinearity : False\n",
      "I0710 07:19:13.615713 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.params_init.method : 'uniform'\n",
      "I0710 07:19:13.615750 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.params_init.scale : 0.04\n",
      "I0710 07:19:13.615787 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.params_init.seed : NoneType\n",
      "I0710 07:19:13.615824 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.qdomain.c_state : NoneType\n",
      "I0710 07:19:13.615861 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.qdomain.default : NoneType\n",
      "I0710 07:19:13.615909 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.qdomain.fullyconnected : NoneType\n",
      "I0710 07:19:13.615944 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.qdomain.m_state : NoneType\n",
      "I0710 07:19:13.615978 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.qdomain.weight : NoneType\n",
      "I0710 07:19:13.616018 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.random_seed : NoneType\n",
      "I0710 07:19:13.616054 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.reset_cell_state : False\n",
      "I0710 07:19:13.616089 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.616124 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.vn.global_vn : False\n",
      "I0710 07:19:13.616158 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.vn.per_step_vn : False\n",
      "I0710 07:19:13.616193 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.vn.scale : NoneType\n",
      "I0710 07:19:13.616228 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.vn.seed : NoneType\n",
      "I0710 07:19:13.616262 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.zero_state_init_params.method : 'zeros'\n",
      "I0710 07:19:13.616297 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.zero_state_init_params.seed : NoneType\n",
      "I0710 07:19:13.616332 140310643746176 base_runner.py:59] task.decoder.rnn_cell_tpl.zo_prob : 0.0\n",
      "I0710 07:19:13.616366 140310643746176 base_runner.py:59] task.decoder.rnn_layers : 8\n",
      "I0710 07:19:13.616401 140310643746176 base_runner.py:59] task.decoder.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.616435 140310643746176 base_runner.py:59] task.decoder.softmax.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.616470 140310643746176 base_runner.py:59] task.decoder.softmax.apply_pruning : False\n",
      "I0710 07:19:13.616504 140310643746176 base_runner.py:59] task.decoder.softmax.chunk_size : 0\n",
      "I0710 07:19:13.616539 140310643746176 base_runner.py:59] task.decoder.softmax.cls : type/lingvo.core.layers/SimpleFullSoftmax\n",
      "I0710 07:19:13.616574 140310643746176 base_runner.py:59] task.decoder.softmax.dtype : float32\n",
      "I0710 07:19:13.616608 140310643746176 base_runner.py:59] task.decoder.softmax.fprop_dtype : NoneType\n",
      "I0710 07:19:13.616643 140310643746176 base_runner.py:59] task.decoder.softmax.inference_driver_name : NoneType\n",
      "I0710 07:19:13.616677 140310643746176 base_runner.py:59] task.decoder.softmax.input_dim : 0\n",
      "I0710 07:19:13.616712 140310643746176 base_runner.py:59] task.decoder.softmax.is_inference : NoneType\n",
      "I0710 07:19:13.616756 140310643746176 base_runner.py:59] task.decoder.softmax.logits_abs_max : NoneType\n",
      "I0710 07:19:13.616792 140310643746176 base_runner.py:59] task.decoder.softmax.name : ''\n",
      "I0710 07:19:13.616827 140310643746176 base_runner.py:59] task.decoder.softmax.num_classes : 16000\n",
      "I0710 07:19:13.616862 140310643746176 base_runner.py:59] task.decoder.softmax.num_sampled : 0\n",
      "I0710 07:19:13.616897 140310643746176 base_runner.py:59] task.decoder.softmax.num_shards : 16\n",
      "I0710 07:19:13.616936 140310643746176 base_runner.py:59] task.decoder.softmax.params_init.method : 'uniform'\n",
      "I0710 07:19:13.616971 140310643746176 base_runner.py:59] task.decoder.softmax.params_init.scale : 0.04\n",
      "I0710 07:19:13.617021 140310643746176 base_runner.py:59] task.decoder.softmax.params_init.seed : NoneType\n",
      "I0710 07:19:13.617054 140310643746176 base_runner.py:59] task.decoder.softmax.qdomain.default : NoneType\n",
      "I0710 07:19:13.617084 140310643746176 base_runner.py:59] task.decoder.softmax.random_seed : NoneType\n",
      "I0710 07:19:13.617115 140310643746176 base_runner.py:59] task.decoder.softmax.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.617146 140310643746176 base_runner.py:59] task.decoder.softmax.use_num_classes_major_weight : False\n",
      "I0710 07:19:13.617185 140310643746176 base_runner.py:59] task.decoder.softmax.vn.global_vn : False\n",
      "I0710 07:19:13.617220 140310643746176 base_runner.py:59] task.decoder.softmax.vn.per_step_vn : False\n",
      "I0710 07:19:13.617254 140310643746176 base_runner.py:59] task.decoder.softmax.vn.scale : 1.0\n",
      "I0710 07:19:13.617308 140310643746176 base_runner.py:59] task.decoder.softmax.vn.seed : NoneType\n",
      "I0710 07:19:13.617345 140310643746176 base_runner.py:59] task.decoder.source_dim : 1024\n",
      "I0710 07:19:13.617386 140310643746176 base_runner.py:59] task.decoder.target_eos_id : 2\n",
      "I0710 07:19:13.617423 140310643746176 base_runner.py:59] task.decoder.target_seq_len : 300\n",
      "I0710 07:19:13.617460 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.617497 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.cls : type/lingvo.core.target_sequence_sampler/TargetSequenceSampler\n",
      "I0710 07:19:13.617534 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.dtype : float32\n",
      "I0710 07:19:13.617571 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.fprop_dtype : NoneType\n",
      "I0710 07:19:13.617609 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.inference_driver_name : NoneType\n",
      "I0710 07:19:13.617646 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.is_inference : NoneType\n",
      "I0710 07:19:13.617683 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.name : 'target_sequence_sampler'\n",
      "I0710 07:19:13.617720 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.params_init.method : 'xavier'\n",
      "I0710 07:19:13.617757 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.params_init.scale : 1.000001\n",
      "I0710 07:19:13.617794 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.params_init.seed : NoneType\n",
      "I0710 07:19:13.617831 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.random_seed : NoneType\n",
      "I0710 07:19:13.617868 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.617920 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.target_eoc_id : -1\n",
      "I0710 07:19:13.617959 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.target_eos_id : 2\n",
      "I0710 07:19:13.617996 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.target_seq_len : 0\n",
      "I0710 07:19:13.618038 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.target_sos_id : 1\n",
      "I0710 07:19:13.618076 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.temperature : 1.0\n",
      "I0710 07:19:13.618113 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.vn.global_vn : False\n",
      "I0710 07:19:13.618150 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.vn.per_step_vn : False\n",
      "I0710 07:19:13.618187 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.vn.scale : NoneType\n",
      "I0710 07:19:13.618224 140310643746176 base_runner.py:59] task.decoder.target_sequence_sampler.vn.seed : NoneType\n",
      "I0710 07:19:13.618261 140310643746176 base_runner.py:59] task.decoder.target_sos_id : 1\n",
      "I0710 07:19:13.618298 140310643746176 base_runner.py:59] task.decoder.unidi_rnn_type : 'func'\n",
      "I0710 07:19:13.618335 140310643746176 base_runner.py:59] task.decoder.use_prev_atten_ctx : False\n",
      "I0710 07:19:13.618373 140310643746176 base_runner.py:59] task.decoder.use_zero_atten_state : False\n",
      "I0710 07:19:13.618411 140310643746176 base_runner.py:59] task.decoder.vn.global_vn : False\n",
      "I0710 07:19:13.618447 140310643746176 base_runner.py:59] task.decoder.vn.per_step_vn : False\n",
      "I0710 07:19:13.618497 140310643746176 base_runner.py:59] task.decoder.vn.scale : NoneType\n",
      "I0710 07:19:13.618598 140310643746176 base_runner.py:59] task.decoder.vn.seed : NoneType\n",
      "I0710 07:19:13.618635 140310643746176 base_runner.py:59] task.dtype : float32\n",
      "I0710 07:19:13.618673 140310643746176 base_runner.py:59] task.encoder.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.618710 140310643746176 base_runner.py:59] task.encoder.bidi_rnn_type : 'func'\n",
      "I0710 07:19:13.618747 140310643746176 base_runner.py:59] task.encoder.cc_schedule : NoneType\n",
      "I0710 07:19:13.618784 140310643746176 base_runner.py:59] task.encoder.cls : type/lingvo.tasks.mt.encoder/MTEncoderBiRNN\n",
      "I0710 07:19:13.618821 140310643746176 base_runner.py:59] task.encoder.dropout_prob : 0.3\n",
      "I0710 07:19:13.618859 140310643746176 base_runner.py:59] task.encoder.dtype : float32\n",
      "I0710 07:19:13.618896 140310643746176 base_runner.py:59] task.encoder.emb.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.618933 140310643746176 base_runner.py:59] task.encoder.emb.cls : type/lingvo.core.layers/EmbeddingLayer\n",
      "I0710 07:19:13.618970 140310643746176 base_runner.py:59] task.encoder.emb.dtype : float32\n",
      "I0710 07:19:13.619012 140310643746176 base_runner.py:59] task.encoder.emb.embedding_dim : 1024\n",
      "I0710 07:19:13.619050 140310643746176 base_runner.py:59] task.encoder.emb.fprop_dtype : NoneType\n",
      "I0710 07:19:13.619087 140310643746176 base_runner.py:59] task.encoder.emb.inference_driver_name : NoneType\n",
      "I0710 07:19:13.619124 140310643746176 base_runner.py:59] task.encoder.emb.is_inference : NoneType\n",
      "I0710 07:19:13.619161 140310643746176 base_runner.py:59] task.encoder.emb.max_num_shards : 16\n",
      "I0710 07:19:13.619198 140310643746176 base_runner.py:59] task.encoder.emb.name : ''\n",
      "I0710 07:19:13.619235 140310643746176 base_runner.py:59] task.encoder.emb.on_ps : True\n",
      "I0710 07:19:13.619272 140310643746176 base_runner.py:59] task.encoder.emb.params_init.method : 'uniform'\n",
      "I0710 07:19:13.619309 140310643746176 base_runner.py:59] task.encoder.emb.params_init.scale : 0.04\n",
      "I0710 07:19:13.619346 140310643746176 base_runner.py:59] task.encoder.emb.params_init.seed : NoneType\n",
      "I0710 07:19:13.619384 140310643746176 base_runner.py:59] task.encoder.emb.random_seed : NoneType\n",
      "I0710 07:19:13.619421 140310643746176 base_runner.py:59] task.encoder.emb.scale_sqrt_depth : False\n",
      "I0710 07:19:13.619457 140310643746176 base_runner.py:59] task.encoder.emb.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.619494 140310643746176 base_runner.py:59] task.encoder.emb.vn.global_vn : False\n",
      "I0710 07:19:13.619530 140310643746176 base_runner.py:59] task.encoder.emb.vn.per_step_vn : False\n",
      "I0710 07:19:13.619567 140310643746176 base_runner.py:59] task.encoder.emb.vn.scale : 1.0\n",
      "I0710 07:19:13.619604 140310643746176 base_runner.py:59] task.encoder.emb.vn.seed : NoneType\n",
      "I0710 07:19:13.619641 140310643746176 base_runner.py:59] task.encoder.emb.vocab_size : 16000\n",
      "I0710 07:19:13.619678 140310643746176 base_runner.py:59] task.encoder.encoder_out_dim : 1024\n",
      "I0710 07:19:13.619714 140310643746176 base_runner.py:59] task.encoder.fprop_dtype : NoneType\n",
      "I0710 07:19:13.619751 140310643746176 base_runner.py:59] task.encoder.inference_driver_name : NoneType\n",
      "I0710 07:19:13.619788 140310643746176 base_runner.py:59] task.encoder.is_inference : NoneType\n",
      "I0710 07:19:13.619824 140310643746176 base_runner.py:59] task.encoder.is_transparent : False\n",
      "I0710 07:19:13.619861 140310643746176 base_runner.py:59] task.encoder.lstm_cell_size : 1024\n",
      "I0710 07:19:13.619898 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.619935 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.apply_pruning : False\n",
      "I0710 07:19:13.619971 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.apply_pruning_to_projection : False\n",
      "I0710 07:19:13.620014 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.bias_init.method : 'constant'\n",
      "I0710 07:19:13.620053 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.bias_init.scale : 0.0\n",
      "I0710 07:19:13.620090 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.bias_init.seed : 0\n",
      "I0710 07:19:13.620127 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.cell_value_cap : 10.0\n",
      "I0710 07:19:13.620164 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.cls : type/lingvo.core.rnn_cell/LayerNormalizedLSTMCellSimple\n",
      "I0710 07:19:13.620202 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.couple_input_forget_gates : False\n",
      "I0710 07:19:13.620239 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.dtype : float32\n",
      "I0710 07:19:13.620276 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.enable_lstm_bias : True\n",
      "I0710 07:19:13.620313 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.forget_gate_bias : 0.0\n",
      "I0710 07:19:13.620351 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.fprop_dtype : NoneType\n",
      "I0710 07:19:13.620388 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.gradient_pruning : False\n",
      "I0710 07:19:13.620425 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.inference_driver_name : NoneType\n",
      "I0710 07:19:13.620463 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.inputs_arity : 1\n",
      "I0710 07:19:13.620500 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.is_inference : NoneType\n",
      "I0710 07:19:13.620537 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.layer_norm_epsilon : 1e-08\n",
      "I0710 07:19:13.620574 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.name : ''\n",
      "I0710 07:19:13.620611 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.num_hidden_nodes : 0\n",
      "I0710 07:19:13.620648 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.num_input_nodes : 0\n",
      "I0710 07:19:13.620686 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.num_output_nodes : 1024\n",
      "I0710 07:19:13.620723 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.output_nonlinearity : False\n",
      "I0710 07:19:13.620772 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.params_init.method : 'uniform'\n",
      "I0710 07:19:13.620810 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.params_init.scale : 0.04\n",
      "I0710 07:19:13.620857 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.params_init.seed : NoneType\n",
      "I0710 07:19:13.620896 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.qdomain.c_state : NoneType\n",
      "I0710 07:19:13.620934 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.qdomain.default : NoneType\n",
      "I0710 07:19:13.620971 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.qdomain.fullyconnected : NoneType\n",
      "I0710 07:19:13.621020 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.qdomain.m_state : NoneType\n",
      "I0710 07:19:13.621059 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.qdomain.weight : NoneType\n",
      "I0710 07:19:13.621096 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.random_seed : NoneType\n",
      "I0710 07:19:13.621133 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.reset_cell_state : False\n",
      "I0710 07:19:13.621171 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.621208 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.vn.global_vn : False\n",
      "I0710 07:19:13.621243 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.vn.per_step_vn : False\n",
      "I0710 07:19:13.621282 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.vn.scale : NoneType\n",
      "I0710 07:19:13.621320 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.vn.seed : NoneType\n",
      "I0710 07:19:13.621357 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.zero_state_init_params.method : 'zeros'\n",
      "I0710 07:19:13.621396 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.zero_state_init_params.seed : NoneType\n",
      "I0710 07:19:13.621433 140310643746176 base_runner.py:59] task.encoder.lstm_tpl.zo_prob : 0.0\n",
      "I0710 07:19:13.621471 140310643746176 base_runner.py:59] task.encoder.name : ''\n",
      "I0710 07:19:13.621509 140310643746176 base_runner.py:59] task.encoder.num_lstm_layers : 6\n",
      "I0710 07:19:13.621546 140310643746176 base_runner.py:59] task.encoder.packed_input : False\n",
      "I0710 07:19:13.621583 140310643746176 base_runner.py:59] task.encoder.params_init.method : 'xavier'\n",
      "I0710 07:19:13.621620 140310643746176 base_runner.py:59] task.encoder.params_init.scale : 1.000001\n",
      "I0710 07:19:13.621658 140310643746176 base_runner.py:59] task.encoder.params_init.seed : NoneType\n",
      "I0710 07:19:13.621695 140310643746176 base_runner.py:59] task.encoder.proj_tpl.activation : 'RELU'\n",
      "I0710 07:19:13.621732 140310643746176 base_runner.py:59] task.encoder.proj_tpl.affine_last : False\n",
      "I0710 07:19:13.621769 140310643746176 base_runner.py:59] task.encoder.proj_tpl.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.621807 140310643746176 base_runner.py:59] task.encoder.proj_tpl.apply_pruning : False\n",
      "I0710 07:19:13.621844 140310643746176 base_runner.py:59] task.encoder.proj_tpl.batch_norm : NoneType\n",
      "I0710 07:19:13.621881 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bias_init : 0.0\n",
      "I0710 07:19:13.621933 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_fold_weights : NoneType\n",
      "I0710 07:19:13.621971 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.add_stats_to_moving_average_variables : NoneType\n",
      "I0710 07:19:13.622025 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.622066 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.cls : type/lingvo.core.bn_layers/BatchNormLayer\n",
      "I0710 07:19:13.622103 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.decay : 0.999\n",
      "I0710 07:19:13.622141 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.dim : 0\n",
      "I0710 07:19:13.622179 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.dtype : float32\n",
      "I0710 07:19:13.622216 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.enable_cross_replica_sum_on_tpu : True\n",
      "I0710 07:19:13.622254 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.fprop_dtype : NoneType\n",
      "I0710 07:19:13.622291 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.gamma_zero_init : False\n",
      "I0710 07:19:13.622329 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.inference_driver_name : NoneType\n",
      "I0710 07:19:13.622366 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.is_inference : NoneType\n",
      "I0710 07:19:13.622403 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.name : ''\n",
      "I0710 07:19:13.622440 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.params_init.method : 'xavier'\n",
      "I0710 07:19:13.622477 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.params_init.scale : 1.000001\n",
      "I0710 07:19:13.622514 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.params_init.seed : NoneType\n",
      "I0710 07:19:13.622551 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.random_seed : NoneType\n",
      "I0710 07:19:13.622588 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.set_padded_output_to_zero : True\n",
      "I0710 07:19:13.622625 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.622661 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.use_fused_batch_norm_for_eval : False\n",
      "I0710 07:19:13.622698 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.use_moving_avg_in_training : False\n",
      "I0710 07:19:13.622740 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.vn.global_vn : False\n",
      "I0710 07:19:13.622776 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.vn.per_step_vn : False\n",
      "I0710 07:19:13.622813 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.vn.scale : NoneType\n",
      "I0710 07:19:13.622872 140310643746176 base_runner.py:59] task.encoder.proj_tpl.bn_params.vn.seed : NoneType\n",
      "I0710 07:19:13.622910 140310643746176 base_runner.py:59] task.encoder.proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer\n",
      "I0710 07:19:13.622949 140310643746176 base_runner.py:59] task.encoder.proj_tpl.dtype : float32\n",
      "I0710 07:19:13.622986 140310643746176 base_runner.py:59] task.encoder.proj_tpl.fprop_dtype : NoneType\n",
      "I0710 07:19:13.623030 140310643746176 base_runner.py:59] task.encoder.proj_tpl.has_bias : False\n",
      "I0710 07:19:13.623069 140310643746176 base_runner.py:59] task.encoder.proj_tpl.inference_driver_name : NoneType\n",
      "I0710 07:19:13.623108 140310643746176 base_runner.py:59] task.encoder.proj_tpl.input_dim : 0\n",
      "I0710 07:19:13.623147 140310643746176 base_runner.py:59] task.encoder.proj_tpl.is_inference : NoneType\n",
      "I0710 07:19:13.623184 140310643746176 base_runner.py:59] task.encoder.proj_tpl.name : ''\n",
      "I0710 07:19:13.623223 140310643746176 base_runner.py:59] task.encoder.proj_tpl.output_dim : 0\n",
      "I0710 07:19:13.623261 140310643746176 base_runner.py:59] task.encoder.proj_tpl.params_init.method : 'xavier'\n",
      "I0710 07:19:13.623299 140310643746176 base_runner.py:59] task.encoder.proj_tpl.params_init.scale : 1.000001\n",
      "I0710 07:19:13.623337 140310643746176 base_runner.py:59] task.encoder.proj_tpl.params_init.seed : NoneType\n",
      "I0710 07:19:13.623375 140310643746176 base_runner.py:59] task.encoder.proj_tpl.qdomain.default : NoneType\n",
      "I0710 07:19:13.623413 140310643746176 base_runner.py:59] task.encoder.proj_tpl.random_seed : NoneType\n",
      "I0710 07:19:13.623451 140310643746176 base_runner.py:59] task.encoder.proj_tpl.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.623490 140310643746176 base_runner.py:59] task.encoder.proj_tpl.use_einsum : False\n",
      "I0710 07:19:13.623528 140310643746176 base_runner.py:59] task.encoder.proj_tpl.vn.global_vn : False\n",
      "I0710 07:19:13.623567 140310643746176 base_runner.py:59] task.encoder.proj_tpl.vn.per_step_vn : False\n",
      "I0710 07:19:13.623605 140310643746176 base_runner.py:59] task.encoder.proj_tpl.vn.scale : NoneType\n",
      "I0710 07:19:13.623644 140310643746176 base_runner.py:59] task.encoder.proj_tpl.vn.seed : NoneType\n",
      "I0710 07:19:13.623682 140310643746176 base_runner.py:59] task.encoder.proj_tpl.weight_norm : False\n",
      "I0710 07:19:13.623721 140310643746176 base_runner.py:59] task.encoder.random_seed : NoneType\n",
      "I0710 07:19:13.623759 140310643746176 base_runner.py:59] task.encoder.residual_start : 2\n",
      "I0710 07:19:13.623798 140310643746176 base_runner.py:59] task.encoder.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.623836 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.add_weight_summaries : True\n",
      "I0710 07:19:13.623875 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.623913 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.cls : type/lingvo.core.layers/WeightedSumLayer\n",
      "I0710 07:19:13.623951 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.dtype : float32\n",
      "I0710 07:19:13.623990 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.fprop_dtype : NoneType\n",
      "I0710 07:19:13.624035 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.global_weight_scale : 1.0\n",
      "I0710 07:19:13.624074 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.inference_driver_name : NoneType\n",
      "I0710 07:19:13.624112 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.is_inference : NoneType\n",
      "I0710 07:19:13.624151 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.minimal_prob : 0.0\n",
      "I0710 07:19:13.624190 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.name : ''\n",
      "I0710 07:19:13.624229 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.num_sources : 0\n",
      "I0710 07:19:13.624268 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.params_init.method : 'xavier'\n",
      "I0710 07:19:13.624306 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.params_init.scale : 1.000001\n",
      "I0710 07:19:13.624345 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.params_init.seed : NoneType\n",
      "I0710 07:19:13.624383 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.random_seed : NoneType\n",
      "I0710 07:19:13.624421 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.624460 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.vn.global_vn : False\n",
      "I0710 07:19:13.624497 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.vn.per_step_vn : False\n",
      "I0710 07:19:13.624536 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.vn.scale : NoneType\n",
      "I0710 07:19:13.624575 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.vn.seed : NoneType\n",
      "I0710 07:19:13.624613 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.weighted_merger_dropout_prob : 0.1\n",
      "I0710 07:19:13.624651 140310643746176 base_runner.py:59] task.encoder.transparent_merger_tpl.weighted_merger_softmax : True\n",
      "I0710 07:19:13.624690 140310643746176 base_runner.py:59] task.encoder.vn.global_vn : False\n",
      "I0710 07:19:13.624729 140310643746176 base_runner.py:59] task.encoder.vn.per_step_vn : False\n",
      "I0710 07:19:13.624767 140310643746176 base_runner.py:59] task.encoder.vn.scale : NoneType\n",
      "I0710 07:19:13.624806 140310643746176 base_runner.py:59] task.encoder.vn.seed : NoneType\n",
      "I0710 07:19:13.624845 140310643746176 base_runner.py:59] task.eval.decoder_samples_per_summary : 0\n",
      "I0710 07:19:13.624883 140310643746176 base_runner.py:59] task.eval.load_checkpoint_from : NoneType\n",
      "I0710 07:19:13.624922 140310643746176 base_runner.py:59] task.eval.samples_per_summary : 2466\n",
      "I0710 07:19:13.624960 140310643746176 base_runner.py:59] task.eval.start_decoder_after : 0\n",
      "I0710 07:19:13.625003 140310643746176 base_runner.py:59] task.eval.start_eval_after : 0\n",
      "I0710 07:19:13.625043 140310643746176 base_runner.py:59] task.fprop_dtype : NoneType\n",
      "I0710 07:19:13.625082 140310643746176 base_runner.py:59] task.inference_driver_name : NoneType\n",
      "I0710 07:19:13.625120 140310643746176 base_runner.py:59] task.input : NoneType\n",
      "I0710 07:19:13.625158 140310643746176 base_runner.py:59] task.is_inference : NoneType\n",
      "I0710 07:19:13.625196 140310643746176 base_runner.py:59] task.name : 'punctuator_rnmt'\n",
      "I0710 07:19:13.625234 140310643746176 base_runner.py:59] task.online_encoder : NoneType\n",
      "I0710 07:19:13.625273 140310643746176 base_runner.py:59] task.params_init.method : 'xavier'\n",
      "I0710 07:19:13.625312 140310643746176 base_runner.py:59] task.params_init.scale : 1.000001\n",
      "I0710 07:19:13.625350 140310643746176 base_runner.py:59] task.params_init.seed : NoneType\n",
      "I0710 07:19:13.625388 140310643746176 base_runner.py:59] task.random_seed : NoneType\n",
      "I0710 07:19:13.625427 140310643746176 base_runner.py:59] task.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.625465 140310643746176 base_runner.py:59] task.train.bprop_variable_exclusion : NoneType\n",
      "I0710 07:19:13.625503 140310643746176 base_runner.py:59] task.train.bprop_variable_filter : NoneType\n",
      "I0710 07:19:13.625541 140310643746176 base_runner.py:59] task.train.clip_gradient_norm_to_value : 0.0\n",
      "I0710 07:19:13.625581 140310643746176 base_runner.py:59] task.train.clip_gradient_single_norm_to_value : 0.0\n",
      "I0710 07:19:13.625619 140310643746176 base_runner.py:59] task.train.colocate_gradients_with_ops : True\n",
      "I0710 07:19:13.625657 140310643746176 base_runner.py:59] task.train.early_stop.metric_history.jobname : 'eval_dev'\n",
      "I0710 07:19:13.625695 140310643746176 base_runner.py:59] task.train.early_stop.metric_history.local_filesystem : False\n",
      "I0710 07:19:13.625733 140310643746176 base_runner.py:59] task.train.early_stop.metric_history.logdir : ''\n",
      "I0710 07:19:13.625772 140310643746176 base_runner.py:59] task.train.early_stop.metric_history.metric : 'log_pplx'\n",
      "I0710 07:19:13.625811 140310643746176 base_runner.py:59] task.train.early_stop.metric_history.minimize : True\n",
      "I0710 07:19:13.625849 140310643746176 base_runner.py:59] task.train.early_stop.metric_history.name : 'MetricHistory'\n",
      "I0710 07:19:13.625888 140310643746176 base_runner.py:59] task.train.early_stop.metric_history.tfevent_file : False\n",
      "I0710 07:19:13.625938 140310643746176 base_runner.py:59] task.train.early_stop.min_steps : 0\n",
      "I0710 07:19:13.625977 140310643746176 base_runner.py:59] task.train.early_stop.name : 'EarlyStop'\n",
      "I0710 07:19:13.626021 140310643746176 base_runner.py:59] task.train.early_stop.tolerance : 0.0\n",
      "I0710 07:19:13.626060 140310643746176 base_runner.py:59] task.train.early_stop.verbose : True\n",
      "I0710 07:19:13.626098 140310643746176 base_runner.py:59] task.train.early_stop.window : 0\n",
      "I0710 07:19:13.626137 140310643746176 base_runner.py:59] task.train.ema_decay : 0.0\n",
      "I0710 07:19:13.626175 140310643746176 base_runner.py:59] task.train.ema_decay_moving_vars : NoneType\n",
      "I0710 07:19:13.626213 140310643746176 base_runner.py:59] task.train.enqueue_max_steps : -1\n",
      "I0710 07:19:13.626251 140310643746176 base_runner.py:59] task.train.gate_gradients : False\n",
      "I0710 07:19:13.626289 140310643746176 base_runner.py:59] task.train.grad_aggregation_method : 1\n",
      "I0710 07:19:13.626327 140310643746176 base_runner.py:59] task.train.grad_norm_to_clip_to_zero : 100000.0\n",
      "I0710 07:19:13.626366 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.626404 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.clip_threshold : 4.0\n",
      "I0710 07:19:13.626442 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.cls : type/lingvo.core.layers/GradNormTracker\n",
      "I0710 07:19:13.626480 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.decay : 0.995\n",
      "I0710 07:19:13.626519 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.dtype : float32\n",
      "I0710 07:19:13.626557 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.fprop_dtype : NoneType\n",
      "I0710 07:19:13.626596 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.grad_norm_clip_cap_min : 0.0\n",
      "I0710 07:19:13.626634 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.grad_norm_lower_cap : 0.01\n",
      "I0710 07:19:13.626672 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.inference_driver_name : NoneType\n",
      "I0710 07:19:13.626710 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.is_inference : NoneType\n",
      "I0710 07:19:13.626748 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.name : 'gradient_norm_tracker'\n",
      "I0710 07:19:13.626786 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.params_init.method : 'xavier'\n",
      "I0710 07:19:13.626830 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.params_init.scale : 1.000001\n",
      "I0710 07:19:13.626868 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.params_init.seed : NoneType\n",
      "I0710 07:19:13.626907 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.random_seed : NoneType\n",
      "I0710 07:19:13.626945 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.626983 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.vn.global_vn : False\n",
      "I0710 07:19:13.627028 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.vn.per_step_vn : False\n",
      "I0710 07:19:13.627067 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.vn.scale : NoneType\n",
      "I0710 07:19:13.627106 140310643746176 base_runner.py:59] task.train.grad_norm_tracker.vn.seed : NoneType\n",
      "I0710 07:19:13.627145 140310643746176 base_runner.py:59] task.train.init_from_checkpoint_rules : {}\n",
      "I0710 07:19:13.627183 140310643746176 base_runner.py:59] task.train.l1_regularizer_weight : NoneType\n",
      "I0710 07:19:13.627222 140310643746176 base_runner.py:59] task.train.l2_regularizer_weight : 1e-05\n",
      "I0710 07:19:13.627260 140310643746176 base_runner.py:59] task.train.learner : NoneType\n",
      "I0710 07:19:13.627298 140310643746176 base_runner.py:59] task.train.learning_rate : 0.0001\n",
      "I0710 07:19:13.627337 140310643746176 base_runner.py:59] task.train.lr_schedule.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.627376 140310643746176 base_runner.py:59] task.train.lr_schedule.cls : type/lingvo.core.schedule/LinearRampupExponentialDecayScaledByNumSplitSchedule\n",
      "I0710 07:19:13.627415 140310643746176 base_runner.py:59] task.train.lr_schedule.decay_end : 1200000\n",
      "I0710 07:19:13.627454 140310643746176 base_runner.py:59] task.train.lr_schedule.decay_start : 400000\n",
      "I0710 07:19:13.627492 140310643746176 base_runner.py:59] task.train.lr_schedule.dtype : float32\n",
      "I0710 07:19:13.627530 140310643746176 base_runner.py:59] task.train.lr_schedule.fprop_dtype : NoneType\n",
      "I0710 07:19:13.627569 140310643746176 base_runner.py:59] task.train.lr_schedule.inference_driver_name : NoneType\n",
      "I0710 07:19:13.627609 140310643746176 base_runner.py:59] task.train.lr_schedule.is_inference : NoneType\n",
      "I0710 07:19:13.627648 140310643746176 base_runner.py:59] task.train.lr_schedule.max : 100000000.0\n",
      "I0710 07:19:13.627686 140310643746176 base_runner.py:59] task.train.lr_schedule.min : 0.5\n",
      "I0710 07:19:13.627724 140310643746176 base_runner.py:59] task.train.lr_schedule.name : 'LRSched'\n",
      "I0710 07:19:13.627763 140310643746176 base_runner.py:59] task.train.lr_schedule.num_splits : 0\n",
      "I0710 07:19:13.627801 140310643746176 base_runner.py:59] task.train.lr_schedule.params_init.method : 'xavier'\n",
      "I0710 07:19:13.627840 140310643746176 base_runner.py:59] task.train.lr_schedule.params_init.scale : 1.000001\n",
      "I0710 07:19:13.627878 140310643746176 base_runner.py:59] task.train.lr_schedule.params_init.seed : NoneType\n",
      "I0710 07:19:13.627916 140310643746176 base_runner.py:59] task.train.lr_schedule.random_seed : NoneType\n",
      "I0710 07:19:13.627954 140310643746176 base_runner.py:59] task.train.lr_schedule.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.627993 140310643746176 base_runner.py:59] task.train.lr_schedule.vn.global_vn : False\n",
      "I0710 07:19:13.628037 140310643746176 base_runner.py:59] task.train.lr_schedule.vn.per_step_vn : False\n",
      "I0710 07:19:13.628076 140310643746176 base_runner.py:59] task.train.lr_schedule.vn.scale : NoneType\n",
      "I0710 07:19:13.628115 140310643746176 base_runner.py:59] task.train.lr_schedule.vn.seed : NoneType\n",
      "I0710 07:19:13.628154 140310643746176 base_runner.py:59] task.train.lr_schedule.warmup : 500\n",
      "I0710 07:19:13.628192 140310643746176 base_runner.py:59] task.train.lr_schedule.warmup_init : 1.0\n",
      "I0710 07:19:13.628231 140310643746176 base_runner.py:59] task.train.max_steps : 4000000\n",
      "I0710 07:19:13.628269 140310643746176 base_runner.py:59] task.train.optimizer.allow_implicit_capture : NoneType\n",
      "I0710 07:19:13.628308 140310643746176 base_runner.py:59] task.train.optimizer.beta1 : 0.9\n",
      "I0710 07:19:13.628346 140310643746176 base_runner.py:59] task.train.optimizer.beta2 : 0.98\n",
      "I0710 07:19:13.628385 140310643746176 base_runner.py:59] task.train.optimizer.cls : type/lingvo.core.optimizer/Adam\n",
      "I0710 07:19:13.628423 140310643746176 base_runner.py:59] task.train.optimizer.dtype : float32\n",
      "I0710 07:19:13.628462 140310643746176 base_runner.py:59] task.train.optimizer.epsilon : 1e-06\n",
      "I0710 07:19:13.628500 140310643746176 base_runner.py:59] task.train.optimizer.fprop_dtype : NoneType\n",
      "I0710 07:19:13.628547 140310643746176 base_runner.py:59] task.train.optimizer.inference_driver_name : NoneType\n",
      "I0710 07:19:13.628586 140310643746176 base_runner.py:59] task.train.optimizer.is_inference : NoneType\n",
      "I0710 07:19:13.628625 140310643746176 base_runner.py:59] task.train.optimizer.name : 'Adam'\n",
      "I0710 07:19:13.628663 140310643746176 base_runner.py:59] task.train.optimizer.params_init.method : 'xavier'\n",
      "I0710 07:19:13.628701 140310643746176 base_runner.py:59] task.train.optimizer.params_init.scale : 1.000001\n",
      "I0710 07:19:13.628739 140310643746176 base_runner.py:59] task.train.optimizer.params_init.seed : NoneType\n",
      "I0710 07:19:13.628777 140310643746176 base_runner.py:59] task.train.optimizer.random_seed : NoneType\n",
      "I0710 07:19:13.628815 140310643746176 base_runner.py:59] task.train.optimizer.skip_lp_regularization : NoneType\n",
      "I0710 07:19:13.628854 140310643746176 base_runner.py:59] task.train.optimizer.use_bf16_gradients_ar : False\n",
      "I0710 07:19:13.628892 140310643746176 base_runner.py:59] task.train.optimizer.vn.global_vn : False\n",
      "I0710 07:19:13.628931 140310643746176 base_runner.py:59] task.train.optimizer.vn.per_step_vn : False\n",
      "I0710 07:19:13.628969 140310643746176 base_runner.py:59] task.train.optimizer.vn.scale : NoneType\n",
      "I0710 07:19:13.629014 140310643746176 base_runner.py:59] task.train.optimizer.vn.seed : NoneType\n",
      "I0710 07:19:13.629053 140310643746176 base_runner.py:59] task.train.pruning_hparams_dict : NoneType\n",
      "I0710 07:19:13.629091 140310643746176 base_runner.py:59] task.train.save_interval_seconds : 600\n",
      "I0710 07:19:13.629130 140310643746176 base_runner.py:59] task.train.save_keep_checkpoint_every_n_hours : 0.5\n",
      "I0710 07:19:13.629169 140310643746176 base_runner.py:59] task.train.save_max_to_keep : 100\n",
      "I0710 07:19:13.629208 140310643746176 base_runner.py:59] task.train.scale_gradients : True\n",
      "I0710 07:19:13.629246 140310643746176 base_runner.py:59] task.train.start_up_delay_steps : 200\n",
      "I0710 07:19:13.629285 140310643746176 base_runner.py:59] task.train.summary_interval_steps : 100\n",
      "I0710 07:19:13.629323 140310643746176 base_runner.py:59] task.train.tpu_steps_per_loop : 100\n",
      "I0710 07:19:13.629361 140310643746176 base_runner.py:59] task.train.vn_start_step : 200000000\n",
      "I0710 07:19:13.629400 140310643746176 base_runner.py:59] task.train.vn_std : 0.0\n",
      "I0710 07:19:13.629438 140310643746176 base_runner.py:59] task.vn.global_vn : False\n",
      "I0710 07:19:13.629477 140310643746176 base_runner.py:59] task.vn.per_step_vn : False\n",
      "I0710 07:19:13.629515 140310643746176 base_runner.py:59] task.vn.scale : NoneType\n",
      "I0710 07:19:13.629554 140310643746176 base_runner.py:59] task.vn.seed : NoneType\n",
      "I0710 07:19:13.629592 140310643746176 base_runner.py:59] train.early_stop.metric_history.jobname : 'eval_dev'\n",
      "I0710 07:19:13.629631 140310643746176 base_runner.py:59] train.early_stop.metric_history.local_filesystem : False\n",
      "I0710 07:19:13.629669 140310643746176 base_runner.py:59] train.early_stop.metric_history.logdir : ''\n",
      "I0710 07:19:13.629719 140310643746176 base_runner.py:59] train.early_stop.metric_history.metric : 'log_pplx'\n",
      "I0710 07:19:13.629756 140310643746176 base_runner.py:59] train.early_stop.metric_history.minimize : True\n",
      "I0710 07:19:13.629793 140310643746176 base_runner.py:59] train.early_stop.metric_history.name : 'MetricHistory'\n",
      "I0710 07:19:13.629830 140310643746176 base_runner.py:59] train.early_stop.metric_history.tfevent_file : False\n",
      "I0710 07:19:13.629867 140310643746176 base_runner.py:59] train.early_stop.min_steps : 0\n",
      "I0710 07:19:13.629916 140310643746176 base_runner.py:59] train.early_stop.name : 'EarlyStop'\n",
      "I0710 07:19:13.629955 140310643746176 base_runner.py:59] train.early_stop.tolerance : 0.0\n",
      "I0710 07:19:13.629992 140310643746176 base_runner.py:59] train.early_stop.verbose : True\n",
      "I0710 07:19:13.630034 140310643746176 base_runner.py:59] train.early_stop.window : 0\n",
      "I0710 07:19:13.630071 140310643746176 base_runner.py:59] train.ema_decay : 0.0\n",
      "I0710 07:19:13.630109 140310643746176 base_runner.py:59] train.ema_decay_moving_vars : NoneType\n",
      "I0710 07:19:13.630146 140310643746176 base_runner.py:59] train.enqueue_max_steps : -1\n",
      "I0710 07:19:13.630183 140310643746176 base_runner.py:59] train.init_from_checkpoint_rules : {}\n",
      "I0710 07:19:13.630220 140310643746176 base_runner.py:59] train.max_steps : 4000000\n",
      "I0710 07:19:13.630257 140310643746176 base_runner.py:59] train.save_interval_seconds : 600\n",
      "I0710 07:19:13.630294 140310643746176 base_runner.py:59] train.save_keep_checkpoint_every_n_hours : 1.0\n",
      "I0710 07:19:13.630330 140310643746176 base_runner.py:59] train.save_max_to_keep : 3\n",
      "I0710 07:19:13.630367 140310643746176 base_runner.py:59] train.start_up_delay_steps : 200\n",
      "I0710 07:19:13.630405 140310643746176 base_runner.py:59] train.summary_interval_steps : 100\n",
      "I0710 07:19:13.630442 140310643746176 base_runner.py:59] train.tpu_steps_per_loop : 100\n",
      "I0710 07:19:13.630479 140310643746176 base_runner.py:59] vn.global_vn : False\n",
      "I0710 07:19:13.630516 140310643746176 base_runner.py:59] vn.per_step_vn : False\n",
      "I0710 07:19:13.630553 140310643746176 base_runner.py:59] vn.scale : NoneType\n",
      "I0710 07:19:13.630590 140310643746176 base_runner.py:59] vn.seed : NoneType\n",
      "I0710 07:19:13.630627 140310643746176 base_runner.py:59] \n",
      "I0710 07:19:13.630684 140310643746176 base_runner.py:60] ============================================================\n",
      "I0710 07:19:13.632175 140310643746176 base_runner.py:111] Starting ...\n",
      "I0710 07:19:13.632400 140310643746176 cluster.py:507] _LeastLoadedPlacer : ['/job:localhost/replica:0/task:0/device:CPU:0']\n",
      "I0710 07:19:13.638810 140310643746176 cluster.py:525] Place variable global_step on /job:localhost/replica:0/task:0/device:CPU:0 8\n",
      "I0710 07:19:13.851271 140310643746176 base_model.py:1068] Training parameters for <class 'lingvo.core.base_model.SingleTaskModel'>: {\n",
      "  early_stop: {\n",
      "    metric_history: {\n",
      "      jobname: \"eval_dev\"\n",
      "      local_filesystem: False\n",
      "      logdir: \"/tmp/punctuator\"\n",
      "      metric: \"log_pplx\"\n",
      "      minimize: True\n",
      "      name: \"MetricHistory\"\n",
      "      tfevent_file: False\n",
      "    }\n",
      "    min_steps: 0\n",
      "    name: \"EarlyStop\"\n",
      "    tolerance: 0.0\n",
      "    verbose: True\n",
      "    window: 0\n",
      "  }\n",
      "  ema_decay: 0.0\n",
      "  ema_decay_moving_vars: None\n",
      "  enqueue_max_steps: -1\n",
      "  init_from_checkpoint_rules: {}\n",
      "  max_steps: 4000000\n",
      "  save_interval_seconds: 600\n",
      "  save_keep_checkpoint_every_n_hours: 1.0\n",
      "  save_max_to_keep: 3\n",
      "  start_up_delay_steps: 200\n",
      "  summary_interval_steps: 100\n",
      "  tpu_steps_per_loop: 100\n",
      "}\n",
      "I0710 07:19:13.859830 140310643746176 base_model.py:280] input_params: {\n",
      "  allow_implicit_capture: None\n",
      "  bucket_adjust_every_n: 0\n",
      "  bucket_batch_limit: [512, 256, 160, 80, 40]\n",
      "  bucket_upper_bound: [10, 20, 30, 60, 120]\n",
      "  cls: <class 'input_generator.PunctuatorInput'>\n",
      "  dtype: <dtype: 'float32'>\n",
      "  file_buffer_size: 10000\n",
      "  file_buffer_size_in_seconds: 0\n",
      "  file_datasource: None\n",
      "  file_parallelism: 1\n",
      "  file_pattern: \"text:/tmp/punctuator_data/train.txt\"\n",
      "  file_random_seed: 0\n",
      "  flush_every_n: 0\n",
      "  fprop_dtype: None\n",
      "  inference_driver_name: None\n",
      "  is_inference: None\n",
      "  name: \"input\"\n",
      "  num_batcher_threads: 1\n",
      "  num_partitions: None\n",
      "  num_samples: 0\n",
      "  pad_to_max_seq_length: False\n",
      "  params_init: {\n",
      "    method: \"xavier\"\n",
      "    scale: 1.000001\n",
      "    seed: None\n",
      "  }\n",
      "  random_seed: None\n",
      "  remote: {\n",
      "    max_inflights_per_target: 32\n",
      "    shardable_batch: False\n",
      "  }\n",
      "  repeat_count: -1\n",
      "  require_sequential_order: False\n",
      "  skip_lp_regularization: None\n",
      "  source_max_length: 122\n",
      "  target_max_length: 122\n",
      "  tokenizer: {\n",
      "    allow_implicit_capture: None\n",
      "    append_eos: True\n",
      "    cls: <class 'lingvo.core.tokenizers.WpmTokenizer'>\n",
      "    dtype: <dtype: 'float32'>\n",
      "    fprop_dtype: None\n",
      "    inference_driver_name: None\n",
      "    is_inference: None\n",
      "    merge_prob: 1.0\n",
      "    name: \"tokenizer\"\n",
      "    pad_to_max_length: False\n",
      "    params_init: {\n",
      "      method: \"xavier\"\n",
      "      scale: 1.000001\n",
      "      seed: None\n",
      "    }\n",
      "    random_seed: None\n",
      "    skip_lp_regularization: None\n",
      "    target_eos_id: 2\n",
      "    target_sos_id: 1\n",
      "    target_unk_id: 0\n",
      "    target_wb_id: -1\n",
      "    vn: {\n",
      "      global_vn: False\n",
      "      per_step_vn: False\n",
      "      scale: None\n",
      "      seed: None\n",
      "    }\n",
      "    vocab_filepath: \"brown_corpus_wpm.16000.vocab\"\n",
      "    vocab_size: 16000\n",
      "  }\n",
      "  tokenizer_dict: {}\n",
      "  tpu_infeed_parallelism: 1\n",
      "  use_chaining: False\n",
      "  use_partitioned_infeed_queue: False\n",
      "  use_per_host_infeed: False\n",
      "  use_within_batch_mixing: False\n",
      "  vn: {\n",
      "    global_vn: False\n",
      "    per_step_vn: False\n",
      "    scale: None\n",
      "    seed: None\n",
      "  }\n",
      "}\n",
      "I0710 07:19:13.905777 140310643746176 base_input_generator.py:673] Building data source <lingvo.core.datasource.SimpleDataSource object at 0x7f9b1c4046d0> with params {\n",
      "  allow_implicit_capture: None\n",
      "  cls: <class 'lingvo.core.datasource.SimpleDataSource'>\n",
      "  dtype: <dtype: 'float32'>\n",
      "  file_pattern: \"text:/tmp/punctuator_data/train.txt\"\n",
      "  file_type: \"\"\n",
      "  fprop_dtype: None\n",
      "  inference_driver_name: None\n",
      "  is_inference: None\n",
      "  name: \"datasource\"\n",
      "  params_init: {\n",
      "    method: \"xavier\"\n",
      "    scale: 1.000001\n",
      "    seed: None\n",
      "  }\n",
      "  random_seed: None\n",
      "  skip_lp_regularization: None\n",
      "  vn: {\n",
      "    global_vn: False\n",
      "    per_step_vn: False\n",
      "    scale: None\n",
      "    seed: None\n",
      "  }\n",
      "} and file_pattern text:/tmp/punctuator_data/train.txt\n",
      "I0710 07:19:13.906135 140310643746176 base_input_generator.py:775] infeed_bucket_batch_limit=[512, 256, 160, 80, 40] num_splits_per_client=1 bucket_batch_limit=[512, 256, 160, 80, 40]\n",
      "I0710 07:19:13.906206 140310643746176 base_input_generator.py:798] infeed_bucket_batch_limit [512, 256, 160, 80, 40]\n",
      "I0710 07:19:15.640953 140310643746176 learner.py:383] Ignoring legacy param start_up_delay_steps=200 for optimization program\n",
      "I0710 07:19:15.641132 140310643746176 learner.py:383] Ignoring legacy param max_steps=4000000 for optimization program\n",
      "I0710 07:19:15.641187 140310643746176 learner.py:383] Ignoring legacy param tpu_steps_per_loop=100 for optimization program\n",
      "I0710 07:19:15.641229 140310643746176 learner.py:383] Ignoring legacy param vn_start_step=200000000 for optimization program\n",
      "I0710 07:19:15.641289 140310643746176 learner.py:383] Ignoring legacy param vn_std=0.0 for optimization program\n",
      "I0710 07:19:15.641330 140310643746176 learner.py:383] Ignoring legacy param early_stop={\n",
      "  metric_history: {\n",
      "    jobname: \"eval_dev\"\n",
      "    local_filesystem: False\n",
      "    logdir: \"/tmp/punctuator\"\n",
      "    metric: \"log_pplx\"\n",
      "    minimize: True\n",
      "    name: \"MetricHistory\"\n",
      "    tfevent_file: False\n",
      "  }\n",
      "  min_steps: 0\n",
      "  name: \"EarlyStop\"\n",
      "  tolerance: 0.0\n",
      "  verbose: True\n",
      "  window: 0\n",
      "} for optimization program\n",
      "I0710 07:19:15.641459 140310643746176 learner.py:383] Ignoring legacy param ema_decay=0.0 for optimization program\n",
      "I0710 07:19:15.641501 140310643746176 learner.py:383] Ignoring legacy param ema_decay_moving_vars=None for optimization program\n",
      "I0710 07:19:15.641539 140310643746176 learner.py:383] Ignoring legacy param init_from_checkpoint_rules={} for optimization program\n",
      "I0710 07:19:15.641578 140310643746176 learner.py:383] Ignoring legacy param pruning_hparams_dict=None for optimization program\n",
      "I0710 07:19:15.641614 140310643746176 learner.py:383] Ignoring legacy param enqueue_max_steps=-1 for optimization program\n",
      "I0710 07:19:15.641650 140310643746176 learner.py:383] Ignoring legacy param save_interval_seconds=600 for optimization program\n",
      "I0710 07:19:15.641686 140310643746176 learner.py:383] Ignoring legacy param save_max_to_keep=100 for optimization program\n",
      "I0710 07:19:15.641722 140310643746176 learner.py:383] Ignoring legacy param save_keep_checkpoint_every_n_hours=0.5 for optimization program\n",
      "I0710 07:19:15.641760 140310643746176 learner.py:383] Ignoring legacy param summary_interval_steps=100 for optimization program\n",
      "I0710 07:19:15.641796 140310643746176 learner.py:383] Ignoring legacy param learner=None for optimization program\n",
      "I0710 07:19:15.642410 140310643746176 learner.py:388] Learner params: allow_implicit_capture : NoneType\n",
      "I0710 07:19:15.642498 140310643746176 learner.py:388] Learner params: bprop_variable_exclusion : NoneType\n",
      "I0710 07:19:15.642549 140310643746176 learner.py:388] Learner params: bprop_variable_filter : NoneType\n",
      "I0710 07:19:15.642592 140310643746176 learner.py:388] Learner params: clip_gradient_norm_to_value : 0.0\n",
      "I0710 07:19:15.642634 140310643746176 learner.py:388] Learner params: clip_gradient_single_norm_to_value : 0.0\n",
      "I0710 07:19:15.642674 140310643746176 learner.py:388] Learner params: cls : type/lingvo.core.learner/Learner\n",
      "I0710 07:19:15.642713 140310643746176 learner.py:388] Learner params: colocate_gradients_with_ops : True\n",
      "I0710 07:19:15.642751 140310643746176 learner.py:388] Learner params: dtype : float32\n",
      "I0710 07:19:15.642790 140310643746176 learner.py:388] Learner params: fprop_dtype : NoneType\n",
      "I0710 07:19:15.642853 140310643746176 learner.py:388] Learner params: gate_gradients : False\n",
      "I0710 07:19:15.642893 140310643746176 learner.py:388] Learner params: grad_aggregation_method : 1\n",
      "I0710 07:19:15.642933 140310643746176 learner.py:388] Learner params: grad_norm_to_clip_to_zero : 100000.0\n",
      "I0710 07:19:15.642991 140310643746176 learner.py:388] Learner params: grad_norm_tracker.allow_implicit_capture : NoneType\n",
      "I0710 07:19:15.643039 140310643746176 learner.py:388] Learner params: grad_norm_tracker.clip_threshold : 4.0\n",
      "I0710 07:19:15.643077 140310643746176 learner.py:388] Learner params: grad_norm_tracker.cls : type/lingvo.core.layers/GradNormTracker\n",
      "I0710 07:19:15.643116 140310643746176 learner.py:388] Learner params: grad_norm_tracker.decay : 0.995\n",
      "I0710 07:19:15.643154 140310643746176 learner.py:388] Learner params: grad_norm_tracker.dtype : float32\n",
      "I0710 07:19:15.643197 140310643746176 learner.py:388] Learner params: grad_norm_tracker.fprop_dtype : NoneType\n",
      "I0710 07:19:15.643232 140310643746176 learner.py:388] Learner params: grad_norm_tracker.grad_norm_clip_cap_min : 0.0\n",
      "I0710 07:19:15.643287 140310643746176 learner.py:388] Learner params: grad_norm_tracker.grad_norm_lower_cap : 0.01\n",
      "I0710 07:19:15.643326 140310643746176 learner.py:388] Learner params: grad_norm_tracker.inference_driver_name : NoneType\n",
      "I0710 07:19:15.643365 140310643746176 learner.py:388] Learner params: grad_norm_tracker.is_inference : NoneType\n",
      "I0710 07:19:15.643404 140310643746176 learner.py:388] Learner params: grad_norm_tracker.name : 'gradient_norm_tracker'\n",
      "I0710 07:19:15.643443 140310643746176 learner.py:388] Learner params: grad_norm_tracker.params_init.method : 'xavier'\n",
      "I0710 07:19:15.643482 140310643746176 learner.py:388] Learner params: grad_norm_tracker.params_init.scale : 1.000001\n",
      "I0710 07:19:15.643521 140310643746176 learner.py:388] Learner params: grad_norm_tracker.params_init.seed : NoneType\n",
      "I0710 07:19:15.643559 140310643746176 learner.py:388] Learner params: grad_norm_tracker.random_seed : NoneType\n",
      "I0710 07:19:15.643610 140310643746176 learner.py:388] Learner params: grad_norm_tracker.skip_lp_regularization : NoneType\n",
      "I0710 07:19:15.643648 140310643746176 learner.py:388] Learner params: grad_norm_tracker.vn.global_vn : False\n",
      "I0710 07:19:15.643695 140310643746176 learner.py:388] Learner params: grad_norm_tracker.vn.per_step_vn : False\n",
      "I0710 07:19:15.643733 140310643746176 learner.py:388] Learner params: grad_norm_tracker.vn.scale : NoneType\n",
      "I0710 07:19:15.643771 140310643746176 learner.py:388] Learner params: grad_norm_tracker.vn.seed : NoneType\n",
      "I0710 07:19:15.643808 140310643746176 learner.py:388] Learner params: inference_driver_name : NoneType\n",
      "I0710 07:19:15.643846 140310643746176 learner.py:388] Learner params: is_inference : NoneType\n",
      "I0710 07:19:15.643883 140310643746176 learner.py:388] Learner params: l1_regularizer_weight : NoneType\n",
      "I0710 07:19:15.643921 140310643746176 learner.py:388] Learner params: l2_regularizer_weight : 1e-05\n",
      "I0710 07:19:15.643958 140310643746176 learner.py:388] Learner params: learning_rate : 0.0001\n",
      "I0710 07:19:15.643996 140310643746176 learner.py:388] Learner params: lr_schedule.allow_implicit_capture : NoneType\n",
      "I0710 07:19:15.644047 140310643746176 learner.py:388] Learner params: lr_schedule.cls : type/lingvo.core.schedule/LinearRampupExponentialDecayScaledByNumSplitSchedule\n",
      "I0710 07:19:15.644086 140310643746176 learner.py:388] Learner params: lr_schedule.decay_end : 1200000\n",
      "I0710 07:19:15.644124 140310643746176 learner.py:388] Learner params: lr_schedule.decay_start : 400000\n",
      "I0710 07:19:15.644161 140310643746176 learner.py:388] Learner params: lr_schedule.dtype : float32\n",
      "I0710 07:19:15.644198 140310643746176 learner.py:388] Learner params: lr_schedule.fprop_dtype : NoneType\n",
      "I0710 07:19:15.644236 140310643746176 learner.py:388] Learner params: lr_schedule.inference_driver_name : NoneType\n",
      "I0710 07:19:15.644273 140310643746176 learner.py:388] Learner params: lr_schedule.is_inference : NoneType\n",
      "I0710 07:19:15.644310 140310643746176 learner.py:388] Learner params: lr_schedule.max : 100000000.0\n",
      "I0710 07:19:15.644347 140310643746176 learner.py:388] Learner params: lr_schedule.min : 0.5\n",
      "I0710 07:19:15.644384 140310643746176 learner.py:388] Learner params: lr_schedule.name : 'LRSched'\n",
      "I0710 07:19:15.644421 140310643746176 learner.py:388] Learner params: lr_schedule.num_splits : 0\n",
      "I0710 07:19:15.644458 140310643746176 learner.py:388] Learner params: lr_schedule.params_init.method : 'xavier'\n",
      "I0710 07:19:15.644494 140310643746176 learner.py:388] Learner params: lr_schedule.params_init.scale : 1.000001\n",
      "I0710 07:19:15.644531 140310643746176 learner.py:388] Learner params: lr_schedule.params_init.seed : NoneType\n",
      "I0710 07:19:15.644568 140310643746176 learner.py:388] Learner params: lr_schedule.random_seed : NoneType\n",
      "I0710 07:19:15.644606 140310643746176 learner.py:388] Learner params: lr_schedule.skip_lp_regularization : NoneType\n",
      "I0710 07:19:15.644643 140310643746176 learner.py:388] Learner params: lr_schedule.vn.global_vn : False\n",
      "I0710 07:19:15.644680 140310643746176 learner.py:388] Learner params: lr_schedule.vn.per_step_vn : False\n",
      "I0710 07:19:15.644717 140310643746176 learner.py:388] Learner params: lr_schedule.vn.scale : NoneType\n",
      "I0710 07:19:15.644754 140310643746176 learner.py:388] Learner params: lr_schedule.vn.seed : NoneType\n",
      "I0710 07:19:15.644791 140310643746176 learner.py:388] Learner params: lr_schedule.warmup : 500\n",
      "I0710 07:19:15.644828 140310643746176 learner.py:388] Learner params: lr_schedule.warmup_init : 1.0\n",
      "I0710 07:19:15.644865 140310643746176 learner.py:388] Learner params: name : 'loss'\n",
      "I0710 07:19:15.644902 140310643746176 learner.py:388] Learner params: optimizer.allow_implicit_capture : NoneType\n",
      "I0710 07:19:15.644939 140310643746176 learner.py:388] Learner params: optimizer.beta1 : 0.9\n",
      "I0710 07:19:15.644976 140310643746176 learner.py:388] Learner params: optimizer.beta2 : 0.98\n",
      "I0710 07:19:15.645014 140310643746176 learner.py:388] Learner params: optimizer.cls : type/lingvo.core.optimizer/Adam\n",
      "I0710 07:19:15.645057 140310643746176 learner.py:388] Learner params: optimizer.dtype : float32\n",
      "I0710 07:19:15.645094 140310643746176 learner.py:388] Learner params: optimizer.epsilon : 1e-06\n",
      "I0710 07:19:15.645131 140310643746176 learner.py:388] Learner params: optimizer.fprop_dtype : NoneType\n",
      "I0710 07:19:15.645169 140310643746176 learner.py:388] Learner params: optimizer.inference_driver_name : NoneType\n",
      "I0710 07:19:15.645206 140310643746176 learner.py:388] Learner params: optimizer.is_inference : NoneType\n",
      "I0710 07:19:15.645243 140310643746176 learner.py:388] Learner params: optimizer.name : 'Adam'\n",
      "I0710 07:19:15.645281 140310643746176 learner.py:388] Learner params: optimizer.params_init.method : 'xavier'\n",
      "I0710 07:19:15.645318 140310643746176 learner.py:388] Learner params: optimizer.params_init.scale : 1.000001\n",
      "I0710 07:19:15.645357 140310643746176 learner.py:388] Learner params: optimizer.params_init.seed : NoneType\n",
      "I0710 07:19:15.645395 140310643746176 learner.py:388] Learner params: optimizer.random_seed : NoneType\n",
      "I0710 07:19:15.645431 140310643746176 learner.py:388] Learner params: optimizer.skip_lp_regularization : NoneType\n",
      "I0710 07:19:15.645469 140310643746176 learner.py:388] Learner params: optimizer.use_bf16_gradients_ar : False\n",
      "I0710 07:19:15.645506 140310643746176 learner.py:388] Learner params: optimizer.vn.global_vn : False\n",
      "I0710 07:19:15.645544 140310643746176 learner.py:388] Learner params: optimizer.vn.per_step_vn : False\n",
      "I0710 07:19:15.645600 140310643746176 learner.py:388] Learner params: optimizer.vn.scale : NoneType\n",
      "I0710 07:19:15.645639 140310643746176 learner.py:388] Learner params: optimizer.vn.seed : NoneType\n",
      "I0710 07:19:15.645677 140310643746176 learner.py:388] Learner params: params_init.method : 'xavier'\n",
      "I0710 07:19:15.645715 140310643746176 learner.py:388] Learner params: params_init.scale : 1.000001\n",
      "I0710 07:19:15.645754 140310643746176 learner.py:388] Learner params: params_init.seed : NoneType\n",
      "I0710 07:19:15.645793 140310643746176 learner.py:388] Learner params: random_seed : NoneType\n",
      "I0710 07:19:15.645831 140310643746176 learner.py:388] Learner params: scale_gradients : True\n",
      "I0710 07:19:15.645870 140310643746176 learner.py:388] Learner params: skip_lp_regularization : NoneType\n",
      "I0710 07:19:15.645926 140310643746176 learner.py:388] Learner params: skip_zero_gradients : NoneType\n",
      "I0710 07:19:15.645968 140310643746176 learner.py:388] Learner params: vn.global_vn : False\n",
      "I0710 07:19:15.646007 140310643746176 learner.py:388] Learner params: vn.per_step_vn : False\n",
      "I0710 07:19:15.646051 140310643746176 learner.py:388] Learner params: vn.scale : NoneType\n",
      "I0710 07:19:15.646090 140310643746176 learner.py:388] Learner params: vn.seed : NoneType\n",
      "I0710 07:19:15.646129 140310643746176 learner.py:388] Learner params: \n",
      "I0710 07:19:15.651376 140310643746176 cluster.py:525] Place variable punctuator_rnmt/gradient_norm_tracker/log_mean/var on /job:localhost/replica:0/task:0/device:CPU:0 12\n",
      "I0710 07:19:15.653061 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/gradient_norm_tracker/log_mean/var:0 shape=() on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.655647 140310643746176 cluster.py:525] Place variable punctuator_rnmt/gradient_norm_tracker/log_mean_squared/var on /job:localhost/replica:0/task:0/device:CPU:0 16\n",
      "I0710 07:19:15.657119 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/gradient_norm_tracker/log_mean_squared/var:0 shape=() on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.659715 140310643746176 cluster.py:525] Place variable punctuator_rnmt/gradient_norm_tracker/total_weight/var on /job:localhost/replica:0/task:0/device:CPU:0 20\n",
      "I0710 07:19:15.661236 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/gradient_norm_tracker/total_weight/var:0 shape=() on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.663659 140310643746176 cluster.py:525] Place variable punctuator_rnmt/gradient_norm_tracker/total_rejections/var on /job:localhost/replica:0/task:0/device:CPU:0 24\n",
      "I0710 07:19:15.665254 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/gradient_norm_tracker/total_rejections/var:0 shape=() on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.667464 140310643746176 schedule.py:432] Peak lr: 1.000000\n",
      "I0710 07:19:15.682512 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_0/var on /job:localhost/replica:0/task:0/device:CPU:0 4096024\n",
      "I0710 07:19:15.684480 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_0/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.689563 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_1/var on /job:localhost/replica:0/task:0/device:CPU:0 8192024\n",
      "I0710 07:19:15.691513 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_1/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.696804 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_2/var on /job:localhost/replica:0/task:0/device:CPU:0 12288024\n",
      "I0710 07:19:15.698671 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_2/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.703845 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_3/var on /job:localhost/replica:0/task:0/device:CPU:0 16384024\n",
      "I0710 07:19:15.705751 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_3/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.710994 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_4/var on /job:localhost/replica:0/task:0/device:CPU:0 20480024\n",
      "I0710 07:19:15.712913 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_4/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.718076 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_5/var on /job:localhost/replica:0/task:0/device:CPU:0 24576024\n",
      "I0710 07:19:15.720160 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_5/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.725564 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_6/var on /job:localhost/replica:0/task:0/device:CPU:0 28672024\n",
      "I0710 07:19:15.727635 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_6/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.733123 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_7/var on /job:localhost/replica:0/task:0/device:CPU:0 32768024\n",
      "I0710 07:19:15.735219 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_7/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.740734 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_8/var on /job:localhost/replica:0/task:0/device:CPU:0 36864024\n",
      "I0710 07:19:15.742871 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_8/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.748241 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_9/var on /job:localhost/replica:0/task:0/device:CPU:0 40960024\n",
      "I0710 07:19:15.750259 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_9/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.755563 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_10/var on /job:localhost/replica:0/task:0/device:CPU:0 45056024\n",
      "I0710 07:19:15.757580 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_10/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.762874 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_11/var on /job:localhost/replica:0/task:0/device:CPU:0 49152024\n",
      "I0710 07:19:15.765309 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_11/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.770534 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_12/var on /job:localhost/replica:0/task:0/device:CPU:0 53248024\n",
      "I0710 07:19:15.772485 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_12/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.777794 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_13/var on /job:localhost/replica:0/task:0/device:CPU:0 57344024\n",
      "I0710 07:19:15.779650 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_13/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.784662 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_14/var on /job:localhost/replica:0/task:0/device:CPU:0 61440024\n",
      "I0710 07:19:15.786747 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_14/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.791935 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_15/var on /job:localhost/replica:0/task:0/device:CPU:0 65536024\n",
      "I0710 07:19:15.793937 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/emb/var_15/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.809653 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_fwd/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 99090456\n",
      "I0710 07:19:15.811695 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L0_rnn_fwd/wm/var:0 shape=(2048, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.814464 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_fwd/b/var on /job:localhost/replica:0/task:0/device:CPU:0 99106840\n",
      "I0710 07:19:15.815970 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L0_rnn_fwd/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.820442 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_fwd/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 99123224\n",
      "I0710 07:19:15.821988 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L0_rnn_fwd/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.830253 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_bak/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 132677656\n",
      "I0710 07:19:15.832111 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L0_rnn_bak/wm/var:0 shape=(2048, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.834682 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_bak/b/var on /job:localhost/replica:0/task:0/device:CPU:0 132694040\n",
      "I0710 07:19:15.836179 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L0_rnn_bak/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.840533 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_bak/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 132710424\n",
      "I0710 07:19:15.842102 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L0_rnn_bak/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.852884 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_fwd/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 183042072\n",
      "I0710 07:19:15.854795 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L1_rnn_fwd/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.857271 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_fwd/b/var on /job:localhost/replica:0/task:0/device:CPU:0 183058456\n",
      "I0710 07:19:15.859050 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L1_rnn_fwd/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.863480 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_fwd/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 183074840\n",
      "I0710 07:19:15.865071 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L1_rnn_fwd/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.873200 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_bak/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 233406488\n",
      "I0710 07:19:15.875186 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L1_rnn_bak/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.877842 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_bak/b/var on /job:localhost/replica:0/task:0/device:CPU:0 233422872\n",
      "I0710 07:19:15.879471 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L1_rnn_bak/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.883930 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_bak/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 233439256\n",
      "I0710 07:19:15.885528 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L1_rnn_bak/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.896357 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_fwd/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 283770904\n",
      "I0710 07:19:15.898391 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L2_rnn_fwd/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.901208 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_fwd/b/var on /job:localhost/replica:0/task:0/device:CPU:0 283787288\n",
      "I0710 07:19:15.902977 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L2_rnn_fwd/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.907384 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_fwd/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 283803672\n",
      "I0710 07:19:15.909071 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L2_rnn_fwd/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.917467 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_bak/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 334135320\n",
      "I0710 07:19:15.919524 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L2_rnn_bak/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.922371 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_bak/b/var on /job:localhost/replica:0/task:0/device:CPU:0 334151704\n",
      "I0710 07:19:15.924013 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L2_rnn_bak/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.928531 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_bak/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 334168088\n",
      "I0710 07:19:15.930086 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L2_rnn_bak/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.940651 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_fwd/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 384499736\n",
      "I0710 07:19:15.942587 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L3_rnn_fwd/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.945210 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_fwd/b/var on /job:localhost/replica:0/task:0/device:CPU:0 384516120\n",
      "I0710 07:19:15.946869 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L3_rnn_fwd/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.951343 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_fwd/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 384532504\n",
      "I0710 07:19:15.952973 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L3_rnn_fwd/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.961116 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_bak/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 434864152\n",
      "I0710 07:19:15.963167 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L3_rnn_bak/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.965891 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_bak/b/var on /job:localhost/replica:0/task:0/device:CPU:0 434880536\n",
      "I0710 07:19:15.967525 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L3_rnn_bak/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.972070 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_bak/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 434896920\n",
      "I0710 07:19:15.973699 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L3_rnn_bak/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.984742 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_fwd/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 485228568\n",
      "I0710 07:19:15.986800 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L4_rnn_fwd/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.989596 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_fwd/b/var on /job:localhost/replica:0/task:0/device:CPU:0 485244952\n",
      "I0710 07:19:15.991566 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L4_rnn_fwd/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:15.995813 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_fwd/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 485261336\n",
      "I0710 07:19:15.997463 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L4_rnn_fwd/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.006086 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_bak/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 535592984\n",
      "I0710 07:19:16.008146 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L4_rnn_bak/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.010932 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_bak/b/var on /job:localhost/replica:0/task:0/device:CPU:0 535609368\n",
      "I0710 07:19:16.012550 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L4_rnn_bak/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.016892 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_bak/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 535625752\n",
      "I0710 07:19:16.018543 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L4_rnn_bak/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.029731 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_fwd/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 585957400\n",
      "I0710 07:19:16.031827 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L5_rnn_fwd/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.034485 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_fwd/b/var on /job:localhost/replica:0/task:0/device:CPU:0 585973784\n",
      "I0710 07:19:16.036087 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L5_rnn_fwd/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.040539 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_fwd/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 585990168\n",
      "I0710 07:19:16.042246 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L5_rnn_fwd/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.050256 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_bak/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 636321816\n",
      "I0710 07:19:16.052247 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L5_rnn_bak/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.054890 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_bak/b/var on /job:localhost/replica:0/task:0/device:CPU:0 636338200\n",
      "I0710 07:19:16.056465 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L5_rnn_bak/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.060995 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_bak/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 636354584\n",
      "I0710 07:19:16.062729 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/L5_rnn_bak/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "W0710 07:19:16.065857 140310643746176 py_utils.py:1534] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.\n",
      "I0710 07:19:16.071254 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/proj/w/var on /job:localhost/replica:0/task:0/device:CPU:0 644743192\n",
      "I0710 07:19:16.073385 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/proj/w/var:0 shape=(2048, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.076099 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/proj/b/var on /job:localhost/replica:0/task:0/device:CPU:0 644747288\n",
      "I0710 07:19:16.077605 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/enc/proj/b/var:0 shape=(1024,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.094175 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_0/var on /job:localhost/replica:0/task:0/device:CPU:0 648843288\n",
      "I0710 07:19:16.096182 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_0/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.101512 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_1/var on /job:localhost/replica:0/task:0/device:CPU:0 652939288\n",
      "I0710 07:19:16.103498 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_1/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.108714 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_2/var on /job:localhost/replica:0/task:0/device:CPU:0 657035288\n",
      "I0710 07:19:16.110765 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_2/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.115886 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_3/var on /job:localhost/replica:0/task:0/device:CPU:0 661131288\n",
      "I0710 07:19:16.117891 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_3/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.123428 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_4/var on /job:localhost/replica:0/task:0/device:CPU:0 665227288\n",
      "I0710 07:19:16.125374 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_4/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.130693 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_5/var on /job:localhost/replica:0/task:0/device:CPU:0 669323288\n",
      "I0710 07:19:16.132684 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_5/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.137964 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_6/var on /job:localhost/replica:0/task:0/device:CPU:0 673419288\n",
      "I0710 07:19:16.139862 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_6/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.144961 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_7/var on /job:localhost/replica:0/task:0/device:CPU:0 677515288\n",
      "I0710 07:19:16.146716 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_7/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.152089 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_8/var on /job:localhost/replica:0/task:0/device:CPU:0 681611288\n",
      "I0710 07:19:16.154123 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_8/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.159388 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_9/var on /job:localhost/replica:0/task:0/device:CPU:0 685707288\n",
      "I0710 07:19:16.161391 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_9/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.166730 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_10/var on /job:localhost/replica:0/task:0/device:CPU:0 689803288\n",
      "I0710 07:19:16.168651 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_10/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.173747 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_11/var on /job:localhost/replica:0/task:0/device:CPU:0 693899288\n",
      "I0710 07:19:16.175678 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_11/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.180911 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_12/var on /job:localhost/replica:0/task:0/device:CPU:0 697995288\n",
      "I0710 07:19:16.182852 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_12/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.187774 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_13/var on /job:localhost/replica:0/task:0/device:CPU:0 702091288\n",
      "I0710 07:19:16.189716 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_13/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.194917 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_14/var on /job:localhost/replica:0/task:0/device:CPU:0 706187288\n",
      "I0710 07:19:16.196927 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_14/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.202150 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_15/var on /job:localhost/replica:0/task:0/device:CPU:0 710283288\n",
      "I0710 07:19:16.204100 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/emb/var_15/var:0 shape=(1000, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.213583 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten_rnn/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 760614936\n",
      "I0710 07:19:16.215691 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten_rnn/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.218535 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten_rnn/b/var on /job:localhost/replica:0/task:0/device:CPU:0 760631320\n",
      "I0710 07:19:16.220135 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten_rnn/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.224490 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten_rnn/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 760647704\n",
      "I0710 07:19:16.226453 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten_rnn/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.235303 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/source_proj/var on /job:localhost/replica:0/task:0/device:CPU:0 764842008\n",
      "I0710 07:19:16.237231 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten/source_proj/var:0 shape=(1024, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.240065 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/source_proj_b/var on /job:localhost/replica:0/task:0/device:CPU:0 764846104\n",
      "I0710 07:19:16.241699 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten/source_proj_b/var:0 shape=(1024,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.247943 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/query_proj/var on /job:localhost/replica:0/task:0/device:CPU:0 769040408\n",
      "I0710 07:19:16.250076 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten/query_proj/var:0 shape=(1024, 1024) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.252838 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/query_proj_b/var on /job:localhost/replica:0/task:0/device:CPU:0 769044504\n",
      "I0710 07:19:16.255036 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten/query_proj_b/var:0 shape=(1024,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.261312 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/inner_att/source_var/var on /job:localhost/replica:0/task:0/device:CPU:0 769306648\n",
      "I0710 07:19:16.263331 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten/inner_att/source_var/var:0 shape=(256, 256) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.268432 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/inner_att/query_var/var on /job:localhost/replica:0/task:0/device:CPU:0 769568792\n",
      "I0710 07:19:16.270448 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten/inner_att/query_var/var:0 shape=(256, 256) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.275441 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/inner_att/hidden_var/var on /job:localhost/replica:0/task:0/device:CPU:0 769569816\n",
      "I0710 07:19:16.277347 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/atten/inner_att/hidden_var/var:0 shape=(256,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.288260 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn1/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 819901464\n",
      "I0710 07:19:16.290395 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn1/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.293021 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn1/b/var on /job:localhost/replica:0/task:0/device:CPU:0 819917848\n",
      "I0710 07:19:16.294689 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn1/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.299180 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn1/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 819934232\n",
      "I0710 07:19:16.300704 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn1/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.308394 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn2/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 870265880\n",
      "I0710 07:19:16.310539 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn2/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.313338 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn2/b/var on /job:localhost/replica:0/task:0/device:CPU:0 870282264\n",
      "I0710 07:19:16.315039 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn2/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.319651 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn2/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 870298648\n",
      "I0710 07:19:16.321343 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn2/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.329364 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn3/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 920630296\n",
      "I0710 07:19:16.331288 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn3/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.334115 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn3/b/var on /job:localhost/replica:0/task:0/device:CPU:0 920646680\n",
      "I0710 07:19:16.335680 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn3/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.339945 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn3/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 920663064\n",
      "I0710 07:19:16.341622 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn3/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.349975 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn4/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 970994712\n",
      "I0710 07:19:16.352009 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn4/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.354794 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn4/b/var on /job:localhost/replica:0/task:0/device:CPU:0 971011096\n",
      "I0710 07:19:16.356435 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn4/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.360873 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn4/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 971027480\n",
      "I0710 07:19:16.362629 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn4/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.370841 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn5/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 1021359128\n",
      "I0710 07:19:16.372917 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn5/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.375799 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn5/b/var on /job:localhost/replica:0/task:0/device:CPU:0 1021375512\n",
      "I0710 07:19:16.377346 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn5/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.381872 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn5/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 1021391896\n",
      "I0710 07:19:16.383661 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn5/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.391635 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn6/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 1071723544\n",
      "I0710 07:19:16.393786 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn6/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.396539 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn6/b/var on /job:localhost/replica:0/task:0/device:CPU:0 1071739928\n",
      "I0710 07:19:16.398155 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn6/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.402874 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn6/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 1071756312\n",
      "I0710 07:19:16.404645 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn6/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.412751 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn7/wm/var on /job:localhost/replica:0/task:0/device:CPU:0 1122087960\n",
      "I0710 07:19:16.414823 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn7/wm/var:0 shape=(3072, 4096) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.417537 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn7/b/var on /job:localhost/replica:0/task:0/device:CPU:0 1122104344\n",
      "I0710 07:19:16.419288 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn7/b/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.424058 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn7/ln_scale/var on /job:localhost/replica:0/task:0/device:CPU:0 1122120728\n",
      "I0710 07:19:16.425762 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/rnn7/ln_scale/var:0 shape=(4096,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.431991 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_0/var on /job:localhost/replica:0/task:0/device:CPU:0 1130312728\n",
      "I0710 07:19:16.433966 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_0/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.439069 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_1/var on /job:localhost/replica:0/task:0/device:CPU:0 1138504728\n",
      "I0710 07:19:16.441023 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_1/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.446451 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_2/var on /job:localhost/replica:0/task:0/device:CPU:0 1146696728\n",
      "I0710 07:19:16.448356 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_2/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.453519 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_3/var on /job:localhost/replica:0/task:0/device:CPU:0 1154888728\n",
      "I0710 07:19:16.455550 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_3/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.461192 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_4/var on /job:localhost/replica:0/task:0/device:CPU:0 1163080728\n",
      "I0710 07:19:16.463415 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_4/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.469063 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_5/var on /job:localhost/replica:0/task:0/device:CPU:0 1171272728\n",
      "I0710 07:19:16.471181 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_5/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.476535 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_6/var on /job:localhost/replica:0/task:0/device:CPU:0 1179464728\n",
      "I0710 07:19:16.478583 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_6/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.484177 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_7/var on /job:localhost/replica:0/task:0/device:CPU:0 1187656728\n",
      "I0710 07:19:16.486114 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_7/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.491448 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_8/var on /job:localhost/replica:0/task:0/device:CPU:0 1195848728\n",
      "I0710 07:19:16.493434 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_8/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.498942 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_9/var on /job:localhost/replica:0/task:0/device:CPU:0 1204040728\n",
      "I0710 07:19:16.501105 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_9/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.506448 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_10/var on /job:localhost/replica:0/task:0/device:CPU:0 1212232728\n",
      "I0710 07:19:16.508372 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_10/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.513611 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_11/var on /job:localhost/replica:0/task:0/device:CPU:0 1220424728\n",
      "I0710 07:19:16.515594 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_11/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.521128 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_12/var on /job:localhost/replica:0/task:0/device:CPU:0 1228616728\n",
      "I0710 07:19:16.523800 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_12/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.529075 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_13/var on /job:localhost/replica:0/task:0/device:CPU:0 1236808728\n",
      "I0710 07:19:16.531121 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_13/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.536556 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_14/var on /job:localhost/replica:0/task:0/device:CPU:0 1245000728\n",
      "I0710 07:19:16.538472 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_14/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.543913 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_15/var on /job:localhost/replica:0/task:0/device:CPU:0 1253192728\n",
      "I0710 07:19:16.545952 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/weight_15/var:0 shape=(2048, 1000) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.548698 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_0/var on /job:localhost/replica:0/task:0/device:CPU:0 1253196728\n",
      "I0710 07:19:16.550462 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_0/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.553240 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_1/var on /job:localhost/replica:0/task:0/device:CPU:0 1253200728\n",
      "I0710 07:19:16.554854 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_1/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.557461 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_2/var on /job:localhost/replica:0/task:0/device:CPU:0 1253204728\n",
      "I0710 07:19:16.559100 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_2/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.561798 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_3/var on /job:localhost/replica:0/task:0/device:CPU:0 1253208728\n",
      "I0710 07:19:16.563526 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_3/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.566393 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_4/var on /job:localhost/replica:0/task:0/device:CPU:0 1253212728\n",
      "I0710 07:19:16.568060 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_4/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.570862 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_5/var on /job:localhost/replica:0/task:0/device:CPU:0 1253216728\n",
      "I0710 07:19:16.572622 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_5/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.575382 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_6/var on /job:localhost/replica:0/task:0/device:CPU:0 1253220728\n",
      "I0710 07:19:16.576933 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_6/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.579576 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_7/var on /job:localhost/replica:0/task:0/device:CPU:0 1253224728\n",
      "I0710 07:19:16.581185 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_7/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.583970 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_8/var on /job:localhost/replica:0/task:0/device:CPU:0 1253228728\n",
      "I0710 07:19:16.585561 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_8/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.588219 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_9/var on /job:localhost/replica:0/task:0/device:CPU:0 1253232728\n",
      "I0710 07:19:16.589866 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_9/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.592632 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_10/var on /job:localhost/replica:0/task:0/device:CPU:0 1253236728\n",
      "I0710 07:19:16.594274 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_10/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.596989 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_11/var on /job:localhost/replica:0/task:0/device:CPU:0 1253240728\n",
      "I0710 07:19:16.598655 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_11/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.601541 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_12/var on /job:localhost/replica:0/task:0/device:CPU:0 1253244728\n",
      "I0710 07:19:16.603168 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_12/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.605813 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_13/var on /job:localhost/replica:0/task:0/device:CPU:0 1253248728\n",
      "I0710 07:19:16.607498 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_13/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.610597 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_14/var on /job:localhost/replica:0/task:0/device:CPU:0 1253252728\n",
      "I0710 07:19:16.612204 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_14/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.614779 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_15/var on /job:localhost/replica:0/task:0/device:CPU:0 1253256728\n",
      "I0710 07:19:16.616314 140310643746176 py_utils.py:1694] Creating var punctuator_rnmt/dec/softmax/bias_15/var:0 shape=(1000,) on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.698814 140310643746176 py_utils.py:1783] === worker 0 ===\n",
      "I0710 07:19:16.701600 140310643746176 py_utils.py:1773] worker 0: dec.beam_search.global_step                                        /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.701690 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[0]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.701737 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[10]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.701779 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[11]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.701816 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[12]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.701853 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[13]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.701910 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[14]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.701963 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[15]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.702013 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[1]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.702091 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[2]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.702127 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[3]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.702161 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[4]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.702196 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[5]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.702230 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[6]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.702265 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[7]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.702299 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[8]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.702333 140310643746176 py_utils.py:1773] worker 0: dec.emb.wm[9]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.702369 140310643746176 py_utils.py:1773] worker 0: dec.frnn[0].cell.b                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.702403 140310643746176 py_utils.py:1773] worker 0: dec.frnn[0].cell.global_step                                       /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.702437 140310643746176 py_utils.py:1773] worker 0: dec.frnn[0].cell.ln_scale                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.702472 140310643746176 py_utils.py:1773] worker 0: dec.frnn[0].cell.wm                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.702506 140310643746176 py_utils.py:1773] worker 0: dec.frnn[0].global_step                                            /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.702541 140310643746176 py_utils.py:1773] worker 0: dec.frnn[1].cell.b                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.702575 140310643746176 py_utils.py:1773] worker 0: dec.frnn[1].cell.global_step                                       /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.702610 140310643746176 py_utils.py:1773] worker 0: dec.frnn[1].cell.ln_scale                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.702654 140310643746176 py_utils.py:1773] worker 0: dec.frnn[1].cell.wm                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.702687 140310643746176 py_utils.py:1773] worker 0: dec.frnn[1].global_step                                            /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.702719 140310643746176 py_utils.py:1773] worker 0: dec.frnn[2].cell.b                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.702751 140310643746176 py_utils.py:1773] worker 0: dec.frnn[2].cell.global_step                                       /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.702784 140310643746176 py_utils.py:1773] worker 0: dec.frnn[2].cell.ln_scale                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.702816 140310643746176 py_utils.py:1773] worker 0: dec.frnn[2].cell.wm                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.702848 140310643746176 py_utils.py:1773] worker 0: dec.frnn[2].global_step                                            /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.702881 140310643746176 py_utils.py:1773] worker 0: dec.frnn[3].cell.b                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.702913 140310643746176 py_utils.py:1773] worker 0: dec.frnn[3].cell.global_step                                       /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.702945 140310643746176 py_utils.py:1773] worker 0: dec.frnn[3].cell.ln_scale                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.702978 140310643746176 py_utils.py:1773] worker 0: dec.frnn[3].cell.wm                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703014 140310643746176 py_utils.py:1773] worker 0: dec.frnn[3].global_step                                            /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703047 140310643746176 py_utils.py:1773] worker 0: dec.frnn[4].cell.b                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703079 140310643746176 py_utils.py:1773] worker 0: dec.frnn[4].cell.global_step                                       /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703112 140310643746176 py_utils.py:1773] worker 0: dec.frnn[4].cell.ln_scale                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703144 140310643746176 py_utils.py:1773] worker 0: dec.frnn[4].cell.wm                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703176 140310643746176 py_utils.py:1773] worker 0: dec.frnn[4].global_step                                            /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703209 140310643746176 py_utils.py:1773] worker 0: dec.frnn[5].cell.b                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703241 140310643746176 py_utils.py:1773] worker 0: dec.frnn[5].cell.global_step                                       /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703278 140310643746176 py_utils.py:1773] worker 0: dec.frnn[5].cell.ln_scale                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703326 140310643746176 py_utils.py:1773] worker 0: dec.frnn[5].cell.wm                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703360 140310643746176 py_utils.py:1773] worker 0: dec.frnn[5].global_step                                            /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703412 140310643746176 py_utils.py:1773] worker 0: dec.frnn[6].cell.b                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703447 140310643746176 py_utils.py:1773] worker 0: dec.frnn[6].cell.global_step                                       /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703482 140310643746176 py_utils.py:1773] worker 0: dec.frnn[6].cell.ln_scale                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703517 140310643746176 py_utils.py:1773] worker 0: dec.frnn[6].cell.wm                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703551 140310643746176 py_utils.py:1773] worker 0: dec.frnn[6].global_step                                            /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703586 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.atten.atten.global_step                        /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703646 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.atten.atten.hidden_var                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703683 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.atten.atten.query_var                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703720 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.atten.atten.source_var                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703757 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.atten.global_step                              /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703794 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.atten.query_proj                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703832 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.atten.query_proj_b                             /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703869 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.atten.source_proj                              /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703906 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.atten.source_proj_b                            /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703943 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.cell.b                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.703979 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.cell.global_step                               /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704021 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.cell.ln_scale                                  /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704058 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.cell.wm                                        /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704095 140310643746176 py_utils.py:1773] worker 0: dec.frnn_with_atten.global_step                                    /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704133 140310643746176 py_utils.py:1773] worker 0: dec.global_step                                                    /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704170 140310643746176 py_utils.py:1773] worker 0: dec.greedy_search.global_step                                      /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704207 140310643746176 py_utils.py:1773] worker 0: dec.smoother.global_step                                           /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704245 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_0                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704282 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_1                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704319 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_10                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704355 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_11                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704392 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_12                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704428 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_13                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704465 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_14                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704502 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_15                                                /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704539 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_2                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704576 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_3                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704622 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_4                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704656 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_5                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704691 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_6                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704725 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_7                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704760 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_8                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704795 140310643746176 py_utils.py:1773] worker 0: dec.softmax.bias_9                                                 /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704830 140310643746176 py_utils.py:1773] worker 0: dec.softmax.global_step                                            /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704864 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_0                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704898 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_1                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704933 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_10                                              /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.704967 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_11                                              /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.705006 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_12                                              /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.705041 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_13                                              /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.705093 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_14                                              /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.705130 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_15                                              /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.705177 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_2                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.705229 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_3                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.705266 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_4                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.705302 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_5                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.705345 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_6                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.705398 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_7                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.705432 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_8                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.705466 140310643746176 py_utils.py:1773] worker 0: dec.softmax.weight_9                                               /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.705500 140310643746176 py_utils.py:1773] worker 0: dec.target_sequence_sampler.global_step                            /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.705534 140310643746176 py_utils.py:1773] worker 0: enc.dropout.global_step                                            /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.705569 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[0]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.705602 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[10]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.705636 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[11]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.705670 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[12]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.705704 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[13]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.705739 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[14]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.705773 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[15]                                                     /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.705807 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[1]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.705841 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[2]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.705875 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[3]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.705922 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[4]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.705957 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[5]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.705991 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[6]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.706030 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[7]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.706065 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[8]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.706099 140310643746176 py_utils.py:1773] worker 0: enc.emb.wm[9]                                                      /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0710 07:19:16.706133 140310643746176 py_utils.py:1773] worker 0: enc.final_proj.b                                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706168 140310643746176 py_utils.py:1773] worker 0: enc.final_proj.global_step                                         /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706203 140310643746176 py_utils.py:1773] worker 0: enc.final_proj.w                                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706237 140310643746176 py_utils.py:1773] worker 0: enc.global_step                                                    /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706271 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].bak_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706305 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].bak_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706340 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].bak_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706374 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].bak_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706408 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].bak_rnn.global_step                                     /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706443 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].fwd_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706477 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].fwd_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706511 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].fwd_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706545 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].fwd_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706579 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].fwd_rnn.global_step                                     /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706613 140310643746176 py_utils.py:1773] worker 0: enc.rnn[0].global_step                                             /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706647 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].bak_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706681 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].bak_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706715 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].bak_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706749 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].bak_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706783 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].bak_rnn.global_step                                     /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706817 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].fwd_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706851 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].fwd_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706885 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].fwd_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706919 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].fwd_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706953 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].fwd_rnn.global_step                                     /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.706987 140310643746176 py_utils.py:1773] worker 0: enc.rnn[1].global_step                                             /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707025 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].bak_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707060 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].bak_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707094 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].bak_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707128 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].bak_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707162 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].bak_rnn.global_step                                     /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707196 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].fwd_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707231 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].fwd_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707267 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].fwd_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707301 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].fwd_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707335 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].fwd_rnn.global_step                                     /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707371 140310643746176 py_utils.py:1773] worker 0: enc.rnn[2].global_step                                             /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707405 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].bak_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707439 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].bak_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707473 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].bak_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707508 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].bak_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707542 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].bak_rnn.global_step                                     /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707576 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].fwd_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707610 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].fwd_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707644 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].fwd_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707678 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].fwd_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707713 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].fwd_rnn.global_step                                     /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707747 140310643746176 py_utils.py:1773] worker 0: enc.rnn[3].global_step                                             /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707799 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].bak_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707832 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].bak_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707868 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].bak_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707904 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].bak_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707952 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].bak_rnn.global_step                                     /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.707985 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].fwd_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708020 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].fwd_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708054 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].fwd_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708087 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].fwd_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708122 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].fwd_rnn.global_step                                     /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708155 140310643746176 py_utils.py:1773] worker 0: enc.rnn[4].global_step                                             /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708189 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].bak_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708224 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].bak_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708257 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].bak_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708291 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].bak_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708325 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].bak_rnn.global_step                                     /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708359 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].fwd_rnn.cell.b                                          /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708411 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].fwd_rnn.cell.global_step                                /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708448 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].fwd_rnn.cell.ln_scale                                   /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708494 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].fwd_rnn.cell.wm                                         /job:localhost/replica:0/task:0/device:CPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708555 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].fwd_rnn.global_step                                     /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708604 140310643746176 py_utils.py:1773] worker 0: enc.rnn[5].global_step                                             /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708657 140310643746176 py_utils.py:1773] worker 0: global_step                                                        /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708694 140310643746176 py_utils.py:1773] worker 0: input._tokenizer_default.global_step                               /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708730 140310643746176 py_utils.py:1773] worker 0: input.datasource.global_step                                       /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708767 140310643746176 py_utils.py:1773] worker 0: input.global_step                                                  /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708841 140310643746176 py_utils.py:1773] worker 0: learners[0].global_step                                            /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708894 140310643746176 py_utils.py:1773] worker 0: learners[0].grad_norm_tracker.global_step                          /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708931 140310643746176 py_utils.py:1773] worker 0: learners[0].lr_schedule.combine.global_step                        /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.708969 140310643746176 py_utils.py:1773] worker 0: learners[0].lr_schedule.combine.schedules[0].global_step           /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.709010 140310643746176 py_utils.py:1773] worker 0: learners[0].lr_schedule.combine.schedules[1].global_step           /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.709047 140310643746176 py_utils.py:1773] worker 0: learners[0].lr_schedule.combine.schedules[2].global_step           /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.709084 140310643746176 py_utils.py:1773] worker 0: learners[0].lr_schedule.combine.schedules[2].linear.global_step    /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.709120 140310643746176 py_utils.py:1773] worker 0: learners[0].lr_schedule.combine.schedules[3].global_step           /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.709157 140310643746176 py_utils.py:1773] worker 0: learners[0].lr_schedule.global_step                                /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.709193 140310643746176 py_utils.py:1773] worker 0: learners[0].optimizer.global_step                                  /job:localhost/replica:0/task:0/device:GPU:0 -> /job:localhost/replica:0/task:0/device:GPU:0\n",
      "I0710 07:19:16.709234 140310643746176 py_utils.py:1789] ==========\n",
      "I0710 07:19:25.171765 140310643746176 base_input_generator.py:138] GlobalBatchSize Tensor(\"fprop/punctuator_rnmt/strided_slice:0\", shape=(), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "I0710 07:19:25.174055 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_0/var:0\n",
      "I0710 07:19:25.174172 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_1/var:0\n",
      "I0710 07:19:25.174226 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_2/var:0\n",
      "I0710 07:19:25.174271 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_3/var:0\n",
      "I0710 07:19:25.174313 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_4/var:0\n",
      "I0710 07:19:25.174352 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_5/var:0\n",
      "I0710 07:19:25.174391 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_6/var:0\n",
      "I0710 07:19:25.174429 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_7/var:0\n",
      "I0710 07:19:25.174467 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_8/var:0\n",
      "I0710 07:19:25.174504 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_9/var:0\n",
      "I0710 07:19:25.174542 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_10/var:0\n",
      "I0710 07:19:25.174579 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_11/var:0\n",
      "I0710 07:19:25.174617 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_12/var:0\n",
      "I0710 07:19:25.174664 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_13/var:0\n",
      "I0710 07:19:25.174699 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_14/var:0\n",
      "I0710 07:19:25.174734 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/emb/var_15/var:0\n",
      "I0710 07:19:25.174768 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn1/b/var:0\n",
      "I0710 07:19:25.174803 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn1/ln_scale/var:0\n",
      "I0710 07:19:25.174838 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn1/wm/var:0\n",
      "I0710 07:19:25.174873 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn2/b/var:0\n",
      "I0710 07:19:25.174954 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn2/ln_scale/var:0\n",
      "I0710 07:19:25.174992 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn2/wm/var:0\n",
      "I0710 07:19:25.175049 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn3/b/var:0\n",
      "I0710 07:19:25.175086 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn3/ln_scale/var:0\n",
      "I0710 07:19:25.175132 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn3/wm/var:0\n",
      "I0710 07:19:25.175167 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn4/b/var:0\n",
      "I0710 07:19:25.175202 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn4/ln_scale/var:0\n",
      "I0710 07:19:25.175237 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn4/wm/var:0\n",
      "I0710 07:19:25.175289 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn5/b/var:0\n",
      "I0710 07:19:25.175325 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn5/ln_scale/var:0\n",
      "I0710 07:19:25.175362 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn5/wm/var:0\n",
      "I0710 07:19:25.175399 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn6/b/var:0\n",
      "I0710 07:19:25.175436 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn6/ln_scale/var:0\n",
      "I0710 07:19:25.175472 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn6/wm/var:0\n",
      "I0710 07:19:25.175509 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn7/b/var:0\n",
      "I0710 07:19:25.175546 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn7/ln_scale/var:0\n",
      "I0710 07:19:25.175582 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/rnn7/wm/var:0\n",
      "I0710 07:19:25.175624 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten/inner_att/hidden_var/var:0\n",
      "I0710 07:19:25.175657 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten/inner_att/query_var/var:0\n",
      "I0710 07:19:25.175689 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten/inner_att/source_var/var:0\n",
      "I0710 07:19:25.175721 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten/query_proj/var:0\n",
      "I0710 07:19:25.175756 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten/query_proj_b/var:0\n",
      "I0710 07:19:25.175793 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten/source_proj/var:0\n",
      "I0710 07:19:25.175830 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten/source_proj_b/var:0\n",
      "I0710 07:19:25.175866 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten_rnn/b/var:0\n",
      "I0710 07:19:25.175902 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten_rnn/ln_scale/var:0\n",
      "I0710 07:19:25.175939 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/atten_rnn/wm/var:0\n",
      "I0710 07:19:25.175977 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_0/var:0\n",
      "I0710 07:19:25.176018 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_1/var:0\n",
      "I0710 07:19:25.176054 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_10/var:0\n",
      "I0710 07:19:25.176091 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_11/var:0\n",
      "I0710 07:19:25.176128 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_12/var:0\n",
      "I0710 07:19:25.176167 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_13/var:0\n",
      "I0710 07:19:25.176200 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_14/var:0\n",
      "I0710 07:19:25.176235 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_15/var:0\n",
      "I0710 07:19:25.176272 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_2/var:0\n",
      "I0710 07:19:25.176308 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_3/var:0\n",
      "I0710 07:19:25.176345 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_4/var:0\n",
      "I0710 07:19:25.176397 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_5/var:0\n",
      "I0710 07:19:25.176434 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_6/var:0\n",
      "I0710 07:19:25.176493 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_7/var:0\n",
      "I0710 07:19:25.176530 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_8/var:0\n",
      "I0710 07:19:25.176577 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/bias_9/var:0\n",
      "I0710 07:19:25.176614 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_0/var:0\n",
      "I0710 07:19:25.176654 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_1/var:0\n",
      "I0710 07:19:25.176693 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_10/var:0\n",
      "I0710 07:19:25.176733 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_11/var:0\n",
      "I0710 07:19:25.176772 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_12/var:0\n",
      "I0710 07:19:25.176811 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_13/var:0\n",
      "I0710 07:19:25.176851 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_14/var:0\n",
      "I0710 07:19:25.176890 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_15/var:0\n",
      "I0710 07:19:25.176930 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_2/var:0\n",
      "I0710 07:19:25.176969 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_3/var:0\n",
      "I0710 07:19:25.177015 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_4/var:0\n",
      "I0710 07:19:25.177054 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_5/var:0\n",
      "I0710 07:19:25.177093 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_6/var:0\n",
      "I0710 07:19:25.177133 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_7/var:0\n",
      "I0710 07:19:25.177172 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_8/var:0\n",
      "I0710 07:19:25.177211 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/dec/softmax/weight_9/var:0\n",
      "I0710 07:19:25.177250 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_0/var:0\n",
      "I0710 07:19:25.177290 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_1/var:0\n",
      "I0710 07:19:25.177330 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_2/var:0\n",
      "I0710 07:19:25.177369 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_3/var:0\n",
      "I0710 07:19:25.177408 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_4/var:0\n",
      "I0710 07:19:25.177448 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_5/var:0\n",
      "I0710 07:19:25.177487 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_6/var:0\n",
      "I0710 07:19:25.177527 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_7/var:0\n",
      "I0710 07:19:25.177566 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_8/var:0\n",
      "I0710 07:19:25.177606 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_9/var:0\n",
      "I0710 07:19:25.177645 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_10/var:0\n",
      "I0710 07:19:25.177685 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_11/var:0\n",
      "I0710 07:19:25.177764 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_12/var:0\n",
      "I0710 07:19:25.177805 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_13/var:0\n",
      "I0710 07:19:25.177858 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_14/var:0\n",
      "I0710 07:19:25.177909 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/emb/var_15/var:0\n",
      "I0710 07:19:25.177974 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/proj/b/var:0\n",
      "I0710 07:19:25.178016 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/proj/w/var:0\n",
      "I0710 07:19:25.178053 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L0_rnn_bak/b/var:0\n",
      "I0710 07:19:25.178091 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L0_rnn_bak/ln_scale/var:0\n",
      "I0710 07:19:25.178147 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L0_rnn_bak/wm/var:0\n",
      "I0710 07:19:25.178187 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L0_rnn_fwd/b/var:0\n",
      "I0710 07:19:25.178226 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L0_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:19:25.178266 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L0_rnn_fwd/wm/var:0\n",
      "I0710 07:19:25.178306 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L1_rnn_bak/b/var:0\n",
      "I0710 07:19:25.178345 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L1_rnn_bak/ln_scale/var:0\n",
      "I0710 07:19:25.178385 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L1_rnn_bak/wm/var:0\n",
      "I0710 07:19:25.178424 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L1_rnn_fwd/b/var:0\n",
      "I0710 07:19:25.178463 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L1_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:19:25.178502 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L1_rnn_fwd/wm/var:0\n",
      "I0710 07:19:25.178542 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L2_rnn_bak/b/var:0\n",
      "I0710 07:19:25.178590 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L2_rnn_bak/ln_scale/var:0\n",
      "I0710 07:19:25.178626 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L2_rnn_bak/wm/var:0\n",
      "I0710 07:19:25.178664 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L2_rnn_fwd/b/var:0\n",
      "I0710 07:19:25.178704 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L2_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:19:25.178743 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L2_rnn_fwd/wm/var:0\n",
      "I0710 07:19:25.178783 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L3_rnn_bak/b/var:0\n",
      "I0710 07:19:25.178822 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L3_rnn_bak/ln_scale/var:0\n",
      "I0710 07:19:25.178862 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L3_rnn_bak/wm/var:0\n",
      "I0710 07:19:25.178901 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L3_rnn_fwd/b/var:0\n",
      "I0710 07:19:25.178941 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L3_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:19:25.178980 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L3_rnn_fwd/wm/var:0\n",
      "I0710 07:19:25.179025 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L4_rnn_bak/b/var:0\n",
      "I0710 07:19:25.179064 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L4_rnn_bak/ln_scale/var:0\n",
      "I0710 07:19:25.179104 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L4_rnn_bak/wm/var:0\n",
      "I0710 07:19:25.179143 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L4_rnn_fwd/b/var:0\n",
      "I0710 07:19:25.179183 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L4_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:19:25.179222 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L4_rnn_fwd/wm/var:0\n",
      "I0710 07:19:25.179261 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L5_rnn_bak/b/var:0\n",
      "I0710 07:19:25.179300 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L5_rnn_bak/ln_scale/var:0\n",
      "I0710 07:19:25.179339 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L5_rnn_bak/wm/var:0\n",
      "I0710 07:19:25.179378 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L5_rnn_fwd/b/var:0\n",
      "I0710 07:19:25.179417 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L5_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:19:25.179456 140310643746176 learner.py:179] loss: bprop variable: punctuator_rnmt/enc/L5_rnn_fwd/wm/var:0\n",
      "I0710 07:19:33.057150 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity:0\n",
      "I0710 07:19:33.057322 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_1:0\n",
      "I0710 07:19:33.057398 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_2:0\n",
      "I0710 07:19:33.057448 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_3:0\n",
      "I0710 07:19:33.057513 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_4:0\n",
      "I0710 07:19:33.057556 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_5:0\n",
      "I0710 07:19:33.057609 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_6:0\n",
      "I0710 07:19:33.057648 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_7:0\n",
      "I0710 07:19:33.057688 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_8:0\n",
      "I0710 07:19:33.057726 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_9:0\n",
      "I0710 07:19:33.057764 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_10:0\n",
      "I0710 07:19:33.057803 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_11:0\n",
      "I0710 07:19:33.057841 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_12:0\n",
      "I0710 07:19:33.057878 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_13:0\n",
      "I0710 07:19:33.057932 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_14:0\n",
      "I0710 07:19:33.057971 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_15:0\n",
      "I0710 07:19:33.058009 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn1/b/var:0\n",
      "I0710 07:19:33.058045 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn1/ln_scale/var:0\n",
      "I0710 07:19:33.058104 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn1/wm/var:0\n",
      "I0710 07:19:33.058142 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn2/b/var:0\n",
      "I0710 07:19:33.058179 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn2/ln_scale/var:0\n",
      "I0710 07:19:33.058216 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn2/wm/var:0\n",
      "I0710 07:19:33.058253 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn3/b/var:0\n",
      "I0710 07:19:33.058290 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn3/ln_scale/var:0\n",
      "I0710 07:19:33.058339 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn3/wm/var:0\n",
      "I0710 07:19:33.058391 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn4/b/var:0\n",
      "I0710 07:19:33.058428 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn4/ln_scale/var:0\n",
      "I0710 07:19:33.058465 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn4/wm/var:0\n",
      "I0710 07:19:33.058501 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn5/b/var:0\n",
      "I0710 07:19:33.058538 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn5/ln_scale/var:0\n",
      "I0710 07:19:33.058585 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn5/wm/var:0\n",
      "I0710 07:19:33.058619 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn6/b/var:0\n",
      "I0710 07:19:33.058654 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn6/ln_scale/var:0\n",
      "I0710 07:19:33.058688 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn6/wm/var:0\n",
      "I0710 07:19:33.058722 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn7/b/var:0\n",
      "I0710 07:19:33.058756 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn7/ln_scale/var:0\n",
      "I0710 07:19:33.058790 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/rnn7/wm/var:0\n",
      "I0710 07:19:33.058825 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten/inner_att/hidden_var/var:0\n",
      "I0710 07:19:33.058860 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten/inner_att/query_var/var:0\n",
      "I0710 07:19:33.058894 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten/inner_att/source_var/var:0\n",
      "I0710 07:19:33.058929 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten/query_proj/var:0\n",
      "I0710 07:19:33.058964 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten/query_proj_b/var:0\n",
      "I0710 07:19:33.058998 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten/source_proj/var:0\n",
      "I0710 07:19:33.059050 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten/source_proj_b/var:0\n",
      "I0710 07:19:33.059092 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten_rnn/b/var:0\n",
      "I0710 07:19:33.059129 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten_rnn/ln_scale/var:0\n",
      "I0710 07:19:33.059166 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/atten_rnn/wm/var:0\n",
      "I0710 07:19:33.059203 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_0/var:0\n",
      "I0710 07:19:33.059239 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_1/var:0\n",
      "I0710 07:19:33.059276 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_10/var:0\n",
      "I0710 07:19:33.059313 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_11/var:0\n",
      "I0710 07:19:33.059350 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_12/var:0\n",
      "I0710 07:19:33.059387 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_13/var:0\n",
      "I0710 07:19:33.059423 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_14/var:0\n",
      "I0710 07:19:33.059458 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_15/var:0\n",
      "I0710 07:19:33.059513 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_2/var:0\n",
      "I0710 07:19:33.059552 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_3/var:0\n",
      "I0710 07:19:33.059591 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_4/var:0\n",
      "I0710 07:19:33.059630 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_5/var:0\n",
      "I0710 07:19:33.059669 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_6/var:0\n",
      "I0710 07:19:33.059708 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_7/var:0\n",
      "I0710 07:19:33.059748 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_8/var:0\n",
      "I0710 07:19:33.059787 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/bias_9/var:0\n",
      "I0710 07:19:33.059826 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_0/var:0\n",
      "I0710 07:19:33.059865 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_1/var:0\n",
      "I0710 07:19:33.059904 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_10/var:0\n",
      "I0710 07:19:33.059943 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_11/var:0\n",
      "I0710 07:19:33.059982 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_12/var:0\n",
      "I0710 07:19:33.060021 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_13/var:0\n",
      "I0710 07:19:33.060063 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_14/var:0\n",
      "I0710 07:19:33.060103 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_15/var:0\n",
      "I0710 07:19:33.060142 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_2/var:0\n",
      "I0710 07:19:33.060192 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_3/var:0\n",
      "I0710 07:19:33.060232 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_4/var:0\n",
      "I0710 07:19:33.060271 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_5/var:0\n",
      "I0710 07:19:33.060310 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_6/var:0\n",
      "I0710 07:19:33.060361 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_7/var:0\n",
      "I0710 07:19:33.060397 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_8/var:0\n",
      "I0710 07:19:33.060434 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/dec/softmax/weight_9/var:0\n",
      "I0710 07:19:33.060476 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_16:0\n",
      "I0710 07:19:33.060517 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_17:0\n",
      "I0710 07:19:33.060557 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_18:0\n",
      "I0710 07:19:33.060597 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_19:0\n",
      "I0710 07:19:33.060637 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_20:0\n",
      "I0710 07:19:33.060678 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_21:0\n",
      "I0710 07:19:33.060718 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_22:0\n",
      "I0710 07:19:33.060758 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_23:0\n",
      "I0710 07:19:33.060798 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_24:0\n",
      "I0710 07:19:33.060838 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_25:0\n",
      "I0710 07:19:33.060877 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_26:0\n",
      "I0710 07:19:33.060917 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_27:0\n",
      "I0710 07:19:33.060956 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_28:0\n",
      "I0710 07:19:33.060996 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_29:0\n",
      "I0710 07:19:33.061036 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_30:0\n",
      "I0710 07:19:33.061106 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: Identity_31:0\n",
      "I0710 07:19:33.061146 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/proj/b/var:0\n",
      "I0710 07:19:33.061186 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/proj/w/var:0\n",
      "I0710 07:19:33.061225 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L0_rnn_bak/b/var:0\n",
      "I0710 07:19:33.061266 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L0_rnn_bak/ln_scale/var:0\n",
      "I0710 07:19:33.061305 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L0_rnn_bak/wm/var:0\n",
      "I0710 07:19:33.061345 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L0_rnn_fwd/b/var:0\n",
      "I0710 07:19:33.061384 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L0_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:19:33.061424 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L0_rnn_fwd/wm/var:0\n",
      "I0710 07:19:33.061463 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L1_rnn_bak/b/var:0\n",
      "I0710 07:19:33.061501 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L1_rnn_bak/ln_scale/var:0\n",
      "I0710 07:19:33.061541 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L1_rnn_bak/wm/var:0\n",
      "I0710 07:19:33.061580 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L1_rnn_fwd/b/var:0\n",
      "I0710 07:19:33.061619 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L1_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:19:33.061658 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L1_rnn_fwd/wm/var:0\n",
      "I0710 07:19:33.061697 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L2_rnn_bak/b/var:0\n",
      "I0710 07:19:33.061747 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L2_rnn_bak/ln_scale/var:0\n",
      "I0710 07:19:33.061783 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L2_rnn_bak/wm/var:0\n",
      "I0710 07:19:33.061819 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L2_rnn_fwd/b/var:0\n",
      "I0710 07:19:33.061856 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L2_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:19:33.061892 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L2_rnn_fwd/wm/var:0\n",
      "I0710 07:19:33.061943 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L3_rnn_bak/b/var:0\n",
      "I0710 07:19:33.061980 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L3_rnn_bak/ln_scale/var:0\n",
      "I0710 07:19:33.062027 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L3_rnn_bak/wm/var:0\n",
      "I0710 07:19:33.062068 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L3_rnn_fwd/b/var:0\n",
      "I0710 07:19:33.062106 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L3_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:19:33.062143 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L3_rnn_fwd/wm/var:0\n",
      "I0710 07:19:33.062179 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L4_rnn_bak/b/var:0\n",
      "I0710 07:19:33.062216 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L4_rnn_bak/ln_scale/var:0\n",
      "I0710 07:19:33.062253 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L4_rnn_bak/wm/var:0\n",
      "I0710 07:19:33.062291 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L4_rnn_fwd/b/var:0\n",
      "I0710 07:19:33.062328 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L4_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:19:33.062365 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L4_rnn_fwd/wm/var:0\n",
      "I0710 07:19:33.062402 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L5_rnn_bak/b/var:0\n",
      "I0710 07:19:33.062438 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L5_rnn_bak/ln_scale/var:0\n",
      "I0710 07:19:33.062475 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L5_rnn_bak/wm/var:0\n",
      "I0710 07:19:33.062512 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L5_rnn_fwd/b/var:0\n",
      "I0710 07:19:33.062548 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L5_rnn_fwd/ln_scale/var:0\n",
      "I0710 07:19:33.062585 140310643746176 py_utils.py:2343] AdjustGradientsWithLpLoss: punctuator_rnmt/enc/L5_rnn_fwd/wm/var:0\n",
      "I0710 07:19:35.008782 140310643746176 learner.py:314] gradient_adjuster=<bound method BaseTask.AdjustGradients of <lingvo.tasks.punctuator.model.RNMTModel object at 0x7f9b1c3d62d0>>\n",
      "I0710 07:19:35.008975 140310643746176 base_model.py:534] BaseTask.AdjustGradients\n",
      "I0710 07:19:36.134192 140310643746176 cluster.py:525] Place variable beta1_power on /job:localhost/replica:0/task:0/device:CPU:0 1253256732\n",
      "I0710 07:19:36.137187 140310643746176 cluster.py:525] Place variable beta2_power on /job:localhost/replica:0/task:0/device:CPU:0 1253256736\n",
      "I0710 07:19:36.141788 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_0/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1257352736\n",
      "I0710 07:19:36.146281 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_0/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1261448736\n",
      "I0710 07:19:36.150796 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_1/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1265544736\n",
      "I0710 07:19:36.155290 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_1/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1269640736\n",
      "I0710 07:19:36.159776 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_2/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1273736736\n",
      "I0710 07:19:36.164374 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_2/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1277832736\n",
      "I0710 07:19:36.168821 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_3/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1281928736\n",
      "I0710 07:19:36.173487 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_3/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1286024736\n",
      "I0710 07:19:36.177925 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_4/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1290120736\n",
      "I0710 07:19:36.182531 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_4/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1294216736\n",
      "I0710 07:19:36.187176 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_5/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1298312736\n",
      "I0710 07:19:36.191904 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_5/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1302408736\n",
      "I0710 07:19:36.196620 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_6/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1306504736\n",
      "I0710 07:19:36.201258 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_6/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1310600736\n",
      "I0710 07:19:36.205721 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_7/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1314696736\n",
      "I0710 07:19:36.210202 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_7/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1318792736\n",
      "I0710 07:19:36.214824 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_8/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1322888736\n",
      "I0710 07:19:36.219424 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_8/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1326984736\n",
      "I0710 07:19:36.223999 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_9/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1331080736\n",
      "I0710 07:19:36.228743 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_9/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1335176736\n",
      "I0710 07:19:36.233256 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_10/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1339272736\n",
      "I0710 07:19:36.237987 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_10/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1343368736\n",
      "I0710 07:19:36.242635 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_11/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1347464736\n",
      "I0710 07:19:36.247145 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_11/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1351560736\n",
      "I0710 07:19:36.251724 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_12/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1355656736\n",
      "I0710 07:19:36.256181 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_12/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1359752736\n",
      "I0710 07:19:36.260853 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_13/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1363848736\n",
      "I0710 07:19:36.265452 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_13/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1367944736\n",
      "I0710 07:19:36.269944 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_14/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1372040736\n",
      "I0710 07:19:36.274759 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_14/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1376136736\n",
      "I0710 07:19:36.279475 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_15/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1380232736\n",
      "I0710 07:19:36.284095 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/emb/var_15/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1384328736\n",
      "I0710 07:19:36.288814 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn1/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1384345120\n",
      "I0710 07:19:36.293806 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn1/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1384361504\n",
      "I0710 07:19:36.298453 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn1/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1384377888\n",
      "I0710 07:19:36.303098 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn1/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1384394272\n",
      "I0710 07:19:36.307720 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn1/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1434725920\n",
      "I0710 07:19:36.312336 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn1/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1485057568\n",
      "I0710 07:19:36.317094 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn2/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1485073952\n",
      "I0710 07:19:36.321456 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn2/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1485090336\n",
      "I0710 07:19:36.326123 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn2/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1485106720\n",
      "I0710 07:19:36.330906 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn2/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1485123104\n",
      "I0710 07:19:36.335690 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn2/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1535454752\n",
      "I0710 07:19:36.340393 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn2/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1585786400\n",
      "I0710 07:19:36.345113 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn3/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1585802784\n",
      "I0710 07:19:36.349652 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn3/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1585819168\n",
      "I0710 07:19:36.354534 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn3/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1585835552\n",
      "I0710 07:19:36.359207 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn3/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1585851936\n",
      "I0710 07:19:36.364037 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn3/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1636183584\n",
      "I0710 07:19:36.369036 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn3/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1686515232\n",
      "I0710 07:19:36.373980 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn4/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1686531616\n",
      "I0710 07:19:36.378733 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn4/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1686548000\n",
      "I0710 07:19:36.383466 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn4/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1686564384\n",
      "I0710 07:19:36.388305 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn4/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1686580768\n",
      "I0710 07:19:36.393253 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn4/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1736912416\n",
      "I0710 07:19:36.398105 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn4/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1787244064\n",
      "I0710 07:19:36.403109 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn5/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1787260448\n",
      "I0710 07:19:36.407811 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn5/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1787276832\n",
      "I0710 07:19:36.412437 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn5/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1787293216\n",
      "I0710 07:19:36.416891 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn5/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1787309600\n",
      "I0710 07:19:36.421384 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn5/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1837641248\n",
      "I0710 07:19:36.428003 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn5/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1887972896\n",
      "I0710 07:19:36.432769 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn6/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1887989280\n",
      "I0710 07:19:36.437299 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn6/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1888005664\n",
      "I0710 07:19:36.442074 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn6/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1888022048\n",
      "I0710 07:19:36.446673 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn6/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1888038432\n",
      "I0710 07:19:36.451297 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn6/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1938370080\n",
      "I0710 07:19:36.455966 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn6/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1988701728\n",
      "I0710 07:19:36.460658 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn7/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1988718112\n",
      "I0710 07:19:36.465371 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn7/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1988734496\n",
      "I0710 07:19:36.469937 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn7/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 1988750880\n",
      "I0710 07:19:36.474629 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn7/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 1988767264\n",
      "I0710 07:19:36.479147 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn7/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2039098912\n",
      "I0710 07:19:36.483924 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/rnn7/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2089430560\n",
      "I0710 07:19:36.487576 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/inner_att/hidden_var/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2089431584\n",
      "I0710 07:19:36.491279 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/inner_att/hidden_var/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2089432608\n",
      "I0710 07:19:36.495848 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/inner_att/query_var/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2089694752\n",
      "I0710 07:19:36.500281 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/inner_att/query_var/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2089956896\n",
      "I0710 07:19:36.505064 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/inner_att/source_var/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2090219040\n",
      "I0710 07:19:36.509723 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/inner_att/source_var/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2090481184\n",
      "I0710 07:19:36.514532 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/query_proj/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2094675488\n",
      "I0710 07:19:36.519172 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/query_proj/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2098869792\n",
      "I0710 07:19:36.523813 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/query_proj_b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2098873888\n",
      "I0710 07:19:36.528365 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/query_proj_b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2098877984\n",
      "I0710 07:19:36.533219 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/source_proj/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2103072288\n",
      "I0710 07:19:36.537687 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/source_proj/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2107266592\n",
      "I0710 07:19:36.542513 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/source_proj_b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2107270688\n",
      "I0710 07:19:36.547241 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten/source_proj_b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2107274784\n",
      "I0710 07:19:36.551821 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten_rnn/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2107291168\n",
      "I0710 07:19:36.556286 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten_rnn/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2107307552\n",
      "I0710 07:19:36.561223 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten_rnn/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2107323936\n",
      "I0710 07:19:36.566131 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten_rnn/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2107340320\n",
      "I0710 07:19:36.570721 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten_rnn/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2157671968\n",
      "I0710 07:19:36.575459 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/atten_rnn/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208003616\n",
      "I0710 07:19:36.580208 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_0/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208007616\n",
      "I0710 07:19:36.584803 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_0/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208011616\n",
      "I0710 07:19:36.589349 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_1/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208015616\n",
      "I0710 07:19:36.594274 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_1/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208019616\n",
      "I0710 07:19:36.598789 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_10/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208023616\n",
      "I0710 07:19:36.603470 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_10/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208027616\n",
      "I0710 07:19:36.607920 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_11/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208031616\n",
      "I0710 07:19:36.612465 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_11/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208035616\n",
      "I0710 07:19:36.616973 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_12/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208039616\n",
      "I0710 07:19:36.621457 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_12/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208043616\n",
      "I0710 07:19:36.626296 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_13/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208047616\n",
      "I0710 07:19:36.630913 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_13/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208051616\n",
      "I0710 07:19:36.635698 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_14/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208055616\n",
      "I0710 07:19:36.640104 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_14/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208059616\n",
      "I0710 07:19:36.644787 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_15/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208063616\n",
      "I0710 07:19:36.649287 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_15/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208067616\n",
      "I0710 07:19:36.653829 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_2/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208071616\n",
      "I0710 07:19:36.658415 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_2/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208075616\n",
      "I0710 07:19:36.662955 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_3/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208079616\n",
      "I0710 07:19:36.667256 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_3/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208083616\n",
      "I0710 07:19:36.671734 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_4/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208087616\n",
      "I0710 07:19:36.676118 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_4/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208091616\n",
      "I0710 07:19:36.680674 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_5/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208095616\n",
      "I0710 07:19:36.685328 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_5/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208099616\n",
      "I0710 07:19:36.690142 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_6/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208103616\n",
      "I0710 07:19:36.694894 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_6/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208107616\n",
      "I0710 07:19:36.699684 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_7/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208111616\n",
      "I0710 07:19:36.704535 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_7/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208115616\n",
      "I0710 07:19:36.709214 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_8/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208119616\n",
      "I0710 07:19:36.713945 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_8/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208123616\n",
      "I0710 07:19:36.718581 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_9/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2208127616\n",
      "I0710 07:19:36.723391 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/bias_9/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2208131616\n",
      "I0710 07:19:36.728202 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_0/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2216323616\n",
      "I0710 07:19:36.733051 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_0/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2224515616\n",
      "I0710 07:19:36.737717 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_1/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2232707616\n",
      "I0710 07:19:36.742538 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_1/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2240899616\n",
      "I0710 07:19:36.747254 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_10/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2249091616\n",
      "I0710 07:19:36.751989 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_10/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2257283616\n",
      "I0710 07:19:36.756630 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_11/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2265475616\n",
      "I0710 07:19:36.761137 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_11/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2273667616\n",
      "I0710 07:19:36.765755 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_12/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2281859616\n",
      "I0710 07:19:36.770358 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_12/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2290051616\n",
      "I0710 07:19:36.775157 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_13/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2298243616\n",
      "I0710 07:19:36.779675 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_13/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2306435616\n",
      "I0710 07:19:36.784367 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_14/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2314627616\n",
      "I0710 07:19:36.788925 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_14/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2322819616\n",
      "I0710 07:19:36.793452 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_15/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2331011616\n",
      "I0710 07:19:36.798045 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_15/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2339203616\n",
      "I0710 07:19:36.802874 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_2/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2347395616\n",
      "I0710 07:19:36.807510 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_2/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2355587616\n",
      "I0710 07:19:36.812233 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_3/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2363779616\n",
      "I0710 07:19:36.816714 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_3/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2371971616\n",
      "I0710 07:19:36.822239 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_4/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2380163616\n",
      "I0710 07:19:36.826963 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_4/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2388355616\n",
      "I0710 07:19:36.831690 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_5/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2396547616\n",
      "I0710 07:19:36.836393 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_5/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2404739616\n",
      "I0710 07:19:36.840948 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_6/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2412931616\n",
      "I0710 07:19:36.845553 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_6/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2421123616\n",
      "I0710 07:19:36.849993 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_7/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2429315616\n",
      "I0710 07:19:36.854662 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_7/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2437507616\n",
      "I0710 07:19:36.859128 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_8/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2445699616\n",
      "I0710 07:19:36.863759 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_8/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2453891616\n",
      "I0710 07:19:36.868216 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_9/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2462083616\n",
      "I0710 07:19:36.872996 140310643746176 cluster.py:525] Place variable punctuator_rnmt/dec/softmax/weight_9/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2470275616\n",
      "I0710 07:19:36.877767 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_0/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2474371616\n",
      "I0710 07:19:36.882747 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_0/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2478467616\n",
      "I0710 07:19:36.887463 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_1/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2482563616\n",
      "I0710 07:19:36.892155 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_1/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2486659616\n",
      "I0710 07:19:36.896732 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_2/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2490755616\n",
      "I0710 07:19:36.901334 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_2/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2494851616\n",
      "I0710 07:19:36.906130 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_3/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2498947616\n",
      "I0710 07:19:36.910875 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_3/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2503043616\n",
      "I0710 07:19:36.915667 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_4/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2507139616\n",
      "I0710 07:19:36.920252 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_4/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2511235616\n",
      "I0710 07:19:36.924770 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_5/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2515331616\n",
      "I0710 07:19:36.929146 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_5/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2519427616\n",
      "I0710 07:19:36.933739 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_6/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2523523616\n",
      "I0710 07:19:36.938220 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_6/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2527619616\n",
      "I0710 07:19:36.942917 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_7/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2531715616\n",
      "I0710 07:19:36.947654 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_7/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2535811616\n",
      "I0710 07:19:36.952402 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_8/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2539907616\n",
      "I0710 07:19:36.956856 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_8/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2544003616\n",
      "I0710 07:19:36.961506 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_9/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2548099616\n",
      "I0710 07:19:36.966383 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_9/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2552195616\n",
      "I0710 07:19:36.970984 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_10/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2556291616\n",
      "I0710 07:19:36.975834 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_10/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2560387616\n",
      "I0710 07:19:36.980778 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_11/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2564483616\n",
      "I0710 07:19:36.985644 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_11/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2568579616\n",
      "I0710 07:19:36.990499 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_12/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2572675616\n",
      "I0710 07:19:36.995300 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_12/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2576771616\n",
      "I0710 07:19:36.999927 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_13/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2580867616\n",
      "I0710 07:19:37.004671 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_13/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2584963616\n",
      "I0710 07:19:37.009153 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_14/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2589059616\n",
      "I0710 07:19:37.013864 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_14/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2593155616\n",
      "I0710 07:19:37.018381 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_15/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2597251616\n",
      "I0710 07:19:37.023172 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/emb/var_15/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2601347616\n",
      "I0710 07:19:37.027744 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/proj/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2601351712\n",
      "I0710 07:19:37.032293 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/proj/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2601355808\n",
      "I0710 07:19:37.036778 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/proj/w/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2609744416\n",
      "I0710 07:19:37.041765 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/proj/w/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2618133024\n",
      "I0710 07:19:37.046671 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_bak/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2618149408\n",
      "I0710 07:19:37.051331 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_bak/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2618165792\n",
      "I0710 07:19:37.056053 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_bak/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2618182176\n",
      "I0710 07:19:37.060861 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_bak/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2618198560\n",
      "I0710 07:19:37.065792 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_bak/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2651752992\n",
      "I0710 07:19:37.070451 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_bak/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2685307424\n",
      "I0710 07:19:37.075224 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_fwd/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2685323808\n",
      "I0710 07:19:37.079891 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_fwd/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2685340192\n",
      "I0710 07:19:37.084798 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_fwd/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2685356576\n",
      "I0710 07:19:37.089482 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_fwd/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2685372960\n",
      "I0710 07:19:37.094295 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_fwd/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2718927392\n",
      "I0710 07:19:37.098961 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L0_rnn_fwd/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2752481824\n",
      "I0710 07:19:37.103674 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_bak/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2752498208\n",
      "I0710 07:19:37.108301 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_bak/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2752514592\n",
      "I0710 07:19:37.112960 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_bak/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2752530976\n",
      "I0710 07:19:37.117420 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_bak/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2752547360\n",
      "I0710 07:19:37.121932 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_bak/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2802879008\n",
      "I0710 07:19:37.126517 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_bak/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2853210656\n",
      "I0710 07:19:37.131073 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_fwd/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2853227040\n",
      "I0710 07:19:37.135461 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_fwd/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2853243424\n",
      "I0710 07:19:37.139958 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_fwd/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2853259808\n",
      "I0710 07:19:37.144710 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_fwd/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2853276192\n",
      "I0710 07:19:37.149420 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_fwd/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2903607840\n",
      "I0710 07:19:37.154145 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L1_rnn_fwd/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2953939488\n",
      "I0710 07:19:37.158755 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_bak/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2953955872\n",
      "I0710 07:19:37.163417 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_bak/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2953972256\n",
      "I0710 07:19:37.168097 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_bak/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 2953988640\n",
      "I0710 07:19:37.172704 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_bak/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 2954005024\n",
      "I0710 07:19:37.177274 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_bak/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3004336672\n",
      "I0710 07:19:37.181888 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_bak/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3054668320\n",
      "I0710 07:19:37.186388 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_fwd/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3054684704\n",
      "I0710 07:19:37.191204 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_fwd/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3054701088\n",
      "I0710 07:19:37.195918 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_fwd/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3054717472\n",
      "I0710 07:19:37.200779 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_fwd/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3054733856\n",
      "I0710 07:19:37.206614 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_fwd/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3105065504\n",
      "I0710 07:19:37.211287 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L2_rnn_fwd/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3155397152\n",
      "I0710 07:19:37.215804 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_bak/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3155413536\n",
      "I0710 07:19:37.220277 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_bak/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3155429920\n",
      "I0710 07:19:37.225067 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_bak/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3155446304\n",
      "I0710 07:19:37.229693 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_bak/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3155462688\n",
      "I0710 07:19:37.234474 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_bak/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3205794336\n",
      "I0710 07:19:37.239003 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_bak/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3256125984\n",
      "I0710 07:19:37.243476 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_fwd/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3256142368\n",
      "I0710 07:19:37.248131 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_fwd/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3256158752\n",
      "I0710 07:19:37.252749 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_fwd/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3256175136\n",
      "I0710 07:19:37.257160 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_fwd/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3256191520\n",
      "I0710 07:19:37.261873 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_fwd/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3306523168\n",
      "I0710 07:19:37.266586 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L3_rnn_fwd/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3356854816\n",
      "I0710 07:19:37.271559 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_bak/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3356871200\n",
      "I0710 07:19:37.276083 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_bak/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3356887584\n",
      "I0710 07:19:37.280768 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_bak/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3356903968\n",
      "I0710 07:19:37.285526 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_bak/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3356920352\n",
      "I0710 07:19:37.289986 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_bak/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3407252000\n",
      "I0710 07:19:37.294845 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_bak/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3457583648\n",
      "I0710 07:19:37.299458 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_fwd/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3457600032\n",
      "I0710 07:19:37.304418 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_fwd/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3457616416\n",
      "I0710 07:19:37.308888 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_fwd/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3457632800\n",
      "I0710 07:19:37.313579 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_fwd/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3457649184\n",
      "I0710 07:19:37.318108 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_fwd/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3507980832\n",
      "I0710 07:19:37.322919 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L4_rnn_fwd/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3558312480\n",
      "I0710 07:19:37.327413 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_bak/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3558328864\n",
      "I0710 07:19:37.332080 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_bak/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3558345248\n",
      "I0710 07:19:37.336703 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_bak/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3558361632\n",
      "I0710 07:19:37.341400 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_bak/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3558378016\n",
      "I0710 07:19:37.346120 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_bak/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3608709664\n",
      "I0710 07:19:37.350889 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_bak/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3659041312\n",
      "I0710 07:19:37.355380 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_fwd/b/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3659057696\n",
      "I0710 07:19:37.359952 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_fwd/b/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3659074080\n",
      "I0710 07:19:37.364769 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_fwd/ln_scale/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3659090464\n",
      "I0710 07:19:37.369250 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_fwd/ln_scale/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3659106848\n",
      "I0710 07:19:37.373709 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_fwd/wm/var/Adam on /job:localhost/replica:0/task:0/device:CPU:0 3709438496\n",
      "I0710 07:19:37.378247 140310643746176 cluster.py:525] Place variable punctuator_rnmt/enc/L5_rnn_fwd/wm/var/Adam_1 on /job:localhost/replica:0/task:0/device:CPU:0 3759770144\n",
      "I0710 07:19:38.176346 140310643746176 trainer.py:380] Trainer number of enqueue ops: 0\n",
      "I0710 07:19:38.176534 140310643746176 trainer.py:389] AttributeError. Expected for single task models.\n",
      "I0710 07:19:43.980799 140310643746176 trainer.py:1609] Starting runners\n",
      "I0710 07:19:43.981286 140295635035904 base_runner.py:192] controller started.\n",
      "I0710 07:19:43.994005 140295626643200 base_runner.py:192] trainer started.\n",
      "I0710 07:19:43.994277 140310643746176 trainer.py:1630] Waiting for runners to finish...\n",
      "I0710 07:19:43.994400 140310643746176 trainer.py:1632] Waiting for thread to finish: <__main__.Controller object at 0x7f9c1c196b50>\n",
      "I0710 07:19:47.691087 140295626643200 base_runner.py:152] trainer: Probably the expected race on global_step: From /job:localhost/replica:0/task:0:\n",
      "Error while reading resource variable global_step from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/global_step)\n",
      "\t [[node global_step/Read/ReadVariableOp (defined at home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/py_utils.py:1766) ]]\n",
      "\n",
      "Original stack trace for 'global_step/Read/ReadVariableOp':\n",
      "  File \"opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1862, in <module>\n",
      "    tf.app.run(main)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"opt/conda/lib/python3.7/site-packages/absl/app.py\", line 299, in run\n",
      "    _run_main(main, args)\n",
      "  File \"opt/conda/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1853, in main\n",
      "    RunnerManager(FLAGS.model).Start()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1849, in Start\n",
      "    self.StartRunners(self.CreateRunners(FLAGS.job.split(','), FLAGS.logdir))\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1593, in CreateRunners\n",
      "    trial)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1552, in _CreateRunner\n",
      "    return self.Trainer(cfg, *common_args)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 373, in __init__\n",
      "    self._model = self.params.Instantiate()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/hyperparams.py\", line 848, in Instantiate\n",
      "    return self.cls(self)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1221, in __init__\n",
      "    super(SingleTaskModel, self).__init__(p)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1150, in __init__\n",
      "    super(SingleTaskBase, self).__init__(params)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1061, in __init__\n",
      "    self._global_step_var = py_utils.GetOrCreateGlobalStepVar()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/py_utils.py\", line 1766, in GetOrCreateGlobalStepVar\n",
      "    return tf.train.get_or_create_global_step()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/training_util.py\", line 161, in get_or_create_global_step\n",
      "    global_step_tensor = create_global_step(graph)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/training_util.py\", line 144, in create_global_step\n",
      "    collections=[ops.GraphKeys.GLOBAL_VARIABLES, ops.GraphKeys.GLOBAL_STEP])\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1573, in get_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1316, in get_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 568, in get_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 520, in _true_getter\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 939, in _get_single_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 259, in __call__\n",
      "    return cls._variable_v1_call(*args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 220, in _variable_v1_call\n",
      "    shape=shape)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 198, in <lambda>\n",
      "    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2598, in default_variable_creator\n",
      "    shape=shape)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 263, in __call__\n",
      "    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1434, in __init__\n",
      "    distribute_strategy=distribute_strategy)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1617, in _init_from_args\n",
      "    value = gen_resource_variable_ops.read_variable_op(handle, dtype)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 483, in read_variable_op\n",
      "    \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3327, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1791, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "I0710 07:19:48.624173 140295635035904 checkpointer.py:90] Load from checkpoint /tmp/punctuator/train/ckpt-00004536.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/punctuator/train/ckpt-00004536\n",
      "I0710 07:19:48.625134 140295635035904 saver.py:1293] Restoring parameters from /tmp/punctuator/train/ckpt-00004536\n",
      "I0710 07:19:48.695130 140295626643200 retry.py:71] Retry: caught exception: _WaitUntilInit while running tensorflow.python.framework.errors_impl.FailedPreconditionError: From /job:localhost/replica:0/task:0:\n",
      "Error while reading resource variable global_step from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/global_step)\n",
      "\t [[node global_step/Read/ReadVariableOp (defined at home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/py_utils.py:1766) ]]\n",
      "\n",
      "Original stack trace for 'global_step/Read/ReadVariableOp':\n",
      "  File \"opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1862, in <module>\n",
      "    tf.app.run(main)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"opt/conda/lib/python3.7/site-packages/absl/app.py\", line 299, in run\n",
      "    _run_main(main, args)\n",
      "  File \"opt/conda/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1853, in main\n",
      "    RunnerManager(FLAGS.model).Start()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1849, in Start\n",
      "    self.StartRunners(self.CreateRunners(FLAGS.job.split(','), FLAGS.logdir))\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1593, in CreateRunners\n",
      "    trial)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1552, in _CreateRunner\n",
      "    return self.Trainer(cfg, *common_args)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 373, in __init__\n",
      "    self._model = self.params.Instantiate()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/hyperparams.py\", line 848, in Instantiate\n",
      "    return self.cls(self)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1221, in __init__\n",
      "    super(SingleTaskModel, self).__init__(p)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1150, in __init__\n",
      "    super(SingleTaskBase, self).__init__(params)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1061, in __init__\n",
      "    self._global_step_var = py_utils.GetOrCreateGlobalStepVar()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/py_utils.py\", line 1766, in GetOrCreateGlobalStepVar\n",
      "    return tf.train.get_or_create_global_step()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/training_util.py\", line 161, in get_or_create_global_step\n",
      "    global_step_tensor = create_global_step(graph)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/training_util.py\", line 144, in create_global_step\n",
      "    collections=[ops.GraphKeys.GLOBAL_VARIABLES, ops.GraphKeys.GLOBAL_STEP])\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1573, in get_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1316, in get_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 568, in get_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 520, in _true_getter\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 939, in _get_single_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 259, in __call__\n",
      "    return cls._variable_v1_call(*args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 220, in _variable_v1_call\n",
      "    shape=shape)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 198, in <lambda>\n",
      "    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2598, in default_variable_creator\n",
      "    shape=shape)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 263, in __call__\n",
      "    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1434, in __init__\n",
      "    distribute_strategy=distribute_strategy)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1617, in _init_from_args\n",
      "    value = gen_resource_variable_ops.read_variable_op(handle, dtype)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 483, in read_variable_op\n",
      "    \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3327, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1791, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      ". Call failed at (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 890, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 415, in Start\n",
      "    self._RunLoop('trainer', self._Loop)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/retry.py\", line 53, in Wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/base_runner.py\", line 193, in _RunLoop\n",
      "    loop_func(*loop_args)\n",
      "Traceback for above exception (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/retry.py\", line 53, in Wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/base_runner.py\", line 149, in _WaitUntilInit\n",
      "    global_step = sess.run(py_utils.GetGlobalStep())\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 958, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1181, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1359, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1384, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "Waiting for 1.51 seconds before retrying.\n",
      "I0710 07:19:48.697140 140295626643200 base_runner.py:152] trainer: Probably the expected race on global_step: From /job:localhost/replica:0/task:0:\n",
      "Error while reading resource variable global_step from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/global_step)\n",
      "\t [[node global_step/Read/ReadVariableOp (defined at home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/py_utils.py:1766) ]]\n",
      "\n",
      "Original stack trace for 'global_step/Read/ReadVariableOp':\n",
      "  File \"opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1862, in <module>\n",
      "    tf.app.run(main)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"opt/conda/lib/python3.7/site-packages/absl/app.py\", line 299, in run\n",
      "    _run_main(main, args)\n",
      "  File \"opt/conda/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1853, in main\n",
      "    RunnerManager(FLAGS.model).Start()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1849, in Start\n",
      "    self.StartRunners(self.CreateRunners(FLAGS.job.split(','), FLAGS.logdir))\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1593, in CreateRunners\n",
      "    trial)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1552, in _CreateRunner\n",
      "    return self.Trainer(cfg, *common_args)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 373, in __init__\n",
      "    self._model = self.params.Instantiate()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/hyperparams.py\", line 848, in Instantiate\n",
      "    return self.cls(self)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1221, in __init__\n",
      "    super(SingleTaskModel, self).__init__(p)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1150, in __init__\n",
      "    super(SingleTaskBase, self).__init__(params)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1061, in __init__\n",
      "    self._global_step_var = py_utils.GetOrCreateGlobalStepVar()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/py_utils.py\", line 1766, in GetOrCreateGlobalStepVar\n",
      "    return tf.train.get_or_create_global_step()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/training_util.py\", line 161, in get_or_create_global_step\n",
      "    global_step_tensor = create_global_step(graph)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/training_util.py\", line 144, in create_global_step\n",
      "    collections=[ops.GraphKeys.GLOBAL_VARIABLES, ops.GraphKeys.GLOBAL_STEP])\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1573, in get_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1316, in get_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 568, in get_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 520, in _true_getter\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 939, in _get_single_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 259, in __call__\n",
      "    return cls._variable_v1_call(*args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 220, in _variable_v1_call\n",
      "    shape=shape)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 198, in <lambda>\n",
      "    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2598, in default_variable_creator\n",
      "    shape=shape)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 263, in __call__\n",
      "    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1434, in __init__\n",
      "    distribute_strategy=distribute_strategy)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1617, in _init_from_args\n",
      "    value = gen_resource_variable_ops.read_variable_op(handle, dtype)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 483, in read_variable_op\n",
      "    \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3327, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1791, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "I0710 07:19:50.206302 140295626643200 retry.py:71] Retry: caught exception: _WaitUntilInit while running tensorflow.python.framework.errors_impl.FailedPreconditionError: From /job:localhost/replica:0/task:0:\n",
      "Error while reading resource variable global_step from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/global_step)\n",
      "\t [[node global_step/Read/ReadVariableOp (defined at home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/py_utils.py:1766) ]]\n",
      "\n",
      "Original stack trace for 'global_step/Read/ReadVariableOp':\n",
      "  File \"opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1862, in <module>\n",
      "    tf.app.run(main)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"opt/conda/lib/python3.7/site-packages/absl/app.py\", line 299, in run\n",
      "    _run_main(main, args)\n",
      "  File \"opt/conda/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1853, in main\n",
      "    RunnerManager(FLAGS.model).Start()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1849, in Start\n",
      "    self.StartRunners(self.CreateRunners(FLAGS.job.split(','), FLAGS.logdir))\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1593, in CreateRunners\n",
      "    trial)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1552, in _CreateRunner\n",
      "    return self.Trainer(cfg, *common_args)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 373, in __init__\n",
      "    self._model = self.params.Instantiate()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/hyperparams.py\", line 848, in Instantiate\n",
      "    return self.cls(self)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1221, in __init__\n",
      "    super(SingleTaskModel, self).__init__(p)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1150, in __init__\n",
      "    super(SingleTaskBase, self).__init__(params)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1061, in __init__\n",
      "    self._global_step_var = py_utils.GetOrCreateGlobalStepVar()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/py_utils.py\", line 1766, in GetOrCreateGlobalStepVar\n",
      "    return tf.train.get_or_create_global_step()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/training_util.py\", line 161, in get_or_create_global_step\n",
      "    global_step_tensor = create_global_step(graph)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/training_util.py\", line 144, in create_global_step\n",
      "    collections=[ops.GraphKeys.GLOBAL_VARIABLES, ops.GraphKeys.GLOBAL_STEP])\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1573, in get_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1316, in get_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 568, in get_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 520, in _true_getter\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 939, in _get_single_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 259, in __call__\n",
      "    return cls._variable_v1_call(*args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 220, in _variable_v1_call\n",
      "    shape=shape)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 198, in <lambda>\n",
      "    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2598, in default_variable_creator\n",
      "    shape=shape)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 263, in __call__\n",
      "    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1434, in __init__\n",
      "    distribute_strategy=distribute_strategy)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1617, in _init_from_args\n",
      "    value = gen_resource_variable_ops.read_variable_op(handle, dtype)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 483, in read_variable_op\n",
      "    \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3327, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1791, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      ". Call failed at (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 890, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 415, in Start\n",
      "    self._RunLoop('trainer', self._Loop)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/retry.py\", line 53, in Wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/base_runner.py\", line 193, in _RunLoop\n",
      "    loop_func(*loop_args)\n",
      "Traceback for above exception (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/retry.py\", line 53, in Wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/base_runner.py\", line 149, in _WaitUntilInit\n",
      "    global_step = sess.run(py_utils.GetGlobalStep())\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 958, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1181, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1359, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1384, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "Waiting for 2.27 seconds before retrying.\n",
      "I0710 07:19:50.208730 140295626643200 base_runner.py:152] trainer: Probably the expected race on global_step: From /job:localhost/replica:0/task:0:\n",
      "Error while reading resource variable global_step from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/global_step)\n",
      "\t [[node global_step/Read/ReadVariableOp (defined at home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/py_utils.py:1766) ]]\n",
      "\n",
      "Original stack trace for 'global_step/Read/ReadVariableOp':\n",
      "  File \"opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1862, in <module>\n",
      "    tf.app.run(main)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"opt/conda/lib/python3.7/site-packages/absl/app.py\", line 299, in run\n",
      "    _run_main(main, args)\n",
      "  File \"opt/conda/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1853, in main\n",
      "    RunnerManager(FLAGS.model).Start()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1849, in Start\n",
      "    self.StartRunners(self.CreateRunners(FLAGS.job.split(','), FLAGS.logdir))\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1593, in CreateRunners\n",
      "    trial)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1552, in _CreateRunner\n",
      "    return self.Trainer(cfg, *common_args)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 373, in __init__\n",
      "    self._model = self.params.Instantiate()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/hyperparams.py\", line 848, in Instantiate\n",
      "    return self.cls(self)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1221, in __init__\n",
      "    super(SingleTaskModel, self).__init__(p)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1150, in __init__\n",
      "    super(SingleTaskBase, self).__init__(params)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1061, in __init__\n",
      "    self._global_step_var = py_utils.GetOrCreateGlobalStepVar()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/py_utils.py\", line 1766, in GetOrCreateGlobalStepVar\n",
      "    return tf.train.get_or_create_global_step()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/training_util.py\", line 161, in get_or_create_global_step\n",
      "    global_step_tensor = create_global_step(graph)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/training_util.py\", line 144, in create_global_step\n",
      "    collections=[ops.GraphKeys.GLOBAL_VARIABLES, ops.GraphKeys.GLOBAL_STEP])\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1573, in get_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1316, in get_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 568, in get_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 520, in _true_getter\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 939, in _get_single_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 259, in __call__\n",
      "    return cls._variable_v1_call(*args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 220, in _variable_v1_call\n",
      "    shape=shape)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 198, in <lambda>\n",
      "    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2598, in default_variable_creator\n",
      "    shape=shape)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 263, in __call__\n",
      "    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1434, in __init__\n",
      "    distribute_strategy=distribute_strategy)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1617, in _init_from_args\n",
      "    value = gen_resource_variable_ops.read_variable_op(handle, dtype)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 483, in read_variable_op\n",
      "    \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3327, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1791, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "I0710 07:19:52.379585 140295635035904 checkpointer.py:92] Load checkpoint done.\n",
      "I0710 07:19:52.481104 140295626643200 retry.py:71] Retry: caught exception: _WaitUntilInit while running tensorflow.python.framework.errors_impl.FailedPreconditionError: From /job:localhost/replica:0/task:0:\n",
      "Error while reading resource variable global_step from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/global_step)\n",
      "\t [[node global_step/Read/ReadVariableOp (defined at home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/py_utils.py:1766) ]]\n",
      "\n",
      "Original stack trace for 'global_step/Read/ReadVariableOp':\n",
      "  File \"opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1862, in <module>\n",
      "    tf.app.run(main)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"opt/conda/lib/python3.7/site-packages/absl/app.py\", line 299, in run\n",
      "    _run_main(main, args)\n",
      "  File \"opt/conda/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1853, in main\n",
      "    RunnerManager(FLAGS.model).Start()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1849, in Start\n",
      "    self.StartRunners(self.CreateRunners(FLAGS.job.split(','), FLAGS.logdir))\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1593, in CreateRunners\n",
      "    trial)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1552, in _CreateRunner\n",
      "    return self.Trainer(cfg, *common_args)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 373, in __init__\n",
      "    self._model = self.params.Instantiate()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/hyperparams.py\", line 848, in Instantiate\n",
      "    return self.cls(self)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1221, in __init__\n",
      "    super(SingleTaskModel, self).__init__(p)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1150, in __init__\n",
      "    super(SingleTaskBase, self).__init__(params)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1061, in __init__\n",
      "    self._global_step_var = py_utils.GetOrCreateGlobalStepVar()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/py_utils.py\", line 1766, in GetOrCreateGlobalStepVar\n",
      "    return tf.train.get_or_create_global_step()\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/training_util.py\", line 161, in get_or_create_global_step\n",
      "    global_step_tensor = create_global_step(graph)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/training_util.py\", line 144, in create_global_step\n",
      "    collections=[ops.GraphKeys.GLOBAL_VARIABLES, ops.GraphKeys.GLOBAL_STEP])\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1573, in get_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1316, in get_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 568, in get_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 520, in _true_getter\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 939, in _get_single_variable\n",
      "    aggregation=aggregation)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 259, in __call__\n",
      "    return cls._variable_v1_call(*args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 220, in _variable_v1_call\n",
      "    shape=shape)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 198, in <lambda>\n",
      "    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2598, in default_variable_creator\n",
      "    shape=shape)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 263, in __call__\n",
      "    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1434, in __init__\n",
      "    distribute_strategy=distribute_strategy)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1617, in _init_from_args\n",
      "    value = gen_resource_variable_ops.read_variable_op(handle, dtype)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 483, in read_variable_op\n",
      "    \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3327, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1791, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      ". Call failed at (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 890, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 415, in Start\n",
      "    self._RunLoop('trainer', self._Loop)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/retry.py\", line 53, in Wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/base_runner.py\", line 193, in _RunLoop\n",
      "    loop_func(*loop_args)\n",
      "Traceback for above exception (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/retry.py\", line 53, in Wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/base_runner.py\", line 149, in _WaitUntilInit\n",
      "    global_step = sess.run(py_utils.GetGlobalStep())\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 958, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1181, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1359, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1384, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "Waiting for 3.41 seconds before retrying.\n",
      "I0710 07:19:52.482028 140295626643200 base_runner.py:111] step:  4536\n",
      "I0710 07:19:53.187035 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "2020-07-10 07:20:15.974390: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-07-10 07:20:16.613920: I ./lingvo/core/ops/input_common.h:74] Create RecordProcessor\n",
      "2020-07-10 07:20:16.623914: I lingvo/core/ops/input_common.cc:34] Input source weights are empty, fall back to legacy behavior.\n",
      "2020-07-10 07:20:16.624034: I lingvo/core/ops/record_yielder.cc:376] 0x7f9441878ff0 Record yielder start\n",
      "2020-07-10 07:20:16.624055: I lingvo/core/ops/record_yielder.cc:378] Randomly seed RecordYielder.\n",
      "2020-07-10 07:20:16.624079: I ./lingvo/core/ops/input_common.h:80] Create batcher\n",
      "2020-07-10 07:20:16.624129: I lingvo/core/ops/record_yielder.cc:485] Epoch 1 /tmp/punctuator_data/train.txt\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 07:20:26.684766 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 07:20:27.185011 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00004536\n",
      "I0710 07:20:27.185226 140295635035904 trainer.py:345] Write summary @4536\n",
      "I0710 07:20:31.144151 140295626643200 summary_utils.py:349] Steps/second: 0.000000, Examples/second: 0.000000\n",
      "I0710 07:20:31.145125 140295626643200 base_runner.py:111] step:  4537, steps/sec: 0.00, examples/sec: 0.00 grad_norm/all/loss:135.98402 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2424124 log_pplx:4.676312 loss:181.73318 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.58862\n",
      "I0710 07:20:39.607878 140295626643200 summary_utils.py:349] Steps/second: 0.118152, Examples/second: 9.452122\n",
      "I0710 07:20:39.608686 140295626643200 trainer.py:508] step:  4538, steps/sec: 0.12, examples/sec: 9.45 grad_norm/all/loss:139.84486 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2427225 log_pplx:4.7488217 loss:197.96651 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.5929\n",
      "I0710 07:20:43.545100 140295626643200 summary_utils.py:349] Steps/second: 0.161279, Examples/second: 19.353420\n",
      "I0710 07:20:43.546243 140295626643200 trainer.py:508] step:  4539, steps/sec: 0.16, examples/sec: 19.35 grad_norm/all/loss:80.660332 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2444968 log_pplx:4.9727859 loss:124.848 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.59784\n",
      "I0710 07:20:46.774302 140295626643200 summary_utils.py:349] Steps/second: 0.191937, Examples/second: 31.733626\n",
      "I0710 07:20:46.775357 140295626643200 trainer.py:508] step:  4540, steps/sec: 0.19, examples/sec: 31.73 grad_norm/all/loss:28.541979 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2438821 log_pplx:4.7861209 loss:75.250534 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:515.60291\n",
      "I0710 07:20:53.479822 140295626643200 summary_utils.py:349] Steps/second: 0.179086, Examples/second: 25.788365\n",
      "I0710 07:20:53.480697 140295626643200 trainer.py:508] step:  4541, steps/sec: 0.18, examples/sec: 25.79 grad_norm/all/loss:95.179443 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2428523 log_pplx:4.7562695 loss:197.74188 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.60767\n",
      "2020-07-10 07:20:56.720760: I ./lingvo/core/ops/input_common.h:74] Create RecordProcessor\n",
      "2020-07-10 07:20:56.834819: I lingvo/core/ops/input_common.cc:34] Input source weights are empty, fall back to legacy behavior.\n",
      "2020-07-10 07:20:56.834969: I lingvo/core/ops/record_yielder.cc:376] 0x7f9ba4a22c00 Record yielder start\n",
      "2020-07-10 07:20:56.834983: I lingvo/core/ops/record_yielder.cc:378] Randomly seed RecordYielder.\n",
      "2020-07-10 07:20:56.835003: I ./lingvo/core/ops/input_common.h:80] Create batcher\n",
      "2020-07-10 07:20:56.835103: I lingvo/core/ops/record_yielder.cc:485] Epoch 1 /tmp/punctuator_data/train.txt\n",
      "I0710 07:21:02.857845 140295626643200 summary_utils.py:349] Steps/second: 0.157661, Examples/second: 20.685095\n",
      "I0710 07:21:02.858873 140295626643200 trainer.py:508] step:  4542, steps/sec: 0.16, examples/sec: 20.69 grad_norm/all/loss:105.10492 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2425231 log_pplx:4.7098999 loss:185.33455 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.61255\n",
      "I0710 07:21:07.960185 140295626643200 summary_utils.py:349] Steps/second: 0.162973, Examples/second: 22.164274\n",
      "I0710 07:21:07.961482 140295626643200 trainer.py:508] step:  4543, steps/sec: 0.16, examples/sec: 22.16 grad_norm/all/loss:117.81812 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2441808 log_pplx:4.9329367 loss:124.95744 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.61731\n",
      "I0710 07:21:23.644355 140295626643200 summary_utils.py:349] Steps/second: 0.133333, Examples/second: 16.304715\n",
      "I0710 07:21:23.645579 140295626643200 trainer.py:508] step:  4544, steps/sec: 0.13, examples/sec: 16.30 grad_norm/all/loss:195.31453 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2418931 log_pplx:4.0761147 loss:300.00201 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:515.62128\n",
      "2020-07-10 07:21:23.674909: I lingvo/core/ops/record_batcher.cc:394] 67 total seconds passed. Total records yielded: 1861. Total records skipped: 1\n",
      "2020-07-10 07:21:23.675247: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "I0710 07:21:33.170982 140295626643200 summary_utils.py:349] Steps/second: 0.128977, Examples/second: 15.090258\n",
      "I0710 07:21:33.172392 140295626643200 trainer.py:508] step:  4545, steps/sec: 0.13, examples/sec: 15.09 grad_norm/all/loss:70.785179 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2427834 log_pplx:4.5930667 loss:188.54538 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.6261\n",
      "I0710 07:21:37.188590 140295626643200 summary_utils.py:349] Steps/second: 0.136272, Examples/second: 18.048469\n",
      "I0710 07:21:37.189692 140295626643200 trainer.py:508] step:  4546, steps/sec: 0.14, examples/sec: 18.05 grad_norm/all/loss:30.884872 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2437956 log_pplx:4.7235923 loss:73.28949 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:515.63098\n",
      "I0710 07:21:47.190530 140295626643200 summary_utils.py:349] Steps/second: 0.131499, Examples/second: 16.726647\n",
      "I0710 07:21:47.191785 140295626643200 trainer.py:508] step:  4547, steps/sec: 0.13, examples/sec: 16.73 grad_norm/all/loss:78.156578 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2430021 log_pplx:4.673924 loss:192.33195 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.63562\n",
      "I0710 07:21:53.233163 140295626643200 summary_utils.py:349] Steps/second: 0.134001, Examples/second: 17.444490\n",
      "I0710 07:21:53.234449 140295626643200 trainer.py:508] step:  4548, steps/sec: 0.13, examples/sec: 17.44 grad_norm/all/loss:50.034176 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2445936 log_pplx:4.8842402 loss:123.99866 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.64008\n",
      "I0710 07:22:04.132134 140295635035904 trainer.py:354] Write summary done: step 4536\n",
      "I0710 07:22:04.347554 140295626643200 summary_utils.py:349] Steps/second: 0.128751, Examples/second: 16.222587\n",
      "I0710 07:22:04.348318 140295626643200 trainer.py:508] step:  4549, steps/sec: 0.13, examples/sec: 16.22 grad_norm/all/loss:73.621956 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2429067 log_pplx:4.7662873 loss:189.69827 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.64429\n",
      "I0710 07:22:10.713748 140295626643200 summary_utils.py:349] Steps/second: 0.130562, Examples/second: 15.988820\n",
      "I0710 07:22:10.714543 140295626643200 trainer.py:508] step:  4550, steps/sec: 0.13, examples/sec: 15.99 grad_norm/all/loss:86.428284 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2427101 log_pplx:4.6187811 loss:184.69351 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.64856\n",
      "I0710 07:22:14.561106 140295626643200 summary_utils.py:349] Steps/second: 0.135374, Examples/second: 16.941134\n",
      "I0710 07:22:14.561929 140295626643200 trainer.py:508] step:  4551, steps/sec: 0.14, examples/sec: 16.94 grad_norm/all/loss:48.838791 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2442654 log_pplx:4.8536038 loss:122.91752 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.65271\n",
      "I0710 07:22:20.618998 140295626643200 summary_utils.py:349] Steps/second: 0.137018, Examples/second: 16.734440\n",
      "I0710 07:22:20.619750 140295626643200 trainer.py:508] step:  4552, steps/sec: 0.14, examples/sec: 16.73 grad_norm/all/loss:119.763 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2428548 log_pplx:4.7931619 loss:189.15013 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.6568\n",
      "I0710 07:22:23.604205 140295626643200 summary_utils.py:349] Steps/second: 0.142273, Examples/second: 18.566596\n",
      "I0710 07:22:23.605023 140295626643200 trainer.py:508] step:  4553, steps/sec: 0.14, examples/sec: 18.57 grad_norm/all/loss:23.305864 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2438878 log_pplx:4.6164589 loss:70.905914 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:515.66132\n",
      "I0710 07:22:34.905891 140295626643200 summary_utils.py:349] Steps/second: 0.137361, Examples/second: 17.194331\n",
      "I0710 07:22:34.906689 140295626643200 trainer.py:508] step:  4554, steps/sec: 0.14, examples/sec: 17.19 grad_norm/all/loss:245.03888 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2420171 log_pplx:4.152441 loss:298.24908 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:515.66571\n",
      "I0710 07:22:41.279592 140295626643200 summary_utils.py:349] Steps/second: 0.138317, Examples/second: 16.966940\n",
      "I0710 07:22:41.280328 140295626643200 trainer.py:508] step:  4555, steps/sec: 0.14, examples/sec: 16.97 grad_norm/all/loss:89.298698 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.24326 log_pplx:4.7610078 loss:194.42766 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.67072\n",
      "I0710 07:22:47.682640 140295626643200 summary_utils.py:349] Steps/second: 0.139155, Examples/second: 16.757182\n",
      "I0710 07:22:47.683399 140295626643200 trainer.py:508] step:  4556, steps/sec: 0.14, examples/sec: 16.76 grad_norm/all/loss:75.043007 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.243041 log_pplx:4.7737794 loss:192.68169 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.6756\n",
      "I0710 07:22:51.567503 140295626643200 summary_utils.py:349] Steps/second: 0.142426, Examples/second: 17.433003\n",
      "I0710 07:22:51.568602 140295626643200 trainer.py:508] step:  4557, steps/sec: 0.14, examples/sec: 17.43 grad_norm/all/loss:40.323093 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2449497 log_pplx:4.9001412 loss:123.20792 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.68054\n",
      "I0710 07:22:58.088679 140295626643200 summary_utils.py:349] Steps/second: 0.142911, Examples/second: 17.203774\n",
      "I0710 07:22:58.089558 140295626643200 trainer.py:508] step:  4558, steps/sec: 0.14, examples/sec: 17.20 grad_norm/all/loss:107.09924 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2434204 log_pplx:4.7504845 loss:194.05728 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.6853\n",
      "I0710 07:23:01.200222 140295626643200 summary_utils.py:349] Steps/second: 0.146612, Examples/second: 18.553067\n",
      "I0710 07:23:01.200960 140295626643200 trainer.py:508] step:  4559, steps/sec: 0.15, examples/sec: 18.55 grad_norm/all/loss:21.552141 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2445867 log_pplx:4.7164965 loss:74.284821 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:515.69031\n",
      "I0710 07:23:03.404586 140295626643200 summary_utils.py:349] Steps/second: 0.151057, Examples/second: 21.647123\n",
      "I0710 07:23:03.405396 140295626643200 trainer.py:508] step:  4560, steps/sec: 0.15, examples/sec: 21.65 grad_norm/all/loss:12.615334 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2429338 log_pplx:4.48001 loss:32.488823 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:515.69513\n",
      "I0710 07:23:07.334985 140295626643200 summary_utils.py:349] Steps/second: 0.153658, Examples/second: 22.126781\n",
      "I0710 07:23:07.335734 140295626643200 trainer.py:508] step:  4561, steps/sec: 0.15, examples/sec: 22.13 grad_norm/all/loss:47.131672 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.245025 log_pplx:4.8730998 loss:124.29451 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.69965\n",
      "I0710 07:23:13.577667 140295626643200 summary_utils.py:349] Steps/second: 0.153909, Examples/second: 21.768909\n",
      "I0710 07:23:13.578459 140295626643200 trainer.py:508] step:  4562, steps/sec: 0.15, examples/sec: 21.77 grad_norm/all/loss:103.28821 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2432017 log_pplx:4.6946411 loss:189.48746 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.70398\n",
      "I0710 07:23:19.688043 140295626643200 summary_utils.py:349] Steps/second: 0.154263, Examples/second: 21.454356\n",
      "I0710 07:23:19.688841 140295626643200 trainer.py:508] step:  4563, steps/sec: 0.15, examples/sec: 21.45 grad_norm/all/loss:87.317528 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2431035 log_pplx:4.6734838 loss:188.6335 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.70874\n",
      "I0710 07:23:22.736280 140295626643200 summary_utils.py:349] Steps/second: 0.157350, Examples/second: 22.565146\n",
      "I0710 07:23:22.737270 140295626643200 trainer.py:508] step:  4564, steps/sec: 0.16, examples/sec: 22.57 grad_norm/all/loss:23.285704 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2443949 log_pplx:4.7462106 loss:73.677505 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:515.71381\n",
      "2020-07-10 07:23:24.006314: I lingvo/core/ops/record_batcher.cc:394] 188 total seconds passed. Total records yielded: 4618. Total records skipped: 2\n",
      "2020-07-10 07:23:24.006366: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "I0710 07:23:33.700052 140295626643200 summary_utils.py:349] Steps/second: 0.153378, Examples/second: 21.429055\n",
      "I0710 07:23:33.701091 140295626643200 trainer.py:508] step:  4565, steps/sec: 0.15, examples/sec: 21.43 grad_norm/all/loss:381.70062 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2421829 log_pplx:4.396955 loss:313.94257 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:515.71863\n",
      "I0710 07:23:39.916646 140295626643200 summary_utils.py:349] Steps/second: 0.153624, Examples/second: 21.147150\n",
      "I0710 07:23:39.917630 140295626643200 trainer.py:508] step:  4566, steps/sec: 0.15, examples/sec: 21.15 grad_norm/all/loss:164.98776 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2435673 log_pplx:4.8658032 loss:198.099 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.72479\n",
      "I0710 07:23:43.722631 140295626643200 summary_utils.py:349] Steps/second: 0.155781, Examples/second: 21.560043\n",
      "I0710 07:23:43.723408 140295626643200 trainer.py:508] step:  4567, steps/sec: 0.16, examples/sec: 21.56 grad_norm/all/loss:75.825211 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2450315 log_pplx:4.9382734 loss:123.27165 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.73083\n",
      "I0710 07:23:49.765445 140295626643200 summary_utils.py:349] Steps/second: 0.156076, Examples/second: 21.306882\n",
      "I0710 07:23:49.766313 140295626643200 trainer.py:508] step:  4568, steps/sec: 0.16, examples/sec: 21.31 grad_norm/all/loss:228.81113 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2431717 log_pplx:4.8651071 loss:194.84756 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.73663\n",
      "I0710 07:23:55.770443 140295626643200 summary_utils.py:349] Steps/second: 0.156383, Examples/second: 21.072563\n",
      "I0710 07:23:55.771288 140295626643200 trainer.py:508] step:  4569, steps/sec: 0.16, examples/sec: 21.07 grad_norm/all/loss:113.92972 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2433228 log_pplx:4.7606573 loss:188.04597 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.742\n",
      "I0710 07:23:59.625420 140295626643200 summary_utils.py:349] Steps/second: 0.158288, Examples/second: 21.450375\n",
      "I0710 07:23:59.626213 140295626643200 trainer.py:508] step:  4570, steps/sec: 0.16, examples/sec: 21.45 grad_norm/all/loss:112.93772 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2452515 log_pplx:4.9267936 loss:123.78569 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.74738\n",
      "I0710 07:24:02.706432 140295626643200 summary_utils.py:349] Steps/second: 0.160709, Examples/second: 22.348031\n",
      "I0710 07:24:02.707334 140295626643200 trainer.py:508] step:  4571, steps/sec: 0.16, examples/sec: 22.35 grad_norm/all/loss:45.910606 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2444521 log_pplx:4.719811 loss:73.507362 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:515.75305\n",
      "I0710 07:24:08.902909 140295626643200 summary_utils.py:349] Steps/second: 0.160728, Examples/second: 22.079482\n",
      "I0710 07:24:08.903697 140295626643200 trainer.py:508] step:  4572, steps/sec: 0.16, examples/sec: 22.08 grad_norm/all/loss:181.84627 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2432445 log_pplx:4.8167429 loss:191.34512 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.75854\n",
      "I0710 07:24:18.051319 140295626643200 summary_utils.py:349] Steps/second: 0.158655, Examples/second: 21.365568\n",
      "I0710 07:24:18.052181 140295626643200 trainer.py:508] step:  4573, steps/sec: 0.16, examples/sec: 21.37 grad_norm/all/loss:212.8031 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2426564 log_pplx:4.1092381 loss:297.4061 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:515.76447\n",
      "I0710 07:24:24.427631 140295626643200 summary_utils.py:349] Steps/second: 0.158605, Examples/second: 21.124519\n",
      "I0710 07:24:24.428601 140295626643200 trainer.py:508] step:  4574, steps/sec: 0.16, examples/sec: 21.12 grad_norm/all/loss:116.5462 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2437129 log_pplx:4.7651086 loss:200.90886 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.76941\n",
      "I0710 07:24:28.301782 140295626643200 summary_utils.py:349] Steps/second: 0.160231, Examples/second: 21.454087\n",
      "I0710 07:24:28.302660 140295626643200 trainer.py:508] step:  4575, steps/sec: 0.16, examples/sec: 21.45 grad_norm/all/loss:102.43079 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2451743 log_pplx:4.8888845 loss:123.16935 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.77448\n",
      "I0710 07:24:35.067494 140295626643200 summary_utils.py:349] Steps/second: 0.159886, Examples/second: 21.186985\n",
      "I0710 07:24:35.068369 140295626643200 trainer.py:508] step:  4576, steps/sec: 0.16, examples/sec: 21.19 grad_norm/all/loss:87.769112 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2437055 log_pplx:4.8710885 loss:193.44312 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.77917\n",
      "I0710 07:24:38.283937 140295626643200 summary_utils.py:349] Steps/second: 0.161852, Examples/second: 21.947095\n",
      "I0710 07:24:38.284756 140295626643200 trainer.py:508] step:  4577, steps/sec: 0.16, examples/sec: 21.95 grad_norm/all/loss:75.146584 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.244761 log_pplx:4.8534474 loss:76.271179 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:515.78387\n",
      "I0710 07:24:44.562216 140295626643200 summary_utils.py:349] Steps/second: 0.161788, Examples/second: 21.719053\n",
      "I0710 07:24:44.563008 140295626643200 trainer.py:508] step:  4578, steps/sec: 0.16, examples/sec: 21.72 grad_norm/all/loss:78.043427 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2435466 log_pplx:4.6958547 loss:188.24509 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.78815\n",
      "I0710 07:24:48.403631 140295626643200 summary_utils.py:349] Steps/second: 0.163259, Examples/second: 22.016684\n",
      "I0710 07:24:48.404426 140295626643200 trainer.py:508] step:  4579, steps/sec: 0.16, examples/sec: 22.02 grad_norm/all/loss:41.660706 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2451247 log_pplx:4.8974237 loss:122.68047 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.79272\n",
      "I0710 07:24:50.625146 140295626643200 summary_utils.py:349] Steps/second: 0.165715, Examples/second: 23.801359\n",
      "I0710 07:24:50.625934 140295626643200 trainer.py:508] step:  4580, steps/sec: 0.17, examples/sec: 23.80 grad_norm/all/loss:12.165463 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2435774 log_pplx:4.4999695 loss:33.204849 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:515.7973\n",
      "I0710 07:24:56.505069 140295626643200 summary_utils.py:349] Steps/second: 0.165812, Examples/second: 23.575443\n",
      "I0710 07:24:56.506030 140295626643200 trainer.py:508] step:  4581, steps/sec: 0.17, examples/sec: 23.58 grad_norm/all/loss:126.34449 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2433581 log_pplx:4.6327686 loss:179.75142 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.80157\n",
      "I0710 07:24:59.544214 140295626643200 summary_utils.py:349] Steps/second: 0.167660, Examples/second: 24.262291\n",
      "I0710 07:24:59.545012 140295626643200 trainer.py:508] step:  4582, steps/sec: 0.17, examples/sec: 24.26 grad_norm/all/loss:24.24894 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2445502 log_pplx:4.6614861 loss:72.835732 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:515.80652\n",
      "I0710 07:25:05.929922 140295626643200 summary_utils.py:349] Steps/second: 0.167403, Examples/second: 23.989601\n",
      "I0710 07:25:05.930680 140295626643200 trainer.py:508] step:  4583, steps/sec: 0.17, examples/sec: 23.99 grad_norm/all/loss:92.266899 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2438942 log_pplx:4.7234964 loss:192.77771 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.81122\n",
      "I0710 07:25:09.729571 140295626643200 summary_utils.py:349] Steps/second: 0.168709, Examples/second: 24.236733\n",
      "I0710 07:25:09.730528 140295626643200 trainer.py:508] step:  4584, steps/sec: 0.17, examples/sec: 24.24 grad_norm/all/loss:41.453957 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2453507 log_pplx:4.8374133 loss:121.72142 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.8158\n",
      "I0710 07:25:15.800397 140295626643200 summary_utils.py:349] Steps/second: 0.168624, Examples/second: 24.000880\n",
      "I0710 07:25:15.801151 140295626643200 trainer.py:508] step:  4585, steps/sec: 0.17, examples/sec: 24.00 grad_norm/all/loss:84.362495 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2437215 log_pplx:4.6396346 loss:187.32526 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.82031\n",
      "I0710 07:25:26.467993 140295626643200 summary_utils.py:349] Steps/second: 0.165920, Examples/second: 23.269372\n",
      "I0710 07:25:26.468784 140295626643200 trainer.py:508] step:  4586, steps/sec: 0.17, examples/sec: 23.27 grad_norm/all/loss:226.03383 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2430371 log_pplx:4.2624345 loss:311.47739 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:515.82477\n",
      "I0710 07:25:32.701197 140295626643200 summary_utils.py:349] Steps/second: 0.165806, Examples/second: 23.053682\n",
      "I0710 07:25:32.701970 140295626643200 trainer.py:508] step:  4587, steps/sec: 0.17, examples/sec: 23.05 grad_norm/all/loss:139.59488 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2437865 log_pplx:4.6065869 loss:188.58215 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.83075\n",
      "I0710 07:25:35.805706 140295626643200 summary_utils.py:349] Steps/second: 0.167399, Examples/second: 23.659042\n",
      "I0710 07:25:35.806508 140295626643200 trainer.py:508] step:  4588, steps/sec: 0.17, examples/sec: 23.66 grad_norm/all/loss:30.266836 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2446275 log_pplx:4.6871052 loss:71.91777 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:515.83655\n",
      "I0710 07:25:39.667073 140295626643200 summary_utils.py:349] Steps/second: 0.168545, Examples/second: 23.881533\n",
      "I0710 07:25:39.667833 140295626643200 trainer.py:508] step:  4589, steps/sec: 0.17, examples/sec: 23.88 grad_norm/all/loss:50.75008 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2453256 log_pplx:4.8181386 loss:119.97166 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.8421\n",
      "I0710 07:25:45.801138 140295626643200 summary_utils.py:349] Steps/second: 0.168437, Examples/second: 23.670222\n",
      "I0710 07:25:45.801951 140295626643200 trainer.py:508] step:  4590, steps/sec: 0.17, examples/sec: 23.67 grad_norm/all/loss:81.245331 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2437069 log_pplx:4.7100568 loss:185.39961 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.84753\n",
      "I0710 07:25:52.040361 140295626643200 summary_utils.py:349] Steps/second: 0.168279, Examples/second: 23.459300\n",
      "I0710 07:25:52.041169 140295626643200 trainer.py:508] step:  4591, steps/sec: 0.17, examples/sec: 23.46 grad_norm/all/loss:82.064857 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2441245 log_pplx:4.5897646 loss:186.00021 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.8526\n",
      "I0710 07:25:55.922271 140295626643200 summary_utils.py:349] Steps/second: 0.169346, Examples/second: 23.671547\n",
      "I0710 07:25:55.923077 140295626643200 trainer.py:508] step:  4592, steps/sec: 0.17, examples/sec: 23.67 grad_norm/all/loss:36.402473 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2457726 log_pplx:4.8844805 loss:122.90574 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.85754\n",
      "I0710 07:26:02.165464 140295626643200 summary_utils.py:349] Steps/second: 0.169173, Examples/second: 23.466768\n",
      "I0710 07:26:02.166288 140295626643200 trainer.py:508] step:  4593, steps/sec: 0.17, examples/sec: 23.47 grad_norm/all/loss:103.48809 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2442126 log_pplx:4.6814685 loss:193.98836 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.8623\n",
      "I0710 07:26:05.212833 140295626643200 summary_utils.py:349] Steps/second: 0.170624, Examples/second: 24.019015\n",
      "I0710 07:26:05.213626 140295626643200 trainer.py:508] step:  4594, steps/sec: 0.17, examples/sec: 24.02 grad_norm/all/loss:43.907822 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2449821 log_pplx:4.6679749 loss:74.031166 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:515.86719\n",
      "I0710 07:26:11.718168 140295626643200 summary_utils.py:349] Steps/second: 0.170301, Examples/second: 23.795123\n",
      "I0710 07:26:11.719095 140295626643200 trainer.py:508] step:  4595, steps/sec: 0.17, examples/sec: 23.80 grad_norm/all/loss:99.345055 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2442504 log_pplx:4.6852965 loss:195.25975 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.87152\n",
      "I0710 07:26:21.575797 140295626643200 summary_utils.py:349] Steps/second: 0.168364, Examples/second: 23.239911\n",
      "I0710 07:26:21.576598 140295626643200 trainer.py:508] step:  4596, steps/sec: 0.17, examples/sec: 23.24 grad_norm/all/loss:236.05914 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2430664 log_pplx:4.1808295 loss:305.5141 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:515.87573\n",
      "I0710 07:26:25.455778 140295626643200 summary_utils.py:349] Steps/second: 0.169342, Examples/second: 23.436997\n",
      "I0710 07:26:25.456578 140295626643200 trainer.py:508] step:  4597, steps/sec: 0.17, examples/sec: 23.44 grad_norm/all/loss:68.790573 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2452612 log_pplx:4.8020368 loss:120.44109 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.88074\n",
      "I0710 07:26:32.075400 140295626643200 summary_utils.py:349] Steps/second: 0.169007, Examples/second: 23.228802\n",
      "I0710 07:26:32.076293 140295626643200 trainer.py:508] step:  4598, steps/sec: 0.17, examples/sec: 23.23 grad_norm/all/loss:161.62933 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2442948 log_pplx:4.7558665 loss:191.42361 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.88544\n",
      "I0710 07:26:38.400794 140295626643200 summary_utils.py:349] Steps/second: 0.168819, Examples/second: 23.046555\n",
      "I0710 07:26:38.401657 140295626643200 trainer.py:508] step:  4599, steps/sec: 0.17, examples/sec: 23.05 grad_norm/all/loss:159.36189 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2438442 log_pplx:4.6483731 loss:185.58629 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.89099\n",
      "I0710 07:26:41.495722 140295626643200 summary_utils.py:349] Steps/second: 0.170109, Examples/second: 23.545196\n",
      "I0710 07:26:41.496479 140295626643200 trainer.py:508] step:  4600, steps/sec: 0.17, examples/sec: 23.55 grad_norm/all/loss:22.819962 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2454063 log_pplx:4.7513237 loss:74.165184 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:515.89722\n",
      "I0710 07:26:47.952036 140295626643200 summary_utils.py:349] Steps/second: 0.169848, Examples/second: 23.354077\n",
      "I0710 07:26:47.952845 140295626643200 trainer.py:508] step:  4601, steps/sec: 0.17, examples/sec: 23.35 grad_norm/all/loss:123.77707 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2443157 log_pplx:4.6909738 loss:188.87033 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.9032\n",
      "I0710 07:26:51.757355 140295626643200 summary_utils.py:349] Steps/second: 0.170777, Examples/second: 23.540961\n",
      "I0710 07:26:51.758147 140295626643200 trainer.py:508] step:  4602, steps/sec: 0.17, examples/sec: 23.54 grad_norm/all/loss:98.317932 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2456968 log_pplx:4.7934403 loss:120.73478 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.90857\n",
      "I0710 07:26:58.040648 140295626643200 summary_utils.py:349] Steps/second: 0.170588, Examples/second: 23.365423\n",
      "I0710 07:26:58.041430 140295626643200 trainer.py:508] step:  4603, steps/sec: 0.17, examples/sec: 23.37 grad_norm/all/loss:70.636108 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.24413 log_pplx:4.6588964 loss:185.24937 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.91333\n",
      "I0710 07:27:00.248555 140295626643200 summary_utils.py:349] Steps/second: 0.172190, Examples/second: 24.548682\n",
      "I0710 07:27:00.249327 140295626643200 trainer.py:508] step:  4604, steps/sec: 0.17, examples/sec: 24.55 grad_norm/all/loss:10.12886 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.243817 log_pplx:4.3923793 loss:33.002892 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:515.91785\n",
      "I0710 07:27:06.630874 140295626643200 summary_utils.py:349] Steps/second: 0.171940, Examples/second: 24.354801\n",
      "I0710 07:27:06.631737 140295626643200 trainer.py:508] step:  4605, steps/sec: 0.17, examples/sec: 24.35 grad_norm/all/loss:139.89062 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.244246 log_pplx:4.7366872 loss:190.11877 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.92212\n",
      "I0710 07:27:09.701424 140295626643200 summary_utils.py:349] Steps/second: 0.173124, Examples/second: 24.809484\n",
      "I0710 07:27:09.702230 140295626643200 trainer.py:508] step:  4606, steps/sec: 0.17, examples/sec: 24.81 grad_norm/all/loss:48.310898 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2457185 log_pplx:4.776114 loss:74.477524 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:515.92706\n",
      "I0710 07:27:13.559581 140295626643200 summary_utils.py:349] Steps/second: 0.173950, Examples/second: 24.969223\n",
      "I0710 07:27:13.560593 140295626643200 trainer.py:508] step:  4607, steps/sec: 0.17, examples/sec: 24.97 grad_norm/all/loss:87.576485 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2456645 log_pplx:4.846849 loss:122.04972 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.93195\n",
      "I0710 07:27:20.004569 140295626643200 summary_utils.py:349] Steps/second: 0.173653, Examples/second: 24.771291\n",
      "I0710 07:27:20.005376 140295626643200 trainer.py:508] step:  4608, steps/sec: 0.17, examples/sec: 24.77 grad_norm/all/loss:138.40596 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2444056 log_pplx:4.7126803 loss:192.27737 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.93707\n",
      "I0710 07:27:29.440081 140295626643200 summary_utils.py:349] Steps/second: 0.172127, Examples/second: 24.308151\n",
      "I0710 07:27:29.440878 140295626643200 trainer.py:508] step:  4609, steps/sec: 0.17, examples/sec: 24.31 grad_norm/all/loss:242.1246 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2435951 log_pplx:4.3339467 loss:312.3692 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:515.94165\n",
      "2020-07-10 07:27:29.462304: I lingvo/core/ops/record_batcher.cc:394] 433 total seconds passed. Total records yielded: 10978. Total records skipped: 3\n",
      "2020-07-10 07:27:29.462415: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 121\n",
      "I0710 07:27:35.784657 140295626643200 summary_utils.py:349] Steps/second: 0.171910, Examples/second: 24.133357\n",
      "I0710 07:27:35.785473 140295626643200 trainer.py:508] step:  4610, steps/sec: 0.17, examples/sec: 24.13 grad_norm/all/loss:185.53413 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2444507 log_pplx:4.7612486 loss:192.05685 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.94672\n",
      "I0710 07:27:39.660595 140295626643200 summary_utils.py:349] Steps/second: 0.172689, Examples/second: 24.288451\n",
      "I0710 07:27:39.661409 140295626643200 trainer.py:508] step:  4611, steps/sec: 0.17, examples/sec: 24.29 grad_norm/all/loss:48.857048 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2457304 log_pplx:4.7926335 loss:118.52783 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.95123\n",
      "I0710 07:27:46.232240 140295626643200 summary_utils.py:349] Steps/second: 0.172379, Examples/second: 24.105464\n",
      "I0710 07:27:46.233023 140295626643200 trainer.py:508] step:  4612, steps/sec: 0.17, examples/sec: 24.11 grad_norm/all/loss:93.333565 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.244705 log_pplx:4.7926216 loss:198.05508 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.95563\n",
      "I0710 07:27:49.327990 140295626643200 summary_utils.py:349] Steps/second: 0.173443, Examples/second: 24.519390\n",
      "I0710 07:27:49.328773 140295626643200 trainer.py:508] step:  4613, steps/sec: 0.17, examples/sec: 24.52 grad_norm/all/loss:47.161556 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2455959 log_pplx:4.7807679 loss:74.886246 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:515.96057\n",
      "I0710 07:27:55.887455 140295626643200 summary_utils.py:349] Steps/second: 0.173134, Examples/second: 24.337636\n",
      "I0710 07:27:55.888373 140295626643200 trainer.py:508] step:  4614, steps/sec: 0.17, examples/sec: 24.34 grad_norm/all/loss:151.99513 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2446718 log_pplx:4.7892399 loss:199.47185 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.96527\n",
      "I0710 07:27:59.750051 140295626643200 summary_utils.py:349] Steps/second: 0.173872, Examples/second: 24.484744\n",
      "I0710 07:27:59.750931 140295626643200 trainer.py:508] step:  4615, steps/sec: 0.17, examples/sec: 24.48 grad_norm/all/loss:50.383461 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2458799 log_pplx:4.7569747 loss:119.75685 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.9707\n",
      "I0710 07:28:06.266703 140295626643200 summary_utils.py:349] Steps/second: 0.173580, Examples/second: 24.309937\n",
      "I0710 07:28:06.267479 140295626643200 trainer.py:508] step:  4616, steps/sec: 0.17, examples/sec: 24.31 grad_norm/all/loss:80.447205 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.244925 log_pplx:4.7306113 loss:198.62654 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.9762\n",
      "I0710 07:28:09.342988 140295626643200 summary_utils.py:349] Steps/second: 0.174597, Examples/second: 24.705432\n",
      "I0710 07:28:09.343730 140295626643200 trainer.py:508] step:  4617, steps/sec: 0.17, examples/sec: 24.71 grad_norm/all/loss:45.392223 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.245437 log_pplx:4.6461248 loss:71.760849 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:515.98181\n",
      "I0710 07:28:15.709606 140295626643200 summary_utils.py:349] Steps/second: 0.174356, Examples/second: 24.539062\n",
      "I0710 07:28:15.710508 140295626643200 trainer.py:508] step:  4618, steps/sec: 0.17, examples/sec: 24.54 grad_norm/all/loss:145.22667 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2447976 log_pplx:4.7140598 loss:193.86569 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.98688\n",
      "I0710 07:28:19.655964 140295626643200 summary_utils.py:349] Steps/second: 0.175022, Examples/second: 24.673873\n",
      "I0710 07:28:19.656930 140295626643200 trainer.py:508] step:  4619, steps/sec: 0.18, examples/sec: 24.67 grad_norm/all/loss:55.874908 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2463394 log_pplx:4.878984 loss:122.64546 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:515.99139\n",
      "I0710 07:28:26.063256 140295626643200 summary_utils.py:349] Steps/second: 0.174767, Examples/second: 24.509438\n",
      "I0710 07:28:26.064022 140295626643200 trainer.py:508] step:  4620, steps/sec: 0.17, examples/sec: 24.51 grad_norm/all/loss:83.160362 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2444887 log_pplx:4.6555243 loss:189.13065 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:515.99548\n",
      "I0710 07:28:36.238525 140295626643200 summary_utils.py:349] Steps/second: 0.173162, Examples/second: 24.077791\n",
      "I0710 07:28:36.239286 140295626643200 trainer.py:508] step:  4621, steps/sec: 0.17, examples/sec: 24.08 grad_norm/all/loss:202.6638 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2438316 log_pplx:4.130815 loss:302.47894 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:515.99963\n",
      "I0710 07:28:42.795465 140295626643200 summary_utils.py:349] Steps/second: 0.172887, Examples/second: 23.919392\n",
      "I0710 07:28:42.796502 140295626643200 trainer.py:508] step:  4622, steps/sec: 0.17, examples/sec: 23.92 grad_norm/all/loss:156.39421 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2446047 log_pplx:4.7076969 loss:191.36789 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.00421\n",
      "I0710 07:28:45.882302 140295626643200 summary_utils.py:349] Steps/second: 0.173829, Examples/second: 24.287596\n",
      "I0710 07:28:45.883059 140295626643200 trainer.py:508] step:  4623, steps/sec: 0.17, examples/sec: 24.29 grad_norm/all/loss:51.76046 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.245993 log_pplx:4.7648897 loss:74.339729 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.00952\n",
      "I0710 07:28:52.382916 140295626643200 summary_utils.py:349] Steps/second: 0.173570, Examples/second: 24.132213\n",
      "I0710 07:28:52.383714 140295626643200 trainer.py:508] step:  4624, steps/sec: 0.17, examples/sec: 24.13 grad_norm/all/loss:66.603706 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.244436 log_pplx:4.6170025 loss:184.85324 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.01489\n",
      "I0710 07:28:56.320574 140295626643200 summary_utils.py:349] Steps/second: 0.174197, Examples/second: 24.260834\n",
      "I0710 07:28:56.321626 140295626643200 trainer.py:508] step:  4625, steps/sec: 0.17, examples/sec: 24.26 grad_norm/all/loss:58.723042 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2463311 log_pplx:4.7947946 loss:119.86986 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.02002\n",
      "I0710 07:29:02.718657 140295626643200 summary_utils.py:349] Steps/second: 0.173973, Examples/second: 24.113791\n",
      "I0710 07:29:02.719457 140295626643200 trainer.py:508] step:  4626, steps/sec: 0.17, examples/sec: 24.11 grad_norm/all/loss:85.659569 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.244771 log_pplx:4.6737609 loss:189.46257 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.02466\n",
      "I0710 07:29:08.957987 140295626643200 summary_utils.py:349] Steps/second: 0.173808, Examples/second: 23.977732\n",
      "I0710 07:29:08.958770 140295626643200 trainer.py:508] step:  4627, steps/sec: 0.17, examples/sec: 23.98 grad_norm/all/loss:95.202179 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2450227 log_pplx:4.7168841 loss:192.15408 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.02924\n",
      "I0710 07:29:12.806220 140295626643200 summary_utils.py:349] Steps/second: 0.174442, Examples/second: 24.107561\n",
      "I0710 07:29:12.807134 140295626643200 trainer.py:508] step:  4628, steps/sec: 0.17, examples/sec: 24.11 grad_norm/all/loss:35.823616 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2464882 log_pplx:4.8263927 loss:120.81065 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.03375\n",
      "I0710 07:29:15.866317 140295626643200 summary_utils.py:349] Steps/second: 0.175331, Examples/second: 24.454848\n",
      "I0710 07:29:15.867136 140295626643200 trainer.py:508] step:  4629, steps/sec: 0.18, examples/sec: 24.45 grad_norm/all/loss:23.803343 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2456686 log_pplx:4.6135035 loss:71.329079 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.03821\n",
      "I0710 07:29:22.100620 140295626643200 summary_utils.py:349] Steps/second: 0.175156, Examples/second: 24.318379\n",
      "I0710 07:29:22.101386 140295626643200 trainer.py:508] step:  4630, steps/sec: 0.18, examples/sec: 24.32 grad_norm/all/loss:158.88512 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2447679 log_pplx:4.7225451 loss:185.83215 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.04242\n",
      "I0710 07:29:24.309789 140295626643200 summary_utils.py:349] Steps/second: 0.176305, Examples/second: 25.177917\n",
      "I0710 07:29:24.310600 140295626643200 trainer.py:508] step:  4631, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:17.046968 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2449225 log_pplx:4.5589747 loss:33.60463 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:516.04706\n",
      "I0710 07:29:35.074470 140295626643200 summary_utils.py:349] Steps/second: 0.174655, Examples/second: 24.753172\n",
      "I0710 07:29:35.075244 140295626643200 trainer.py:508] step:  4632, steps/sec: 0.17, examples/sec: 24.75 grad_norm/all/loss:383.91132 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.244156 log_pplx:4.2684536 loss:309.67633 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.05133\n",
      "I0710 07:29:41.459685 140295626643200 summary_utils.py:349] Steps/second: 0.174445, Examples/second: 24.611336\n",
      "I0710 07:29:41.460504 140295626643200 trainer.py:508] step:  4633, steps/sec: 0.17, examples/sec: 24.61 grad_norm/all/loss:222.556 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2446829 log_pplx:4.7671843 loss:191.16409 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.0567\n",
      "I0710 07:29:45.576523 140295626643200 summary_utils.py:349] Steps/second: 0.174954, Examples/second: 24.717172\n",
      "I0710 07:29:45.577347 140295626643200 trainer.py:508] step:  4634, steps/sec: 0.17, examples/sec: 24.72 grad_norm/all/loss:75.579254 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2463603 log_pplx:4.8100777 loss:119.4703 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.06146\n",
      "I0710 07:29:52.249230 140295626643200 summary_utils.py:349] Steps/second: 0.174655, Examples/second: 24.565810\n",
      "I0710 07:29:52.250050 140295626643200 trainer.py:508] step:  4635, steps/sec: 0.17, examples/sec: 24.57 grad_norm/all/loss:173.13309 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2449538 log_pplx:4.7273822 loss:192.52261 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.06586\n",
      "I0710 07:29:54.523320 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 07:29:57.430866 140295626643200 summary_utils.py:349] Steps/second: 0.174823, Examples/second: 24.793095\n",
      "I0710 07:29:57.431640 140295626643200 trainer.py:508] step:  4636, steps/sec: 0.17, examples/sec: 24.79 grad_norm/all/loss:20.449041 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2462908 log_pplx:4.6816649 loss:72.565811 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.06995\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 07:29:59.286592 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 07:29:59.798341 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00004635\n",
      "I0710 07:30:03.818486 140295626643200 summary_utils.py:349] Steps/second: 0.174619, Examples/second: 24.656248\n",
      "I0710 07:30:03.819361 140295626643200 base_runner.py:111] step:  4637, steps/sec: 0.17, examples/sec: 24.66 grad_norm/all/loss:112.5118 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.24474 log_pplx:4.7207999 loss:194.55598 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.07397\n",
      "I0710 07:30:04.525767 140295635035904 trainer.py:345] Write summary @4637\n",
      "2020-07-10 07:30:06.849883: I lingvo/core/ops/record_batcher.cc:394] 550 total seconds passed. Total records yielded: 896. Total records skipped: 0\n",
      "I0710 07:30:11.795296 140295626643200 summary_utils.py:349] Steps/second: 0.173943, Examples/second: 24.593082\n",
      "I0710 07:30:11.796539 140295626643200 trainer.py:508] step:  4638, steps/sec: 0.17, examples/sec: 24.59 grad_norm/all/loss:123.03408 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2468648 log_pplx:4.9522896 loss:125.32389 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.07855\n",
      "I0710 07:30:21.104362 140295626643200 summary_utils.py:349] Steps/second: 0.172893, Examples/second: 24.340627\n",
      "I0710 07:30:21.105614 140295626643200 trainer.py:508] step:  4639, steps/sec: 0.17, examples/sec: 24.34 grad_norm/all/loss:94.771126 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2452271 log_pplx:4.7268291 loss:189.07318 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.08392\n",
      "I0710 07:30:36.707086 140295626643200 summary_utils.py:349] Steps/second: 0.170090, Examples/second: 23.779530\n",
      "I0710 07:30:36.708188 140295626643200 trainer.py:508] step:  4640, steps/sec: 0.17, examples/sec: 23.78 grad_norm/all/loss:315.73688 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2441816 log_pplx:4.1447725 loss:295.41864 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.08984\n",
      "I0710 07:30:46.449175 140295626643200 summary_utils.py:349] Steps/second: 0.169022, Examples/second: 23.533045\n",
      "I0710 07:30:46.774812 140295626643200 trainer.py:508] step:  4641, steps/sec: 0.17, examples/sec: 23.53 grad_norm/all/loss:86.087463 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2449253 log_pplx:4.6767845 loss:193.79425 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.09583\n",
      "I0710 07:30:52.799814 140295626643200 summary_utils.py:349] Steps/second: 0.168904, Examples/second: 23.550017\n",
      "I0710 07:30:52.801206 140295626643200 trainer.py:508] step:  4642, steps/sec: 0.17, examples/sec: 23.55 grad_norm/all/loss:64.125679 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2465513 log_pplx:4.849885 loss:121.94429 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.10199\n",
      "I0710 07:30:57.321575 140295626643200 summary_utils.py:349] Steps/second: 0.169281, Examples/second: 23.788791\n",
      "I0710 07:30:57.323208 140295626643200 trainer.py:508] step:  4643, steps/sec: 0.17, examples/sec: 23.79 grad_norm/all/loss:53.093807 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2464145 log_pplx:4.7506094 loss:74.061264 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.10773\n",
      "I0710 07:31:02.539182 140295635035904 trainer.py:354] Write summary done: step 4637\n",
      "I0710 07:31:06.097847 140295626643200 summary_utils.py:349] Steps/second: 0.168516, Examples/second: 23.585973\n",
      "I0710 07:31:06.098706 140295626643200 trainer.py:508] step:  4644, steps/sec: 0.17, examples/sec: 23.59 grad_norm/all/loss:120.72749 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2449162 log_pplx:4.6180587 loss:184.95326 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.11304\n",
      "I0710 07:31:12.466290 140295626643200 summary_utils.py:349] Steps/second: 0.168402, Examples/second: 23.476502\n",
      "I0710 07:31:12.467086 140295626643200 trainer.py:508] step:  4645, steps/sec: 0.17, examples/sec: 23.48 grad_norm/all/loss:136.29645 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2451202 log_pplx:4.7325001 loss:189.12253 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.11853\n",
      "I0710 07:31:16.270457 140295626643200 summary_utils.py:349] Steps/second: 0.168959, Examples/second: 23.586080\n",
      "I0710 07:31:16.271211 140295626643200 trainer.py:508] step:  4646, steps/sec: 0.17, examples/sec: 23.59 grad_norm/all/loss:72.516762 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.246551 log_pplx:4.881083 loss:121.44746 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.12439\n",
      "I0710 07:31:22.722419 140295626643200 summary_utils.py:349] Steps/second: 0.168821, Examples/second: 23.475308\n",
      "I0710 07:31:22.723213 140295626643200 trainer.py:508] step:  4647, steps/sec: 0.17, examples/sec: 23.48 grad_norm/all/loss:83.798126 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2452723 log_pplx:4.8041821 loss:196.01062 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.12976\n",
      "I0710 07:31:25.760523 140295626643200 summary_utils.py:349] Steps/second: 0.169565, Examples/second: 23.757427\n",
      "I0710 07:31:25.761289 140295626643200 trainer.py:508] step:  4648, steps/sec: 0.17, examples/sec: 23.76 grad_norm/all/loss:23.016544 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2466577 log_pplx:4.7311273 loss:75.67955 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.13513\n",
      "I0710 07:31:32.105422 140295626643200 summary_utils.py:349] Steps/second: 0.169450, Examples/second: 23.650403\n",
      "I0710 07:31:32.106207 140295626643200 trainer.py:508] step:  4649, steps/sec: 0.17, examples/sec: 23.65 grad_norm/all/loss:81.486641 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2453144 log_pplx:4.7556353 loss:191.59265 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.14032\n",
      "I0710 07:31:38.619896 140295626643200 summary_utils.py:349] Steps/second: 0.169295, Examples/second: 23.539434\n",
      "I0710 07:31:38.620793 140295626643200 trainer.py:508] step:  4650, steps/sec: 0.17, examples/sec: 23.54 grad_norm/all/loss:93.683762 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2455195 log_pplx:4.6905794 loss:195.65579 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.14539\n",
      "I0710 07:31:42.398499 140295626643200 summary_utils.py:349] Steps/second: 0.169831, Examples/second: 23.645286\n",
      "I0710 07:31:42.399265 140295626643200 trainer.py:508] step:  4651, steps/sec: 0.17, examples/sec: 23.65 grad_norm/all/loss:45.562218 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2471191 log_pplx:4.8830171 loss:125.49355 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.15027\n",
      "I0710 07:31:52.561048 140295626643200 summary_utils.py:349] Steps/second: 0.168766, Examples/second: 23.351344\n",
      "I0710 07:31:52.561795 140295626643200 trainer.py:508] step:  4652, steps/sec: 0.17, examples/sec: 23.35 grad_norm/all/loss:306.95816 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2442259 log_pplx:4.2804818 loss:301.66693 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.15491\n",
      "I0710 07:31:59.021309 140295626643200 summary_utils.py:349] Steps/second: 0.168635, Examples/second: 23.248338\n",
      "I0710 07:31:59.022210 140295626643200 trainer.py:508] step:  4653, steps/sec: 0.17, examples/sec: 23.25 grad_norm/all/loss:107.61456 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2454473 log_pplx:4.7456236 loss:192.37573 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.16058\n",
      "I0710 07:32:02.171807 140295626643200 summary_utils.py:349] Steps/second: 0.169313, Examples/second: 23.512808\n",
      "I0710 07:32:02.172789 140295626643200 trainer.py:508] step:  4654, steps/sec: 0.17, examples/sec: 23.51 grad_norm/all/loss:40.067307 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2466618 log_pplx:4.7199197 loss:73.12188 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.16595\n",
      "I0710 07:32:04.378781 140295626643200 summary_utils.py:349] Steps/second: 0.170217, Examples/second: 24.176519\n",
      "I0710 07:32:04.379558 140295626643200 trainer.py:508] step:  4655, steps/sec: 0.17, examples/sec: 24.18 grad_norm/all/loss:15.386142 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2452823 log_pplx:4.5014491 loss:33.101475 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:516.17133\n",
      "I0710 07:32:10.702382 140295626643200 summary_utils.py:349] Steps/second: 0.170107, Examples/second: 24.072336\n",
      "I0710 07:32:10.703290 140295626643200 trainer.py:508] step:  4656, steps/sec: 0.17, examples/sec: 24.07 grad_norm/all/loss:230.80923 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.245664 log_pplx:4.9454665 loss:205.48413 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.17645\n",
      "I0710 07:32:14.510304 140295626643200 summary_utils.py:349] Steps/second: 0.170608, Examples/second: 24.169490\n",
      "I0710 07:32:14.511075 140295626643200 trainer.py:508] step:  4657, steps/sec: 0.17, examples/sec: 24.17 grad_norm/all/loss:54.347183 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2470632 log_pplx:4.8535471 loss:120.58031 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.18127\n",
      "I0710 07:32:20.748454 140295626643200 summary_utils.py:349] Steps/second: 0.170518, Examples/second: 24.069753\n",
      "I0710 07:32:20.749212 140295626643200 trainer.py:508] step:  4658, steps/sec: 0.17, examples/sec: 24.07 grad_norm/all/loss:88.14122 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2453688 log_pplx:4.7268634 loss:186.94746 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.18604\n",
      "I0710 07:32:26.832289 140295626643200 summary_utils.py:349] Steps/second: 0.170465, Examples/second: 23.976924\n",
      "I0710 07:32:26.833052 140295626643200 trainer.py:508] step:  4659, steps/sec: 0.17, examples/sec: 23.98 grad_norm/all/loss:78.179611 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2455814 log_pplx:4.7298708 loss:191.26414 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.19092\n",
      "I0710 07:32:29.891734 140295626643200 summary_utils.py:349] Steps/second: 0.171131, Examples/second: 24.231039\n",
      "I0710 07:32:29.892634 140295626643200 trainer.py:508] step:  4660, steps/sec: 0.17, examples/sec: 24.23 grad_norm/all/loss:35.59655 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2467259 log_pplx:4.6782804 loss:71.672714 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.19592\n",
      "I0710 07:32:33.781393 140295626643200 summary_utils.py:349] Steps/second: 0.171594, Examples/second: 24.322024\n",
      "I0710 07:32:33.782198 140295626643200 trainer.py:508] step:  4661, steps/sec: 0.17, examples/sec: 24.32 grad_norm/all/loss:38.897095 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2473984 log_pplx:4.839736 loss:122.35458 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.20044\n",
      "I0710 07:32:40.374595 140295626643200 summary_utils.py:349] Steps/second: 0.171414, Examples/second: 24.211826\n",
      "I0710 07:32:40.375378 140295626643200 trainer.py:508] step:  4662, steps/sec: 0.17, examples/sec: 24.21 grad_norm/all/loss:124.89031 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2457415 log_pplx:4.7752657 loss:200.50148 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.20483\n",
      "I0710 07:32:51.480941 140295626643200 summary_utils.py:349] Steps/second: 0.170193, Examples/second: 23.902636\n",
      "I0710 07:32:51.481836 140295626643200 trainer.py:508] step:  4663, steps/sec: 0.17, examples/sec: 23.90 grad_norm/all/loss:209.6205 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2453473 log_pplx:4.2779846 loss:316.35693 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.20953\n",
      "I0710 07:32:57.812826 140295626643200 summary_utils.py:349] Steps/second: 0.170089, Examples/second: 23.807079\n",
      "I0710 07:32:57.813612 140295626643200 trainer.py:508] step:  4664, steps/sec: 0.17, examples/sec: 23.81 grad_norm/all/loss:70.539894 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2451617 log_pplx:4.6471152 loss:177.51981 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.21478\n",
      "I0710 07:33:01.706077 140295626643200 summary_utils.py:349] Steps/second: 0.170539, Examples/second: 23.896764\n",
      "I0710 07:33:01.706849 140295626643200 trainer.py:508] step:  4665, steps/sec: 0.17, examples/sec: 23.90 grad_norm/all/loss:123.78506 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2473831 log_pplx:4.9594078 loss:125.69 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.22021\n",
      "I0710 07:33:04.705679 140295626643200 summary_utils.py:349] Steps/second: 0.171187, Examples/second: 24.141360\n",
      "I0710 07:33:04.706492 140295626643200 trainer.py:508] step:  4666, steps/sec: 0.17, examples/sec: 24.14 grad_norm/all/loss:45.131828 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2471037 log_pplx:4.8055921 loss:74.242661 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.22485\n",
      "I0710 07:33:11.261847 140295626643200 summary_utils.py:349] Steps/second: 0.171026, Examples/second: 24.038383\n",
      "I0710 07:33:11.262632 140295626643200 trainer.py:508] step:  4667, steps/sec: 0.17, examples/sec: 24.04 grad_norm/all/loss:151.55458 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2458748 log_pplx:4.6119637 loss:189.83997 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.22919\n",
      "I0710 07:33:17.434341 140295626643200 summary_utils.py:349] Steps/second: 0.170954, Examples/second: 23.949152\n",
      "I0710 07:33:17.435120 140295626643200 trainer.py:508] step:  4668, steps/sec: 0.17, examples/sec: 23.95 grad_norm/all/loss:129.50658 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2455151 log_pplx:4.6586061 loss:184.42258 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.23413\n",
      "I0710 07:33:21.290219 140295626643200 summary_utils.py:349] Steps/second: 0.171396, Examples/second: 24.037000\n",
      "I0710 07:33:21.291204 140295626643200 trainer.py:508] step:  4669, steps/sec: 0.17, examples/sec: 24.04 grad_norm/all/loss:35.698925 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2471027 log_pplx:4.7875581 loss:117.8936 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.23926\n",
      "I0710 07:33:27.628248 140295626643200 summary_utils.py:349] Steps/second: 0.171285, Examples/second: 23.943826\n",
      "I0710 07:33:27.628971 140295626643200 trainer.py:508] step:  4670, steps/sec: 0.17, examples/sec: 23.94 grad_norm/all/loss:130.55965 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2460406 log_pplx:4.7206511 loss:197.55923 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.24426\n",
      "I0710 07:33:30.696941 140295626643200 summary_utils.py:349] Steps/second: 0.171893, Examples/second: 24.177965\n",
      "I0710 07:33:30.697712 140295626643200 trainer.py:508] step:  4671, steps/sec: 0.17, examples/sec: 24.18 grad_norm/all/loss:27.476889 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2466295 log_pplx:4.6061921 loss:69.812607 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.24915\n",
      "I0710 07:33:41.372281 140295626643200 summary_utils.py:349] Steps/second: 0.170837, Examples/second: 23.901959\n",
      "I0710 07:33:41.373111 140295626643200 trainer.py:508] step:  4672, steps/sec: 0.17, examples/sec: 23.90 grad_norm/all/loss:210.29381 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.244934 log_pplx:4.2379069 loss:305.55313 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.25385\n",
      "I0710 07:33:47.742832 140295626643200 summary_utils.py:349] Steps/second: 0.170726, Examples/second: 23.811238\n",
      "I0710 07:33:47.743639 140295626643200 trainer.py:508] step:  4673, steps/sec: 0.17, examples/sec: 23.81 grad_norm/all/loss:140.45996 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2460138 log_pplx:4.7243977 loss:190.74757 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.2594\n",
      "I0710 07:33:51.621271 140295626643200 summary_utils.py:349] Steps/second: 0.171148, Examples/second: 23.895750\n",
      "I0710 07:33:51.622210 140295626643200 trainer.py:508] step:  4674, steps/sec: 0.17, examples/sec: 23.90 grad_norm/all/loss:59.18243 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2475965 log_pplx:4.8514519 loss:122.10498 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.26501\n",
      "I0710 07:33:57.978746 140295626643200 summary_utils.py:349] Steps/second: 0.171039, Examples/second: 23.806615\n",
      "I0710 07:33:57.979510 140295626643200 trainer.py:508] step:  4675, steps/sec: 0.17, examples/sec: 23.81 grad_norm/all/loss:144.90154 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2463446 log_pplx:4.7700605 loss:201.05806 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.27075\n",
      "I0710 07:34:04.057393 140295626643200 summary_utils.py:349] Steps/second: 0.170990, Examples/second: 23.727010\n",
      "I0710 07:34:04.058261 140295626643200 trainer.py:508] step:  4676, steps/sec: 0.17, examples/sec: 23.73 grad_norm/all/loss:120.83206 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2459464 log_pplx:4.731741 loss:186.96292 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.27679\n",
      "I0710 07:34:06.252139 140295626643200 summary_utils.py:349] Steps/second: 0.171756, Examples/second: 24.291260\n",
      "I0710 07:34:06.252927 140295626643200 trainer.py:508] step:  4677, steps/sec: 0.17, examples/sec: 24.29 grad_norm/all/loss:15.460604 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2459062 log_pplx:4.4874811 loss:32.174889 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:516.28296\n",
      "I0710 07:34:10.056777 140295626643200 summary_utils.py:349] Steps/second: 0.172180, Examples/second: 24.373785\n",
      "I0710 07:34:10.057547 140295626643200 trainer.py:508] step:  4678, steps/sec: 0.17, examples/sec: 24.37 grad_norm/all/loss:75.30748 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2476542 log_pplx:4.9102139 loss:122.23364 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.28876\n",
      "I0710 07:34:13.127513 140295626643200 summary_utils.py:349] Steps/second: 0.172753, Examples/second: 24.594172\n",
      "I0710 07:34:13.128262 140295626643200 trainer.py:508] step:  4679, steps/sec: 0.17, examples/sec: 24.59 grad_norm/all/loss:23.666622 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2471923 log_pplx:4.6586828 loss:72.846512 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.29376\n",
      "I0710 07:34:19.479009 140295626643200 summary_utils.py:349] Steps/second: 0.172636, Examples/second: 24.502169\n",
      "I0710 07:34:19.479757 140295626643200 trainer.py:508] step:  4680, steps/sec: 0.17, examples/sec: 24.50 grad_norm/all/loss:174.41927 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2462212 log_pplx:4.7122588 loss:192.26016 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.2984\n",
      "I0710 07:34:25.729676 140295626643200 summary_utils.py:349] Steps/second: 0.172541, Examples/second: 24.414515\n",
      "I0710 07:34:25.730472 140295626643200 trainer.py:508] step:  4681, steps/sec: 0.17, examples/sec: 24.41 grad_norm/all/loss:90.836044 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2462109 log_pplx:4.7366681 loss:191.42061 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.30219\n",
      "I0710 07:34:29.477877 140295626643200 summary_utils.py:349] Steps/second: 0.172962, Examples/second: 24.496212\n",
      "I0710 07:34:29.478823 140295626643200 trainer.py:508] step:  4682, steps/sec: 0.17, examples/sec: 24.50 grad_norm/all/loss:49.233662 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2479026 log_pplx:4.7848897 loss:123.98845 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.30554\n",
      "I0710 07:34:35.868515 140295626643200 summary_utils.py:349] Steps/second: 0.172837, Examples/second: 24.405595\n",
      "I0710 07:34:35.869326 140295626643200 trainer.py:508] step:  4683, steps/sec: 0.17, examples/sec: 24.41 grad_norm/all/loss:157.17686 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2463809 log_pplx:4.8331709 loss:202.38902 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.30896\n",
      "I0710 07:34:46.710817 140295626643200 summary_utils.py:349] Steps/second: 0.171816, Examples/second: 24.143064\n",
      "I0710 07:34:46.711662 140295626643200 trainer.py:508] step:  4684, steps/sec: 0.17, examples/sec: 24.14 grad_norm/all/loss:451.93448 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2455124 log_pplx:4.4807453 loss:324.96606 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.31274\n",
      "I0710 07:34:53.500246 140295626643200 summary_utils.py:349] Steps/second: 0.171623, Examples/second: 24.045752\n",
      "I0710 07:34:53.501124 140295626643200 trainer.py:508] step:  4685, steps/sec: 0.17, examples/sec: 24.05 grad_norm/all/loss:129.49316 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2464895 log_pplx:4.7589922 loss:205.35051 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.31775\n",
      "I0710 07:34:56.722955 140295626643200 summary_utils.py:349] Steps/second: 0.172139, Examples/second: 24.251981\n",
      "I0710 07:34:56.723757 140295626643200 trainer.py:508] step:  4686, steps/sec: 0.17, examples/sec: 24.25 grad_norm/all/loss:40.26749 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2475646 log_pplx:4.7765212 loss:74.85704 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.32288\n",
      "I0710 07:35:00.819885 140295626643200 summary_utils.py:349] Steps/second: 0.172478, Examples/second: 24.321710\n",
      "I0710 07:35:00.820674 140295626643200 trainer.py:508] step:  4687, steps/sec: 0.17, examples/sec: 24.32 grad_norm/all/loss:137.59406 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2477993 log_pplx:4.9095163 loss:124.60966 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.32788\n",
      "I0710 07:35:07.239394 140295626643200 summary_utils.py:349] Steps/second: 0.172356, Examples/second: 24.234809\n",
      "I0710 07:35:07.240170 140295626643200 trainer.py:508] step:  4688, steps/sec: 0.17, examples/sec: 24.23 grad_norm/all/loss:285.79898 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2463844 log_pplx:4.8587599 loss:200.48457 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.3324\n",
      "I0710 07:35:13.642414 140295626643200 summary_utils.py:349] Steps/second: 0.172238, Examples/second: 24.149623\n",
      "I0710 07:35:13.643243 140295626643200 trainer.py:508] step:  4689, steps/sec: 0.17, examples/sec: 24.15 grad_norm/all/loss:117.50213 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2461293 log_pplx:4.7476072 loss:192.69351 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.33685\n",
      "I0710 07:35:17.475682 140295626643200 summary_utils.py:349] Steps/second: 0.172622, Examples/second: 24.225699\n",
      "I0710 07:35:17.476428 140295626643200 trainer.py:508] step:  4690, steps/sec: 0.17, examples/sec: 24.23 grad_norm/all/loss:158.99268 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2479442 log_pplx:4.9723859 loss:127.23093 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.34149\n",
      "I0710 07:35:26.717984 140295626643200 summary_utils.py:349] Steps/second: 0.171957, Examples/second: 24.020356\n",
      "I0710 07:35:26.718960 140295626643200 trainer.py:508] step:  4691, steps/sec: 0.17, examples/sec: 24.02 grad_norm/all/loss:241.89764 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2455511 log_pplx:4.3389449 loss:308.93286 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.34729\n",
      "2020-07-10 07:35:30.008162: I lingvo/core/ops/record_batcher.cc:394] 914 total seconds passed. Total records yielded: 22881. Total records skipped: 9\n",
      "2020-07-10 07:35:30.008212: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 07:35:30.008219: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 07:35:30.008225: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 07:35:30.008231: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 07:35:30.008236: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 07:35:30.008243: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "I0710 07:35:33.064736 140295626643200 summary_utils.py:349] Steps/second: 0.171855, Examples/second: 23.940024\n",
      "I0710 07:35:33.065543 140295626643200 trainer.py:508] step:  4692, steps/sec: 0.17, examples/sec: 23.94 grad_norm/all/loss:253.51982 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2462395 log_pplx:4.7966681 loss:189.70822 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.3526\n",
      "I0710 07:35:36.059988 140295626643200 summary_utils.py:349] Steps/second: 0.172392, Examples/second: 24.143683\n",
      "I0710 07:35:36.060762 140295626643200 trainer.py:508] step:  4693, steps/sec: 0.17, examples/sec: 24.14 grad_norm/all/loss:35.748566 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2473854 log_pplx:4.7121372 loss:71.749649 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.35895\n",
      "I0710 07:35:42.423251 140295626643200 summary_utils.py:349] Steps/second: 0.172285, Examples/second: 24.062881\n",
      "I0710 07:35:42.424209 140295626643200 trainer.py:508] step:  4694, steps/sec: 0.17, examples/sec: 24.06 grad_norm/all/loss:103.59832 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2467012 log_pplx:4.738698 loss:192.33189 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.36523\n",
      "I0710 07:35:46.323776 140295626643200 summary_utils.py:349] Steps/second: 0.172644, Examples/second: 24.135153\n",
      "I0710 07:35:46.324543 140295626643200 trainer.py:508] step:  4695, steps/sec: 0.17, examples/sec: 24.14 grad_norm/all/loss:96.302521 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.24805 log_pplx:4.8216195 loss:123.16225 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.3714\n",
      "I0710 07:35:52.448918 140295626643200 summary_utils.py:349] Steps/second: 0.172581, Examples/second: 24.061528\n",
      "I0710 07:35:52.449683 140295626643200 trainer.py:508] step:  4696, steps/sec: 0.17, examples/sec: 24.06 grad_norm/all/loss:116.32372 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2467675 log_pplx:4.7812386 loss:196.15031 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.37701\n",
      "I0710 07:35:55.505011 140295626643200 summary_utils.py:349] Steps/second: 0.173093, Examples/second: 24.258925\n",
      "I0710 07:35:55.505789 140295626643200 trainer.py:508] step:  4697, steps/sec: 0.17, examples/sec: 24.26 grad_norm/all/loss:52.769829 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.247571 log_pplx:4.7811627 loss:75.116554 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.38239\n",
      "I0710 07:36:01.699430 140295626643200 summary_utils.py:349] Steps/second: 0.173015, Examples/second: 24.183411\n",
      "I0710 07:36:01.700209 140295626643200 trainer.py:508] step:  4698, steps/sec: 0.17, examples/sec: 24.18 grad_norm/all/loss:80.575813 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2463003 log_pplx:4.747963 loss:186.59494 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.38715\n",
      "I0710 07:36:05.579946 140295626643200 summary_utils.py:349] Steps/second: 0.173367, Examples/second: 24.254209\n",
      "I0710 07:36:05.580736 140295626643200 trainer.py:508] step:  4699, steps/sec: 0.17, examples/sec: 24.25 grad_norm/all/loss:75.524834 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2480001 log_pplx:4.8753314 loss:123.86391 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.39154\n",
      "I0710 07:36:11.706969 140295626643200 summary_utils.py:349] Steps/second: 0.173300, Examples/second: 24.181267\n",
      "I0710 07:36:11.707988 140295626643200 trainer.py:508] step:  4700, steps/sec: 0.17, examples/sec: 24.18 grad_norm/all/loss:104.85099 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2464796 log_pplx:4.7370782 loss:191.31876 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.39539\n",
      "I0710 07:36:21.293880 140295626643200 summary_utils.py:349] Steps/second: 0.172604, Examples/second: 23.979379\n",
      "I0710 07:36:21.294763 140295626643200 trainer.py:508] step:  4701, steps/sec: 0.17, examples/sec: 23.98 grad_norm/all/loss:219.10728 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2452946 log_pplx:4.1237373 loss:292.68225 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.39923\n",
      "I0710 07:36:23.486024 140295626643200 summary_utils.py:349] Steps/second: 0.173257, Examples/second: 24.461804\n",
      "I0710 07:36:23.486781 140295626643200 trainer.py:508] step:  4702, steps/sec: 0.17, examples/sec: 24.46 grad_norm/all/loss:13.985608 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.246501 log_pplx:4.6462889 loss:33.449654 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:516.40417\n",
      "I0710 07:36:29.820007 140295626643200 summary_utils.py:349] Steps/second: 0.173156, Examples/second: 24.383633\n",
      "I0710 07:36:29.820794 140295626643200 trainer.py:508] step:  4703, steps/sec: 0.17, examples/sec: 24.38 grad_norm/all/loss:145.50687 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2467099 log_pplx:4.7212191 loss:195.10437 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.409\n",
      "I0710 07:36:32.895412 140295626643200 summary_utils.py:349] Steps/second: 0.173642, Examples/second: 24.571842\n",
      "I0710 07:36:32.896221 140295626643200 trainer.py:508] step:  4704, steps/sec: 0.17, examples/sec: 24.57 grad_norm/all/loss:44.693096 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2479155 log_pplx:4.7589402 loss:74.507156 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.41449\n",
      "I0710 07:36:36.707288 140295626643200 summary_utils.py:349] Steps/second: 0.173992, Examples/second: 24.640543\n",
      "I0710 07:36:36.708119 140295626643200 trainer.py:508] step:  4705, steps/sec: 0.17, examples/sec: 24.64 grad_norm/all/loss:48.722202 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2480875 log_pplx:4.8378458 loss:120.16001 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.41992\n",
      "I0710 07:36:42.972484 140295626643200 summary_utils.py:349] Steps/second: 0.173899, Examples/second: 24.564009\n",
      "I0710 07:36:42.973309 140295626643200 trainer.py:508] step:  4706, steps/sec: 0.17, examples/sec: 24.56 grad_norm/all/loss:118.03656 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.246877 log_pplx:4.7377863 loss:196.79578 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.42542\n",
      "I0710 07:36:49.076653 140295626643200 summary_utils.py:349] Steps/second: 0.173836, Examples/second: 24.492488\n",
      "I0710 07:36:49.077467 140295626643200 trainer.py:508] step:  4707, steps/sec: 0.17, examples/sec: 24.49 grad_norm/all/loss:110.58165 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.246502 log_pplx:4.7399426 loss:185.39102 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.43091\n",
      "I0710 07:36:52.895901 140295626643200 summary_utils.py:349] Steps/second: 0.174178, Examples/second: 24.560181\n",
      "I0710 07:36:52.896670 140295626643200 trainer.py:508] step:  4708, steps/sec: 0.17, examples/sec: 24.56 grad_norm/all/loss:43.785023 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.24825 log_pplx:4.8243709 loss:120.88065 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.4364\n",
      "I0710 07:36:59.162471 140295626643200 summary_utils.py:349] Steps/second: 0.174086, Examples/second: 24.485376\n",
      "I0710 07:36:59.163322 140295626643200 trainer.py:508] step:  4709, steps/sec: 0.17, examples/sec: 24.49 grad_norm/all/loss:74.06221 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2469186 log_pplx:4.7237964 loss:195.62424 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.44153\n",
      "I0710 07:37:02.231603 140295626643200 summary_utils.py:349] Steps/second: 0.174556, Examples/second: 24.667854\n",
      "I0710 07:37:02.232456 140295626643200 trainer.py:508] step:  4710, steps/sec: 0.17, examples/sec: 24.67 grad_norm/all/loss:30.112959 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2479116 log_pplx:4.6626406 loss:73.035896 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.44666\n",
      "I0710 07:37:13.368752 140295626643200 summary_utils.py:349] Steps/second: 0.173614, Examples/second: 24.433645\n",
      "I0710 07:37:13.369507 140295626643200 trainer.py:508] step:  4711, steps/sec: 0.17, examples/sec: 24.43 grad_norm/all/loss:249.35912 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.245989 log_pplx:4.2606726 loss:303.67938 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.45142\n",
      "I0710 07:37:19.682085 140295626643200 summary_utils.py:349] Steps/second: 0.173519, Examples/second: 24.360017\n",
      "I0710 07:37:19.683018 140295626643200 trainer.py:508] step:  4712, steps/sec: 0.17, examples/sec: 24.36 grad_norm/all/loss:91.445511 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2470903 log_pplx:4.7974644 loss:193.45776 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.45721\n",
      "I0710 07:37:23.512490 140295626643200 summary_utils.py:349] Steps/second: 0.173850, Examples/second: 24.425893\n",
      "I0710 07:37:23.513308 140295626643200 trainer.py:508] step:  4713, steps/sec: 0.17, examples/sec: 24.43 grad_norm/all/loss:49.88979 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2481483 log_pplx:4.7136312 loss:117.84078 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.46265\n",
      "I0710 07:37:29.574134 140295626643200 summary_utils.py:349] Steps/second: 0.173797, Examples/second: 24.359063\n",
      "I0710 07:37:29.574954 140295626643200 trainer.py:508] step:  4714, steps/sec: 0.17, examples/sec: 24.36 grad_norm/all/loss:121.18698 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2467748 log_pplx:4.6963024 loss:188.14563 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.46802\n",
      "I0710 07:37:36.007862 140295626643200 summary_utils.py:349] Steps/second: 0.173682, Examples/second: 24.284205\n",
      "I0710 07:37:36.008731 140295626643200 trainer.py:508] step:  4715, steps/sec: 0.17, examples/sec: 24.28 grad_norm/all/loss:99.641968 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2468156 log_pplx:4.6787715 loss:191.01083 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.47266\n",
      "I0710 07:37:39.088079 140295626643200 summary_utils.py:349] Steps/second: 0.174134, Examples/second: 24.460479\n",
      "I0710 07:37:39.088869 140295626643200 trainer.py:508] step:  4716, steps/sec: 0.17, examples/sec: 24.46 grad_norm/all/loss:31.310802 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2482156 log_pplx:4.7662888 loss:74.659447 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.47723\n",
      "I0710 07:37:42.980961 140295626643200 summary_utils.py:349] Steps/second: 0.174446, Examples/second: 24.523258\n",
      "I0710 07:37:42.981673 140295626643200 trainer.py:508] step:  4717, steps/sec: 0.17, examples/sec: 24.52 grad_norm/all/loss:72.120644 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2486491 log_pplx:4.8138013 loss:120.67599 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.48151\n",
      "I0710 07:37:49.395456 140295626643200 summary_utils.py:349] Steps/second: 0.174332, Examples/second: 24.448802\n",
      "I0710 07:37:49.396396 140295626643200 trainer.py:508] step:  4718, steps/sec: 0.17, examples/sec: 24.45 grad_norm/all/loss:124.97447 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2466449 log_pplx:4.6954713 loss:182.47777 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.48596\n",
      "I0710 07:37:55.767273 140295626643200 summary_utils.py:349] Steps/second: 0.174226, Examples/second: 24.376256\n",
      "I0710 07:37:55.768085 140295626643200 trainer.py:508] step:  4719, steps/sec: 0.17, examples/sec: 24.38 grad_norm/all/loss:92.516518 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2473204 log_pplx:4.8065133 loss:200.31145 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.49078\n",
      "I0710 07:37:59.557291 140295626643200 summary_utils.py:349] Steps/second: 0.174550, Examples/second: 24.440747\n",
      "I0710 07:37:59.558129 140295626643200 trainer.py:508] step:  4720, steps/sec: 0.17, examples/sec: 24.44 grad_norm/all/loss:44.723625 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2487516 log_pplx:4.7779555 loss:120.73295 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.49579\n",
      "I0710 07:38:05.781230 140295626643200 summary_utils.py:349] Steps/second: 0.174468, Examples/second: 24.372366\n",
      "I0710 07:38:05.782055 140295626643200 trainer.py:508] step:  4721, steps/sec: 0.17, examples/sec: 24.37 grad_norm/all/loss:137.28401 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.24713 log_pplx:4.7614722 loss:192.95866 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.50037\n",
      "I0710 07:38:08.767292 140295626643200 summary_utils.py:349] Steps/second: 0.174921, Examples/second: 24.545606\n",
      "I0710 07:38:08.768061 140295626643200 trainer.py:508] step:  4722, steps/sec: 0.17, examples/sec: 24.55 grad_norm/all/loss:44.796997 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2479918 log_pplx:4.6500192 loss:71.439545 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.50452\n",
      "I0710 07:38:19.579272 140295626643200 summary_utils.py:349] Steps/second: 0.174086, Examples/second: 24.334655\n",
      "I0710 07:38:19.580006 140295626643200 trainer.py:508] step:  4723, steps/sec: 0.17, examples/sec: 24.33 grad_norm/all/loss:234.66373 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2463663 log_pplx:4.340127 loss:318.67383 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.50836\n",
      "I0710 07:38:21.774923 140295626643200 summary_utils.py:349] Steps/second: 0.174663, Examples/second: 24.762973\n",
      "I0710 07:38:21.775702 140295626643200 trainer.py:508] step:  4724, steps/sec: 0.17, examples/sec: 24.76 grad_norm/all/loss:14.199721 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2471097 log_pplx:4.4320531 loss:32.677734 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:516.51233\n",
      "I0710 07:38:28.276124 140295626643200 summary_utils.py:349] Steps/second: 0.174538, Examples/second: 24.687783\n",
      "I0710 07:38:28.276931 140295626643200 trainer.py:508] step:  4725, steps/sec: 0.17, examples/sec: 24.69 grad_norm/all/loss:169.37553 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2471567 log_pplx:4.7385845 loss:188.89182 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.51624\n",
      "I0710 07:38:32.188144 140295626643200 summary_utils.py:349] Steps/second: 0.174831, Examples/second: 24.746449\n",
      "I0710 07:38:32.188916 140295626643200 trainer.py:508] step:  4726, steps/sec: 0.17, examples/sec: 24.75 grad_norm/all/loss:65.533417 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2485459 log_pplx:4.8638906 loss:122.26605 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.52081\n",
      "I0710 07:38:38.172333 140295626643200 summary_utils.py:349] Steps/second: 0.174788, Examples/second: 24.683813\n",
      "I0710 07:38:38.173072 140295626643200 trainer.py:508] step:  4727, steps/sec: 0.17, examples/sec: 24.68 grad_norm/all/loss:140.44746 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2473099 log_pplx:4.7629485 loss:188.85091 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.52515\n",
      "I0710 07:38:41.242895 140295626643200 summary_utils.py:349] Steps/second: 0.175213, Examples/second: 24.849125\n",
      "I0710 07:38:41.243713 140295626643200 trainer.py:508] step:  4728, steps/sec: 0.18, examples/sec: 24.85 grad_norm/all/loss:39.628506 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2481978 log_pplx:4.6698871 loss:71.963684 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.53015\n",
      "I0710 07:38:47.797489 140295626643200 summary_utils.py:349] Steps/second: 0.175078, Examples/second: 24.773554\n",
      "I0710 07:38:47.798286 140295626643200 trainer.py:508] step:  4729, steps/sec: 0.18, examples/sec: 24.77 grad_norm/all/loss:113.20559 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2472483 log_pplx:4.7208762 loss:193.61494 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.53516\n",
      "I0710 07:38:51.628819 140295626643200 summary_utils.py:349] Steps/second: 0.175377, Examples/second: 24.832695\n",
      "I0710 07:38:51.629708 140295626643200 trainer.py:508] step:  4730, steps/sec: 0.18, examples/sec: 24.83 grad_norm/all/loss:61.08009 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2489349 log_pplx:4.8218069 loss:120.54518 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.54022\n",
      "I0710 07:38:57.987310 140295626643200 summary_utils.py:349] Steps/second: 0.175273, Examples/second: 24.762316\n",
      "I0710 07:38:57.988106 140295626643200 trainer.py:508] step:  4731, steps/sec: 0.18, examples/sec: 24.76 grad_norm/all/loss:108.45361 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2472063 log_pplx:4.7619839 loss:192.80081 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.54486\n",
      "I0710 07:39:04.145630 140295626643200 summary_utils.py:349] Steps/second: 0.175202, Examples/second: 24.697182\n",
      "I0710 07:39:04.146483 140295626643200 trainer.py:508] step:  4732, steps/sec: 0.18, examples/sec: 24.70 grad_norm/all/loss:80.157158 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2473415 log_pplx:4.7155414 loss:190.80258 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.54913\n",
      "I0710 07:39:07.218605 140295626643200 summary_utils.py:349] Steps/second: 0.175616, Examples/second: 24.858557\n",
      "I0710 07:39:07.219362 140295626643200 trainer.py:508] step:  4733, steps/sec: 0.18, examples/sec: 24.86 grad_norm/all/loss:40.13493 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2486101 log_pplx:4.6956029 loss:73.36879 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.55371\n",
      "I0710 07:39:17.755400 140295626643200 summary_utils.py:349] Steps/second: 0.174861, Examples/second: 24.661569\n",
      "I0710 07:39:17.756152 140295626643200 trainer.py:508] step:  4734, steps/sec: 0.17, examples/sec: 24.66 grad_norm/all/loss:347.24893 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2463778 log_pplx:4.4095855 loss:316.05704 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.55817\n",
      "I0710 07:39:24.077097 140295626643200 summary_utils.py:349] Steps/second: 0.174768, Examples/second: 24.594572\n",
      "I0710 07:39:24.077861 140295626643200 trainer.py:508] step:  4735, steps/sec: 0.17, examples/sec: 24.59 grad_norm/all/loss:79.028793 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2469841 log_pplx:4.7085233 loss:180.39529 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.56329\n",
      "I0710 07:39:27.887132 140295626643200 summary_utils.py:349] Steps/second: 0.175062, Examples/second: 24.652891\n",
      "I0710 07:39:27.887916 140295626643200 trainer.py:508] step:  4736, steps/sec: 0.18, examples/sec: 24.65 grad_norm/all/loss:46.370785 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2486932 log_pplx:4.7702074 loss:121.58067 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.56836\n",
      "I0710 07:39:34.159487 140295626643200 summary_utils.py:349] Steps/second: 0.174976, Examples/second: 24.587597\n",
      "I0710 07:39:34.160251 140295626643200 base_runner.py:111] step:  4737, steps/sec: 0.17, examples/sec: 24.59 grad_norm/all/loss:229.75005 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2475568 log_pplx:4.8601317 loss:199.99442 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.57324\n",
      "I0710 07:39:40.670500 140295626643200 summary_utils.py:349] Steps/second: 0.174855, Examples/second: 24.517925\n",
      "I0710 07:39:40.671336 140295626643200 trainer.py:508] step:  4738, steps/sec: 0.17, examples/sec: 24.52 grad_norm/all/loss:179.625 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.247514 log_pplx:4.6790714 loss:192.0174 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.57739\n",
      "I0710 07:39:43.023530 140295635035904 trainer.py:345] Write summary @4738\n",
      "2020-07-10 07:39:44.961935: I lingvo/core/ops/record_batcher.cc:394] 1128 total seconds passed. Total records yielded: 1130. Total records skipped: 0\n",
      "I0710 07:39:47.564173 140295626643200 summary_utils.py:349] Steps/second: 0.174677, Examples/second: 24.510127\n",
      "I0710 07:39:47.565378 140295626643200 trainer.py:508] step:  4739, steps/sec: 0.17, examples/sec: 24.51 grad_norm/all/loss:69.434753 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2489854 log_pplx:4.801281 loss:121.68247 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.58112\n",
      "I0710 07:39:51.702697 140295626643200 summary_utils.py:349] Steps/second: 0.174916, Examples/second: 24.643307\n",
      "I0710 07:39:51.703776 140295626643200 trainer.py:508] step:  4740, steps/sec: 0.17, examples/sec: 24.64 grad_norm/all/loss:26.640776 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2487512 log_pplx:4.6557627 loss:71.818771 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.58521\n",
      "I0710 07:40:01.367338 140295626643200 summary_utils.py:349] Steps/second: 0.174326, Examples/second: 24.508146\n",
      "I0710 07:40:01.368417 140295626643200 trainer.py:508] step:  4741, steps/sec: 0.17, examples/sec: 24.51 grad_norm/all/loss:213.81676 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2474803 log_pplx:4.879158 loss:192.84871 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.58905\n",
      "I0710 07:40:16.544731 140295626643200 summary_utils.py:349] Steps/second: 0.172937, Examples/second: 24.228099\n",
      "I0710 07:40:16.546247 140295626643200 trainer.py:508] step:  4742, steps/sec: 0.17, examples/sec: 24.23 grad_norm/all/loss:201.73825 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2465107 log_pplx:4.2266145 loss:295.33469 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.59399\n",
      "I0710 07:40:26.938736 140295626643200 summary_utils.py:349] Steps/second: 0.172270, Examples/second: 24.084405\n",
      "I0710 07:40:26.939692 140295626643200 trainer.py:508] step:  4743, steps/sec: 0.17, examples/sec: 24.08 grad_norm/all/loss:152.95934 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2478768 log_pplx:4.8121624 loss:198.38139 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.59955\n",
      "I0710 07:40:28.837958 140295635035904 trainer.py:354] Write summary done: step 4738\n",
      "I0710 07:40:28.843414 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 07:40:31.907518 140295626643200 summary_utils.py:349] Steps/second: 0.172390, Examples/second: 24.117991\n",
      "I0710 07:40:31.908297 140295626643200 trainer.py:508] step:  4744, steps/sec: 0.17, examples/sec: 24.12 grad_norm/all/loss:70.641808 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2493649 log_pplx:4.8917904 loss:123.24253 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.60596\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 07:40:33.671977 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 07:40:34.154551 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00004743\n",
      "I0710 07:40:38.318890 140295626643200 summary_utils.py:349] Steps/second: 0.172303, Examples/second: 24.056170\n",
      "I0710 07:40:38.319653 140295626643200 trainer.py:508] step:  4745, steps/sec: 0.17, examples/sec: 24.06 grad_norm/all/loss:87.173508 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2477928 log_pplx:4.7067327 loss:194.03506 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.61188\n",
      "I0710 07:40:41.412309 140295626643200 summary_utils.py:349] Steps/second: 0.172689, Examples/second: 24.206206\n",
      "I0710 07:40:41.413097 140295626643200 trainer.py:508] step:  4746, steps/sec: 0.17, examples/sec: 24.21 grad_norm/all/loss:77.763786 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2488806 log_pplx:4.7651258 loss:74.008362 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.61755\n",
      "I0710 07:40:43.610984 140295626643200 summary_utils.py:349] Steps/second: 0.173201, Examples/second: 24.584590\n",
      "I0710 07:40:43.611751 140295626643200 trainer.py:508] step:  4747, steps/sec: 0.17, examples/sec: 24.58 grad_norm/all/loss:15.722224 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.247363 log_pplx:4.4324484 loss:32.82782 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:516.62268\n",
      "I0710 07:40:49.949140 140295626643200 summary_utils.py:349] Steps/second: 0.173120, Examples/second: 24.522381\n",
      "I0710 07:40:49.950024 140295626643200 trainer.py:508] step:  4748, steps/sec: 0.17, examples/sec: 24.52 grad_norm/all/loss:98.079575 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2473922 log_pplx:4.7409954 loss:182.5876 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.62738\n",
      "I0710 07:40:53.772331 140295626643200 summary_utils.py:349] Steps/second: 0.173397, Examples/second: 24.576565\n",
      "I0710 07:40:53.773098 140295626643200 trainer.py:508] step:  4749, steps/sec: 0.17, examples/sec: 24.58 grad_norm/all/loss:41.54155 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2494553 log_pplx:4.7793336 loss:119.57295 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.63177\n",
      "I0710 07:41:00.108830 140295626643200 summary_utils.py:349] Steps/second: 0.173317, Examples/second: 24.514944\n",
      "I0710 07:41:00.109840 140295626643200 trainer.py:508] step:  4750, steps/sec: 0.17, examples/sec: 24.51 grad_norm/all/loss:93.5186 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2477598 log_pplx:4.7790627 loss:192.89491 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.63574\n",
      "I0710 07:41:06.529574 140295626643200 summary_utils.py:349] Steps/second: 0.173225, Examples/second: 24.452289\n",
      "I0710 07:41:08.321280 140295626643200 trainer.py:508] step:  4751, steps/sec: 0.17, examples/sec: 24.45 grad_norm/all/loss:100.02738 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2478517 log_pplx:4.7264891 loss:189.70944 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.63983\n",
      "I0710 07:41:11.427639 140295626643200 summary_utils.py:349] Steps/second: 0.173347, Examples/second: 24.562127\n",
      "I0710 07:41:11.428607 140295626643200 trainer.py:508] step:  4752, steps/sec: 0.17, examples/sec: 24.56 grad_norm/all/loss:62.502163 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2488929 log_pplx:4.8306756 loss:75.517036 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.64417\n",
      "I0710 07:41:21.659692 140295626643200 summary_utils.py:349] Steps/second: 0.172729, Examples/second: 24.393140\n",
      "I0710 07:41:21.660463 140295626643200 trainer.py:508] step:  4753, steps/sec: 0.17, examples/sec: 24.39 grad_norm/all/loss:208.7123 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.247159 log_pplx:4.351532 loss:322.66611 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.64868\n",
      "I0710 07:41:25.513947 140295626643200 summary_utils.py:349] Steps/second: 0.172995, Examples/second: 24.445742\n",
      "I0710 07:41:25.514761 140295626643200 trainer.py:508] step:  4754, steps/sec: 0.17, examples/sec: 24.45 grad_norm/all/loss:44.283905 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2495396 log_pplx:4.8434448 loss:122.62997 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.65411\n",
      "I0710 07:41:31.882170 140295626643200 summary_utils.py:349] Steps/second: 0.172915, Examples/second: 24.385717\n",
      "I0710 07:41:31.883236 140295626643200 trainer.py:508] step:  4755, steps/sec: 0.17, examples/sec: 24.39 grad_norm/all/loss:213.96478 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2477309 log_pplx:4.730505 loss:190.22542 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.65948\n",
      "I0710 07:41:38.515486 140295626643200 summary_utils.py:349] Steps/second: 0.172799, Examples/second: 24.321207\n",
      "I0710 07:41:38.516268 140295626643200 trainer.py:508] step:  4756, steps/sec: 0.17, examples/sec: 24.32 grad_norm/all/loss:119.47536 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2477782 log_pplx:4.6831021 loss:191.89012 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.66431\n",
      "I0710 07:41:42.453437 140295626643200 summary_utils.py:349] Steps/second: 0.173050, Examples/second: 24.371725\n",
      "I0710 07:41:42.454303 140295626643200 trainer.py:508] step:  4757, steps/sec: 0.17, examples/sec: 24.37 grad_norm/all/loss:64.371284 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2495733 log_pplx:4.8734283 loss:122.50581 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.66895\n",
      "I0710 07:41:48.526522 140295626643200 summary_utils.py:349] Steps/second: 0.173010, Examples/second: 24.318482\n",
      "I0710 07:41:48.527322 140295626643200 trainer.py:508] step:  4758, steps/sec: 0.17, examples/sec: 24.32 grad_norm/all/loss:135.46631 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2479004 log_pplx:4.738502 loss:187.70393 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.67383\n",
      "I0710 07:41:51.495620 140295626643200 summary_utils.py:349] Steps/second: 0.173390, Examples/second: 24.462033\n",
      "I0710 07:41:51.496406 140295626643200 trainer.py:508] step:  4759, steps/sec: 0.17, examples/sec: 24.46 grad_norm/all/loss:23.926945 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2493533 log_pplx:4.6922579 loss:75.167763 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.6792\n",
      "I0710 07:41:58.045368 140295626643200 summary_utils.py:349] Steps/second: 0.173284, Examples/second: 24.399698\n",
      "I0710 07:41:58.046189 140295626643200 trainer.py:508] step:  4760, steps/sec: 0.17, examples/sec: 24.40 grad_norm/all/loss:106.50738 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2480159 log_pplx:4.7482772 loss:197.0535 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.68427\n",
      "I0710 07:42:07.735687 140295626643200 summary_utils.py:349] Steps/second: 0.172761, Examples/second: 24.248192\n",
      "I0710 07:42:07.736521 140295626643200 trainer.py:508] step:  4761, steps/sec: 0.17, examples/sec: 24.25 grad_norm/all/loss:203.69786 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2470615 log_pplx:4.1825337 loss:299.36484 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.68958\n",
      "I0710 07:42:14.305195 140295626643200 summary_utils.py:349] Steps/second: 0.172657, Examples/second: 24.187341\n",
      "I0710 07:42:14.306062 140295626643200 trainer.py:508] step:  4762, steps/sec: 0.17, examples/sec: 24.19 grad_norm/all/loss:79.704628 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2480356 log_pplx:4.7080164 loss:197.08932 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.69525\n",
      "I0710 07:42:18.107339 140295626643200 summary_utils.py:349] Steps/second: 0.172920, Examples/second: 24.239398\n",
      "I0710 07:42:18.108090 140295626643200 trainer.py:508] step:  4763, steps/sec: 0.17, examples/sec: 24.24 grad_norm/all/loss:146.88239 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2495524 log_pplx:4.8756986 loss:122.44096 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.70105\n",
      "I0710 07:42:24.679905 140295626643200 summary_utils.py:349] Steps/second: 0.172816, Examples/second: 24.179016\n",
      "I0710 07:42:24.680670 140295626643200 trainer.py:508] step:  4764, steps/sec: 0.17, examples/sec: 24.18 grad_norm/all/loss:91.262199 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2482105 log_pplx:4.6636415 loss:190.68465 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.70605\n",
      "I0710 07:42:27.721229 140295626643200 summary_utils.py:349] Steps/second: 0.173176, Examples/second: 24.317604\n",
      "I0710 07:42:27.722115 140295626643200 trainer.py:508] step:  4765, steps/sec: 0.17, examples/sec: 24.32 grad_norm/all/loss:35.681263 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2491066 log_pplx:4.732388 loss:73.832642 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.71094\n",
      "I0710 07:42:31.562836 140295626643200 summary_utils.py:349] Steps/second: 0.173430, Examples/second: 24.368029\n",
      "I0710 07:42:31.563596 140295626643200 trainer.py:508] step:  4766, steps/sec: 0.17, examples/sec: 24.37 grad_norm/all/loss:50.631313 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2497082 log_pplx:4.785924 loss:121.56245 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.71539\n",
      "I0710 07:42:37.870811 140295626643200 summary_utils.py:349] Steps/second: 0.173359, Examples/second: 24.312469\n",
      "I0710 07:42:37.871701 140295626643200 trainer.py:508] step:  4767, steps/sec: 0.17, examples/sec: 24.31 grad_norm/all/loss:179.74751 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2480167 log_pplx:4.7058821 loss:192.17647 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.71991\n",
      "I0710 07:42:44.198770 140295626643200 summary_utils.py:349] Steps/second: 0.173286, Examples/second: 24.257071\n",
      "I0710 07:42:44.199549 140295626643200 trainer.py:508] step:  4768, steps/sec: 0.17, examples/sec: 24.26 grad_norm/all/loss:112.71841 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2484767 log_pplx:4.6937218 loss:192.55995 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.72516\n",
      "I0710 07:42:46.390189 140295626643200 summary_utils.py:349] Steps/second: 0.173751, Examples/second: 24.600710\n",
      "I0710 07:42:46.391013 140295626643200 trainer.py:508] step:  4769, steps/sec: 0.17, examples/sec: 24.60 grad_norm/all/loss:10.229277 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2477701 log_pplx:4.3384414 loss:31.56385 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:516.73059\n",
      "I0710 07:42:52.848497 140295626643200 summary_utils.py:349] Steps/second: 0.173660, Examples/second: 24.541920\n",
      "I0710 07:42:52.849391 140295626643200 trainer.py:508] step:  4770, steps/sec: 0.17, examples/sec: 24.54 grad_norm/all/loss:200.83202 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2482854 log_pplx:4.8047848 loss:192.4917 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.73578\n",
      "I0710 07:42:56.674419 140295626643200 summary_utils.py:349] Steps/second: 0.173909, Examples/second: 24.591050\n",
      "I0710 07:42:56.675212 140295626643200 trainer.py:508] step:  4771, steps/sec: 0.17, examples/sec: 24.59 grad_norm/all/loss:39.65527 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2494103 log_pplx:4.7846842 loss:117.79295 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.74017\n",
      "I0710 07:42:59.709602 140295626643200 summary_utils.py:349] Steps/second: 0.174259, Examples/second: 24.725534\n",
      "I0710 07:42:59.710376 140295626643200 trainer.py:508] step:  4772, steps/sec: 0.17, examples/sec: 24.73 grad_norm/all/loss:22.931446 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2494335 log_pplx:4.6173477 loss:72.867508 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.74438\n",
      "I0710 07:43:06.139687 140295626643200 summary_utils.py:349] Steps/second: 0.174170, Examples/second: 24.667240\n",
      "I0710 07:43:06.140484 140295626643200 trainer.py:508] step:  4773, steps/sec: 0.17, examples/sec: 24.67 grad_norm/all/loss:103.71251 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2484756 log_pplx:4.7230582 loss:195.88884 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.74829\n",
      "I0710 07:43:15.846384 140295626643200 summary_utils.py:349] Steps/second: 0.173664, Examples/second: 24.521100\n",
      "I0710 07:43:15.847240 140295626643200 trainer.py:508] step:  4774, steps/sec: 0.17, examples/sec: 24.52 grad_norm/all/loss:285.80826 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2470425 log_pplx:4.4017897 loss:319.4599 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.75195\n",
      "I0710 07:43:22.131586 140295626643200 summary_utils.py:349] Steps/second: 0.173598, Examples/second: 24.467038\n",
      "I0710 07:43:22.132389 140295626643200 trainer.py:508] step:  4775, steps/sec: 0.17, examples/sec: 24.47 grad_norm/all/loss:73.886414 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2482842 log_pplx:4.8089552 loss:193.8009 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.75635\n",
      "I0710 07:43:26.000990 140295626643200 summary_utils.py:349] Steps/second: 0.173836, Examples/second: 24.514553\n",
      "I0710 07:43:26.001745 140295626643200 trainer.py:508] step:  4776, steps/sec: 0.17, examples/sec: 24.51 grad_norm/all/loss:63.128468 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2500031 log_pplx:4.7791348 loss:119.50822 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.7608\n",
      "I0710 07:43:29.067550 140295626643200 summary_utils.py:349] Steps/second: 0.174175, Examples/second: 24.645783\n",
      "I0710 07:43:29.068319 140295626643200 trainer.py:508] step:  4777, steps/sec: 0.17, examples/sec: 24.65 grad_norm/all/loss:20.365419 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2492311 log_pplx:4.6029053 loss:70.733719 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.76556\n",
      "I0710 07:43:35.268730 140295626643200 summary_utils.py:349] Steps/second: 0.174117, Examples/second: 24.593162\n",
      "I0710 07:43:35.269500 140295626643200 trainer.py:508] step:  4778, steps/sec: 0.17, examples/sec: 24.59 grad_norm/all/loss:172.44073 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2483358 log_pplx:4.7858663 loss:187.18718 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.77008\n",
      "I0710 07:43:41.647246 140295626643200 summary_utils.py:349] Steps/second: 0.174038, Examples/second: 24.537882\n",
      "I0710 07:43:41.648026 140295626643200 trainer.py:508] step:  4779, steps/sec: 0.17, examples/sec: 24.54 grad_norm/all/loss:104.107 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2484561 log_pplx:4.7363958 loss:189.81108 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.77423\n",
      "I0710 07:43:45.574330 140295626643200 summary_utils.py:349] Steps/second: 0.174265, Examples/second: 24.583519\n",
      "I0710 07:43:45.575564 140295626643200 trainer.py:508] step:  4780, steps/sec: 0.17, examples/sec: 24.58 grad_norm/all/loss:32.772072 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2498937 log_pplx:4.7289362 loss:119.16917 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.7782\n",
      "I0710 07:43:51.588700 140295626643200 summary_utils.py:349] Steps/second: 0.174230, Examples/second: 24.535067\n",
      "I0710 07:43:51.589470 140295626643200 trainer.py:508] step:  4781, steps/sec: 0.17, examples/sec: 24.54 grad_norm/all/loss:144.61209 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2482682 log_pplx:4.7493839 loss:184.21675 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.7821\n",
      "I0710 07:43:54.631588 140295626643200 summary_utils.py:349] Steps/second: 0.174565, Examples/second: 24.664275\n",
      "I0710 07:43:54.632341 140295626643200 trainer.py:508] step:  4782, steps/sec: 0.17, examples/sec: 24.66 grad_norm/all/loss:28.502789 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2497579 log_pplx:4.6379156 loss:72.213799 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.78662\n",
      "I0710 07:44:04.502305 140295626643200 summary_utils.py:349] Steps/second: 0.174054, Examples/second: 24.520324\n",
      "I0710 07:44:04.503387 140295626643200 trainer.py:508] step:  4783, steps/sec: 0.17, examples/sec: 24.52 grad_norm/all/loss:261.68353 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.247321 log_pplx:4.3209991 loss:308.08725 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.79095\n",
      "I0710 07:44:11.053461 140295626643200 summary_utils.py:349] Steps/second: 0.173955, Examples/second: 24.463535\n",
      "I0710 07:44:11.054325 140295626643200 trainer.py:508] step:  4784, steps/sec: 0.17, examples/sec: 24.46 grad_norm/all/loss:137.10278 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2487564 log_pplx:4.7570748 loss:196.05093 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.79553\n",
      "I0710 07:44:14.957388 140295626643200 summary_utils.py:349] Steps/second: 0.174180, Examples/second: 24.508833\n",
      "I0710 07:44:14.958164 140295626643200 trainer.py:508] step:  4785, steps/sec: 0.17, examples/sec: 24.51 grad_norm/all/loss:72.764404 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2502314 log_pplx:4.8456435 loss:121.83763 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.8009\n",
      "I0710 07:44:21.243752 140295626643200 summary_utils.py:349] Steps/second: 0.174114, Examples/second: 24.457038\n",
      "I0710 07:44:21.244601 140295626643200 trainer.py:508] step:  4786, steps/sec: 0.17, examples/sec: 24.46 grad_norm/all/loss:140.86963 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2483501 log_pplx:4.7469168 loss:188.45259 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.80609\n",
      "I0710 07:44:24.300026 140295626643200 summary_utils.py:349] Steps/second: 0.174440, Examples/second: 24.583509\n",
      "I0710 07:44:24.300922 140295626643200 trainer.py:508] step:  4787, steps/sec: 0.17, examples/sec: 24.58 grad_norm/all/loss:36.308025 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2499684 log_pplx:4.7073555 loss:74.453453 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.81171\n",
      "I0710 07:44:30.583721 140295626643200 summary_utils.py:349] Steps/second: 0.174373, Examples/second: 24.531770\n",
      "I0710 07:44:30.584490 140295626643200 trainer.py:508] step:  4788, steps/sec: 0.17, examples/sec: 24.53 grad_norm/all/loss:80.442993 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2485701 log_pplx:4.6532168 loss:185.83783 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.81689\n",
      "I0710 07:44:34.347938 140295626643200 summary_utils.py:349] Steps/second: 0.174612, Examples/second: 24.578650\n",
      "I0710 07:44:34.348692 140295626643200 trainer.py:508] step:  4789, steps/sec: 0.17, examples/sec: 24.58 grad_norm/all/loss:61.663216 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2502033 log_pplx:4.8285232 loss:122.49358 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.82172\n",
      "I0710 07:44:40.709180 140295626643200 summary_utils.py:349] Steps/second: 0.174535, Examples/second: 24.525978\n",
      "I0710 07:44:40.710075 140295626643200 trainer.py:508] step:  4790, steps/sec: 0.17, examples/sec: 24.53 grad_norm/all/loss:73.022652 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2486422 log_pplx:4.7489777 loss:187.34717 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.82593\n",
      "I0710 07:44:46.889711 140295626643200 summary_utils.py:349] Steps/second: 0.174481, Examples/second: 24.476805\n",
      "I0710 07:44:46.890552 140295626643200 trainer.py:508] step:  4791, steps/sec: 0.17, examples/sec: 24.48 grad_norm/all/loss:75.077057 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2483639 log_pplx:4.6473479 loss:183.45406 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.8299\n",
      "I0710 07:44:50.645764 140295626643200 summary_utils.py:349] Steps/second: 0.174717, Examples/second: 24.523440\n",
      "I0710 07:44:50.646600 140295626643200 trainer.py:508] step:  4792, steps/sec: 0.17, examples/sec: 24.52 grad_norm/all/loss:64.815437 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2501322 log_pplx:4.7861724 loss:119.74406 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.83374\n",
      "I0710 07:44:53.715652 140295626643200 summary_utils.py:349] Steps/second: 0.175034, Examples/second: 24.647000\n",
      "I0710 07:44:53.716668 140295626643200 trainer.py:508] step:  4793, steps/sec: 0.18, examples/sec: 24.65 grad_norm/all/loss:37.591064 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2498226 log_pplx:4.7186136 loss:73.617744 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.83777\n",
      "I0710 07:45:03.694018 140295626643200 summary_utils.py:349] Steps/second: 0.174527, Examples/second: 24.507150\n",
      "I0710 07:45:03.694879 140295626643200 trainer.py:508] step:  4794, steps/sec: 0.17, examples/sec: 24.51 grad_norm/all/loss:326.53296 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2476933 log_pplx:4.4269347 loss:315.97247 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.84174\n",
      "I0710 07:45:05.876157 140295626643200 summary_utils.py:349] Steps/second: 0.174947, Examples/second: 24.818069\n",
      "I0710 07:45:05.876888 140295626643200 trainer.py:508] step:  4795, steps/sec: 0.17, examples/sec: 24.82 grad_norm/all/loss:11.074485 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2484629 log_pplx:4.3785405 loss:31.949661 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:516.8465\n",
      "I0710 07:45:12.247895 140295626643200 summary_utils.py:349] Steps/second: 0.174870, Examples/second: 24.765315\n",
      "I0710 07:45:12.248706 140295626643200 trainer.py:508] step:  4796, steps/sec: 0.17, examples/sec: 24.77 grad_norm/all/loss:505.60559 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2485613 log_pplx:5.0301466 loss:204.53833 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.85107\n",
      "I0710 07:45:19.081686 140295626643200 summary_utils.py:349] Steps/second: 0.174739, Examples/second: 24.705339\n",
      "I0710 07:45:19.082649 140295626643200 trainer.py:508] step:  4797, steps/sec: 0.17, examples/sec: 24.71 grad_norm/all/loss:174.10953 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.248855 log_pplx:4.8397021 loss:192.62015 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.85522\n",
      "I0710 07:45:23.048345 140295626643200 summary_utils.py:349] Steps/second: 0.174944, Examples/second: 24.746898\n",
      "I0710 07:45:23.049105 140295626643200 trainer.py:508] step:  4798, steps/sec: 0.17, examples/sec: 24.75 grad_norm/all/loss:52.956409 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2504762 log_pplx:4.8418193 loss:121.3481 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.85919\n",
      "I0710 07:45:26.099050 140295626643200 summary_utils.py:349] Steps/second: 0.175256, Examples/second: 24.867640\n",
      "I0710 07:45:26.099913 140295626643200 trainer.py:508] step:  4799, steps/sec: 0.18, examples/sec: 24.87 grad_norm/all/loss:25.255274 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2502345 log_pplx:4.5975404 loss:74.314941 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.86322\n",
      "I0710 07:45:32.367614 140295626643200 summary_utils.py:349] Steps/second: 0.175190, Examples/second: 24.817092\n",
      "I0710 07:45:32.368367 140295626643200 trainer.py:508] step:  4800, steps/sec: 0.18, examples/sec: 24.82 grad_norm/all/loss:411.70193 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2488854 log_pplx:5.328402 loss:211.73735 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.86707\n",
      "I0710 07:45:36.237241 140295626643200 summary_utils.py:349] Steps/second: 0.175404, Examples/second: 24.859592\n",
      "I0710 07:45:36.238152 140295626643200 trainer.py:508] step:  4801, steps/sec: 0.18, examples/sec: 24.86 grad_norm/all/loss:86.123932 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2504863 log_pplx:4.8855252 loss:122.16866 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.87299\n",
      "I0710 07:45:42.274145 140295626643200 summary_utils.py:349] Steps/second: 0.175365, Examples/second: 24.813220\n",
      "I0710 07:45:42.274915 140295626643200 trainer.py:508] step:  4802, steps/sec: 0.18, examples/sec: 24.81 grad_norm/all/loss:196.4648 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2488421 log_pplx:4.8710947 loss:191.61667 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.87897\n",
      "I0710 07:45:48.616414 140295626643200 summary_utils.py:349] Steps/second: 0.175292, Examples/second: 24.762232\n",
      "I0710 07:45:48.617243 140295626643200 trainer.py:508] step:  4803, steps/sec: 0.18, examples/sec: 24.76 grad_norm/all/loss:80.41906 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2491677 log_pplx:4.7798061 loss:196.21103 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.88531\n",
      "I0710 07:45:51.705461 140295626643200 summary_utils.py:349] Steps/second: 0.175593, Examples/second: 24.880286\n",
      "I0710 07:45:51.706267 140295626643200 trainer.py:508] step:  4804, steps/sec: 0.18, examples/sec: 24.88 grad_norm/all/loss:122.01753 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2502471 log_pplx:5.0252485 loss:78.362473 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.8913\n",
      "I0710 07:45:58.046084 140295626643200 summary_utils.py:349] Steps/second: 0.175519, Examples/second: 24.829362\n",
      "I0710 07:45:58.046832 140295626643200 trainer.py:508] step:  4805, steps/sec: 0.18, examples/sec: 24.83 grad_norm/all/loss:292.03998 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2490094 log_pplx:4.9416976 loss:200.9418 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.89636\n",
      "I0710 07:46:01.888094 140295626643200 summary_utils.py:349] Steps/second: 0.175732, Examples/second: 24.871567\n",
      "I0710 07:46:01.888878 140295626643200 trainer.py:508] step:  4806, steps/sec: 0.18, examples/sec: 24.87 grad_norm/all/loss:224.90297 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2504535 log_pplx:5.1200495 loss:129.34526 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.90021\n",
      "I0710 07:46:08.035262 140295626643200 summary_utils.py:349] Steps/second: 0.175679, Examples/second: 24.824140\n",
      "I0710 07:46:08.035998 140295626643200 trainer.py:508] step:  4807, steps/sec: 0.18, examples/sec: 24.82 grad_norm/all/loss:96.58017 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2487966 log_pplx:4.8433132 loss:196.09366 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.90326\n",
      "I0710 07:46:18.628701 140295626643200 summary_utils.py:349] Steps/second: 0.175123, Examples/second: 24.680053\n",
      "I0710 07:46:18.629490 140295626643200 trainer.py:508] step:  4808, steps/sec: 0.18, examples/sec: 24.68 grad_norm/all/loss:187.33025 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2482129 log_pplx:4.3461089 loss:313.4631 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.90619\n",
      "I0710 07:46:24.975342 140295626643200 summary_utils.py:349] Steps/second: 0.175051, Examples/second: 24.630733\n",
      "I0710 07:46:24.976213 140295626643200 trainer.py:508] step:  4809, steps/sec: 0.18, examples/sec: 24.63 grad_norm/all/loss:194.14671 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2492067 log_pplx:4.8974938 loss:197.369 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.91022\n",
      "I0710 07:46:28.797484 140295626643200 summary_utils.py:349] Steps/second: 0.175264, Examples/second: 24.673013\n",
      "I0710 07:46:28.798582 140295626643200 trainer.py:508] step:  4810, steps/sec: 0.18, examples/sec: 24.67 grad_norm/all/loss:136.29309 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2506599 log_pplx:4.9395528 loss:124.41499 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.91504\n",
      "I0710 07:46:35.369707 140295626643200 summary_utils.py:349] Steps/second: 0.175167, Examples/second: 24.620491\n",
      "I0710 07:46:35.370535 140295626643200 trainer.py:508] step:  4811, steps/sec: 0.18, examples/sec: 24.62 grad_norm/all/loss:151.10852 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2492341 log_pplx:4.8422031 loss:200.40669 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.92053\n",
      "I0710 07:46:38.414338 140295626643200 summary_utils.py:349] Steps/second: 0.175464, Examples/second: 24.736003\n",
      "I0710 07:46:38.415082 140295626643200 trainer.py:508] step:  4812, steps/sec: 0.18, examples/sec: 24.74 grad_norm/all/loss:65.575844 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2502451 log_pplx:4.8816791 loss:76.524132 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.92645\n",
      "I0710 07:46:44.782221 140295626643200 summary_utils.py:349] Steps/second: 0.175390, Examples/second: 24.686744\n",
      "I0710 07:46:44.782984 140295626643200 trainer.py:508] step:  4813, steps/sec: 0.18, examples/sec: 24.69 grad_norm/all/loss:141.55739 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2490005 log_pplx:4.844285 loss:194.07419 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.9325\n",
      "I0710 07:46:48.649097 140295626643200 summary_utils.py:349] Steps/second: 0.175594, Examples/second: 24.727657\n",
      "I0710 07:46:48.649986 140295626643200 trainer.py:508] step:  4814, steps/sec: 0.18, examples/sec: 24.73 grad_norm/all/loss:76.838989 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2507063 log_pplx:4.915288 loss:125.21696 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.93756\n",
      "I0710 07:46:54.734013 140295626643200 summary_utils.py:349] Steps/second: 0.175551, Examples/second: 24.683159\n",
      "I0710 07:46:54.734847 140295626643200 trainer.py:508] step:  4815, steps/sec: 0.18, examples/sec: 24.68 grad_norm/all/loss:149.95953 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2490515 log_pplx:4.843142 loss:190.75925 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.9422\n",
      "I0710 07:46:57.732766 140295626643200 summary_utils.py:349] Steps/second: 0.175849, Examples/second: 24.797859\n",
      "I0710 07:46:57.733605 140295626643200 trainer.py:508] step:  4816, steps/sec: 0.18, examples/sec: 24.80 grad_norm/all/loss:25.605606 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2503997 log_pplx:4.6936207 loss:73.869522 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.94623\n",
      "I0710 07:46:59.948524 140295626643200 summary_utils.py:349] Steps/second: 0.176233, Examples/second: 25.085530\n",
      "I0710 07:46:59.949281 140295626643200 trainer.py:508] step:  4817, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:11.391252 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.249386 log_pplx:4.5577426 loss:33.310688 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:516.95013\n",
      "I0710 07:47:06.296598 140295626643200 summary_utils.py:349] Steps/second: 0.176159, Examples/second: 25.035852\n",
      "I0710 07:47:06.297374 140295626643200 trainer.py:508] step:  4818, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:78.666092 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2490147 log_pplx:4.7012362 loss:185.69885 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.95386\n",
      "I0710 07:47:10.197270 140295626643200 summary_utils.py:349] Steps/second: 0.176354, Examples/second: 25.074840\n",
      "I0710 07:47:10.198120 140295626643200 trainer.py:508] step:  4819, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:45.215878 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2509043 log_pplx:4.8488235 loss:122.97829 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.95789\n",
      "I0710 07:47:16.378162 140295626643200 summary_utils.py:349] Steps/second: 0.176298, Examples/second: 25.028127\n",
      "I0710 07:47:16.378994 140295626643200 trainer.py:508] step:  4820, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:88.481834 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2486711 log_pplx:4.6962519 loss:181.45145 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.96185\n",
      "I0710 07:47:26.144953 140295626643200 summary_utils.py:349] Steps/second: 0.175851, Examples/second: 24.901536\n",
      "I0710 07:47:26.145870 140295626643200 trainer.py:508] step:  4821, steps/sec: 0.18, examples/sec: 24.90 grad_norm/all/loss:187.42929 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2483996 log_pplx:4.4555945 loss:314.78775 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:516.96619\n",
      "I0710 07:47:32.429845 140295626643200 summary_utils.py:349] Steps/second: 0.175786, Examples/second: 24.854349\n",
      "I0710 07:47:32.430647 140295626643200 trainer.py:508] step:  4822, steps/sec: 0.18, examples/sec: 24.85 grad_norm/all/loss:76.585121 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2492067 log_pplx:4.751709 loss:189.2962 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.97156\n",
      "I0710 07:47:36.238035 140295626643200 summary_utils.py:349] Steps/second: 0.175990, Examples/second: 24.894562\n",
      "I0710 07:47:36.238823 140295626643200 trainer.py:508] step:  4823, steps/sec: 0.18, examples/sec: 24.89 grad_norm/all/loss:54.43652 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.251575 log_pplx:4.8274512 loss:123.40173 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.97705\n",
      "I0710 07:47:42.477698 140295626643200 summary_utils.py:349] Steps/second: 0.175930, Examples/second: 24.848383\n",
      "I0710 07:47:42.478524 140295626643200 trainer.py:508] step:  4824, steps/sec: 0.18, examples/sec: 24.85 grad_norm/all/loss:89.425949 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.249652 log_pplx:4.7468929 loss:198.42012 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.98212\n",
      "I0710 07:47:45.526855 140295626643200 summary_utils.py:349] Steps/second: 0.176213, Examples/second: 24.958659\n",
      "I0710 07:47:45.527576 140295626643200 trainer.py:508] step:  4825, steps/sec: 0.18, examples/sec: 24.96 grad_norm/all/loss:43.302734 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2508315 log_pplx:4.7589674 loss:73.763992 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:516.98706\n",
      "I0710 07:47:51.512456 140295626643200 summary_utils.py:349] Steps/second: 0.176180, Examples/second: 24.916356\n",
      "I0710 07:47:51.513410 140295626643200 trainer.py:508] step:  4826, steps/sec: 0.18, examples/sec: 24.92 grad_norm/all/loss:74.401657 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2493858 log_pplx:4.7989502 loss:197.0569 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:516.99158\n",
      "I0710 07:47:55.314270 140295626643200 summary_utils.py:349] Steps/second: 0.176381, Examples/second: 24.956055\n",
      "I0710 07:47:55.315187 140295626643200 trainer.py:508] step:  4827, steps/sec: 0.18, examples/sec: 24.96 grad_norm/all/loss:36.085064 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2511771 log_pplx:4.7645636 loss:120.81148 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:516.99603\n",
      "I0710 07:48:01.690963 140295626643200 summary_utils.py:349] Steps/second: 0.176305, Examples/second: 24.908109\n",
      "I0710 07:48:01.691762 140295626643200 trainer.py:508] step:  4828, steps/sec: 0.18, examples/sec: 24.91 grad_norm/all/loss:87.81366 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2493527 log_pplx:4.6723166 loss:191.68176 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.00031\n",
      "I0710 07:48:04.719545 140295626643200 summary_utils.py:349] Steps/second: 0.176587, Examples/second: 25.017305\n",
      "I0710 07:48:04.720402 140295626643200 trainer.py:508] step:  4829, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:22.175549 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2504518 log_pplx:4.5695405 loss:70.57798 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.00452\n",
      "I0710 07:48:10.912608 140295626643200 summary_utils.py:349] Steps/second: 0.176531, Examples/second: 24.972158\n",
      "I0710 07:48:10.913367 140295626643200 trainer.py:508] step:  4830, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:72.99752 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2492998 log_pplx:4.6571398 loss:186.05273 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.00854\n",
      "I0710 07:48:14.748018 140295626643200 summary_utils.py:349] Steps/second: 0.176725, Examples/second: 25.010762\n",
      "I0710 07:48:14.748957 140295626643200 trainer.py:508] step:  4831, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:36.876625 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2513363 log_pplx:4.8409305 loss:121.35606 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.01239\n",
      "I0710 07:48:20.984946 140295626643200 summary_utils.py:349] Steps/second: 0.176664, Examples/second: 24.965255\n",
      "I0710 07:48:20.985662 140295626643200 trainer.py:508] step:  4832, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:69.020767 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2496593 log_pplx:4.7651005 loss:196.26257 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.01587\n",
      "I0710 07:48:32.180775 140295626643200 summary_utils.py:349] Steps/second: 0.176082, Examples/second: 24.822779\n",
      "I0710 07:48:32.181513 140295626643200 trainer.py:508] step:  4833, steps/sec: 0.18, examples/sec: 24.82 grad_norm/all/loss:223.22426 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2489789 log_pplx:4.3296952 loss:324.94363 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.01953\n",
      "I0710 07:48:38.182166 140295626643200 summary_utils.py:349] Steps/second: 0.176048, Examples/second: 24.781896\n",
      "I0710 07:48:38.182937 140295626643200 trainer.py:508] step:  4834, steps/sec: 0.18, examples/sec: 24.78 grad_norm/all/loss:78.02623 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.249483 log_pplx:4.7119079 loss:193.48271 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.02277\n",
      "I0710 07:48:41.233590 140295626643200 summary_utils.py:349] Steps/second: 0.176322, Examples/second: 24.888624\n",
      "I0710 07:48:41.234374 140295626643200 trainer.py:508] step:  4835, steps/sec: 0.18, examples/sec: 24.89 grad_norm/all/loss:27.662306 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2509941 log_pplx:4.698916 loss:74.521866 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.02637\n",
      "I0710 07:48:45.037868 140295626643200 summary_utils.py:349] Steps/second: 0.176516, Examples/second: 24.927184\n",
      "I0710 07:48:45.038790 140295626643200 trainer.py:508] step:  4836, steps/sec: 0.18, examples/sec: 24.93 grad_norm/all/loss:37.661514 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2514764 log_pplx:4.8717227 loss:123.22412 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.02997\n",
      "I0710 07:48:51.322832 140295626643200 summary_utils.py:349] Steps/second: 0.176452, Examples/second: 24.882091\n",
      "I0710 07:48:51.323619 140295626643200 base_runner.py:111] step:  4837, steps/sec: 0.18, examples/sec: 24.88 grad_norm/all/loss:95.299362 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2498564 log_pplx:4.7827539 loss:194.89725 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.03357\n",
      "I0710 07:48:57.608554 140295626643200 summary_utils.py:349] Steps/second: 0.176388, Examples/second: 24.837319\n",
      "I0710 07:48:57.609289 140295626643200 trainer.py:508] step:  4838, steps/sec: 0.18, examples/sec: 24.84 grad_norm/all/loss:69.983841 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2496133 log_pplx:4.7734437 loss:189.86372 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.0379\n",
      "I0710 07:48:59.314631 140295635035904 trainer.py:345] Write summary @4838\n",
      "2020-07-10 07:49:01.620851: I lingvo/core/ops/record_batcher.cc:394] 1685 total seconds passed. Total records yielded: 1310. Total records skipped: 0\n",
      "I0710 07:49:02.483622 140295626643200 summary_utils.py:349] Steps/second: 0.176470, Examples/second: 25.065747\n",
      "I0710 07:49:02.485072 140295626643200 trainer.py:508] step:  4839, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:10.987543 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2498463 log_pplx:4.4866562 loss:33.816418 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:517.0426\n",
      "I0710 07:49:08.003479 140295626643200 summary_utils.py:349] Steps/second: 0.176485, Examples/second: 25.078351\n",
      "I0710 07:49:08.005182 140295626643200 trainer.py:508] step:  4840, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:58.346306 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2510811 log_pplx:4.789547 loss:119.08012 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.04706\n",
      "I0710 07:49:16.917658 140295626643200 summary_utils.py:349] Steps/second: 0.176153, Examples/second: 24.995170\n",
      "I0710 07:49:16.918790 140295626643200 trainer.py:508] step:  4841, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:71.413498 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2497845 log_pplx:4.8100162 loss:193.48291 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.05109\n",
      "I0710 07:49:20.773039 140295626643200 summary_utils.py:349] Steps/second: 0.176338, Examples/second: 25.087463\n",
      "I0710 07:49:20.774157 140295626643200 trainer.py:508] step:  4842, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:28.931339 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2507102 log_pplx:4.5751982 loss:71.004936 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.05505\n",
      "I0710 07:49:29.717937 140295626643200 summary_utils.py:349] Steps/second: 0.176006, Examples/second: 25.004404\n",
      "I0710 07:49:29.718950 140295626643200 trainer.py:508] step:  4843, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:125.02303 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2499454 log_pplx:4.7751403 loss:201.39156 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.05865\n",
      "I0710 07:49:35.062912 140295626643200 summary_utils.py:349] Steps/second: 0.176040, Examples/second: 25.019515\n",
      "I0710 07:49:35.064546 140295626643200 trainer.py:508] step:  4844, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:35.332787 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2516918 log_pplx:4.8475952 loss:121.61405 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.06287\n",
      "I0710 07:49:45.330567 140295626643200 summary_utils.py:349] Steps/second: 0.175580, Examples/second: 24.918675\n",
      "I0710 07:49:45.331830 140295626643200 trainer.py:508] step:  4845, steps/sec: 0.18, examples/sec: 24.92 grad_norm/all/loss:122.60767 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2500548 log_pplx:4.7140226 loss:189.97513 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.06696\n",
      "I0710 07:49:54.078147 140295635035904 trainer.py:354] Write summary done: step 4838\n",
      "I0710 07:49:59.851801 140295626643200 summary_utils.py:349] Steps/second: 0.174704, Examples/second: 24.736706\n",
      "I0710 07:49:59.852551 140295626643200 trainer.py:508] step:  4846, steps/sec: 0.17, examples/sec: 24.74 grad_norm/all/loss:299.10147 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2492874 log_pplx:4.4305377 loss:331.84726 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.07074\n",
      "I0710 07:50:06.062509 140295626643200 summary_utils.py:349] Steps/second: 0.174656, Examples/second: 24.695221\n",
      "I0710 07:50:06.063417 140295626643200 trainer.py:508] step:  4847, steps/sec: 0.17, examples/sec: 24.70 grad_norm/all/loss:94.934753 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2496353 log_pplx:4.6797123 loss:183.32771 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.07581\n",
      "I0710 07:50:09.852229 140295626643200 summary_utils.py:349] Steps/second: 0.174846, Examples/second: 24.732558\n",
      "I0710 07:50:09.852964 140295626643200 trainer.py:508] step:  4848, steps/sec: 0.17, examples/sec: 24.73 grad_norm/all/loss:39.443272 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2511863 log_pplx:4.7581086 loss:119.19063 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.08063\n",
      "I0710 07:50:12.820822 140295626643200 summary_utils.py:349] Steps/second: 0.175116, Examples/second: 24.835034\n",
      "I0710 07:50:12.821919 140295626643200 trainer.py:508] step:  4849, steps/sec: 0.18, examples/sec: 24.84 grad_norm/all/loss:21.401926 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2508744 log_pplx:4.5658717 loss:71.020706 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.08539\n",
      "I0710 07:50:19.257120 140295626643200 summary_utils.py:349] Steps/second: 0.175045, Examples/second: 24.790380\n",
      "I0710 07:50:19.257880 140295626643200 trainer.py:508] step:  4850, steps/sec: 0.18, examples/sec: 24.79 grad_norm/all/loss:224.11617 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2495745 log_pplx:4.7070279 loss:188.10461 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.08997\n",
      "I0710 07:50:25.348663 140295626643200 summary_utils.py:349] Steps/second: 0.175008, Examples/second: 24.750802\n",
      "I0710 07:50:25.349463 140295626643200 trainer.py:508] step:  4851, steps/sec: 0.18, examples/sec: 24.75 grad_norm/all/loss:117.99332 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2498688 log_pplx:4.6856518 loss:185.20036 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.09375\n",
      "I0710 07:50:29.341548 140295626643200 summary_utils.py:349] Steps/second: 0.175175, Examples/second: 24.784821\n",
      "I0710 07:50:29.342428 140295626643200 trainer.py:508] step:  4852, steps/sec: 0.18, examples/sec: 24.78 grad_norm/all/loss:83.105125 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2513901 log_pplx:4.7963424 loss:119.60879 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.09711\n",
      "I0710 07:50:34.125965 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 07:50:36.023771 140295626643200 summary_utils.py:349] Steps/second: 0.175081, Examples/second: 24.737384\n",
      "I0710 07:50:36.024582 140295626643200 trainer.py:508] step:  4853, steps/sec: 0.18, examples/sec: 24.74 grad_norm/all/loss:183.09094 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2499074 log_pplx:4.7205067 loss:187.87617 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.10101\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 07:50:39.290347 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 07:50:39.744712 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00004852\n",
      "I0710 07:50:45.849581 140295626643200 summary_utils.py:349] Steps/second: 0.174684, Examples/second: 24.625485\n",
      "I0710 07:50:45.850365 140295626643200 trainer.py:508] step:  4854, steps/sec: 0.17, examples/sec: 24.63 grad_norm/all/loss:296.47574 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2491678 log_pplx:4.4933567 loss:328.9137 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.10559\n",
      "I0710 07:50:48.790144 140295626643200 summary_utils.py:349] Steps/second: 0.174952, Examples/second: 24.726487\n",
      "I0710 07:50:48.790862 140295626643200 trainer.py:508] step:  4855, steps/sec: 0.17, examples/sec: 24.73 grad_norm/all/loss:33.640106 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2512565 log_pplx:4.6486645 loss:72.199577 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.11066\n",
      "I0710 07:50:55.080200 140295626643200 summary_utils.py:349] Steps/second: 0.174896, Examples/second: 24.685076\n",
      "I0710 07:50:55.080968 140295626643200 trainer.py:508] step:  4856, steps/sec: 0.17, examples/sec: 24.69 grad_norm/all/loss:188.05383 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2503734 log_pplx:4.9216652 loss:204.55673 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.1156\n",
      "I0710 07:50:58.868815 140295626643200 summary_utils.py:349] Steps/second: 0.175081, Examples/second: 24.721448\n",
      "I0710 07:50:58.869614 140295626643200 trainer.py:508] step:  4857, steps/sec: 0.18, examples/sec: 24.72 grad_norm/all/loss:90.001945 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2516857 log_pplx:4.8462267 loss:120.42873 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.12122\n",
      "I0710 07:51:04.587155 140295626643200 summary_utils.py:349] Steps/second: 0.175080, Examples/second: 24.687978\n",
      "I0710 07:51:04.587971 140295626643200 trainer.py:508] step:  4858, steps/sec: 0.18, examples/sec: 24.69 grad_norm/all/loss:97.16687 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2499506 log_pplx:4.799706 loss:187.12856 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.12653\n",
      "I0710 07:51:10.975291 140295626643200 summary_utils.py:349] Steps/second: 0.175016, Examples/second: 24.645740\n",
      "I0710 07:51:10.976075 140295626643200 trainer.py:508] step:  4859, steps/sec: 0.18, examples/sec: 24.65 grad_norm/all/loss:82.493065 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2503806 log_pplx:4.8203063 loss:198.77737 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.13202\n",
      "I0710 07:51:14.083534 140295626643200 summary_utils.py:349] Steps/second: 0.175263, Examples/second: 24.743082\n",
      "I0710 07:51:14.084437 140295626643200 trainer.py:508] step:  4860, steps/sec: 0.18, examples/sec: 24.74 grad_norm/all/loss:30.912754 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2513008 log_pplx:4.5567198 loss:71.198746 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.13757\n",
      "2020-07-10 07:51:16.850082: I lingvo/core/ops/record_yielder.cc:532] Epoch 2: total records 46838\n",
      "2020-07-10 07:51:16.850150: I lingvo/core/ops/record_yielder.cc:485] Epoch 2 /tmp/punctuator_data/train.txt\n",
      "I0710 07:51:17.945785 140295626643200 summary_utils.py:349] Steps/second: 0.175438, Examples/second: 24.777973\n",
      "I0710 07:51:17.946671 140295626643200 trainer.py:508] step:  4861, steps/sec: 0.18, examples/sec: 24.78 grad_norm/all/loss:76.331245 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2519257 log_pplx:4.7822313 loss:122.06645 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.14276\n",
      "I0710 07:51:24.429822 140295626643200 summary_utils.py:349] Steps/second: 0.175364, Examples/second: 24.734449\n",
      "I0710 07:51:26.435814 140295626643200 trainer.py:508] step:  4862, steps/sec: 0.18, examples/sec: 24.73 grad_norm/all/loss:92.112717 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2497237 log_pplx:4.6824083 loss:185.36482 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.14752\n",
      "I0710 07:51:28.615999 140295626643200 summary_utils.py:349] Steps/second: 0.175507, Examples/second: 24.954349\n",
      "I0710 07:51:28.616825 140295626643200 trainer.py:508] step:  4863, steps/sec: 0.18, examples/sec: 24.95 grad_norm/all/loss:12.993051 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.249826 log_pplx:4.4394708 loss:32.368248 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:517.1521\n",
      "2020-07-10 07:51:31.019995: I lingvo/core/ops/record_batcher.cc:394] 1875 total seconds passed. Total records yielded: 47179. Total records skipped: 21\n",
      "2020-07-10 07:51:31.020047: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 07:51:31.020054: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 07:51:31.020059: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 07:51:31.020072: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 07:51:31.020084: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 07:51:31.020089: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 07:51:31.020096: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 07:51:31.020103: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 07:51:31.020116: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 07:51:31.020128: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "I0710 07:51:39.081716 140295626643200 summary_utils.py:349] Steps/second: 0.175059, Examples/second: 24.835948\n",
      "I0710 07:51:39.082588 140295626643200 trainer.py:508] step:  4864, steps/sec: 0.18, examples/sec: 24.84 grad_norm/all/loss:308.1431 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2493174 log_pplx:4.4734783 loss:324.43903 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.15649\n",
      "I0710 07:51:45.146964 140295626643200 summary_utils.py:349] Steps/second: 0.175026, Examples/second: 24.798256\n",
      "I0710 07:51:45.147799 140295626643200 trainer.py:508] step:  4865, steps/sec: 0.18, examples/sec: 24.80 grad_norm/all/loss:80.109039 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2505934 log_pplx:4.7358871 loss:190.20509 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.16125\n",
      "I0710 07:51:48.917180 140295626643200 summary_utils.py:349] Steps/second: 0.175208, Examples/second: 24.833672\n",
      "I0710 07:51:48.917985 140295626643200 trainer.py:508] step:  4866, steps/sec: 0.18, examples/sec: 24.83 grad_norm/all/loss:111.17846 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2515284 log_pplx:4.8909817 loss:122.76363 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.16614\n",
      "I0710 07:51:51.943783 140295626643200 summary_utils.py:349] Steps/second: 0.175457, Examples/second: 24.929822\n",
      "I0710 07:51:51.944554 140295626643200 trainer.py:508] step:  4867, steps/sec: 0.18, examples/sec: 24.93 grad_norm/all/loss:44.650578 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2514559 log_pplx:4.6142755 loss:71.953857 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.17139\n",
      "I0710 07:51:58.498247 140295626643200 summary_utils.py:349] Steps/second: 0.175378, Examples/second: 24.885633\n",
      "I0710 07:51:58.499033 140295626643200 trainer.py:508] step:  4868, steps/sec: 0.18, examples/sec: 24.89 grad_norm/all/loss:347.46661 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2506094 log_pplx:4.8110723 loss:200.02031 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.17657\n",
      "I0710 07:52:04.736791 140295626643200 summary_utils.py:349] Steps/second: 0.175328, Examples/second: 24.845893\n",
      "I0710 07:52:04.737570 140295626643200 trainer.py:508] step:  4869, steps/sec: 0.18, examples/sec: 24.85 grad_norm/all/loss:193.82326 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2505368 log_pplx:4.703351 loss:188.66316 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.18158\n",
      "I0710 07:52:08.525624 140295626643200 summary_utils.py:349] Steps/second: 0.175505, Examples/second: 24.880606\n",
      "I0710 07:52:08.526449 140295626643200 trainer.py:508] step:  4870, steps/sec: 0.18, examples/sec: 24.88 grad_norm/all/loss:51.536221 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2521909 log_pplx:4.6648841 loss:118.92539 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.18652\n",
      "I0710 07:52:14.762948 140295626643200 summary_utils.py:349] Steps/second: 0.175455, Examples/second: 24.841108\n",
      "I0710 07:52:14.763756 140295626643200 trainer.py:508] step:  4871, steps/sec: 0.18, examples/sec: 24.84 grad_norm/all/loss:249.51776 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.250376 log_pplx:4.5366778 loss:184.13242 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.19135\n",
      "I0710 07:52:17.811493 140295626643200 summary_utils.py:349] Steps/second: 0.175699, Examples/second: 24.935656\n",
      "I0710 07:52:17.812264 140295626643200 trainer.py:508] step:  4872, steps/sec: 0.18, examples/sec: 24.94 grad_norm/all/loss:30.15357 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.251215 log_pplx:4.5243831 loss:68.76709 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.19708\n",
      "I0710 07:52:24.198427 140295626643200 summary_utils.py:349] Steps/second: 0.175635, Examples/second: 24.894223\n",
      "I0710 07:52:24.199245 140295626643200 trainer.py:508] step:  4873, steps/sec: 0.18, examples/sec: 24.89 grad_norm/all/loss:211.25726 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2502233 log_pplx:4.4942446 loss:181.90453 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.20227\n",
      "I0710 07:52:28.000632 140295626643200 summary_utils.py:349] Steps/second: 0.175809, Examples/second: 24.928314\n",
      "I0710 07:52:28.001423 140295626643200 trainer.py:508] step:  4874, steps/sec: 0.18, examples/sec: 24.93 grad_norm/all/loss:89.250458 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2522807 log_pplx:4.7445951 loss:119.23763 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.20807\n",
      "I0710 07:52:34.147477 140295626643200 summary_utils.py:349] Steps/second: 0.175767, Examples/second: 24.890233\n",
      "I0710 07:52:34.148232 140295626643200 trainer.py:508] step:  4875, steps/sec: 0.18, examples/sec: 24.89 grad_norm/all/loss:91.46418 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.250545 log_pplx:4.5227504 loss:186.84612 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.21313\n",
      "I0710 07:52:45.166141 140295626643200 summary_utils.py:349] Steps/second: 0.175282, Examples/second: 24.769108\n",
      "I0710 07:52:45.166997 140295626643200 trainer.py:508] step:  4876, steps/sec: 0.18, examples/sec: 24.77 grad_norm/all/loss:201.25142 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2495008 log_pplx:3.9786477 loss:281.48935 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.21808\n",
      "I0710 07:52:51.405980 140295626643200 summary_utils.py:349] Steps/second: 0.175234, Examples/second: 24.730683\n",
      "I0710 07:52:51.406783 140295626643200 trainer.py:508] step:  4877, steps/sec: 0.18, examples/sec: 24.73 grad_norm/all/loss:102.99551 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2508109 log_pplx:4.5145636 loss:186.56435 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.22302\n",
      "I0710 07:52:54.346946 140295626643200 summary_utils.py:349] Steps/second: 0.175483, Examples/second: 24.824995\n",
      "I0710 07:52:54.347710 140295626643200 trainer.py:508] step:  4878, steps/sec: 0.18, examples/sec: 24.82 grad_norm/all/loss:72.135544 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2519972 log_pplx:4.6764731 loss:74.220734 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.22784\n",
      "I0710 07:52:58.136518 140295626643200 summary_utils.py:349] Steps/second: 0.175656, Examples/second: 24.858855\n",
      "I0710 07:52:58.137287 140295626643200 trainer.py:508] step:  4879, steps/sec: 0.18, examples/sec: 24.86 grad_norm/all/loss:82.345116 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2521245 log_pplx:4.7224422 loss:118.20866 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.23218\n",
      "I0710 07:53:04.551767 140295626643200 summary_utils.py:349] Steps/second: 0.175591, Examples/second: 24.818169\n",
      "I0710 07:53:04.552662 140295626643200 trainer.py:508] step:  4880, steps/sec: 0.18, examples/sec: 24.82 grad_norm/all/loss:114.68939 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2505876 log_pplx:4.5264616 loss:181.17163 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.23621\n",
      "I0710 07:53:10.827827 140295626643200 summary_utils.py:349] Steps/second: 0.175539, Examples/second: 24.779510\n",
      "I0710 07:53:10.828752 140295626643200 trainer.py:508] step:  4881, steps/sec: 0.18, examples/sec: 24.78 grad_norm/all/loss:128.80505 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2506257 log_pplx:4.3950553 loss:180.52689 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.24109\n",
      "I0710 07:53:16.676134 140295626643200 summary_utils.py:349] Steps/second: 0.175525, Examples/second: 24.746481\n",
      "I0710 07:53:16.676889 140295626643200 trainer.py:508] step:  4882, steps/sec: 0.18, examples/sec: 24.75 grad_norm/all/loss:79.188148 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2505817 log_pplx:4.549057 loss:179.06226 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.24634\n",
      "I0710 07:53:20.482934 140295626643200 summary_utils.py:349] Steps/second: 0.175693, Examples/second: 24.779891\n",
      "I0710 07:53:20.483787 140295626643200 trainer.py:508] step:  4883, steps/sec: 0.18, examples/sec: 24.78 grad_norm/all/loss:45.782135 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2526369 log_pplx:4.6952486 loss:118.76045 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.25171\n",
      "I0710 07:53:23.449090 140295626643200 summary_utils.py:349] Steps/second: 0.175936, Examples/second: 24.872422\n",
      "I0710 07:53:23.449826 140295626643200 trainer.py:508] step:  4884, steps/sec: 0.18, examples/sec: 24.87 grad_norm/all/loss:34.227352 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2518889 log_pplx:4.5390596 loss:70.319969 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.25714\n",
      "I0710 07:53:29.386987 140295626643200 summary_utils.py:349] Steps/second: 0.175914, Examples/second: 24.838205\n",
      "I0710 07:53:29.387750 140295626643200 trainer.py:508] step:  4885, steps/sec: 0.18, examples/sec: 24.84 grad_norm/all/loss:122.1153 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2509019 log_pplx:4.502831 loss:181.52037 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.26233\n",
      "I0710 07:53:37.695803 140295626643200 summary_utils.py:349] Steps/second: 0.175681, Examples/second: 24.754453\n",
      "I0710 07:53:37.696532 140295626643200 trainer.py:508] step:  4886, steps/sec: 0.18, examples/sec: 24.75 grad_norm/all/loss:171.74266 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.249794 log_pplx:3.8751976 loss:263.41656 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.26733\n",
      "I0710 07:53:39.881207 140295626643200 summary_utils.py:349] Steps/second: 0.175991, Examples/second: 24.984701\n",
      "I0710 07:53:39.882369 140295626643200 trainer.py:508] step:  4887, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:13.537483 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2506374 log_pplx:4.3441043 loss:32.402603 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:517.27283\n",
      "I0710 07:53:46.178798 140295626643200 summary_utils.py:349] Steps/second: 0.175937, Examples/second: 24.945933\n",
      "I0710 07:53:46.179562 140295626643200 trainer.py:508] step:  4888, steps/sec: 0.18, examples/sec: 24.95 grad_norm/all/loss:172.18591 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2508048 log_pplx:4.5216537 loss:177.13579 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.27802\n",
      "I0710 07:53:49.947005 140295626643200 summary_utils.py:349] Steps/second: 0.176105, Examples/second: 24.978952\n",
      "I0710 07:53:49.947757 140295626643200 trainer.py:508] step:  4889, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:47.273762 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2524295 log_pplx:4.6974039 loss:117.64062 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.28253\n",
      "I0710 07:53:56.006273 140295626643200 summary_utils.py:349] Steps/second: 0.176072, Examples/second: 24.943361\n",
      "I0710 07:53:56.007016 140295626643200 trainer.py:508] step:  4890, steps/sec: 0.18, examples/sec: 24.94 grad_norm/all/loss:76.829536 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2512795 log_pplx:4.5394301 loss:186.45711 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.28723\n",
      "I0710 07:53:58.988262 140295626643200 summary_utils.py:349] Steps/second: 0.176309, Examples/second: 25.033816\n",
      "I0710 07:53:58.989017 140295626643200 trainer.py:508] step:  4891, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:56.266052 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2519637 log_pplx:4.5863314 loss:71.464355 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.29187\n",
      "I0710 07:54:05.257401 140295626643200 summary_utils.py:349] Steps/second: 0.176256, Examples/second: 24.995616\n",
      "I0710 07:54:05.258198 140295626643200 trainer.py:508] step:  4892, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:87.647163 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2509468 log_pplx:4.5327272 loss:182.8389 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.29669\n",
      "I0710 07:54:08.925224 140295626643200 summary_utils.py:349] Steps/second: 0.176431, Examples/second: 25.029475\n",
      "I0710 07:54:08.926016 140295626643200 trainer.py:508] step:  4893, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:43.513138 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2524734 log_pplx:4.7251115 loss:116.68073 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.3017\n",
      "I0710 07:54:15.345237 140295626643200 summary_utils.py:349] Steps/second: 0.176366, Examples/second: 24.989612\n",
      "I0710 07:54:15.346078 140295626643200 trainer.py:508] step:  4894, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:88.529427 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2510066 log_pplx:4.4373341 loss:178.99095 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.30658\n",
      "I0710 07:54:21.600760 140295626643200 summary_utils.py:349] Steps/second: 0.176315, Examples/second: 24.952023\n",
      "I0710 07:54:21.601547 140295626643200 trainer.py:508] step:  4895, steps/sec: 0.18, examples/sec: 24.95 grad_norm/all/loss:72.823517 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2510024 log_pplx:4.4839315 loss:185.52267 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.31152\n",
      "I0710 07:54:25.339907 140295626643200 summary_utils.py:349] Steps/second: 0.176483, Examples/second: 24.984813\n",
      "I0710 07:54:25.340892 140295626643200 trainer.py:508] step:  4896, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:106.35774 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2527001 log_pplx:4.7055321 loss:117.72652 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.31628\n",
      "I0710 07:54:28.321045 140295626643200 summary_utils.py:349] Steps/second: 0.176715, Examples/second: 25.073915\n",
      "I0710 07:54:28.321807 140295626643200 trainer.py:508] step:  4897, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:68.553329 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2522466 log_pplx:4.6337585 loss:71.624146 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.32031\n",
      "I0710 07:54:34.644691 140295626643200 summary_utils.py:349] Steps/second: 0.176658, Examples/second: 25.035472\n",
      "I0710 07:54:34.645516 140295626643200 trainer.py:508] step:  4898, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:73.806923 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2507905 log_pplx:4.4246349 loss:174.2753 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.32373\n",
      "I0710 07:54:43.360680 140295626643200 summary_utils.py:349] Steps/second: 0.176395, Examples/second: 24.948635\n",
      "I0710 07:54:43.361486 140295626643200 trainer.py:508] step:  4899, steps/sec: 0.18, examples/sec: 24.95 grad_norm/all/loss:258.96527 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2501061 log_pplx:3.9220238 loss:282.09155 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.32745\n",
      "I0710 07:54:49.713291 140295626643200 summary_utils.py:349] Steps/second: 0.176336, Examples/second: 24.910507\n",
      "I0710 07:54:49.714261 140295626643200 trainer.py:508] step:  4900, steps/sec: 0.18, examples/sec: 24.91 grad_norm/all/loss:112.09412 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2510362 log_pplx:4.4007797 loss:183.4025 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.3324\n",
      "I0710 07:54:53.529124 140295626643200 summary_utils.py:349] Steps/second: 0.176495, Examples/second: 24.941997\n",
      "I0710 07:54:53.529916 140295626643200 trainer.py:508] step:  4901, steps/sec: 0.18, examples/sec: 24.94 grad_norm/all/loss:36.089176 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2525703 log_pplx:4.6421905 loss:115.88069 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.33746\n",
      "I0710 07:54:59.780084 140295626643200 summary_utils.py:349] Steps/second: 0.176445, Examples/second: 24.905301\n",
      "I0710 07:54:59.780984 140295626643200 trainer.py:508] step:  4902, steps/sec: 0.18, examples/sec: 24.91 grad_norm/all/loss:85.058174 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2511888 log_pplx:4.4329576 loss:175.98843 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.34247\n",
      "I0710 07:55:02.772036 140295626643200 summary_utils.py:349] Steps/second: 0.176673, Examples/second: 24.992906\n",
      "I0710 07:55:02.772833 140295626643200 trainer.py:508] step:  4903, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:24.645708 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2522452 log_pplx:4.4986033 loss:69.658066 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.34747\n",
      "I0710 07:55:09.057997 140295626643200 summary_utils.py:349] Steps/second: 0.176619, Examples/second: 24.955799\n",
      "I0710 07:55:09.059066 140295626643200 trainer.py:508] step:  4904, steps/sec: 0.18, examples/sec: 24.96 grad_norm/all/loss:86.665039 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.251143 log_pplx:4.434207 loss:181.2482 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.3526\n",
      "I0710 07:55:12.789556 140295626643200 summary_utils.py:349] Steps/second: 0.176783, Examples/second: 24.987926\n",
      "I0710 07:55:12.790330 140295626643200 trainer.py:508] step:  4905, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:67.866394 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2526703 log_pplx:4.5436149 loss:114.35709 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.35724\n",
      "I0710 07:55:19.120387 140295626643200 summary_utils.py:349] Steps/second: 0.176726, Examples/second: 24.950476\n",
      "I0710 07:55:19.121182 140295626643200 trainer.py:508] step:  4906, steps/sec: 0.18, examples/sec: 24.95 grad_norm/all/loss:86.916618 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2514899 log_pplx:4.4413347 loss:184.9816 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.36218\n",
      "I0710 07:55:28.018041 140295626643200 summary_utils.py:349] Steps/second: 0.176453, Examples/second: 24.863680\n",
      "I0710 07:55:28.018794 140295626643200 trainer.py:508] step:  4907, steps/sec: 0.18, examples/sec: 24.86 grad_norm/all/loss:269.7757 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2504045 log_pplx:3.9113932 loss:280.64243 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.36755\n",
      "I0710 07:55:34.218503 140295626643200 summary_utils.py:349] Steps/second: 0.176408, Examples/second: 24.828414\n",
      "I0710 07:55:34.219531 140295626643200 trainer.py:508] step:  4908, steps/sec: 0.18, examples/sec: 24.83 grad_norm/all/loss:73.494095 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2515948 log_pplx:4.4384365 loss:180.86629 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.37231\n",
      "I0710 07:55:37.374777 140295626643200 summary_utils.py:349] Steps/second: 0.176619, Examples/second: 24.912752\n",
      "I0710 07:55:37.375885 140295626643200 trainer.py:508] step:  4909, steps/sec: 0.18, examples/sec: 24.91 grad_norm/all/loss:22.113712 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2525656 log_pplx:4.5017443 loss:70.673859 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.37756\n",
      "I0710 07:55:44.025435 140295626643200 summary_utils.py:349] Steps/second: 0.176536, Examples/second: 24.872197\n",
      "I0710 07:55:44.026303 140295626643200 trainer.py:508] step:  4910, steps/sec: 0.18, examples/sec: 24.87 grad_norm/all/loss:117.94302 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2515761 log_pplx:4.4535336 loss:183.31857 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.38269\n",
      "I0710 07:55:46.174789 140295626643200 summary_utils.py:349] Steps/second: 0.176830, Examples/second: 25.088998\n",
      "I0710 07:55:46.175654 140295626643200 trainer.py:508] step:  4911, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:10.979442 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2509813 log_pplx:4.2965922 loss:31.561489 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:517.38824\n",
      "I0710 07:55:50.004492 140295626643200 summary_utils.py:349] Steps/second: 0.176982, Examples/second: 25.119164\n",
      "I0710 07:55:50.005471 140295626643200 trainer.py:508] step:  4912, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:69.086311 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.252728 log_pplx:4.6311631 loss:115.14231 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.39355\n",
      "I0710 07:55:56.357232 140295626643200 summary_utils.py:349] Steps/second: 0.176923, Examples/second: 25.081720\n",
      "I0710 07:55:56.358003 140295626643200 trainer.py:508] step:  4913, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:96.783974 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2514552 log_pplx:4.5439963 loss:181.36226 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.39838\n",
      "I0710 07:56:02.605376 140295626643200 summary_utils.py:349] Steps/second: 0.176874, Examples/second: 25.045729\n",
      "I0710 07:56:02.606478 140295626643200 trainer.py:508] step:  4914, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:81.470947 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2512828 log_pplx:4.452692 loss:178.55295 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.40344\n",
      "I0710 07:56:06.340173 140295626643200 summary_utils.py:349] Steps/second: 0.177033, Examples/second: 25.076855\n",
      "I0710 07:56:06.340939 140295626643200 trainer.py:508] step:  4915, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:44.140579 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2534517 log_pplx:4.7174692 loss:119.08661 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.40894\n",
      "I0710 07:56:09.370766 140295626643200 summary_utils.py:349] Steps/second: 0.177250, Examples/second: 25.161038\n",
      "I0710 07:56:09.371625 140295626643200 trainer.py:508] step:  4916, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:21.767134 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.252669 log_pplx:4.5649786 loss:70.311371 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.41406\n",
      "I0710 07:56:15.750321 140295626643200 summary_utils.py:349] Steps/second: 0.177189, Examples/second: 25.123494\n",
      "I0710 07:56:15.751108 140295626643200 trainer.py:508] step:  4917, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:102.41216 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2513915 log_pplx:4.5194511 loss:180.89104 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.41876\n",
      "I0710 07:56:22.212175 140295626643200 summary_utils.py:349] Steps/second: 0.177121, Examples/second: 25.085214\n",
      "I0710 07:56:22.212938 140295626643200 trainer.py:508] step:  4918, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:86.691147 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2517945 log_pplx:4.5380864 loss:187.13934 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.42371\n",
      "I0710 07:56:25.983545 140295626643200 summary_utils.py:349] Steps/second: 0.177275, Examples/second: 25.115561\n",
      "I0710 07:56:25.984279 140295626643200 trainer.py:508] step:  4919, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:42.56234 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2530367 log_pplx:4.6226373 loss:115.6815 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.42908\n",
      "I0710 07:56:34.891832 140295626643200 summary_utils.py:349] Steps/second: 0.177008, Examples/second: 25.030645\n",
      "I0710 07:56:34.892585 140295626643200 trainer.py:508] step:  4920, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:250.45834 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2505655 log_pplx:3.9270713 loss:273.12778 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.43427\n",
      "I0710 07:56:40.973683 140295626643200 summary_utils.py:349] Steps/second: 0.176972, Examples/second: 24.997356\n",
      "I0710 07:56:40.974619 140295626643200 trainer.py:508] step:  4921, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:284.58609 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2514262 log_pplx:4.5494819 loss:178.79463 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.44086\n",
      "I0710 07:56:43.978232 140295626643200 summary_utils.py:349] Steps/second: 0.177188, Examples/second: 25.080608\n",
      "I0710 07:56:43.978996 140295626643200 trainer.py:508] step:  4922, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:22.678534 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2529546 log_pplx:4.5382104 loss:71.459091 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.44672\n",
      "I0710 07:56:50.407867 140295626643200 summary_utils.py:349] Steps/second: 0.177124, Examples/second: 25.043321\n",
      "I0710 07:56:50.408627 140295626643200 trainer.py:508] step:  4923, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:86.553612 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2514758 log_pplx:4.4430399 loss:174.27826 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.45221\n",
      "I0710 07:56:54.287586 140295626643200 summary_utils.py:349] Steps/second: 0.177267, Examples/second: 25.072105\n",
      "I0710 07:56:54.288345 140295626643200 trainer.py:508] step:  4924, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:71.148048 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2532341 log_pplx:4.725533 loss:119.20158 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.45758\n",
      "I0710 07:57:00.712064 140295626643200 summary_utils.py:349] Steps/second: 0.177204, Examples/second: 25.035077\n",
      "I0710 07:57:00.712852 140295626643200 trainer.py:508] step:  4925, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:110.94824 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2523607 log_pplx:4.4655881 loss:194.9229 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.46307\n",
      "I0710 07:57:07.002347 140295626643200 summary_utils.py:349] Steps/second: 0.177152, Examples/second: 24.999793\n",
      "I0710 07:57:07.003113 140295626643200 trainer.py:508] step:  4926, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:92.489235 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.251435 log_pplx:4.4522214 loss:173.97055 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.46869\n",
      "I0710 07:57:10.918459 140295626643200 summary_utils.py:349] Steps/second: 0.177291, Examples/second: 25.028022\n",
      "I0710 07:57:10.919266 140295626643200 trainer.py:508] step:  4927, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:48.90295 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2535824 log_pplx:4.6887383 loss:117.59941 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.4743\n",
      "I0710 07:57:13.975754 140295626643200 summary_utils.py:349] Steps/second: 0.177499, Examples/second: 25.109500\n",
      "I0710 07:57:13.976495 140295626643200 trainer.py:508] step:  4928, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:31.612799 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2527804 log_pplx:4.4939084 loss:69.56781 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.47931\n",
      "I0710 07:57:23.582293 140295626643200 summary_utils.py:349] Steps/second: 0.177180, Examples/second: 25.018553\n",
      "I0710 07:57:23.583110 140295626643200 trainer.py:508] step:  4929, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:209.0481 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2511488 log_pplx:3.8547466 loss:288.62418 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.48383\n",
      "I0710 07:57:29.878703 140295626643200 summary_utils.py:349] Steps/second: 0.177128, Examples/second: 24.983611\n",
      "I0710 07:57:29.879463 140295626643200 trainer.py:508] step:  4930, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:68.919701 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2516444 log_pplx:4.4053564 loss:174.56226 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.48938\n",
      "I0710 07:57:35.992457 140295626643200 summary_utils.py:349] Steps/second: 0.177091, Examples/second: 24.950915\n",
      "I0710 07:57:35.993276 140295626643200 trainer.py:508] step:  4931, steps/sec: 0.18, examples/sec: 24.95 grad_norm/all/loss:79.505417 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2517935 log_pplx:4.4385071 loss:177.65125 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.49487\n",
      "I0710 07:57:39.908013 140295626643200 summary_utils.py:349] Steps/second: 0.177228, Examples/second: 24.978869\n",
      "I0710 07:57:39.908868 140295626643200 trainer.py:508] step:  4932, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:64.12458 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2533556 log_pplx:4.6086426 loss:114.72638 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.50031\n",
      "I0710 07:57:42.086018 140295626643200 summary_utils.py:349] Steps/second: 0.177504, Examples/second: 25.183982\n",
      "I0710 07:57:42.086808 140295626643200 trainer.py:508] step:  4933, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:11.270469 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2516524 log_pplx:4.3160272 loss:31.552521 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:517.50525\n",
      "I0710 07:57:48.327208 140295626643200 summary_utils.py:349] Steps/second: 0.177455, Examples/second: 25.149484\n",
      "I0710 07:57:48.328072 140295626643200 trainer.py:508] step:  4934, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:151.74294 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2520744 log_pplx:4.4401302 loss:180.60229 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.51001\n",
      "I0710 07:57:51.413294 140295626643200 summary_utils.py:349] Steps/second: 0.177657, Examples/second: 25.229112\n",
      "I0710 07:57:51.414077 140295626643200 trainer.py:508] step:  4935, steps/sec: 0.18, examples/sec: 25.23 grad_norm/all/loss:23.21999 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2533768 log_pplx:4.5187063 loss:71.204926 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.51501\n",
      "I0710 07:57:57.677396 140295626643200 summary_utils.py:349] Steps/second: 0.177607, Examples/second: 25.194375\n",
      "I0710 07:57:57.678404 140295626643200 trainer.py:508] step:  4936, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:73.792847 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2520331 log_pplx:4.466033 loss:181.09763 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.51996\n",
      "I0710 07:58:01.557004 140295626643200 summary_utils.py:349] Steps/second: 0.177745, Examples/second: 25.222039\n",
      "I0710 07:58:01.557782 140295626643200 base_runner.py:111] step:  4937, steps/sec: 0.18, examples/sec: 25.22 grad_norm/all/loss:52.627571 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2538061 log_pplx:4.6768265 loss:117.50526 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.5249\n",
      "I0710 07:58:07.961662 140295626643200 summary_utils.py:349] Steps/second: 0.177684, Examples/second: 25.185909\n",
      "I0710 07:58:07.962493 140295626643200 trainer.py:508] step:  4938, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:108.79997 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2522013 log_pplx:4.5575438 loss:185.71992 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.52954\n",
      "I0710 07:58:11.044751 140295626643200 summary_utils.py:349] Steps/second: 0.177884, Examples/second: 25.264828\n",
      "I0710 07:58:11.045506 140295626643200 trainer.py:508] step:  4939, steps/sec: 0.18, examples/sec: 25.26 grad_norm/all/loss:23.182232 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2530529 log_pplx:4.434835 loss:69.20768 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.53394\n",
      "I0710 07:58:14.545201 140295635035904 trainer.py:345] Write summary @4939\n",
      "2020-07-10 07:58:16.040359: I lingvo/core/ops/record_batcher.cc:394] 2240 total seconds passed. Total records yielded: 1426. Total records skipped: 0\n",
      "I0710 07:58:21.648161 140295626643200 summary_utils.py:349] Steps/second: 0.177494, Examples/second: 25.182075\n",
      "I0710 07:58:21.649577 140295626643200 trainer.py:508] step:  4940, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:74.316841 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2521179 log_pplx:4.4482079 loss:177.92831 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.53821\n",
      "I0710 07:58:38.497063 140295626643200 summary_utils.py:349] Steps/second: 0.176623, Examples/second: 25.014068\n",
      "I0710 07:58:38.497999 140295626643200 trainer.py:508] step:  4941, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:376.19186 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.251671 log_pplx:4.0223842 loss:303.69 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.54279\n",
      "I0710 07:58:44.053176 140295626643200 summary_utils.py:349] Steps/second: 0.176632, Examples/second: 25.023235\n",
      "I0710 07:58:44.055232 140295626643200 trainer.py:508] step:  4942, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:51.773163 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2537943 log_pplx:4.6934457 loss:119.09618 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.5481\n",
      "I0710 07:58:54.190916 140295626643200 summary_utils.py:349] Steps/second: 0.176288, Examples/second: 24.947823\n",
      "I0710 07:58:54.192167 140295626643200 trainer.py:508] step:  4943, steps/sec: 0.18, examples/sec: 24.95 grad_norm/all/loss:224.37578 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.252254 log_pplx:4.569129 loss:185.44954 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.55377\n",
      "I0710 07:58:59.606609 140295635035904 trainer.py:354] Write summary done: step 4939\n",
      "I0710 07:59:03.302263 140295626643200 summary_utils.py:349] Steps/second: 0.176026, Examples/second: 24.884112\n",
      "I0710 07:59:03.303174 140295626643200 trainer.py:508] step:  4944, steps/sec: 0.18, examples/sec: 24.88 grad_norm/all/loss:188.69704 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2524331 log_pplx:4.5075521 loss:184.80965 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.55939\n",
      "I0710 07:59:06.399773 140295626643200 summary_utils.py:349] Steps/second: 0.176222, Examples/second: 24.961391\n",
      "I0710 07:59:06.400620 140295626643200 trainer.py:508] step:  4945, steps/sec: 0.18, examples/sec: 24.96 grad_norm/all/loss:24.723982 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2537569 log_pplx:4.5724759 loss:72.177238 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.56525\n",
      "I0710 07:59:10.209779 140295626643200 summary_utils.py:349] Steps/second: 0.176364, Examples/second: 24.989375\n",
      "I0710 07:59:10.210592 140295626643200 trainer.py:508] step:  4946, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:87.757607 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2539756 log_pplx:4.6883936 loss:117.53218 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.57092\n",
      "I0710 07:59:16.575497 140295626643200 summary_utils.py:349] Steps/second: 0.176311, Examples/second: 24.955370\n",
      "I0710 07:59:16.576351 140295626643200 trainer.py:508] step:  4947, steps/sec: 0.18, examples/sec: 24.96 grad_norm/all/loss:229.99118 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2518096 log_pplx:4.5190935 loss:175.56677 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.57697\n",
      "I0710 07:59:22.977742 140295626643200 summary_utils.py:349] Steps/second: 0.176256, Examples/second: 24.921161\n",
      "I0710 07:59:22.978619 140295626643200 trainer.py:508] step:  4948, steps/sec: 0.18, examples/sec: 24.92 grad_norm/all/loss:106.38853 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2523252 log_pplx:4.4751778 loss:181.18875 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.58429\n",
      "I0710 07:59:26.836375 140295626643200 summary_utils.py:349] Steps/second: 0.176393, Examples/second: 24.948493\n",
      "I0710 07:59:26.837187 140295626643200 trainer.py:508] step:  4949, steps/sec: 0.18, examples/sec: 24.95 grad_norm/all/loss:115.19994 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2542449 log_pplx:4.6594739 loss:118.75834 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.59155\n",
      "I0710 07:59:37.895640 140295626643200 summary_utils.py:349] Steps/second: 0.175988, Examples/second: 24.847966\n",
      "I0710 07:59:37.896398 140295626643200 trainer.py:508] step:  4950, steps/sec: 0.18, examples/sec: 24.85 grad_norm/all/loss:442.57294 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.251547 log_pplx:4.0744519 loss:299.77783 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.5976\n",
      "I0710 07:59:40.951039 140295626643200 summary_utils.py:349] Steps/second: 0.176185, Examples/second: 24.924602\n",
      "I0710 07:59:40.951800 140295626643200 trainer.py:508] step:  4951, steps/sec: 0.18, examples/sec: 24.92 grad_norm/all/loss:83.734367 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2532926 log_pplx:4.6352491 loss:72.588722 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.60352\n",
      "I0710 07:59:47.229252 140295626643200 summary_utils.py:349] Steps/second: 0.176140, Examples/second: 24.892140\n",
      "I0710 07:59:47.230077 140295626643200 trainer.py:508] step:  4952, steps/sec: 0.18, examples/sec: 24.89 grad_norm/all/loss:83.821999 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2522212 log_pplx:4.4837136 loss:182.26297 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.60864\n",
      "I0710 07:59:53.548070 140295626643200 summary_utils.py:349] Steps/second: 0.176092, Examples/second: 24.859424\n",
      "I0710 07:59:53.548914 140295626643200 trainer.py:508] step:  4953, steps/sec: 0.18, examples/sec: 24.86 grad_norm/all/loss:121.09002 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.25252 log_pplx:4.5670128 loss:186.10576 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.61322\n",
      "I0710 07:59:57.349572 140295626643200 summary_utils.py:349] Steps/second: 0.176232, Examples/second: 24.887104\n",
      "I0710 07:59:57.350398 140295626643200 trainer.py:508] step:  4954, steps/sec: 0.18, examples/sec: 24.89 grad_norm/all/loss:44.330273 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2542132 log_pplx:4.7342882 loss:120.2805 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.61835\n",
      "I0710 08:00:03.644539 140295626643200 summary_utils.py:349] Steps/second: 0.176185, Examples/second: 24.854791\n",
      "I0710 08:00:03.645345 140295626643200 trainer.py:508] step:  4955, steps/sec: 0.18, examples/sec: 24.85 grad_norm/all/loss:169.75 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2527211 log_pplx:4.6587834 loss:192.75717 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.62305\n",
      "I0710 08:00:05.812431 140295626643200 summary_utils.py:349] Steps/second: 0.176446, Examples/second: 25.047709\n",
      "I0710 08:00:05.813320 140295626643200 trainer.py:508] step:  4956, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:9.997571 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2524954 log_pplx:4.3232093 loss:31.925888 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:517.62854\n",
      "I0710 08:00:08.793033 140295626643200 summary_utils.py:349] Steps/second: 0.176645, Examples/second: 25.123979\n",
      "I0710 08:00:08.793768 140295626643200 trainer.py:508] step:  4957, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:42.295696 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2532673 log_pplx:4.5778375 loss:70.080254 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.63367\n",
      "I0710 08:00:15.278038 140295626643200 summary_utils.py:349] Steps/second: 0.176584, Examples/second: 25.089195\n",
      "I0710 08:00:15.278913 140295626643200 trainer.py:508] step:  4958, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:86.21785 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2528744 log_pplx:4.4505754 loss:183.58623 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.63879\n",
      "I0710 08:00:19.145422 140295626643200 summary_utils.py:349] Steps/second: 0.176717, Examples/second: 25.115565\n",
      "I0710 08:00:19.146368 140295626643200 trainer.py:508] step:  4959, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:69.228577 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2540172 log_pplx:4.6423254 loss:116.37727 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.64423\n",
      "I0710 08:00:25.411155 140295626643200 summary_utils.py:349] Steps/second: 0.176672, Examples/second: 25.083251\n",
      "I0710 08:00:25.411949 140295626643200 trainer.py:508] step:  4960, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:120.47397 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2524399 log_pplx:4.5663962 loss:181.22885 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.64923\n",
      "I0710 08:00:31.398046 140295626643200 summary_utils.py:349] Steps/second: 0.176648, Examples/second: 25.054016\n",
      "I0710 08:00:31.398860 140295626643200 trainer.py:508] step:  4961, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:77.666473 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.252636 log_pplx:4.4612122 loss:178.89462 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.65381\n",
      "I0710 08:00:34.475686 140295626643200 summary_utils.py:349] Steps/second: 0.176838, Examples/second: 25.128452\n",
      "I0710 08:00:34.476550 140295626643200 trainer.py:508] step:  4962, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:37.507774 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2539313 log_pplx:4.5432105 loss:71.64431 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.65808\n",
      "I0710 08:00:38.389881 140295626643200 summary_utils.py:349] Steps/second: 0.176966, Examples/second: 25.154059\n",
      "I0710 08:00:38.390760 140295626643200 trainer.py:508] step:  4963, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:68.650337 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2542137 log_pplx:4.683217 loss:115.85109 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.66241\n",
      "I0710 08:00:39.693278 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 08:00:44.779258 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 08:00:45.078501 140295626643200 summary_utils.py:349] Steps/second: 0.176890, Examples/second: 25.117502\n",
      "I0710 08:00:45.265094 140295626643200 trainer.py:508] step:  4964, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:113.84194 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2526191 log_pplx:4.4402075 loss:176.94226 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.66693\n",
      "I0710 08:00:45.266148 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00004963\n",
      "I0710 08:00:55.689866 140295626643200 summary_utils.py:349] Steps/second: 0.176528, Examples/second: 25.024070\n",
      "I0710 08:00:55.690774 140295626643200 trainer.py:508] step:  4965, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:428.05557 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2519994 log_pplx:4.2142344 loss:321.44073 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.67194\n",
      "I0710 08:01:01.764499 140295626643200 summary_utils.py:349] Steps/second: 0.176498, Examples/second: 24.994443\n",
      "I0710 08:01:01.765345 140295626643200 trainer.py:508] step:  4966, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:147.64864 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2527211 log_pplx:4.5518446 loss:177.57884 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.67877\n",
      "I0710 08:01:08.258454 140295626643200 summary_utils.py:349] Steps/second: 0.176438, Examples/second: 24.960668\n",
      "I0710 08:01:08.259270 140295626643200 trainer.py:508] step:  4967, steps/sec: 0.18, examples/sec: 24.96 grad_norm/all/loss:229.71387 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2530107 log_pplx:4.6149149 loss:190.36523 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.68506\n",
      "I0710 08:01:12.116490 140295626643200 summary_utils.py:349] Steps/second: 0.176569, Examples/second: 24.986764\n",
      "I0710 08:01:12.117277 140295626643200 trainer.py:508] step:  4968, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:64.528717 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2545869 log_pplx:4.73071 loss:118.47472 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.69067\n",
      "I0710 08:01:15.226556 140295626643200 summary_utils.py:349] Steps/second: 0.176753, Examples/second: 25.059712\n",
      "I0710 08:01:15.227334 140295626643200 trainer.py:508] step:  4969, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:53.840965 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2537732 log_pplx:4.4930577 loss:69.361588 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.69604\n",
      "I0710 08:01:21.774466 140295626643200 summary_utils.py:349] Steps/second: 0.176689, Examples/second: 25.025399\n",
      "I0710 08:01:21.775214 140295626643200 trainer.py:508] step:  4970, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:103.28423 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.252923 log_pplx:4.3951492 loss:175.5862 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.70093\n",
      "I0710 08:01:28.244008 140295626643200 summary_utils.py:349] Steps/second: 0.176631, Examples/second: 24.992066\n",
      "I0710 08:01:28.244811 140295626643200 trainer.py:508] step:  4971, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:133.76453 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2529538 log_pplx:4.5219278 loss:180.25534 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.70605\n",
      "I0710 08:01:32.147756 140295626643200 summary_utils.py:349] Steps/second: 0.176757, Examples/second: 25.017436\n",
      "I0710 08:01:32.148663 140295626643200 trainer.py:508] step:  4972, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:54.747318 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2547566 log_pplx:4.657866 loss:116.94155 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.71167\n",
      "I0710 08:01:42.573775 140295626643200 summary_utils.py:349] Steps/second: 0.176416, Examples/second: 24.928082\n",
      "I0710 08:01:42.574604 140295626643200 trainer.py:508] step:  4973, steps/sec: 0.18, examples/sec: 24.93 grad_norm/all/loss:254.66733 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2522782 log_pplx:4.0248599 loss:289.6893 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.71729\n",
      "I0710 08:01:45.680789 140295626643200 summary_utils.py:349] Steps/second: 0.176599, Examples/second: 25.000236\n",
      "I0710 08:01:45.681573 140295626643200 trainer.py:508] step:  4974, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:27.585047 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2544341 log_pplx:4.6254959 loss:72.454056 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.7226\n",
      "I0710 08:01:51.968269 140295626643200 summary_utils.py:349] Steps/second: 0.176554, Examples/second: 24.969122\n",
      "I0710 08:01:51.969226 140295626643200 trainer.py:508] step:  4975, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:90.240959 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.252674 log_pplx:4.5270891 loss:178.70683 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.7276\n",
      "I0710 08:01:58.166830 140295626643200 summary_utils.py:349] Steps/second: 0.176516, Examples/second: 24.939057\n",
      "I0710 08:01:58.167665 140295626643200 trainer.py:508] step:  4976, steps/sec: 0.18, examples/sec: 24.94 grad_norm/all/loss:96.415993 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2531055 log_pplx:4.4688973 loss:181.93999 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.73242\n",
      "I0710 08:02:01.998907 140295626643200 summary_utils.py:349] Steps/second: 0.176646, Examples/second: 24.964924\n",
      "I0710 08:02:01.999704 140295626643200 trainer.py:508] step:  4977, steps/sec: 0.18, examples/sec: 24.96 grad_norm/all/loss:50.017136 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.254698 log_pplx:4.6642742 loss:116.63601 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.73688\n",
      "I0710 08:02:08.335209 140295626643200 summary_utils.py:349] Steps/second: 0.176598, Examples/second: 24.933615\n",
      "I0710 08:02:08.336018 140295626643200 trainer.py:508] step:  4978, steps/sec: 0.18, examples/sec: 24.93 grad_norm/all/loss:102.41067 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.252856 log_pplx:4.461216 loss:177.27757 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.74103\n",
      "I0710 08:02:10.531379 140295626643200 summary_utils.py:349] Steps/second: 0.176843, Examples/second: 25.116556\n",
      "I0710 08:02:10.532164 140295626643200 trainer.py:508] step:  4979, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:10.845588 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2529423 log_pplx:4.3310642 loss:31.882385 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:517.74554\n",
      "I0710 08:02:16.996429 140295626643200 summary_utils.py:349] Steps/second: 0.176786, Examples/second: 25.083681\n",
      "I0710 08:02:16.997239 140295626643200 trainer.py:508] step:  4980, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:114.18972 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2531655 log_pplx:4.481822 loss:188.96481 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.74988\n",
      "I0710 08:02:20.117232 140295626643200 summary_utils.py:349] Steps/second: 0.176965, Examples/second: 25.154515\n",
      "I0710 08:02:20.118136 140295626643200 trainer.py:508] step:  4981, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:27.926476 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2540586 log_pplx:4.5147986 loss:70.42028 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.75476\n",
      "I0710 08:02:24.029050 140295626643200 summary_utils.py:349] Steps/second: 0.177087, Examples/second: 25.179028\n",
      "I0710 08:02:24.029970 140295626643200 trainer.py:508] step:  4982, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:62.690453 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2546687 log_pplx:4.6309595 loss:115.80292 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.75958\n",
      "I0710 08:02:30.442337 140295626643200 summary_utils.py:349] Steps/second: 0.177033, Examples/second: 25.146686\n",
      "I0710 08:02:30.443145 140295626643200 trainer.py:508] step:  4983, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:78.082977 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2534817 log_pplx:4.5519171 loss:185.83203 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.76404\n",
      "I0710 08:02:36.803789 140295626643200 summary_utils.py:349] Steps/second: 0.176983, Examples/second: 25.115023\n",
      "I0710 08:02:36.804575 140295626643200 trainer.py:508] step:  4984, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:74.539703 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2533704 log_pplx:4.4369402 loss:181.19353 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.76849\n",
      "I0710 08:02:40.642276 140295626643200 summary_utils.py:349] Steps/second: 0.177110, Examples/second: 25.140165\n",
      "I0710 08:02:40.643078 140295626643200 trainer.py:508] step:  4985, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:36.942337 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2550309 log_pplx:4.6256747 loss:118.15708 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.77325\n",
      "I0710 08:02:47.180099 140295626643200 summary_utils.py:349] Steps/second: 0.177048, Examples/second: 25.106900\n",
      "I0710 08:02:47.180859 140295626643200 trainer.py:508] step:  4986, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:74.391472 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2535007 log_pplx:4.4687734 loss:182.2701 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.77783\n",
      "I0710 08:02:50.237448 140295626643200 summary_utils.py:349] Steps/second: 0.177229, Examples/second: 25.177492\n",
      "I0710 08:02:50.238276 140295626643200 trainer.py:508] step:  4987, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:29.250242 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.254382 log_pplx:4.5429411 loss:70.273605 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.78265\n",
      "I0710 08:02:59.605642 140295626643200 summary_utils.py:349] Steps/second: 0.176970, Examples/second: 25.100635\n",
      "I0710 08:02:59.606446 140295626643200 trainer.py:508] step:  4988, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:281.33096 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2524302 log_pplx:4.0177975 loss:286.36853 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.78729\n",
      "I0710 08:03:05.985871 140295626643200 summary_utils.py:349] Steps/second: 0.176919, Examples/second: 25.069264\n",
      "I0710 08:03:05.986656 140295626643200 trainer.py:508] step:  4989, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:69.39669 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2530426 log_pplx:4.3457084 loss:175.72958 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.79309\n",
      "I0710 08:03:09.855331 140295626643200 summary_utils.py:349] Steps/second: 0.177042, Examples/second: 25.093884\n",
      "I0710 08:03:09.856060 140295626643200 trainer.py:508] step:  4990, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:42.537888 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2550334 log_pplx:4.6793041 loss:116.54392 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.79889\n",
      "I0710 08:03:16.181266 140295626643200 summary_utils.py:349] Steps/second: 0.176995, Examples/second: 25.063185\n",
      "I0710 08:03:16.182221 140295626643200 trainer.py:508] step:  4991, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:111.06365 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2530209 log_pplx:4.3688507 loss:167.21776 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.80469\n",
      "I0710 08:03:22.589922 140295626643200 summary_utils.py:349] Steps/second: 0.176943, Examples/second: 25.031833\n",
      "I0710 08:03:22.590680 140295626643200 trainer.py:508] step:  4992, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:180.57161 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2533181 log_pplx:4.5045285 loss:185.86809 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.81\n",
      "I0710 08:03:25.669808 140295626643200 summary_utils.py:349] Steps/second: 0.177120, Examples/second: 25.101323\n",
      "I0710 08:03:25.670636 140295626643200 trainer.py:508] step:  4993, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:23.291603 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2543271 log_pplx:4.4514532 loss:70.266884 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.81482\n",
      "I0710 08:03:29.603136 140295626643200 summary_utils.py:349] Steps/second: 0.177238, Examples/second: 25.125085\n",
      "I0710 08:03:29.603943 140295626643200 trainer.py:508] step:  4994, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:77.061264 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2553431 log_pplx:4.6405482 loss:117.81191 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.81921\n",
      "I0710 08:03:36.069787 140295626643200 summary_utils.py:349] Steps/second: 0.177181, Examples/second: 25.093179\n",
      "I0710 08:03:36.070657 140295626643200 trainer.py:508] step:  4995, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:176.27359 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2535352 log_pplx:4.473999 loss:184.55244 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.8241\n",
      "I0710 08:03:42.156542 140295626643200 summary_utils.py:349] Steps/second: 0.177151, Examples/second: 25.065106\n",
      "I0710 08:03:42.157323 140295626643200 trainer.py:508] step:  4996, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:97.031181 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2534826 log_pplx:4.5018177 loss:181.98599 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.82953\n",
      "I0710 08:03:46.047693 140295626643200 summary_utils.py:349] Steps/second: 0.177271, Examples/second: 25.089179\n",
      "I0710 08:03:46.048443 140295626643200 trainer.py:508] step:  4997, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:86.528885 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2551061 log_pplx:4.6631079 loss:116.75256 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.83514\n",
      "I0710 08:03:52.618550 140295626643200 summary_utils.py:349] Steps/second: 0.177207, Examples/second: 25.056560\n",
      "I0710 08:03:52.619338 140295626643200 trainer.py:508] step:  4998, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:122.22564 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.253503 log_pplx:4.4672422 loss:179.80649 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.84015\n",
      "I0710 08:03:55.705785 140295626643200 summary_utils.py:349] Steps/second: 0.177381, Examples/second: 25.125150\n",
      "I0710 08:03:55.706625 140295626643200 trainer.py:508] step:  4999, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:67.475517 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2546172 log_pplx:4.6009359 loss:70.631546 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.84485\n",
      "I0710 08:04:05.352195 140295626643200 summary_utils.py:349] Steps/second: 0.177109, Examples/second: 25.047739\n",
      "I0710 08:04:05.353108 140295626643200 trainer.py:508] step:  5000, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:375.57181 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2528197 log_pplx:4.0961962 loss:295.33575 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.84906\n",
      "I0710 08:04:11.655403 140295626643200 summary_utils.py:349] Steps/second: 0.177065, Examples/second: 25.018019\n",
      "I0710 08:04:11.656282 140295626643200 trainer.py:508] step:  5001, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:146.798 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2535844 log_pplx:4.4610395 loss:179.1665 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.85345\n",
      "I0710 08:04:18.138728 140295626643200 summary_utils.py:349] Steps/second: 0.177008, Examples/second: 24.986729\n",
      "I0710 08:04:18.139518 140295626643200 trainer.py:508] step:  5002, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:178.08627 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2538158 log_pplx:4.5693111 loss:187.28461 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.85846\n",
      "I0710 08:04:20.360620 140295626643200 summary_utils.py:349] Steps/second: 0.177239, Examples/second: 25.160348\n",
      "I0710 08:04:20.361548 140295626643200 trainer.py:508] step:  5003, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:12.367503 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2534704 log_pplx:4.3166218 loss:31.379818 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:517.86401\n",
      "I0710 08:04:24.210672 140295626643200 summary_utils.py:349] Steps/second: 0.177360, Examples/second: 25.184324\n",
      "I0710 08:04:24.211497 140295626643200 trainer.py:508] step:  5004, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:61.300163 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2555553 log_pplx:4.6673565 loss:120.94287 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.86938\n",
      "I0710 08:04:27.245484 140295626643200 summary_utils.py:349] Steps/second: 0.177535, Examples/second: 25.252444\n",
      "I0710 08:04:27.246381 140295626643200 trainer.py:508] step:  5005, steps/sec: 0.18, examples/sec: 25.25 grad_norm/all/loss:44.693985 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2550309 log_pplx:4.6133876 loss:72.138237 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.87427\n",
      "I0710 08:04:33.365844 140295626643200 summary_utils.py:349] Steps/second: 0.177502, Examples/second: 25.224227\n",
      "I0710 08:04:33.366700 140295626643200 trainer.py:508] step:  5006, steps/sec: 0.18, examples/sec: 25.22 grad_norm/all/loss:131.99832 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2537922 log_pplx:4.5460978 loss:183.43506 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.87927\n",
      "I0710 08:04:39.791603 140295626643200 summary_utils.py:349] Steps/second: 0.177449, Examples/second: 25.193236\n",
      "I0710 08:04:39.792444 140295626643200 trainer.py:508] step:  5007, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:107.01894 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2538558 log_pplx:4.4814429 loss:177.52116 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.88403\n",
      "I0710 08:04:43.706148 140295626643200 summary_utils.py:349] Steps/second: 0.177564, Examples/second: 25.216376\n",
      "I0710 08:04:43.706974 140295626643200 trainer.py:508] step:  5008, steps/sec: 0.18, examples/sec: 25.22 grad_norm/all/loss:40.96085 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2557207 log_pplx:4.741015 loss:120.98479 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.88843\n",
      "I0710 08:04:54.280354 140295626643200 summary_utils.py:349] Steps/second: 0.177235, Examples/second: 25.131272\n",
      "I0710 08:04:54.281131 140295626643200 trainer.py:508] step:  5009, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:453.70993 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2526166 log_pplx:4.1374292 loss:296.23993 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.89276\n",
      "I0710 08:05:00.506765 140295626643200 summary_utils.py:349] Steps/second: 0.177196, Examples/second: 25.102622\n",
      "I0710 08:05:00.507619 140295626643200 trainer.py:508] step:  5010, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:78.867821 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2541553 log_pplx:4.4449539 loss:181.24301 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.8988\n",
      "I0710 08:05:03.567845 140295626643200 summary_utils.py:349] Steps/second: 0.177367, Examples/second: 25.169662\n",
      "I0710 08:05:03.568614 140295626643200 trainer.py:508] step:  5011, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:23.822655 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2551131 log_pplx:4.4992843 loss:70.881294 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.90546\n",
      "I0710 08:05:09.966624 140295626643200 summary_utils.py:349] Steps/second: 0.177317, Examples/second: 25.139404\n",
      "I0710 08:05:09.967378 140295626643200 trainer.py:508] step:  5012, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:155.87833 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.254047 log_pplx:4.4222665 loss:180.20737 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.9118\n",
      "I0710 08:05:13.786039 140295626643200 summary_utils.py:349] Steps/second: 0.177437, Examples/second: 25.163254\n",
      "I0710 08:05:13.786814 140295626643200 trainer.py:508] step:  5013, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:48.884529 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.255173 log_pplx:4.6282873 loss:115.82289 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.91815\n",
      "I0710 08:05:20.109881 140295626643200 summary_utils.py:349] Steps/second: 0.177392, Examples/second: 25.133827\n",
      "I0710 08:05:20.110895 140295626643200 trainer.py:508] step:  5014, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:114.98506 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2541758 log_pplx:4.5386562 loss:189.6591 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.92426\n",
      "I0710 08:05:26.411718 140295626643200 summary_utils.py:349] Steps/second: 0.177348, Examples/second: 25.104743\n",
      "I0710 08:05:26.412513 140295626643200 trainer.py:508] step:  5015, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:112.1159 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2542833 log_pplx:4.5409756 loss:186.23674 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.93036\n",
      "I0710 08:05:29.490960 140295626643200 summary_utils.py:349] Steps/second: 0.177516, Examples/second: 25.170968\n",
      "I0710 08:05:29.491705 140295626643200 trainer.py:508] step:  5016, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:24.858761 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2548916 log_pplx:4.4676652 loss:69.370979 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.93628\n",
      "I0710 08:05:40.030772 140295626643200 summary_utils.py:349] Steps/second: 0.177195, Examples/second: 25.087798\n",
      "I0710 08:05:40.031520 140295626643200 trainer.py:508] step:  5017, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:261.54541 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.253304 log_pplx:4.0800567 loss:298.45615 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.94177\n",
      "I0710 08:05:43.941674 140295626643200 summary_utils.py:349] Steps/second: 0.177308, Examples/second: 25.110610\n",
      "I0710 08:05:43.942694 140295626643200 trainer.py:508] step:  5018, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:106.58517 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.255571 log_pplx:4.6508346 loss:117.63706 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.94702\n",
      "I0710 08:05:50.150929 140295626643200 summary_utils.py:349] Steps/second: 0.177271, Examples/second: 25.082689\n",
      "I0710 08:05:50.151741 140295626643200 trainer.py:508] step:  5019, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:264.10187 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.254079 log_pplx:4.6208754 loss:181.71591 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.95282\n",
      "I0710 08:05:56.884518 140295626643200 summary_utils.py:349] Steps/second: 0.177200, Examples/second: 25.050075\n",
      "I0710 08:05:56.885362 140295626643200 trainer.py:508] step:  5020, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:167.91011 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2541806 log_pplx:4.5853338 loss:186.39383 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.95978\n",
      "I0710 08:06:01.009961 140295626643200 summary_utils.py:349] Steps/second: 0.177298, Examples/second: 25.070830\n",
      "I0710 08:06:01.011055 140295626643200 trainer.py:508] step:  5021, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:141.07625 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2562034 log_pplx:4.7979808 loss:121.86871 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.96729\n",
      "I0710 08:06:07.641872 140295626643200 summary_utils.py:349] Steps/second: 0.177234, Examples/second: 25.039305\n",
      "I0710 08:06:07.642830 140295626643200 trainer.py:508] step:  5022, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:209.41202 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2537884 log_pplx:4.5163307 loss:176.24982 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.97345\n",
      "I0710 08:06:10.724740 140295626643200 summary_utils.py:349] Steps/second: 0.177399, Examples/second: 25.104573\n",
      "I0710 08:06:10.725489 140295626643200 trainer.py:508] step:  5023, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:80.371323 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2556244 log_pplx:4.6927357 loss:73.085693 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:517.97839\n",
      "I0710 08:06:17.181271 140295626643200 summary_utils.py:349] Steps/second: 0.177346, Examples/second: 25.074679\n",
      "I0710 08:06:17.182321 140295626643200 trainer.py:508] step:  5024, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:119.94553 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2543007 log_pplx:4.5853968 loss:185.36464 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.98279\n",
      "I0710 08:06:23.481121 140295626643200 summary_utils.py:349] Steps/second: 0.177304, Examples/second: 25.046352\n",
      "I0710 08:06:23.481921 140295626643200 trainer.py:508] step:  5025, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:88.001198 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2541006 log_pplx:4.5037518 loss:178.40486 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:517.98682\n",
      "I0710 08:06:27.280231 140295626643200 summary_utils.py:349] Steps/second: 0.177422, Examples/second: 25.069880\n",
      "I0710 08:06:27.281203 140295626643200 trainer.py:508] step:  5026, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:73.662224 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.255914 log_pplx:4.6003766 loss:116.13078 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:517.99103\n",
      "I0710 08:06:38.310437 140295626643200 summary_utils.py:349] Steps/second: 0.177076, Examples/second: 24.984404\n",
      "I0710 08:06:38.311217 140295626643200 trainer.py:508] step:  5027, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:249.80623 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2535347 log_pplx:3.863349 loss:285.79126 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:517.99561\n",
      "I0710 08:06:40.523563 140295626643200 summary_utils.py:349] Steps/second: 0.177296, Examples/second: 25.149317\n",
      "I0710 08:06:40.524405 140295626643200 trainer.py:508] step:  5028, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:10.791563 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2545907 log_pplx:4.4172373 loss:32.939476 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:518.00134\n",
      "I0710 08:06:46.833863 140295626643200 summary_utils.py:349] Steps/second: 0.177253, Examples/second: 25.120964\n",
      "I0710 08:06:46.834652 140295626643200 trainer.py:508] step:  5029, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:132.30278 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2545053 log_pplx:4.4677429 loss:181.61374 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.00684\n",
      "I0710 08:06:49.950696 140295626643200 summary_utils.py:349] Steps/second: 0.177414, Examples/second: 25.184913\n",
      "I0710 08:06:49.951495 140295626643200 trainer.py:508] step:  5030, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:65.670113 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.255409 log_pplx:4.5905972 loss:71.423225 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.01276\n",
      "I0710 08:06:53.818350 140295626643200 summary_utils.py:349] Steps/second: 0.177527, Examples/second: 25.207407\n",
      "I0710 08:06:53.819099 140295626643200 trainer.py:508] step:  5031, steps/sec: 0.18, examples/sec: 25.21 grad_norm/all/loss:50.252472 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2557967 log_pplx:4.625463 loss:116.41713 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.0188\n",
      "I0710 08:06:59.918868 140295626643200 summary_utils.py:349] Steps/second: 0.177497, Examples/second: 25.180951\n",
      "I0710 08:06:59.919643 140295626643200 trainer.py:508] step:  5032, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:120.58463 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2544085 log_pplx:4.3835411 loss:175.12245 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.02484\n",
      "I0710 08:07:06.243347 140295626643200 summary_utils.py:349] Steps/second: 0.177453, Examples/second: 25.152596\n",
      "I0710 08:07:06.244249 140295626643200 trainer.py:508] step:  5033, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:186.30742 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2546506 log_pplx:4.6241817 loss:190.22726 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.03003\n",
      "I0710 08:07:09.331570 140295626643200 summary_utils.py:349] Steps/second: 0.177615, Examples/second: 25.216324\n",
      "I0710 08:07:09.332336 140295626643200 trainer.py:508] step:  5034, steps/sec: 0.18, examples/sec: 25.22 grad_norm/all/loss:25.005533 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2559462 log_pplx:4.575139 loss:70.217659 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.03412\n",
      "I0710 08:07:15.864790 140295626643200 summary_utils.py:349] Steps/second: 0.177558, Examples/second: 25.186109\n",
      "I0710 08:07:15.865616 140295626643200 trainer.py:508] step:  5035, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:89.107933 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2548866 log_pplx:4.42731 loss:187.16454 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.03796\n",
      "I0710 08:07:19.776107 140295626643200 summary_utils.py:349] Steps/second: 0.177667, Examples/second: 25.208002\n",
      "I0710 08:07:19.776936 140295626643200 trainer.py:508] step:  5036, steps/sec: 0.18, examples/sec: 25.21 grad_norm/all/loss:44.576195 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2563907 log_pplx:4.5936255 loss:116.62065 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.04218\n",
      "I0710 08:07:31.087713 140295626643200 summary_utils.py:349] Steps/second: 0.177309, Examples/second: 25.121070\n",
      "I0710 08:07:31.088555 140295626643200 base_runner.py:111] step:  5037, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:186.09752 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.253517 log_pplx:3.8510334 loss:273.51968 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.04633\n",
      "I0710 08:07:37.658697 140295626643200 summary_utils.py:349] Steps/second: 0.177250, Examples/second: 25.090973\n",
      "I0710 08:07:37.659536 140295626643200 trainer.py:508] step:  5038, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:152.18222 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2550973 log_pplx:4.6241035 loss:197.04462 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.05151\n",
      "I0710 08:07:44.030822 140295626643200 summary_utils.py:349] Steps/second: 0.177204, Examples/second: 25.062775\n",
      "I0710 08:07:44.031648 140295626643200 trainer.py:508] step:  5039, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:80.945282 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2550654 log_pplx:4.5044374 loss:189.29898 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.05762\n",
      "I0710 08:07:47.887837 140295626643200 summary_utils.py:349] Steps/second: 0.177316, Examples/second: 25.085101\n",
      "I0710 08:07:47.888644 140295626643200 trainer.py:508] step:  5040, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:77.466621 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2560747 log_pplx:4.6597972 loss:115.38823 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.06403\n",
      "I0710 08:07:50.039356 140295635035904 trainer.py:345] Write summary @5040\n",
      "I0710 08:07:54.476796 140295626643200 summary_utils.py:349] Steps/second: 0.177257, Examples/second: 25.117005\n",
      "I0710 08:07:54.478176 140295626643200 trainer.py:508] step:  5041, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:47.965157 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2557964 log_pplx:4.521666 loss:70.633369 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.0697\n",
      "I0710 08:08:03.430298 140295626643200 summary_utils.py:349] Steps/second: 0.177051, Examples/second: 25.066209\n",
      "I0710 08:08:03.431346 140295626643200 trainer.py:508] step:  5042, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:84.243202 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2543209 log_pplx:4.4691153 loss:172.95474 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.07489\n",
      "I0710 08:08:12.481218 140295626643200 summary_utils.py:349] Steps/second: 0.176840, Examples/second: 25.014879\n",
      "I0710 08:08:12.482467 140295626643200 trainer.py:508] step:  5043, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:90.953972 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.254967 log_pplx:4.5036383 loss:186.28174 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.07983\n",
      "I0710 08:08:17.890545 140295626643200 summary_utils.py:349] Steps/second: 0.176856, Examples/second: 25.023491\n",
      "I0710 08:08:17.891828 140295626643200 trainer.py:508] step:  5044, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:38.074375 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2563376 log_pplx:4.5322328 loss:115.03373 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.08447\n",
      "I0710 08:08:27.154469 140295626643200 summary_utils.py:349] Steps/second: 0.176634, Examples/second: 24.970704\n",
      "I0710 08:08:27.155577 140295626643200 trainer.py:508] step:  5045, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:73.214455 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2550614 log_pplx:4.4994388 loss:182.33975 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.08887\n",
      "I0710 08:08:43.990522 140295626643200 summary_utils.py:349] Steps/second: 0.175951, Examples/second: 24.839204\n",
      "I0710 08:08:43.991716 140295626643200 trainer.py:508] step:  5046, steps/sec: 0.18, examples/sec: 24.84 grad_norm/all/loss:268.01791 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2537471 log_pplx:3.9050112 loss:274.42465 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.09338\n",
      "I0710 08:08:46.657751 140295635035904 trainer.py:354] Write summary done: step 5040\n",
      "I0710 08:08:48.231570 140295626643200 summary_utils.py:349] Steps/second: 0.176039, Examples/second: 24.891206\n",
      "I0710 08:08:48.232359 140295626643200 trainer.py:508] step:  5047, steps/sec: 0.18, examples/sec: 24.89 grad_norm/all/loss:33.546982 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2561578 log_pplx:4.5083246 loss:70.583458 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.09747\n",
      "I0710 08:08:54.675226 140295626643200 summary_utils.py:349] Steps/second: 0.175993, Examples/second: 24.863519\n",
      "I0710 08:08:54.676223 140295626643200 trainer.py:508] step:  5048, steps/sec: 0.18, examples/sec: 24.86 grad_norm/all/loss:229.32733 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2550055 log_pplx:4.5872059 loss:189.62363 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.10162\n",
      "I0710 08:08:58.562388 140295626643200 summary_utils.py:349] Steps/second: 0.176101, Examples/second: 24.885309\n",
      "I0710 08:08:58.563225 140295626643200 trainer.py:508] step:  5049, steps/sec: 0.18, examples/sec: 24.89 grad_norm/all/loss:80.060989 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2563746 log_pplx:4.6629219 loss:116.68962 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.10669\n",
      "I0710 08:09:00.795924 140295626643200 summary_utils.py:349] Steps/second: 0.176310, Examples/second: 25.042172\n",
      "I0710 08:09:00.796757 140295626643200 trainer.py:508] step:  5050, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:22.338816 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.25474 log_pplx:4.4194779 loss:31.790888 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:518.11206\n",
      "I0710 08:09:07.046640 140295626643200 summary_utils.py:349] Steps/second: 0.176275, Examples/second: 25.015926\n",
      "I0710 08:09:07.047379 140295626643200 trainer.py:508] step:  5051, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:112.96883 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2551008 log_pplx:4.4521623 loss:182.03778 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.11713\n",
      "I0710 08:09:13.376407 140295626643200 summary_utils.py:349] Steps/second: 0.176235, Examples/second: 24.989116\n",
      "I0710 08:09:13.377242 140295626643200 trainer.py:508] step:  5052, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:118.39196 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.255398 log_pplx:4.5867181 loss:189.03012 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.12219\n",
      "I0710 08:09:16.388814 140295626643200 summary_utils.py:349] Steps/second: 0.176396, Examples/second: 25.050896\n",
      "I0710 08:09:16.389657 140295626643200 trainer.py:508] step:  5053, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:35.644855 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2564349 log_pplx:4.621727 loss:73.044952 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.12714\n",
      "I0710 08:09:20.263597 140295626643200 summary_utils.py:349] Steps/second: 0.176504, Examples/second: 25.072382\n",
      "I0710 08:09:20.264415 140295626643200 trainer.py:508] step:  5054, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:74.816513 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2563607 log_pplx:4.6065168 loss:115.62356 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.13159\n",
      "I0710 08:09:26.681826 140295626643200 summary_utils.py:349] Steps/second: 0.176458, Examples/second: 25.044816\n",
      "I0710 08:09:26.682728 140295626643200 trainer.py:508] step:  5055, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:138.01659 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2550502 log_pplx:4.464644 loss:185.89662 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.13556\n",
      "I0710 08:09:33.054629 140295626643200 summary_utils.py:349] Steps/second: 0.176416, Examples/second: 25.017757\n",
      "I0710 08:09:33.055435 140295626643200 trainer.py:508] step:  5056, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:118.03725 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.255313 log_pplx:4.4910903 loss:187.55916 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.13983\n",
      "I0710 08:09:39.253527 140295626643200 summary_utils.py:349] Steps/second: 0.176384, Examples/second: 24.992289\n",
      "I0710 08:09:39.254368 140295626643200 trainer.py:508] step:  5057, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:79.49157 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2552371 log_pplx:4.4615827 loss:185.43452 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.14435\n",
      "I0710 08:09:43.092540 140295626643200 summary_utils.py:349] Steps/second: 0.176494, Examples/second: 25.013988\n",
      "I0710 08:09:43.093301 140295626643200 trainer.py:508] step:  5058, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:42.520115 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.256644 log_pplx:4.5845571 loss:114.0695 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.14899\n",
      "I0710 08:09:54.061810 140295626643200 summary_utils.py:349] Steps/second: 0.176178, Examples/second: 24.934881\n",
      "I0710 08:09:54.062671 140295626643200 trainer.py:508] step:  5059, steps/sec: 0.18, examples/sec: 24.93 grad_norm/all/loss:235.69833 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2542557 log_pplx:3.98265 loss:294.01913 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.15344\n",
      "I0710 08:09:57.163611 140295626643200 summary_utils.py:349] Steps/second: 0.176331, Examples/second: 24.995116\n",
      "I0710 08:09:57.164380 140295626643200 trainer.py:508] step:  5060, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:32.178486 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2562958 log_pplx:4.5385032 loss:70.329071 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.15839\n",
      "I0710 08:10:03.255052 140295626643200 summary_utils.py:349] Steps/second: 0.176306, Examples/second: 24.970805\n",
      "I0710 08:10:03.255916 140295626643200 trainer.py:508] step:  5061, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:328.10974 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2550291 log_pplx:4.5723276 loss:180.1497 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.16333\n",
      "I0710 08:10:07.105527 140295626643200 summary_utils.py:349] Steps/second: 0.176414, Examples/second: 24.992260\n",
      "I0710 08:10:07.106389 140295626643200 trainer.py:508] step:  5062, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:44.048527 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2569034 log_pplx:4.5908895 loss:116.40773 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.16779\n",
      "I0710 08:10:13.534249 140295626643200 summary_utils.py:349] Steps/second: 0.176369, Examples/second: 24.965212\n",
      "I0710 08:10:13.535103 140295626643200 trainer.py:508] step:  5063, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:89.25621 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2549402 log_pplx:4.4422565 loss:182.63225 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.1723\n",
      "I0710 08:10:16.551742 140295626643200 summary_utils.py:349] Steps/second: 0.176525, Examples/second: 25.025729\n",
      "I0710 08:10:16.552516 140295626643200 trainer.py:508] step:  5064, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:45.585884 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2563677 log_pplx:4.4991264 loss:70.334 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.17682\n",
      "I0710 08:10:22.826345 140295626643200 summary_utils.py:349] Steps/second: 0.176489, Examples/second: 24.999982\n",
      "I0710 08:10:22.827105 140295626643200 trainer.py:508] step:  5065, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:163.64108 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2554163 log_pplx:4.5922723 loss:193.10506 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.18127\n",
      "I0710 08:10:26.696455 140295626643200 summary_utils.py:349] Steps/second: 0.176595, Examples/second: 25.021096\n",
      "I0710 08:10:26.697204 140295626643200 trainer.py:508] step:  5066, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:88.409866 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2568251 log_pplx:4.6005645 loss:116.65306 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.1864\n",
      "I0710 08:10:32.783344 140295626643200 summary_utils.py:349] Steps/second: 0.176570, Examples/second: 24.997009\n",
      "I0710 08:10:32.784314 140295626643200 trainer.py:508] step:  5067, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:106.53665 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2550839 log_pplx:4.3959579 loss:171.11267 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.19177\n",
      "I0710 08:10:43.679653 140295626643200 summary_utils.py:349] Steps/second: 0.176263, Examples/second: 24.919872\n",
      "I0710 08:10:43.680457 140295626643200 trainer.py:508] step:  5068, steps/sec: 0.18, examples/sec: 24.92 grad_norm/all/loss:244.81807 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2544004 log_pplx:3.9277039 loss:285.44586 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.19757\n",
      "I0710 08:10:46.752618 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 08:10:50.422222 140295626643200 summary_utils.py:349] Steps/second: 0.176201, Examples/second: 24.890719\n",
      "I0710 08:10:50.423362 140295626643200 trainer.py:508] step:  5069, steps/sec: 0.18, examples/sec: 24.89 grad_norm/all/loss:142.56009 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2552292 log_pplx:4.4233212 loss:175.88232 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.20276\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 08:10:52.086027 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 08:10:52.619843 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00005068\n",
      "I0710 08:10:53.499413 140295626643200 summary_utils.py:349] Steps/second: 0.176353, Examples/second: 24.950078\n",
      "I0710 08:10:53.500236 140295626643200 trainer.py:508] step:  5070, steps/sec: 0.18, examples/sec: 24.95 grad_norm/all/loss:123.59005 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2566775 log_pplx:4.8230882 loss:76.641876 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.20764\n",
      "I0710 08:11:00.103719 140295626643200 summary_utils.py:349] Steps/second: 0.176298, Examples/second: 24.922089\n",
      "I0710 08:11:00.104547 140295626643200 trainer.py:508] step:  5071, steps/sec: 0.18, examples/sec: 24.92 grad_norm/all/loss:101.24121 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2556195 log_pplx:4.5151429 loss:190.7648 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.21191\n",
      "I0710 08:11:04.058691 140295626643200 summary_utils.py:349] Steps/second: 0.176398, Examples/second: 24.942345\n",
      "I0710 08:11:04.059477 140295626643200 trainer.py:508] step:  5072, steps/sec: 0.18, examples/sec: 24.94 grad_norm/all/loss:108.84949 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2573646 log_pplx:4.7891688 loss:121.10609 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.21674\n",
      "I0710 08:11:10.613787 140295626643200 summary_utils.py:349] Steps/second: 0.176347, Examples/second: 24.914873\n",
      "I0710 08:11:10.614628 140295626643200 trainer.py:508] step:  5073, steps/sec: 0.18, examples/sec: 24.91 grad_norm/all/loss:160.03453 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2556717 log_pplx:4.5958347 loss:188.2569 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.22107\n",
      "I0710 08:11:12.915393 140295626643200 summary_utils.py:349] Steps/second: 0.176542, Examples/second: 25.064344\n",
      "I0710 08:11:12.916400 140295626643200 trainer.py:508] step:  5074, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:11.819451 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.255411 log_pplx:4.336184 loss:31.987825 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:518.22644\n",
      "I0710 08:11:19.337979 140295626643200 summary_utils.py:349] Steps/second: 0.176498, Examples/second: 25.037778\n",
      "I0710 08:11:19.338833 140295626643200 trainer.py:508] step:  5075, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:200.13058 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.255477 log_pplx:4.5196142 loss:182.30994 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.23151\n",
      "I0710 08:11:23.340789 140295626643200 summary_utils.py:349] Steps/second: 0.176594, Examples/second: 25.057363\n",
      "I0710 08:11:23.341608 140295626643200 trainer.py:508] step:  5076, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:44.560612 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2572014 log_pplx:4.5914707 loss:116.04942 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.23724\n",
      "I0710 08:11:33.683743 140295626643200 summary_utils.py:349] Steps/second: 0.176324, Examples/second: 24.985800\n",
      "I0710 08:11:33.684546 140295626643200 trainer.py:508] step:  5077, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:451.8959 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2550764 log_pplx:4.1624193 loss:311.14081 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.24255\n",
      "I0710 08:11:39.814944 140295626643200 summary_utils.py:349] Steps/second: 0.176298, Examples/second: 24.961948\n",
      "I0710 08:11:39.815735 140295626643200 trainer.py:508] step:  5078, steps/sec: 0.18, examples/sec: 24.96 grad_norm/all/loss:231.60455 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.25547 log_pplx:4.5344868 loss:178.43205 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.25\n",
      "I0710 08:11:42.911739 140295626643200 summary_utils.py:349] Steps/second: 0.176446, Examples/second: 25.020122\n",
      "I0710 08:11:42.912508 140295626643200 trainer.py:508] step:  5079, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:29.579962 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2567172 log_pplx:4.4742203 loss:70.101952 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.25647\n",
      "I0710 08:11:49.293528 140295626643200 summary_utils.py:349] Steps/second: 0.176405, Examples/second: 24.994239\n",
      "I0710 08:11:49.294333 140295626643200 trainer.py:508] step:  5080, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:317.28381 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2557806 log_pplx:4.6542139 loss:186.45944 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.26251\n",
      "I0710 08:11:53.107185 140295626643200 summary_utils.py:349] Steps/second: 0.176511, Examples/second: 25.015226\n",
      "I0710 08:11:53.107941 140295626643200 trainer.py:508] step:  5081, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:51.517349 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2568761 log_pplx:4.5643492 loss:113.0247 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.26776\n",
      "I0710 08:11:59.388736 140295626643200 summary_utils.py:349] Steps/second: 0.176476, Examples/second: 24.990249\n",
      "I0710 08:11:59.389526 140295626643200 trainer.py:508] step:  5082, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:151.68539 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.255686 log_pplx:4.5727482 loss:183.31003 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.27252\n",
      "I0710 08:12:02.463043 140295626643200 summary_utils.py:349] Steps/second: 0.176624, Examples/second: 25.048209\n",
      "I0710 08:12:02.463827 140295626643200 trainer.py:508] step:  5083, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:56.199223 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2566072 log_pplx:4.5797915 loss:70.754204 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.27686\n",
      "I0710 08:12:08.776430 140295626643200 summary_utils.py:349] Steps/second: 0.176586, Examples/second: 25.022983\n",
      "I0710 08:12:08.777246 140295626643200 trainer.py:508] step:  5084, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:162.46497 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2557814 log_pplx:4.5730152 loss:187.15067 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.28137\n",
      "I0710 08:12:12.571021 140295626643200 summary_utils.py:349] Steps/second: 0.176693, Examples/second: 25.043957\n",
      "I0710 08:12:12.571875 140295626643200 trainer.py:508] step:  5085, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:139.18639 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2575035 log_pplx:4.8512135 loss:123.88786 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.28644\n",
      "I0710 08:12:18.819579 140295626643200 summary_utils.py:349] Steps/second: 0.176659, Examples/second: 25.019344\n",
      "I0710 08:12:18.820520 140295626643200 trainer.py:508] step:  5086, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:169.71857 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2557069 log_pplx:4.5402846 loss:181.15735 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.29224\n",
      "I0710 08:12:29.506074 140295626643200 summary_utils.py:349] Steps/second: 0.176375, Examples/second: 24.946431\n",
      "I0710 08:12:29.506818 140295626643200 trainer.py:508] step:  5087, steps/sec: 0.18, examples/sec: 24.95 grad_norm/all/loss:246.03081 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2549388 log_pplx:4.0671 loss:297.81339 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.29858\n",
      "I0710 08:12:35.891030 140295626643200 summary_utils.py:349] Steps/second: 0.176334, Examples/second: 24.921059\n",
      "I0710 08:12:35.892060 140295626643200 trainer.py:508] step:  5088, steps/sec: 0.18, examples/sec: 24.92 grad_norm/all/loss:74.59211 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2561309 log_pplx:4.4866967 loss:185.80534 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.30511\n",
      "I0710 08:12:38.965157 140295626643200 summary_utils.py:349] Steps/second: 0.176481, Examples/second: 24.978412\n",
      "I0710 08:12:38.965970 140295626643200 trainer.py:508] step:  5089, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:61.939693 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2569767 log_pplx:4.6238084 loss:71.23555 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.3114\n",
      "I0710 08:12:42.934834 140295626643200 summary_utils.py:349] Steps/second: 0.176576, Examples/second: 24.997839\n",
      "I0710 08:12:42.935657 140295626643200 trainer.py:508] step:  5090, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:128.0778 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2574775 log_pplx:4.700963 loss:119.522 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.31714\n",
      "I0710 08:12:49.149562 140295626643200 summary_utils.py:349] Steps/second: 0.176545, Examples/second: 24.973826\n",
      "I0710 08:12:49.150374 140295626643200 trainer.py:508] step:  5091, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:91.50827 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2559935 log_pplx:4.5098948 loss:187.55527 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.3219\n",
      "I0710 08:12:55.283548 140295626643200 summary_utils.py:349] Steps/second: 0.176519, Examples/second: 24.950548\n",
      "I0710 08:12:55.284430 140295626643200 trainer.py:508] step:  5092, steps/sec: 0.18, examples/sec: 24.95 grad_norm/all/loss:75.609001 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2558955 log_pplx:4.4667807 loss:178.11287 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.32629\n",
      "I0710 08:12:59.105002 140295626643200 summary_utils.py:349] Steps/second: 0.176622, Examples/second: 24.971086\n",
      "I0710 08:12:59.105734 140295626643200 trainer.py:508] step:  5093, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:56.326317 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2575035 log_pplx:4.6397371 loss:116.6604 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.33063\n",
      "I0710 08:13:05.502882 140295626643200 summary_utils.py:349] Steps/second: 0.176581, Examples/second: 24.945800\n",
      "I0710 08:13:05.503640 140295626643200 trainer.py:508] step:  5094, steps/sec: 0.18, examples/sec: 24.95 grad_norm/all/loss:158.08971 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2562886 log_pplx:4.4805584 loss:188.07146 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.33459\n",
      "I0710 08:13:08.456075 140295626643200 summary_utils.py:349] Steps/second: 0.176733, Examples/second: 25.003548\n",
      "I0710 08:13:08.456960 140295626643200 trainer.py:508] step:  5095, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:23.924648 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2572689 log_pplx:4.5397854 loss:70.490799 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.33911\n",
      "I0710 08:13:14.617526 140295626643200 summary_utils.py:349] Steps/second: 0.176705, Examples/second: 24.980138\n",
      "I0710 08:13:14.618309 140295626643200 trainer.py:508] step:  5096, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:128.30536 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2559693 log_pplx:4.5062222 loss:179.85458 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.34332\n",
      "I0710 08:13:18.437288 140295626643200 summary_utils.py:349] Steps/second: 0.176807, Examples/second: 25.000528\n",
      "I0710 08:13:18.438076 140295626643200 trainer.py:508] step:  5097, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:37.965088 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2576915 log_pplx:4.5740528 loss:114.55144 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.34796\n",
      "I0710 08:13:24.582406 140295626643200 summary_utils.py:349] Steps/second: 0.176780, Examples/second: 24.977326\n",
      "I0710 08:13:24.583164 140295626643200 trainer.py:508] step:  5098, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:69.329803 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2560432 log_pplx:4.4906149 loss:177.15475 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.35242\n",
      "I0710 08:13:26.762305 140295626643200 summary_utils.py:349] Steps/second: 0.176973, Examples/second: 25.121408\n",
      "I0710 08:13:26.763069 140295626643200 trainer.py:508] step:  5099, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:13.555466 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2558599 log_pplx:4.2908864 loss:31.578243 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:518.35693\n",
      "I0710 08:13:37.261641 140295626643200 summary_utils.py:349] Steps/second: 0.176704, Examples/second: 25.051179\n",
      "I0710 08:13:37.262458 140295626643200 trainer.py:508] step:  5100, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:195.85228 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.255353 log_pplx:4.0888162 loss:296.13251 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.36102\n",
      "I0710 08:13:43.697825 140295626643200 summary_utils.py:349] Steps/second: 0.176661, Examples/second: 25.025734\n",
      "I0710 08:13:43.698618 140295626643200 trainer.py:508] step:  5101, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:111.44711 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2559019 log_pplx:4.5236487 loss:185.35652 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.36591\n",
      "I0710 08:13:46.714145 140295626643200 summary_utils.py:349] Steps/second: 0.176807, Examples/second: 25.082223\n",
      "I0710 08:13:46.714955 140295626643200 trainer.py:508] step:  5102, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:27.227341 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2573566 log_pplx:4.4157043 loss:69.357613 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.37054\n",
      "I0710 08:13:50.597104 140295626643200 summary_utils.py:349] Steps/second: 0.176905, Examples/second: 25.101791\n",
      "I0710 08:13:50.598050 140295626643200 trainer.py:508] step:  5103, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:41.372585 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2577503 log_pplx:4.6222324 loss:117.08694 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.37512\n",
      "I0710 08:13:56.587927 140295626643200 summary_utils.py:349] Steps/second: 0.176887, Examples/second: 25.079835\n",
      "I0710 08:13:56.588809 140295626643200 trainer.py:508] step:  5104, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:89.016167 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2562463 log_pplx:4.4455795 loss:181.60193 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.37939\n",
      "I0710 08:14:02.902046 140295626643200 summary_utils.py:349] Steps/second: 0.176850, Examples/second: 25.055438\n",
      "I0710 08:14:02.902904 140295626643200 trainer.py:508] step:  5105, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:67.654266 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2558827 log_pplx:4.3759871 loss:171.26518 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.38354\n",
      "I0710 08:14:09.131316 140295626643200 summary_utils.py:349] Steps/second: 0.176819, Examples/second: 25.031797\n",
      "I0710 08:14:09.132068 140295626643200 trainer.py:508] step:  5106, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:76.158852 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2564201 log_pplx:4.4767022 loss:190.7075 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.38776\n",
      "I0710 08:14:12.945333 140295626643200 summary_utils.py:349] Steps/second: 0.176920, Examples/second: 25.051825\n",
      "I0710 08:14:12.946142 140295626643200 trainer.py:508] step:  5107, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:60.496803 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2578586 log_pplx:4.6053252 loss:117.37823 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.39178\n",
      "I0710 08:14:15.997754 140295626643200 summary_utils.py:349] Steps/second: 0.177062, Examples/second: 25.107496\n",
      "I0710 08:14:15.998573 140295626643200 trainer.py:508] step:  5108, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:31.889341 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2577808 log_pplx:4.5844903 loss:70.934242 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.39606\n",
      "I0710 08:14:21.890629 140295626643200 summary_utils.py:349] Steps/second: 0.177049, Examples/second: 25.086463\n",
      "I0710 08:14:21.891488 140295626643200 trainer.py:508] step:  5109, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:88.521034 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2560226 log_pplx:4.4016418 loss:175.40543 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.40033\n",
      "I0710 08:14:31.268402 140295626643200 summary_utils.py:349] Steps/second: 0.176845, Examples/second: 25.026201\n",
      "I0710 08:14:31.269278 140295626643200 trainer.py:508] step:  5110, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:241.92696 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2555246 log_pplx:3.8870897 loss:272.19348 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.40472\n",
      "I0710 08:14:37.487579 140295626643200 summary_utils.py:349] Steps/second: 0.176814, Examples/second: 25.002900\n",
      "I0710 08:14:37.488417 140295626643200 trainer.py:508] step:  5111, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:108.62436 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2562847 log_pplx:4.5170503 loss:177.57654 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.41016\n",
      "I0710 08:14:41.345257 140295626643200 summary_utils.py:349] Steps/second: 0.176912, Examples/second: 25.022452\n",
      "I0710 08:14:41.346122 140295626643200 trainer.py:508] step:  5112, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:93.922729 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2575332 log_pplx:4.5682888 loss:113.92171 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.41528\n",
      "I0710 08:14:47.456401 140295626643200 summary_utils.py:349] Steps/second: 0.176887, Examples/second: 25.000060\n",
      "I0710 08:14:47.457161 140295626643200 trainer.py:508] step:  5113, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:138.85211 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2569141 log_pplx:4.616437 loss:191.58215 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.41968\n",
      "I0710 08:14:50.509539 140295626643200 summary_utils.py:349] Steps/second: 0.177028, Examples/second: 25.055184\n",
      "I0710 08:14:50.510366 140295626643200 trainer.py:508] step:  5114, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:31.243824 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2576573 log_pplx:4.5762124 loss:71.753571 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.42377\n",
      "I0710 08:14:56.705522 140295626643200 summary_utils.py:349] Steps/second: 0.176999, Examples/second: 25.032143\n",
      "I0710 08:14:56.706509 140295626643200 trainer.py:508] step:  5115, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:84.754021 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2563654 log_pplx:4.3843145 loss:175.04375 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.42743\n",
      "I0710 08:15:00.556488 140295626643200 summary_utils.py:349] Steps/second: 0.177096, Examples/second: 25.051597\n",
      "I0710 08:15:00.557259 140295626643200 trainer.py:508] step:  5116, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:63.47897 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2580909 log_pplx:4.6172705 loss:116.61493 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.43134\n",
      "I0710 08:15:06.883958 140295626643200 summary_utils.py:349] Steps/second: 0.177059, Examples/second: 25.027629\n",
      "I0710 08:15:06.884777 140295626643200 trainer.py:508] step:  5117, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:141.02275 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2564151 log_pplx:4.4758801 loss:181.04935 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.43536\n",
      "I0710 08:15:09.961890 140295626643200 summary_utils.py:349] Steps/second: 0.177198, Examples/second: 25.082212\n",
      "I0710 08:15:09.962847 140295626643200 trainer.py:508] step:  5118, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:20.535746 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.257648 log_pplx:4.4471626 loss:69.730118 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.43988\n",
      "I0710 08:15:20.858484 140295626643200 summary_utils.py:349] Steps/second: 0.176915, Examples/second: 25.011290\n",
      "I0710 08:15:20.859332 140295626643200 trainer.py:508] step:  5119, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:292.09171 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2558295 log_pplx:4.0277534 loss:300.16837 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.44434\n",
      "I0710 08:15:27.072875 140295626643200 summary_utils.py:349] Steps/second: 0.176885, Examples/second: 24.988405\n",
      "I0710 08:15:27.073656 140295626643200 trainer.py:508] step:  5120, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:78.321785 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2565637 log_pplx:4.439045 loss:181.94534 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.44818\n",
      "I0710 08:15:30.974308 140295626643200 summary_utils.py:349] Steps/second: 0.176979, Examples/second: 25.007348\n",
      "I0710 08:15:30.975143 140295626643200 trainer.py:508] step:  5121, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:42.292126 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2583184 log_pplx:4.5871696 loss:115.88338 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.45215\n",
      "I0710 08:15:37.183061 140295626643200 summary_utils.py:349] Steps/second: 0.176949, Examples/second: 24.984582\n",
      "I0710 08:15:37.183858 140295626643200 trainer.py:508] step:  5122, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:87.723831 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2566041 log_pplx:4.4613218 loss:178.7317 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.45642\n",
      "I0710 08:15:39.455666 140295626643200 summary_utils.py:349] Steps/second: 0.177130, Examples/second: 25.122181\n",
      "I0710 08:15:39.456453 140295626643200 trainer.py:508] step:  5123, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:12.137615 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2565036 log_pplx:4.3262205 loss:31.821379 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:518.461\n",
      "I0710 08:15:46.006262 140295626643200 summary_utils.py:349] Steps/second: 0.177081, Examples/second: 25.096670\n",
      "I0710 08:15:46.007012 140295626643200 trainer.py:508] step:  5124, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:97.96891 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2570287 log_pplx:4.4925733 loss:191.10286 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.46533\n",
      "I0710 08:15:49.091884 140295626643200 summary_utils.py:349] Steps/second: 0.177218, Examples/second: 25.150487\n",
      "I0710 08:15:49.092714 140295626643200 trainer.py:508] step:  5125, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:39.852074 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2579437 log_pplx:4.4822226 loss:70.630028 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.47015\n",
      "I0710 08:15:52.977504 140295626643200 summary_utils.py:349] Steps/second: 0.177312, Examples/second: 25.169234\n",
      "I0710 08:15:52.978320 140295626643200 trainer.py:508] step:  5126, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:72.42334 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2584343 log_pplx:4.6864243 loss:118.15647 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.47461\n",
      "I0710 08:15:59.344258 140295626643200 summary_utils.py:349] Steps/second: 0.177273, Examples/second: 25.145123\n",
      "I0710 08:15:59.344999 140295626643200 trainer.py:508] step:  5127, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:72.295982 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.256721 log_pplx:4.4669251 loss:177.33693 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.47864\n",
      "I0710 08:16:05.979062 140295626643200 summary_utils.py:349] Steps/second: 0.177220, Examples/second: 25.119085\n",
      "I0710 08:16:05.979892 140295626643200 trainer.py:508] step:  5128, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:73.917427 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2567716 log_pplx:4.3911858 loss:176.90988 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.48285\n",
      "I0710 08:16:09.824549 140295626643200 summary_utils.py:349] Steps/second: 0.177316, Examples/second: 25.138076\n",
      "I0710 08:16:09.825277 140295626643200 trainer.py:508] step:  5129, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:42.939568 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2581577 log_pplx:4.606554 loss:114.0698 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.48724\n",
      "I0710 08:16:16.305914 140295626643200 summary_utils.py:349] Steps/second: 0.177271, Examples/second: 25.113285\n",
      "I0710 08:16:16.306756 140295626643200 trainer.py:508] step:  5130, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:70.01828 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2562842 log_pplx:4.3350005 loss:172.80396 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.49152\n",
      "I0710 08:16:19.554879 140295626643200 summary_utils.py:349] Steps/second: 0.177398, Examples/second: 25.165372\n",
      "I0710 08:16:19.555918 140295626643200 trainer.py:508] step:  5131, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:37.115009 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2580456 log_pplx:4.467967 loss:68.904434 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.49609\n",
      "I0710 08:16:29.132538 140295626643200 summary_utils.py:349] Steps/second: 0.177189, Examples/second: 25.105507\n",
      "I0710 08:16:29.133352 140295626643200 trainer.py:508] step:  5132, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:319.0665 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2556731 log_pplx:4.0076146 loss:277.82791 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.50061\n",
      "I0710 08:16:35.058584 140295626643200 summary_utils.py:349] Steps/second: 0.177175, Examples/second: 25.085062\n",
      "I0710 08:16:35.059378 140295626643200 trainer.py:508] step:  5133, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:184.23102 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2566949 log_pplx:4.4364243 loss:172.74326 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.50665\n",
      "I0710 08:16:38.941858 140295626643200 summary_utils.py:349] Steps/second: 0.177267, Examples/second: 25.103646\n",
      "I0710 08:16:38.942678 140295626643200 trainer.py:508] step:  5134, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:50.828724 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2583469 log_pplx:4.5769897 loss:113.82401 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.51202\n",
      "I0710 08:16:45.334052 140295626643200 summary_utils.py:349] Steps/second: 0.177228, Examples/second: 25.079798\n",
      "I0710 08:16:45.334840 140295626643200 trainer.py:508] step:  5135, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:217.23984 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2570066 log_pplx:4.580266 loss:181.60757 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.51709\n",
      "I0710 08:16:52.003960 140295626643200 summary_utils.py:349] Steps/second: 0.177174, Examples/second: 25.053982\n",
      "I0710 08:16:52.004687 140295626643200 trainer.py:508] step:  5136, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:77.091034 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2569647 log_pplx:4.4833231 loss:184.32065 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.52173\n",
      "I0710 08:16:55.141758 140295626643200 summary_utils.py:349] Steps/second: 0.177305, Examples/second: 25.106401\n",
      "I0710 08:16:55.142624 140295626643200 base_runner.py:111] step:  5137, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:37.632324 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2581375 log_pplx:4.4808016 loss:70.047523 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.52637\n",
      "I0710 08:16:59.139504 140295626643200 summary_utils.py:349] Steps/second: 0.177391, Examples/second: 25.124002\n",
      "I0710 08:16:59.140332 140295626643200 trainer.py:508] step:  5138, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:134.24243 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2582977 log_pplx:4.6593795 loss:116.74657 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.53101\n",
      "I0710 08:17:05.516180 140295626643200 summary_utils.py:349] Steps/second: 0.177352, Examples/second: 25.100372\n",
      "I0710 08:17:05.516930 140295626643200 trainer.py:508] step:  5139, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:231.5293 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2567362 log_pplx:4.5605607 loss:183.10649 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.53638\n",
      "I0710 08:17:11.766083 140295626643200 summary_utils.py:349] Steps/second: 0.177321, Examples/second: 25.077766\n",
      "I0710 08:17:11.766881 140295626643200 trainer.py:508] step:  5140, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:125.5733 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.257068 log_pplx:4.5209918 loss:177.50545 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.54248\n",
      "I0710 08:17:15.610740 140295626643200 summary_utils.py:349] Steps/second: 0.177414, Examples/second: 25.096443\n",
      "I0710 08:17:15.611505 140295626643200 trainer.py:508] step:  5141, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:114.86355 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2588879 log_pplx:4.7173562 loss:120.02724 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.54865\n",
      "I0710 08:17:17.065429 140295635035904 trainer.py:345] Write summary @5141\n",
      "2020-07-10 08:17:18.530306: I lingvo/core/ops/record_batcher.cc:394] 3382 total seconds passed. Total records yielded: 1687. Total records skipped: 0\n",
      "I0710 08:17:22.220475 140295626643200 summary_utils.py:349] Steps/second: 0.177363, Examples/second: 25.122862\n",
      "I0710 08:17:22.221670 140295626643200 trainer.py:508] step:  5142, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:125.41323 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2582442 log_pplx:4.7493639 loss:74.69117 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.55389\n",
      "I0710 08:17:31.701573 140295626643200 summary_utils.py:349] Steps/second: 0.177164, Examples/second: 25.076615\n",
      "I0710 08:17:31.702808 140295626643200 trainer.py:508] step:  5143, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:235.42502 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2566187 log_pplx:4.4469886 loss:172.37639 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.55841\n",
      "I0710 08:17:34.449967 140295626643200 summary_utils.py:349] Steps/second: 0.177314, Examples/second: 25.206045\n",
      "I0710 08:17:34.451347 140295626643200 trainer.py:508] step:  5144, steps/sec: 0.18, examples/sec: 25.21 grad_norm/all/loss:14.426589 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2571701 log_pplx:4.3151789 loss:31.816021 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:518.56226\n",
      "I0710 08:17:50.046687 140295626643200 summary_utils.py:349] Steps/second: 0.176801, Examples/second: 25.103358\n",
      "I0710 08:17:50.048146 140295626643200 trainer.py:508] step:  5145, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:250.62747 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2564534 log_pplx:4.0034437 loss:295.85446 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.56586\n",
      "I0710 08:17:56.010575 140295626643200 summary_utils.py:349] Steps/second: 0.176785, Examples/second: 25.106344\n",
      "I0710 08:17:56.012018 140295626643200 trainer.py:508] step:  5146, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:42.467522 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2587464 log_pplx:4.5757747 loss:115.39531 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.56946\n",
      "I0710 08:18:02.877243 140295635035904 trainer.py:354] Write summary done: step 5141\n",
      "I0710 08:18:05.480620 140295626643200 summary_utils.py:349] Steps/second: 0.176590, Examples/second: 25.060674\n",
      "I0710 08:18:05.481367 140295626643200 trainer.py:508] step:  5147, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:137.52158 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2573395 log_pplx:4.5717688 loss:186.6996 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.57318\n",
      "I0710 08:18:12.024252 140295626643200 summary_utils.py:349] Steps/second: 0.176545, Examples/second: 25.036406\n",
      "I0710 08:18:12.025105 140295626643200 trainer.py:508] step:  5148, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:246.6339 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2574123 log_pplx:4.600049 loss:189.46452 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.57758\n",
      "I0710 08:18:15.834999 140295626643200 summary_utils.py:349] Steps/second: 0.176639, Examples/second: 25.055049\n",
      "I0710 08:18:15.835790 140295626643200 trainer.py:508] step:  5149, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:48.696598 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2587737 log_pplx:4.642529 loss:115.4539 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.58313\n",
      "I0710 08:18:22.249220 140295626643200 summary_utils.py:349] Steps/second: 0.176601, Examples/second: 25.031798\n",
      "I0710 08:18:22.250036 140295626643200 trainer.py:508] step:  5150, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:81.244041 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2571781 log_pplx:4.4649801 loss:178.43178 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.58856\n",
      "I0710 08:18:25.288212 140295626643200 summary_utils.py:349] Steps/second: 0.176734, Examples/second: 25.083589\n",
      "I0710 08:18:25.289075 140295626643200 trainer.py:508] step:  5151, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:40.453068 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2583779 log_pplx:4.4852948 loss:69.434464 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.59375\n",
      "I0710 08:18:31.756448 140295626643200 summary_utils.py:349] Steps/second: 0.176693, Examples/second: 25.059959\n",
      "I0710 08:18:31.757260 140295626643200 trainer.py:508] step:  5152, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:197.96277 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2576323 log_pplx:4.5671282 loss:186.3959 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.59882\n",
      "I0710 08:18:35.570879 140295626643200 summary_utils.py:349] Steps/second: 0.176787, Examples/second: 25.078444\n",
      "I0710 08:18:35.571835 140295626643200 trainer.py:508] step:  5153, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:90.867058 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.259002 log_pplx:4.719142 loss:118.47997 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.60303\n",
      "I0710 08:18:44.656260 140295626643200 summary_utils.py:349] Steps/second: 0.176613, Examples/second: 25.024674\n",
      "I0710 08:18:44.657055 140295626643200 trainer.py:508] step:  5154, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:195.69478 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2564381 log_pplx:3.9110382 loss:286.58136 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.60663\n",
      "I0710 08:18:51.007977 140295626643200 summary_utils.py:349] Steps/second: 0.176578, Examples/second: 25.002116\n",
      "I0710 08:18:51.008789 140295626643200 trainer.py:508] step:  5155, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:77.474457 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2574803 log_pplx:4.5209332 loss:185.5278 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.6109\n",
      "I0710 08:18:54.071840 140295626643200 summary_utils.py:349] Steps/second: 0.176709, Examples/second: 25.053329\n",
      "I0710 08:18:54.072646 140295626643200 trainer.py:508] step:  5156, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:34.443104 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2586296 log_pplx:4.5163069 loss:68.750191 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.61523\n",
      "I0710 08:19:00.268940 140295626643200 summary_utils.py:349] Steps/second: 0.176682, Examples/second: 25.031883\n",
      "I0710 08:19:00.269724 140295626643200 trainer.py:508] step:  5157, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:94.043465 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2575752 log_pplx:4.4535594 loss:184.48868 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.61969\n",
      "I0710 08:19:04.095042 140295626643200 summary_utils.py:349] Steps/second: 0.176774, Examples/second: 25.050165\n",
      "I0710 08:19:04.096118 140295626643200 trainer.py:508] step:  5158, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:50.731903 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2591813 log_pplx:4.6554375 loss:117.20064 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.62433\n",
      "I0710 08:19:10.050085 140295626643200 summary_utils.py:349] Steps/second: 0.176759, Examples/second: 25.030507\n",
      "I0710 08:19:10.050868 140295626643200 trainer.py:508] step:  5159, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:84.50235 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2570864 log_pplx:4.5226302 loss:172.42526 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.62903\n",
      "I0710 08:19:16.274048 140295626643200 summary_utils.py:349] Steps/second: 0.176731, Examples/second: 25.009008\n",
      "I0710 08:19:16.274816 140295626643200 trainer.py:508] step:  5160, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:71.786407 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2571921 log_pplx:4.4558973 loss:170.49376 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.63361\n",
      "I0710 08:19:19.342742 140295626643200 summary_utils.py:349] Steps/second: 0.176861, Examples/second: 25.059814\n",
      "I0710 08:19:19.343796 140295626643200 trainer.py:508] step:  5161, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:26.500937 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2588485 log_pplx:4.5227456 loss:71.339256 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.63824\n",
      "I0710 08:19:25.207598 140295626643200 summary_utils.py:349] Steps/second: 0.176850, Examples/second: 25.040864\n",
      "I0710 08:19:25.208676 140295626643200 trainer.py:508] step:  5162, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:102.89056 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2574002 log_pplx:4.5166097 loss:180.09981 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.64252\n",
      "I0710 08:19:29.023068 140295626643200 summary_utils.py:349] Steps/second: 0.176942, Examples/second: 25.059083\n",
      "I0710 08:19:29.023811 140295626643200 trainer.py:508] step:  5163, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:45.494431 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2590915 log_pplx:4.6459007 loss:116.49595 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.64648\n",
      "I0710 08:19:39.600570 140295626643200 summary_utils.py:349] Steps/second: 0.176697, Examples/second: 24.995657\n",
      "I0710 08:19:39.601370 140295626643200 trainer.py:508] step:  5164, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:185.55504 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2566516 log_pplx:3.9218597 loss:287.76645 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.65027\n",
      "I0710 08:19:45.831704 140295626643200 summary_utils.py:349] Steps/second: 0.176668, Examples/second: 24.974347\n",
      "I0710 08:19:45.832452 140295626643200 trainer.py:508] step:  5165, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:76.809547 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2576201 log_pplx:4.4984388 loss:175.88895 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.6546\n",
      "I0710 08:19:49.703129 140295626643200 summary_utils.py:349] Steps/second: 0.176757, Examples/second: 24.992139\n",
      "I0710 08:19:49.703921 140295626643200 trainer.py:508] step:  5166, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:39.735157 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2593474 log_pplx:4.5967355 loss:116.46979 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.65912\n",
      "I0710 08:19:56.007629 140295626643200 summary_utils.py:349] Steps/second: 0.176725, Examples/second: 24.970381\n",
      "I0710 08:19:56.008433 140295626643200 trainer.py:508] step:  5167, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:83.926384 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2582583 log_pplx:4.5101323 loss:194.33035 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.6637\n",
      "I0710 08:19:59.043064 140295626643200 summary_utils.py:349] Steps/second: 0.176855, Examples/second: 25.020889\n",
      "I0710 08:19:59.043886 140295626643200 trainer.py:508] step:  5168, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:33.886131 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2587628 log_pplx:4.4953041 loss:70.449844 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.66852\n",
      "I0710 08:20:01.244529 140295626643200 summary_utils.py:349] Steps/second: 0.177026, Examples/second: 25.148873\n",
      "I0710 08:20:01.245321 140295626643200 trainer.py:508] step:  5169, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:17.627712 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2576404 log_pplx:4.3387518 loss:31.922029 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:518.6731\n",
      "I0710 08:20:07.218003 140295626643200 summary_utils.py:349] Steps/second: 0.177010, Examples/second: 25.129235\n",
      "I0710 08:20:07.218817 140295626643200 trainer.py:508] step:  5170, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:82.437622 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2572882 log_pplx:4.5105357 loss:173.14819 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.67731\n",
      "I0710 08:20:13.680316 140295626643200 summary_utils.py:349] Steps/second: 0.176970, Examples/second: 25.106237\n",
      "I0710 08:20:13.681390 140295626643200 trainer.py:508] step:  5171, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:81.157364 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.257738 log_pplx:4.5159526 loss:180.35587 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.68158\n",
      "I0710 08:20:17.603437 140295626643200 summary_utils.py:349] Steps/second: 0.177055, Examples/second: 25.123386\n",
      "I0710 08:20:17.604212 140295626643200 trainer.py:508] step:  5172, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:42.991058 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2592316 log_pplx:4.5738683 loss:115.80463 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.68591\n",
      "I0710 08:20:28.306587 140295626643200 summary_utils.py:349] Steps/second: 0.176806, Examples/second: 25.059753\n",
      "I0710 08:20:28.307501 140295626643200 trainer.py:508] step:  5173, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:258.10611 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2565483 log_pplx:4.047205 loss:285.02441 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.69019\n",
      "I0710 08:20:34.916280 140295626643200 summary_utils.py:349] Steps/second: 0.176759, Examples/second: 25.035989\n",
      "I0710 08:20:34.917281 140295626643200 trainer.py:508] step:  5174, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:105.91441 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2578692 log_pplx:4.5309086 loss:184.7478 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.69531\n",
      "I0710 08:20:38.010336 140295626643200 summary_utils.py:349] Steps/second: 0.176885, Examples/second: 25.085489\n",
      "I0710 08:20:38.011132 140295626643200 trainer.py:508] step:  5175, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:23.663662 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2590852 log_pplx:4.4741845 loss:69.140129 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.70038\n",
      "I0710 08:20:41.855047 140295626643200 summary_utils.py:349] Steps/second: 0.176973, Examples/second: 25.103090\n",
      "I0710 08:20:41.855810 140295626643200 trainer.py:508] step:  5176, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:43.957035 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.259434 log_pplx:4.5939903 loss:116.40025 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.7052\n",
      "I0710 08:20:48.245148 140295626643200 summary_utils.py:349] Steps/second: 0.176937, Examples/second: 25.080859\n",
      "I0710 08:20:48.246047 140295626643200 trainer.py:508] step:  5177, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:148.94562 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2577049 log_pplx:4.4543438 loss:181.23611 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.70978\n",
      "I0710 08:20:53.023155 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 08:20:54.777287 140295626643200 summary_utils.py:349] Steps/second: 0.176894, Examples/second: 25.057724\n",
      "I0710 08:20:54.778360 140295626643200 trainer.py:508] step:  5178, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:81.034126 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2579957 log_pplx:4.5261254 loss:184.77908 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.71417\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 08:20:58.281358 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 08:20:58.781492 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00005177\n",
      "I0710 08:21:01.073107 140295626643200 summary_utils.py:349] Steps/second: 0.176863, Examples/second: 25.036303\n",
      "I0710 08:21:01.073876 140295626643200 trainer.py:508] step:  5179, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:79.750938 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2575359 log_pplx:4.4459214 loss:174.44682 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.71887\n",
      "I0710 08:21:04.172582 140295626643200 summary_utils.py:349] Steps/second: 0.176987, Examples/second: 25.085408\n",
      "I0710 08:21:04.173438 140295626643200 trainer.py:508] step:  5180, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:29.758694 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2588856 log_pplx:4.4412112 loss:68.786728 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.72369\n",
      "I0710 08:21:08.028780 140295626643200 summary_utils.py:349] Steps/second: 0.177075, Examples/second: 25.102804\n",
      "I0710 08:21:08.029661 140295626643200 trainer.py:508] step:  5181, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:51.278183 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2594866 log_pplx:4.6323051 loss:116.18401 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.72839\n",
      "I0710 08:21:13.995387 140295626643200 summary_utils.py:349] Steps/second: 0.177059, Examples/second: 25.083649\n",
      "I0710 08:21:13.996164 140295626643200 trainer.py:508] step:  5182, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:94.265427 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2576293 log_pplx:4.4927316 loss:178.08064 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.73328\n",
      "I0710 08:21:20.157384 140295626643200 summary_utils.py:349] Steps/second: 0.177034, Examples/second: 25.063214\n",
      "I0710 08:21:20.158250 140295626643200 trainer.py:508] step:  5183, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:85.983536 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2580096 log_pplx:4.4786038 loss:179.70398 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.7384\n",
      "I0710 08:21:24.071358 140295626643200 summary_utils.py:349] Steps/second: 0.177118, Examples/second: 25.080160\n",
      "I0710 08:21:24.072235 140295626643200 trainer.py:508] step:  5184, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:99.394424 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2600279 log_pplx:4.6923079 loss:118.5981 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.74365\n",
      "I0710 08:21:27.201403 140295626643200 summary_utils.py:349] Steps/second: 0.177240, Examples/second: 25.128709\n",
      "I0710 08:21:27.202270 140295626643200 trainer.py:508] step:  5185, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:41.627842 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2594699 log_pplx:4.476428 loss:70.066589 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.74811\n",
      "I0710 08:21:37.865029 140295626643200 summary_utils.py:349] Steps/second: 0.176997, Examples/second: 25.066539\n",
      "I0710 08:21:37.865845 140295626643200 trainer.py:508] step:  5186, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:434.60446 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2570552 log_pplx:4.1468391 loss:290.38239 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.75214\n",
      "I0710 08:21:45.932397 140295626643200 summary_utils.py:349] Steps/second: 0.176881, Examples/second: 25.033279\n",
      "I0710 08:21:45.933202 140295626643200 trainer.py:508] step:  5187, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:114.63013 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2579248 log_pplx:4.4248791 loss:173.06807 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.75684\n",
      "I0710 08:21:51.763703 140295626643200 summary_utils.py:349] Steps/second: 0.176872, Examples/second: 25.015354\n",
      "I0710 08:21:51.764475 140295626643200 trainer.py:508] step:  5188, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:173.32631 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.258064 log_pplx:4.5352573 loss:179.14265 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.76196\n",
      "I0710 08:21:55.607638 140295626643200 summary_utils.py:349] Steps/second: 0.176959, Examples/second: 25.032681\n",
      "I0710 08:21:55.608435 140295626643200 trainer.py:508] step:  5189, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:37.327702 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2595922 log_pplx:4.5522785 loss:112.72579 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.76764\n",
      "I0710 08:22:02.206050 140295626643200 summary_utils.py:349] Steps/second: 0.176914, Examples/second: 25.009605\n",
      "I0710 08:22:02.206821 140295626643200 trainer.py:508] step:  5190, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:129.02184 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2580801 log_pplx:4.4521942 loss:184.48779 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.77325\n",
      "I0710 08:22:04.394003 140295626643200 summary_utils.py:349] Steps/second: 0.177080, Examples/second: 25.133420\n",
      "I0710 08:22:04.394759 140295626643200 trainer.py:508] step:  5191, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:10.292603 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2579918 log_pplx:4.3310285 loss:31.298447 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:518.77887\n",
      "2020-07-10 08:22:04.995274: I lingvo/core/ops/record_yielder.cc:532] Epoch 3: total records 46838\n",
      "2020-07-10 08:22:04.995331: I lingvo/core/ops/record_yielder.cc:485] Epoch 3 /tmp/punctuator_data/train.txt\n",
      "I0710 08:22:07.492652 140295626643200 summary_utils.py:349] Steps/second: 0.177202, Examples/second: 25.181608\n",
      "I0710 08:22:07.493410 140295626643200 trainer.py:508] step:  5192, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:32.228333 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2591634 log_pplx:4.4276147 loss:70.201912 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.78424\n",
      "I0710 08:22:14.032372 140295626643200 summary_utils.py:349] Steps/second: 0.177159, Examples/second: 25.158740\n",
      "I0710 08:22:14.033264 140295626643200 trainer.py:508] step:  5193, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:126.95953 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2581158 log_pplx:4.6055794 loss:181.69009 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.78918\n",
      "I0710 08:22:23.942539 140295626643200 summary_utils.py:349] Steps/second: 0.176955, Examples/second: 25.102360\n",
      "I0710 08:22:23.943300 140295626643200 trainer.py:508] step:  5194, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:581.57227 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2570277 log_pplx:4.2157998 loss:303.74838 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.7934\n",
      "I0710 08:22:27.743746 140295626643200 summary_utils.py:349] Steps/second: 0.177044, Examples/second: 25.119736\n",
      "I0710 08:22:27.744544 140295626643200 trainer.py:508] step:  5195, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:176.77228 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.259838 log_pplx:4.709374 loss:120.17733 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.79974\n",
      "I0710 08:22:33.972738 140295626643200 summary_utils.py:349] Steps/second: 0.177016, Examples/second: 25.099195\n",
      "I0710 08:22:33.973672 140295626643200 trainer.py:508] step:  5196, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:423.3042 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2579423 log_pplx:4.6975403 loss:183.61511 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.80554\n",
      "I0710 08:22:40.548170 140295626643200 summary_utils.py:349] Steps/second: 0.176972, Examples/second: 25.076393\n",
      "I0710 08:22:40.549013 140295626643200 trainer.py:508] step:  5197, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:180.63728 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2579693 log_pplx:4.3484383 loss:170.78493 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.81024\n",
      "I0710 08:22:43.598621 140295626643200 summary_utils.py:349] Steps/second: 0.177095, Examples/second: 25.124486\n",
      "I0710 08:22:43.599374 140295626643200 trainer.py:508] step:  5198, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:75.446228 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2596281 log_pplx:4.552012 loss:72.085388 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.81458\n",
      "I0710 08:22:47.484812 140295626643200 summary_utils.py:349] Steps/second: 0.177179, Examples/second: 25.141177\n",
      "I0710 08:22:47.485634 140295626643200 trainer.py:508] step:  5199, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:242.7352 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2602072 log_pplx:4.9500575 loss:126.25741 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.81946\n",
      "I0710 08:22:54.013393 140295626643200 summary_utils.py:349] Steps/second: 0.177137, Examples/second: 25.118698\n",
      "I0710 08:22:54.014204 140295626643200 trainer.py:508] step:  5200, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:291.61697 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2582572 log_pplx:4.5098896 loss:180.39555 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.82587\n",
      "I0710 08:23:00.480558 140295626643200 summary_utils.py:349] Steps/second: 0.177098, Examples/second: 25.096708\n",
      "I0710 08:23:00.481403 140295626643200 trainer.py:508] step:  5201, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:177.5118 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.258267 log_pplx:4.3340588 loss:174.55423 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.83331\n",
      "I0710 08:23:04.291455 140295626643200 summary_utils.py:349] Steps/second: 0.177185, Examples/second: 25.113856\n",
      "I0710 08:23:04.292259 140295626643200 trainer.py:508] step:  5202, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:60.330021 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2602358 log_pplx:4.6735435 loss:118.0946 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.84082\n",
      "I0710 08:23:10.547868 140295626643200 summary_utils.py:349] Steps/second: 0.177156, Examples/second: 25.093341\n",
      "I0710 08:23:10.548774 140295626643200 trainer.py:508] step:  5203, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:218.62689 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2590711 log_pplx:4.5369029 loss:185.89961 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.84766\n",
      "I0710 08:23:19.886084 140295626643200 summary_utils.py:349] Steps/second: 0.176982, Examples/second: 25.041779\n",
      "I0710 08:23:19.886868 140295626643200 trainer.py:508] step:  5204, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:366.4133 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2579294 log_pplx:3.9228847 loss:290.8819 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.85339\n",
      "I0710 08:23:22.948247 140295626643200 summary_utils.py:349] Steps/second: 0.177104, Examples/second: 25.089320\n",
      "I0710 08:23:22.949097 140295626643200 trainer.py:508] step:  5205, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:110.43304 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2596992 log_pplx:4.8489923 loss:74.951027 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.85797\n",
      "I0710 08:23:29.534499 140295626643200 summary_utils.py:349] Steps/second: 0.177060, Examples/second: 25.066759\n",
      "I0710 08:23:29.535260 140295626643200 trainer.py:508] step:  5206, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:122.25614 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2583534 log_pplx:4.3682032 loss:172.2164 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.86206\n",
      "2020-07-10 08:23:32.011886: I lingvo/core/ops/record_batcher.cc:394] 3796 total seconds passed. Total records yielded: 96031. Total records skipped: 42\n",
      "2020-07-10 08:23:32.011948: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 08:23:32.011961: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 08:23:32.011977: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 08:23:32.011993: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 08:23:32.012018: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 08:23:32.012029: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 121\n",
      "2020-07-10 08:23:32.012038: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 08:23:32.012046: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 08:23:32.012061: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 08:23:32.012074: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "I0710 08:23:33.394033 140295626643200 summary_utils.py:349] Steps/second: 0.177143, Examples/second: 25.083483\n",
      "I0710 08:23:33.394951 140295626643200 trainer.py:508] step:  5207, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:109.8977 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2605397 log_pplx:4.721416 loss:119.89449 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.86591\n",
      "I0710 08:23:39.903223 140295626643200 summary_utils.py:349] Steps/second: 0.177103, Examples/second: 25.061504\n",
      "I0710 08:23:39.904011 140295626643200 trainer.py:508] step:  5208, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:127.93284 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2583069 log_pplx:4.3102922 loss:171.54962 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.86926\n",
      "I0710 08:23:45.870212 140295626643200 summary_utils.py:349] Steps/second: 0.177088, Examples/second: 25.043178\n",
      "I0710 08:23:45.870995 140295626643200 trainer.py:508] step:  5209, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:150.96405 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2586265 log_pplx:4.3976874 loss:172.1145 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.87329\n",
      "I0710 08:23:48.974356 140295626643200 summary_utils.py:349] Steps/second: 0.177206, Examples/second: 25.090116\n",
      "I0710 08:23:48.975122 140295626643200 trainer.py:508] step:  5210, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:29.729448 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2598565 log_pplx:4.4668455 loss:68.712646 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.87817\n",
      "I0710 08:23:55.443601 140295626643200 summary_utils.py:349] Steps/second: 0.177168, Examples/second: 25.068479\n",
      "I0710 08:23:55.444399 140295626643200 trainer.py:508] step:  5211, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:157.54539 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2588348 log_pplx:4.3820848 loss:178.78906 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.88287\n",
      "I0710 08:23:59.362047 140295626643200 summary_utils.py:349] Steps/second: 0.177248, Examples/second: 25.084699\n",
      "I0710 08:23:59.362820 140295626643200 trainer.py:508] step:  5212, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:35.351398 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2602735 log_pplx:4.4355874 loss:111.13918 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.88831\n",
      "I0710 08:24:08.476961 140295626643200 summary_utils.py:349] Steps/second: 0.177087, Examples/second: 25.035281\n",
      "I0710 08:24:08.477725 140295626643200 trainer.py:508] step:  5213, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:389.30884 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2577519 log_pplx:3.8608701 loss:281.3609 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.89374\n",
      "I0710 08:24:14.528305 140295626643200 summary_utils.py:349] Steps/second: 0.177068, Examples/second: 25.016581\n",
      "I0710 08:24:14.529192 140295626643200 trainer.py:508] step:  5214, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:81.242271 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2588162 log_pplx:4.3261428 loss:173.09981 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.90082\n",
      "I0710 08:24:16.735044 140295626643200 summary_utils.py:349] Steps/second: 0.177228, Examples/second: 25.135986\n",
      "I0710 08:24:16.735810 140295626643200 trainer.py:508] step:  5215, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:10.125409 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2587727 log_pplx:4.2788687 loss:31.523228 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:518.90802\n",
      "I0710 08:24:22.981419 140295626643200 summary_utils.py:349] Steps/second: 0.177200, Examples/second: 25.115889\n",
      "I0710 08:24:22.982513 140295626643200 trainer.py:508] step:  5216, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:234.58725 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2589172 log_pplx:4.3945937 loss:177.26691 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.91486\n",
      "I0710 08:24:26.774699 140295626643200 summary_utils.py:349] Steps/second: 0.177285, Examples/second: 25.132765\n",
      "I0710 08:24:26.775601 140295626643200 trainer.py:508] step:  5217, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:70.764977 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.260628 log_pplx:4.6108336 loss:116.50999 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.92102\n",
      "I0710 08:24:29.861257 140295626643200 summary_utils.py:349] Steps/second: 0.177403, Examples/second: 25.179246\n",
      "I0710 08:24:29.862050 140295626643200 trainer.py:508] step:  5218, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:27.952278 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2602977 log_pplx:4.4771838 loss:70.025963 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.9267\n",
      "I0710 08:24:36.114334 140295626643200 summary_utils.py:349] Steps/second: 0.177375, Examples/second: 25.159103\n",
      "I0710 08:24:36.115114 140295626643200 trainer.py:508] step:  5219, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:155.80405 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2588878 log_pplx:4.3276753 loss:172.78244 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.93201\n",
      "I0710 08:24:42.579259 140295626643200 summary_utils.py:349] Steps/second: 0.177336, Examples/second: 25.137643\n",
      "I0710 08:24:42.580105 140295626643200 trainer.py:508] step:  5220, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:89.129387 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.258919 log_pplx:4.3680091 loss:177.61417 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.93665\n",
      "I0710 08:24:46.453121 140295626643200 summary_utils.py:349] Steps/second: 0.177418, Examples/second: 25.153885\n",
      "I0710 08:24:46.453970 140295626643200 trainer.py:508] step:  5221, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:61.711704 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2604986 log_pplx:4.5623946 loss:115.02938 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.94122\n",
      "I0710 08:24:52.944184 140295626643200 summary_utils.py:349] Steps/second: 0.177378, Examples/second: 25.132322\n",
      "I0710 08:24:52.944940 140295626643200 trainer.py:508] step:  5222, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:112.89159 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2591332 log_pplx:4.3616104 loss:178.22629 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.9458\n",
      "I0710 08:25:03.673540 140295626643200 summary_utils.py:349] Steps/second: 0.177145, Examples/second: 25.073018\n",
      "I0710 08:25:03.674375 140295626643200 trainer.py:508] step:  5223, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:176.47327 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.258085 log_pplx:3.6461449 loss:265.25702 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:518.95068\n",
      "I0710 08:25:06.755543 140295626643200 summary_utils.py:349] Steps/second: 0.177262, Examples/second: 25.119134\n",
      "I0710 08:25:06.756455 140295626643200 trainer.py:508] step:  5224, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:48.582489 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2603751 log_pplx:4.5223136 loss:71.155777 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.95544\n",
      "I0710 08:25:13.101068 140295626643200 summary_utils.py:349] Steps/second: 0.177230, Examples/second: 25.098682\n",
      "I0710 08:25:13.101832 140295626643200 trainer.py:508] step:  5225, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:111.42253 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2592711 log_pplx:4.3325095 loss:176.98303 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.96039\n",
      "I0710 08:25:16.948744 140295626643200 summary_utils.py:349] Steps/second: 0.177312, Examples/second: 25.115005\n",
      "I0710 08:25:16.949576 140295626643200 trainer.py:508] step:  5226, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:57.790752 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.260707 log_pplx:4.4991422 loss:113.60335 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.96545\n",
      "I0710 08:25:23.305821 140295626643200 summary_utils.py:349] Steps/second: 0.177279, Examples/second: 25.094538\n",
      "I0710 08:25:23.306635 140295626643200 trainer.py:508] step:  5227, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:74.516342 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2589884 log_pplx:4.2428226 loss:169.44772 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.97058\n",
      "I0710 08:25:29.744351 140295626643200 summary_utils.py:349] Steps/second: 0.177243, Examples/second: 25.073615\n",
      "I0710 08:25:29.745155 140295626643200 trainer.py:508] step:  5228, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:85.227562 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2591242 log_pplx:4.2571702 loss:174.54398 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.97595\n",
      "I0710 08:25:32.823069 140295626643200 summary_utils.py:349] Steps/second: 0.177360, Examples/second: 25.119443\n",
      "I0710 08:25:32.823847 140295626643200 trainer.py:508] step:  5229, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:35.797791 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2601631 log_pplx:4.3609352 loss:68.275894 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:518.98145\n",
      "I0710 08:25:38.982276 140295626643200 summary_utils.py:349] Steps/second: 0.177336, Examples/second: 25.100323\n",
      "I0710 08:25:38.983049 140295626643200 trainer.py:508] step:  5230, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:100.98932 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2589403 log_pplx:4.3011189 loss:171.39958 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.98663\n",
      "I0710 08:25:42.747756 140295626643200 summary_utils.py:349] Steps/second: 0.177421, Examples/second: 25.117065\n",
      "I0710 08:25:42.748728 140295626643200 trainer.py:508] step:  5231, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:103.88703 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2608513 log_pplx:4.625504 loss:116.47598 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:518.99207\n",
      "I0710 08:25:49.139858 140295626643200 summary_utils.py:349] Steps/second: 0.177387, Examples/second: 25.096505\n",
      "I0710 08:25:49.140674 140295626643200 trainer.py:508] step:  5232, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:94.471413 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2596333 log_pplx:4.314188 loss:178.55347 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:518.99677\n",
      "I0710 08:25:55.463830 140295626643200 summary_utils.py:349] Steps/second: 0.177356, Examples/second: 25.076449\n",
      "I0710 08:25:55.464595 140295626643200 trainer.py:508] step:  5233, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:75.050682 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2592273 log_pplx:4.2299128 loss:169.88388 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.0014\n",
      "I0710 08:26:06.843595 140295626643200 summary_utils.py:349] Steps/second: 0.177097, Examples/second: 25.014105\n",
      "I0710 08:26:06.844465 140295626643200 trainer.py:508] step:  5234, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:196.91312 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2579993 log_pplx:3.6463149 loss:254.87744 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.00629\n",
      "I0710 08:26:10.713711 140295626643200 summary_utils.py:349] Steps/second: 0.177177, Examples/second: 25.030146\n",
      "I0710 08:26:10.714738 140295626643200 trainer.py:508] step:  5235, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:40.110813 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2607493 log_pplx:4.4524708 loss:111.36743 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.01147\n",
      "I0710 08:26:13.785358 140295626643200 summary_utils.py:349] Steps/second: 0.177292, Examples/second: 25.075576\n",
      "I0710 08:26:13.786462 140295626643200 trainer.py:508] step:  5236, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:21.867136 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2605469 log_pplx:4.3540936 loss:66.859138 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.01666\n",
      "I0710 08:26:19.920114 140295626643200 summary_utils.py:349] Steps/second: 0.177270, Examples/second: 25.056879\n",
      "I0710 08:26:19.920917 140295626643200 base_runner.py:111] step:  5237, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:136.4064 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2595674 log_pplx:4.3402109 loss:175.019 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.02161\n",
      "I0710 08:26:22.119356 140295626643200 summary_utils.py:349] Steps/second: 0.177425, Examples/second: 25.172520\n",
      "I0710 08:26:22.120157 140295626643200 trainer.py:508] step:  5238, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:11.656341 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2591338 log_pplx:4.2410297 loss:31.319012 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:519.02692\n",
      "I0710 08:26:28.335862 140295626643200 summary_utils.py:349] Steps/second: 0.177399, Examples/second: 25.153191\n",
      "I0710 08:26:28.336749 140295626643200 trainer.py:508] step:  5239, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:84.992119 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2592059 log_pplx:4.2500987 loss:171.59775 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.03192\n",
      "I0710 08:26:32.196355 140295626643200 summary_utils.py:349] Steps/second: 0.177478, Examples/second: 25.169070\n",
      "I0710 08:26:32.197179 140295626643200 trainer.py:508] step:  5240, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:37.976116 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2612671 log_pplx:4.4394841 loss:112.15247 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.03674\n",
      "I0710 08:26:38.818036 140295626643200 summary_utils.py:349] Steps/second: 0.177434, Examples/second: 25.147228\n",
      "I0710 08:26:38.818870 140295626643200 trainer.py:508] step:  5241, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:86.843002 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2595212 log_pplx:4.2297349 loss:173.6835 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.04132\n",
      "I0710 08:26:42.079493 140295626643200 summary_utils.py:349] Steps/second: 0.177540, Examples/second: 25.191042\n",
      "I0710 08:26:42.080285 140295626643200 trainer.py:508] step:  5242, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:23.959667 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.260396 log_pplx:4.3276157 loss:67.06115 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.04547\n",
      "I0710 08:26:43.338536 140295635035904 trainer.py:345] Write summary @5242\n",
      "I0710 08:26:53.762125 140295626643200 summary_utils.py:349] Steps/second: 0.177270, Examples/second: 25.137234\n",
      "I0710 08:26:53.763427 140295626643200 trainer.py:508] step:  5243, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:96.451973 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2591984 log_pplx:4.2934084 loss:168.40894 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.04944\n",
      "I0710 08:26:59.230507 140295626643200 summary_utils.py:349] Steps/second: 0.177278, Examples/second: 25.142886\n",
      "I0710 08:26:59.231539 140295626643200 trainer.py:508] step:  5244, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:38.186649 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2611107 log_pplx:4.4412656 loss:112.25301 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.05298\n",
      "I0710 08:27:08.411294 140295626643200 summary_utils.py:349] Steps/second: 0.177121, Examples/second: 25.105153\n",
      "I0710 08:27:08.412731 140295626643200 trainer.py:508] step:  5245, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:84.807846 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2594042 log_pplx:4.1911097 loss:173.40715 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.05658\n",
      "I0710 08:27:17.364858 140295626643200 summary_utils.py:349] Steps/second: 0.176975, Examples/second: 25.069014\n",
      "I0710 08:27:17.365690 140295626643200 trainer.py:508] step:  5246, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:69.479462 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.259228 log_pplx:4.1634603 loss:166.74657 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.06024\n",
      "I0710 08:27:32.035163 140295626643200 summary_utils.py:349] Steps/second: 0.176578, Examples/second: 24.987497\n",
      "I0710 08:27:32.036392 140295626643200 trainer.py:508] step:  5247, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:182.56212 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2586811 log_pplx:3.5620549 loss:257.62558 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.06427\n",
      "I0710 08:27:36.457171 140295626643200 summary_utils.py:349] Steps/second: 0.176632, Examples/second: 25.023644\n",
      "I0710 08:27:36.458456 140295626643200 trainer.py:508] step:  5248, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:28.787857 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.260837 log_pplx:4.4170532 loss:70.051704 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.06927\n",
      "I0710 08:27:42.281452 140295626643200 summary_utils.py:349] Steps/second: 0.176625, Examples/second: 25.027181\n",
      "I0710 08:27:42.282959 140295626643200 trainer.py:508] step:  5249, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:48.298965 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2611887 log_pplx:4.4956779 loss:111.99857 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.07428\n",
      "I0710 08:27:52.685586 140295635035904 trainer.py:354] Write summary done: step 5242\n",
      "I0710 08:27:53.118556 140295626643200 summary_utils.py:349] Steps/second: 0.176399, Examples/second: 24.979871\n",
      "I0710 08:27:53.119330 140295626643200 trainer.py:508] step:  5250, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:80.445892 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2598002 log_pplx:4.1886063 loss:171.73283 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.07959\n",
      "I0710 08:27:59.801190 140295626643200 summary_utils.py:349] Steps/second: 0.176355, Examples/second: 24.958400\n",
      "I0710 08:27:59.802049 140295626643200 trainer.py:508] step:  5251, steps/sec: 0.18, examples/sec: 24.96 grad_norm/all/loss:87.47406 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2599041 log_pplx:4.2193608 loss:171.51701 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.08521\n",
      "I0710 08:28:03.692731 140295626643200 summary_utils.py:349] Steps/second: 0.176432, Examples/second: 24.973914\n",
      "I0710 08:28:03.693542 140295626643200 trainer.py:508] step:  5252, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:43.850346 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2614326 log_pplx:4.4909267 loss:113.03102 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.09106\n",
      "I0710 08:28:06.758385 140295626643200 summary_utils.py:349] Steps/second: 0.176545, Examples/second: 25.018159\n",
      "I0710 08:28:06.759255 140295626643200 trainer.py:508] step:  5253, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:29.656368 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2607982 log_pplx:4.3571091 loss:67.603264 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.09644\n",
      "I0710 08:28:13.013533 140295626643200 summary_utils.py:349] Steps/second: 0.176520, Examples/second: 24.999327\n",
      "I0710 08:28:13.014508 140295626643200 trainer.py:508] step:  5254, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:78.437843 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2601091 log_pplx:4.1462827 loss:172.17436 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.10132\n",
      "I0710 08:28:19.368163 140295626643200 summary_utils.py:349] Steps/second: 0.176490, Examples/second: 24.979942\n",
      "I0710 08:28:19.369219 140295626643200 trainer.py:508] step:  5255, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:81.738213 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2597539 log_pplx:4.18999 loss:167.44247 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.10602\n",
      "I0710 08:28:30.459052 140295626643200 summary_utils.py:349] Steps/second: 0.176255, Examples/second: 24.921832\n",
      "I0710 08:28:30.459813 140295626643200 trainer.py:508] step:  5256, steps/sec: 0.18, examples/sec: 24.92 grad_norm/all/loss:182.93362 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2593673 log_pplx:3.6498425 loss:272.73447 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.11072\n",
      "I0710 08:28:34.317172 140295626643200 summary_utils.py:349] Steps/second: 0.176333, Examples/second: 24.937469\n",
      "I0710 08:28:34.317946 140295626643200 trainer.py:508] step:  5257, steps/sec: 0.18, examples/sec: 24.94 grad_norm/all/loss:43.394089 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.261408 log_pplx:4.4971023 loss:113.01781 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.11584\n",
      "I0710 08:28:40.761612 140295626643200 summary_utils.py:349] Steps/second: 0.176300, Examples/second: 24.917734\n",
      "I0710 08:28:40.762656 140295626643200 trainer.py:508] step:  5258, steps/sec: 0.18, examples/sec: 24.92 grad_norm/all/loss:128.45877 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2602034 log_pplx:4.3255982 loss:179.51233 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.12103\n",
      "I0710 08:28:47.034338 140295626643200 summary_utils.py:349] Steps/second: 0.176274, Examples/second: 24.899105\n",
      "I0710 08:28:47.035147 140295626643200 trainer.py:508] step:  5259, steps/sec: 0.18, examples/sec: 24.90 grad_norm/all/loss:85.627968 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2598486 log_pplx:4.2376089 loss:168.55089 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.12671\n",
      "I0710 08:28:50.120378 140295626643200 summary_utils.py:349] Steps/second: 0.176386, Examples/second: 24.942814\n",
      "I0710 08:28:50.121286 140295626643200 trainer.py:508] step:  5260, steps/sec: 0.18, examples/sec: 24.94 grad_norm/all/loss:28.811659 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.260994 log_pplx:4.330277 loss:68.066536 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.13275\n",
      "I0710 08:28:53.970391 140295626643200 summary_utils.py:349] Steps/second: 0.176464, Examples/second: 24.958405\n",
      "I0710 08:28:53.971311 140295626643200 trainer.py:508] step:  5261, steps/sec: 0.18, examples/sec: 24.96 grad_norm/all/loss:71.080246 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2615442 log_pplx:4.5362124 loss:113.03676 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.13849\n",
      "I0710 08:28:56.158116 140295626643200 summary_utils.py:349] Steps/second: 0.176613, Examples/second: 25.069830\n",
      "I0710 08:28:56.158916 140295626643200 trainer.py:508] step:  5262, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:11.159032 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2600352 log_pplx:4.3038449 loss:31.354181 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:519.14362\n",
      "I0710 08:29:02.600708 140295626643200 summary_utils.py:349] Steps/second: 0.176580, Examples/second: 25.050003\n",
      "I0710 08:29:02.601481 140295626643200 trainer.py:508] step:  5263, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:115.02162 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2600628 log_pplx:4.2100477 loss:170.98056 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.14838\n",
      "I0710 08:29:08.908310 140295626643200 summary_utils.py:349] Steps/second: 0.176552, Examples/second: 25.031060\n",
      "I0710 08:29:08.909268 140295626643200 trainer.py:508] step:  5264, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:111.43565 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2601335 log_pplx:4.1598229 loss:171.33269 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.15302\n",
      "I0710 08:29:12.747609 140295626643200 summary_utils.py:349] Steps/second: 0.176630, Examples/second: 25.046563\n",
      "I0710 08:29:12.748408 140295626643200 trainer.py:508] step:  5265, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:51.582336 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2616006 log_pplx:4.4519582 loss:110.71462 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.15729\n",
      "I0710 08:29:19.067717 140295626643200 summary_utils.py:349] Steps/second: 0.176602, Examples/second: 25.027595\n",
      "I0710 08:29:19.068493 140295626643200 trainer.py:508] step:  5266, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:70.20829 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2601805 log_pplx:4.2605524 loss:171.91328 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.1618\n",
      "I0710 08:29:22.095757 140295626643200 summary_utils.py:349] Steps/second: 0.176715, Examples/second: 25.071221\n",
      "I0710 08:29:22.096556 140295626643200 trainer.py:508] step:  5267, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:39.380005 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2612982 log_pplx:4.3632917 loss:67.051514 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.16638\n",
      "I0710 08:29:32.755805 140295626643200 summary_utils.py:349] Steps/second: 0.176501, Examples/second: 25.016348\n",
      "I0710 08:29:32.756588 140295626643200 trainer.py:508] step:  5268, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:227.18172 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2589446 log_pplx:3.6607919 loss:258.72647 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.17096\n",
      "I0710 08:29:39.395195 140295626643200 summary_utils.py:349] Steps/second: 0.176460, Examples/second: 24.995594\n",
      "I0710 08:29:39.395999 140295626643200 trainer.py:508] step:  5269, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:123.35129 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2602572 log_pplx:4.2470994 loss:174.55576 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.17578\n",
      "I0710 08:29:43.183727 140295626643200 summary_utils.py:349] Steps/second: 0.176540, Examples/second: 25.011322\n",
      "I0710 08:29:43.184528 140295626643200 trainer.py:508] step:  5270, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:36.362194 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2616539 log_pplx:4.3590584 loss:108.73125 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.18115\n",
      "I0710 08:29:49.580414 140295626643200 summary_utils.py:349] Steps/second: 0.176509, Examples/second: 24.992087\n",
      "I0710 08:29:49.581223 140295626643200 trainer.py:508] step:  5271, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:94.164665 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2600533 log_pplx:4.2379498 loss:170.36557 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.1864\n",
      "I0710 08:29:55.871984 140295626643200 summary_utils.py:349] Steps/second: 0.176482, Examples/second: 24.973541\n",
      "I0710 08:29:55.872744 140295626643200 trainer.py:508] step:  5272, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:152.1058 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2606341 log_pplx:4.2960086 loss:174.84756 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.19159\n",
      "I0710 08:29:58.915636 140295626643200 summary_utils.py:349] Steps/second: 0.176593, Examples/second: 25.016727\n",
      "I0710 08:29:58.916421 140295626643200 trainer.py:508] step:  5273, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:38.240845 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.261236 log_pplx:4.3819165 loss:66.824226 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.19659\n",
      "I0710 08:30:05.528543 140295626643200 summary_utils.py:349] Steps/second: 0.176553, Examples/second: 24.996261\n",
      "I0710 08:30:05.529313 140295626643200 trainer.py:508] step:  5274, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:75.077057 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2606217 log_pplx:4.2498393 loss:177.32454 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.20117\n",
      "I0710 08:30:09.441276 140295626643200 summary_utils.py:349] Steps/second: 0.176627, Examples/second: 25.011146\n",
      "I0710 08:30:09.442136 140295626643200 trainer.py:508] step:  5275, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:36.129162 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2617735 log_pplx:4.4457078 loss:111.3094 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.20605\n",
      "I0710 08:30:15.850460 140295626643200 summary_utils.py:349] Steps/second: 0.176595, Examples/second: 24.991957\n",
      "I0710 08:30:15.851271 140295626643200 trainer.py:508] step:  5276, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:131.521 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2601786 log_pplx:4.2730961 loss:166.75757 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.21088\n",
      "I0710 08:30:25.944724 140295626643200 summary_utils.py:349] Steps/second: 0.176409, Examples/second: 24.941353\n",
      "I0710 08:30:25.945526 140295626643200 trainer.py:508] step:  5277, steps/sec: 0.18, examples/sec: 24.94 grad_norm/all/loss:172.40779 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2595567 log_pplx:3.6294842 loss:262.23022 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.21649\n",
      "I0710 08:30:32.354207 140295626643200 summary_utils.py:349] Steps/second: 0.176378, Examples/second: 24.922344\n",
      "I0710 08:30:32.355005 140295626643200 trainer.py:508] step:  5278, steps/sec: 0.18, examples/sec: 24.92 grad_norm/all/loss:87.232407 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2601469 log_pplx:4.1371565 loss:166.62398 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.22302\n",
      "I0710 08:30:36.262353 140295626643200 summary_utils.py:349] Steps/second: 0.176452, Examples/second: 24.937230\n",
      "I0710 08:30:36.263172 140295626643200 trainer.py:508] step:  5279, steps/sec: 0.18, examples/sec: 24.94 grad_norm/all/loss:55.589794 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2620804 log_pplx:4.4559259 loss:112.59566 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.22968\n",
      "I0710 08:30:39.367300 140295626643200 summary_utils.py:349] Steps/second: 0.176559, Examples/second: 24.979664\n",
      "I0710 08:30:39.368250 140295626643200 trainer.py:508] step:  5280, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:33.734306 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2618057 log_pplx:4.3492522 loss:68.228897 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.23584\n",
      "I0710 08:30:45.702948 140295626643200 summary_utils.py:349] Steps/second: 0.176531, Examples/second: 24.961095\n",
      "I0710 08:30:45.703735 140295626643200 trainer.py:508] step:  5281, steps/sec: 0.18, examples/sec: 24.96 grad_norm/all/loss:126.37449 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2606686 log_pplx:4.3053889 loss:175.82132 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.24152\n",
      "I0710 08:30:52.215756 140295626643200 summary_utils.py:349] Steps/second: 0.176495, Examples/second: 24.941534\n",
      "I0710 08:30:52.216751 140295626643200 trainer.py:508] step:  5282, steps/sec: 0.18, examples/sec: 24.94 grad_norm/all/loss:99.999222 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2606502 log_pplx:4.237206 loss:177.59189 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.24658\n",
      "I0710 08:30:56.167327 140295626643200 summary_utils.py:349] Steps/second: 0.176567, Examples/second: 24.956076\n",
      "I0710 08:30:56.168197 140295626643200 trainer.py:508] step:  5283, steps/sec: 0.18, examples/sec: 24.96 grad_norm/all/loss:37.010914 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2617612 log_pplx:4.412045 loss:111.84534 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.2514\n",
      "I0710 08:31:02.519783 140295626643200 summary_utils.py:349] Steps/second: 0.176538, Examples/second: 24.937517\n",
      "I0710 08:31:02.520522 140295626643200 trainer.py:508] step:  5284, steps/sec: 0.18, examples/sec: 24.94 grad_norm/all/loss:89.740631 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2608093 log_pplx:4.1714091 loss:174.8342 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.25616\n",
      "I0710 08:31:02.865234 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 08:31:05.768371 140295626643200 summary_utils.py:349] Steps/second: 0.176639, Examples/second: 24.978840\n",
      "I0710 08:31:05.769138 140295626643200 trainer.py:508] step:  5285, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:32.43166 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2617712 log_pplx:4.4048367 loss:68.825577 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.26111\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 08:31:08.193671 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 08:31:08.659406 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00005284\n",
      "I0710 08:31:16.556934 140295626643200 summary_utils.py:349] Steps/second: 0.176426, Examples/second: 24.924785\n",
      "I0710 08:31:16.557787 140295626643200 trainer.py:508] step:  5286, steps/sec: 0.18, examples/sec: 24.92 grad_norm/all/loss:208.36734 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2595326 log_pplx:3.5966916 loss:255.99448 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.26605\n",
      "I0710 08:31:23.010574 140295626643200 summary_utils.py:349] Steps/second: 0.176393, Examples/second: 24.905768\n",
      "I0710 08:31:23.011347 140295626643200 trainer.py:508] step:  5287, steps/sec: 0.18, examples/sec: 24.91 grad_norm/all/loss:94.382355 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2605327 log_pplx:4.2481704 loss:176.24594 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.27173\n",
      "I0710 08:31:26.790910 140295626643200 summary_utils.py:349] Steps/second: 0.176471, Examples/second: 24.921241\n",
      "I0710 08:31:26.791703 140295626643200 trainer.py:508] step:  5288, steps/sec: 0.18, examples/sec: 24.92 grad_norm/all/loss:43.015636 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2621871 log_pplx:4.4722371 loss:112.6165 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.27789\n",
      "I0710 08:31:28.953698 140295626643200 summary_utils.py:349] Steps/second: 0.176617, Examples/second: 25.028832\n",
      "I0710 08:31:28.954583 140295626643200 trainer.py:508] step:  5289, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:9.7460051 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2602066 log_pplx:4.1526151 loss:30.374105 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:519.28387\n",
      "I0710 08:31:35.284311 140295626643200 summary_utils.py:349] Steps/second: 0.176589, Examples/second: 25.010435\n",
      "I0710 08:31:35.285183 140295626643200 trainer.py:508] step:  5290, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:118.14137 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2602953 log_pplx:4.1940212 loss:163.46198 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.28955\n",
      "I0710 08:31:38.313973 140295626643200 summary_utils.py:349] Steps/second: 0.176698, Examples/second: 25.052671\n",
      "I0710 08:31:38.314847 140295626643200 trainer.py:508] step:  5291, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:25.596827 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2619239 log_pplx:4.3626261 loss:67.688873 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.29486\n",
      "I0710 08:31:44.688272 140295626643200 summary_utils.py:349] Steps/second: 0.176668, Examples/second: 25.034023\n",
      "I0710 08:31:44.689074 140295626643200 trainer.py:508] step:  5292, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:149.87518 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2608513 log_pplx:4.3007565 loss:172.35281 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.2998\n",
      "I0710 08:31:48.517330 140295626643200 summary_utils.py:349] Steps/second: 0.176744, Examples/second: 25.049019\n",
      "I0710 08:31:48.518215 140295626643200 trainer.py:508] step:  5293, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:34.13102 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2618874 log_pplx:4.350317 loss:106.99062 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.30444\n",
      "I0710 08:31:55.261321 140295626643200 summary_utils.py:349] Steps/second: 0.176699, Examples/second: 25.028261\n",
      "I0710 08:31:55.262249 140295626643200 trainer.py:508] step:  5294, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:98.221725 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2607098 log_pplx:4.2198062 loss:169.056 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.30896\n",
      "I0710 08:32:06.173647 140295626643200 summary_utils.py:349] Steps/second: 0.176483, Examples/second: 24.973985\n",
      "I0710 08:32:06.174559 140295626643200 trainer.py:508] step:  5295, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:226.66574 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2598333 log_pplx:3.6317737 loss:267.7525 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.3139\n",
      "I0710 08:32:12.655925 140295626643200 summary_utils.py:349] Steps/second: 0.176450, Examples/second: 24.954947\n",
      "I0710 08:32:12.656737 140295626643200 trainer.py:508] step:  5296, steps/sec: 0.18, examples/sec: 24.95 grad_norm/all/loss:155.65465 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2608408 log_pplx:4.3013659 loss:177.70016 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.31989\n",
      "I0710 08:32:15.703040 140295626643200 summary_utils.py:349] Steps/second: 0.176557, Examples/second: 24.996754\n",
      "I0710 08:32:15.703841 140295626643200 trainer.py:508] step:  5297, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:33.230049 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2619554 log_pplx:4.3624964 loss:68.3685 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.32654\n",
      "I0710 08:32:19.544229 140295626643200 summary_utils.py:349] Steps/second: 0.176632, Examples/second: 25.011605\n",
      "I0710 08:32:19.545024 140295626643200 trainer.py:508] step:  5298, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:61.244911 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2626704 log_pplx:4.4856129 loss:112.89726 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.3327\n",
      "I0710 08:32:25.896484 140295626643200 summary_utils.py:349] Steps/second: 0.176603, Examples/second: 24.993324\n",
      "I0710 08:32:25.897234 140295626643200 trainer.py:508] step:  5299, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:119.85207 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2612281 log_pplx:4.1838956 loss:175.67133 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.3385\n",
      "I0710 08:32:32.373925 140295626643200 summary_utils.py:349] Steps/second: 0.176570, Examples/second: 24.974372\n",
      "I0710 08:32:32.374722 140295626643200 trainer.py:508] step:  5300, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:102.89452 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2610084 log_pplx:4.2423286 loss:174.41273 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.34406\n",
      "I0710 08:32:36.220607 140295626643200 summary_utils.py:349] Steps/second: 0.176644, Examples/second: 24.989154\n",
      "I0710 08:32:36.221624 140295626643200 trainer.py:508] step:  5301, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:46.982468 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2625054 log_pplx:4.4313335 loss:110.61717 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.34943\n",
      "I0710 08:32:39.224876 140295626643200 summary_utils.py:349] Steps/second: 0.176753, Examples/second: 25.030957\n",
      "I0710 08:32:39.225841 140295626643200 trainer.py:508] step:  5302, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:27.980104 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2622546 log_pplx:4.3577456 loss:67.493988 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.35437\n",
      "I0710 08:32:45.701622 140295626643200 summary_utils.py:349] Steps/second: 0.176719, Examples/second: 25.012011\n",
      "I0710 08:32:45.702511 140295626643200 trainer.py:508] step:  5303, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:99.354362 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2613881 log_pplx:4.1949482 loss:177.18411 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.35925\n",
      "I0710 08:32:51.895589 140295626643200 summary_utils.py:349] Steps/second: 0.176698, Examples/second: 24.994751\n",
      "I0710 08:32:51.896350 140295626643200 trainer.py:508] step:  5304, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:83.54525 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2608167 log_pplx:4.1651683 loss:170.9281 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.36438\n",
      "I0710 08:32:55.723970 140295626643200 summary_utils.py:349] Steps/second: 0.176772, Examples/second: 25.009553\n",
      "I0710 08:32:55.724768 140295626643200 trainer.py:508] step:  5305, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:46.324211 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2629964 log_pplx:4.4692063 loss:112.48434 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.36963\n",
      "I0710 08:33:01.938572 140295626643200 summary_utils.py:349] Steps/second: 0.176749, Examples/second: 24.992218\n",
      "I0710 08:33:01.939437 140295626643200 trainer.py:508] step:  5306, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:90.294968 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.260668 log_pplx:4.2008543 loss:166.45886 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.375\n",
      "I0710 08:33:12.863082 140295626643200 summary_utils.py:349] Steps/second: 0.176536, Examples/second: 24.938792\n",
      "I0710 08:33:12.863865 140295626643200 trainer.py:508] step:  5307, steps/sec: 0.18, examples/sec: 24.94 grad_norm/all/loss:216.02017 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2606654 log_pplx:3.6705139 loss:273.63681 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.38007\n",
      "I0710 08:33:15.919510 140295626643200 summary_utils.py:349] Steps/second: 0.176641, Examples/second: 24.979980\n",
      "I0710 08:33:15.920270 140295626643200 trainer.py:508] step:  5308, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:23.986624 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2624036 log_pplx:4.3933764 loss:69.229996 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.3847\n",
      "I0710 08:33:22.364520 140295626643200 summary_utils.py:349] Steps/second: 0.176610, Examples/second: 24.961450\n",
      "I0710 08:33:22.365329 140295626643200 trainer.py:508] step:  5309, steps/sec: 0.18, examples/sec: 24.96 grad_norm/all/loss:124.76598 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2615367 log_pplx:4.2660723 loss:173.62914 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.38934\n",
      "I0710 08:33:26.145371 140295626643200 summary_utils.py:349] Steps/second: 0.176686, Examples/second: 24.976450\n",
      "I0710 08:33:26.146344 140295626643200 trainer.py:508] step:  5310, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:39.879131 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2627813 log_pplx:4.4226766 loss:109.93115 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.39392\n",
      "I0710 08:33:32.235774 140295626643200 summary_utils.py:349] Steps/second: 0.176668, Examples/second: 24.959989\n",
      "I0710 08:33:32.236522 140295626643200 trainer.py:508] step:  5311, steps/sec: 0.18, examples/sec: 24.96 grad_norm/all/loss:83.255165 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2612246 log_pplx:4.2909083 loss:171.52907 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.39856\n",
      "I0710 08:33:34.436337 140295626643200 summary_utils.py:349] Steps/second: 0.176808, Examples/second: 25.064266\n",
      "I0710 08:33:34.437244 140295626643200 trainer.py:508] step:  5312, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:9.9941044 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2612185 log_pplx:4.2201581 loss:31.321486 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:519.40375\n",
      "I0710 08:33:40.838575 140295626643200 summary_utils.py:349] Steps/second: 0.176778, Examples/second: 25.045935\n",
      "I0710 08:33:40.839447 140295626643200 trainer.py:508] step:  5313, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:84.263222 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.261163 log_pplx:4.2294793 loss:169.49638 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.40869\n",
      "I0710 08:33:44.744175 140295626643200 summary_utils.py:349] Steps/second: 0.176848, Examples/second: 25.060087\n",
      "I0710 08:33:44.745017 140295626643200 trainer.py:508] step:  5314, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:39.167427 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2631018 log_pplx:4.4344668 loss:111.02794 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.414\n",
      "I0710 08:33:47.825077 140295626643200 summary_utils.py:349] Steps/second: 0.176952, Examples/second: 25.100753\n",
      "I0710 08:33:47.825865 140295626643200 trainer.py:508] step:  5315, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:24.515364 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2627946 log_pplx:4.3687067 loss:69.06311 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.41901\n",
      "I0710 08:33:54.295271 140295626643200 summary_utils.py:349] Steps/second: 0.176919, Examples/second: 25.082037\n",
      "I0710 08:33:54.296080 140295626643200 trainer.py:508] step:  5316, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:71.218079 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2614219 log_pplx:4.216795 loss:171.9398 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.42365\n",
      "I0710 08:33:58.058620 140295626643200 summary_utils.py:349] Steps/second: 0.176995, Examples/second: 25.096925\n",
      "I0710 08:33:58.059392 140295626643200 trainer.py:508] step:  5317, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:42.873905 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.262817 log_pplx:4.4282088 loss:113.55589 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.42841\n",
      "I0710 08:34:04.537867 140295626643200 summary_utils.py:349] Steps/second: 0.176961, Examples/second: 25.078207\n",
      "I0710 08:34:04.538925 140295626643200 trainer.py:508] step:  5318, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:77.830956 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2613708 log_pplx:4.2171817 loss:172.48274 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.43262\n",
      "I0710 08:34:14.186836 140295626643200 summary_utils.py:349] Steps/second: 0.176801, Examples/second: 25.032542\n",
      "I0710 08:34:14.187630 140295626643200 trainer.py:508] step:  5319, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:289.03992 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2606362 log_pplx:3.5644987 loss:250.22778 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.43726\n",
      "I0710 08:34:17.196390 140295626643200 summary_utils.py:349] Steps/second: 0.176907, Examples/second: 25.073360\n",
      "I0710 08:34:17.197362 140295626643200 trainer.py:508] step:  5320, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:28.277819 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2626261 log_pplx:4.3042636 loss:67.607201 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.44366\n",
      "I0710 08:34:23.396742 140295626643200 summary_utils.py:349] Steps/second: 0.176885, Examples/second: 25.056334\n",
      "I0710 08:34:23.397490 140295626643200 trainer.py:508] step:  5321, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:165.60706 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2611705 log_pplx:4.1795449 loss:166.39815 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.45001\n",
      "I0710 08:34:29.468162 140295626643200 summary_utils.py:349] Steps/second: 0.176869, Examples/second: 25.040083\n",
      "I0710 08:34:29.468913 140295626643200 trainer.py:508] step:  5322, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:124.00624 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2613441 log_pplx:4.3210959 loss:171.70952 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.4566\n",
      "I0710 08:34:33.315706 140295626643200 summary_utils.py:349] Steps/second: 0.176940, Examples/second: 25.054413\n",
      "I0710 08:34:33.316499 140295626643200 trainer.py:508] step:  5323, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:46.506859 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2629857 log_pplx:4.4751635 loss:113.38945 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.46326\n",
      "I0710 08:34:39.604873 140295626643200 summary_utils.py:349] Steps/second: 0.176915, Examples/second: 25.036975\n",
      "I0710 08:34:39.605653 140295626643200 trainer.py:508] step:  5324, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:105.4632 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2611985 log_pplx:4.194725 loss:163.96133 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.4693\n",
      "I0710 08:34:42.691627 140295626643200 summary_utils.py:349] Steps/second: 0.177017, Examples/second: 25.077122\n",
      "I0710 08:34:42.692495 140295626643200 trainer.py:508] step:  5325, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:41.354561 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.262688 log_pplx:4.3726587 loss:68.117821 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.47473\n",
      "I0710 08:34:49.097553 140295626643200 summary_utils.py:349] Steps/second: 0.176987, Examples/second: 25.059033\n",
      "I0710 08:34:49.098429 140295626643200 trainer.py:508] step:  5326, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:91.617966 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2618822 log_pplx:4.1966133 loss:172.37589 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.47968\n",
      "I0710 08:34:52.885073 140295626643200 summary_utils.py:349] Steps/second: 0.177061, Examples/second: 25.073621\n",
      "I0710 08:34:52.885863 140295626643200 trainer.py:508] step:  5327, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:45.534599 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2631193 log_pplx:4.3947964 loss:111.71023 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.48444\n",
      "I0710 08:34:59.032152 140295626643200 summary_utils.py:349] Steps/second: 0.177041, Examples/second: 25.057029\n",
      "I0710 08:34:59.032953 140295626643200 trainer.py:508] step:  5328, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:77.650742 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2616817 log_pplx:4.2674198 loss:174.27075 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.48907\n",
      "I0710 08:35:05.426666 140295626643200 summary_utils.py:349] Steps/second: 0.177012, Examples/second: 25.039098\n",
      "I0710 08:35:05.427480 140295626643200 trainer.py:508] step:  5329, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:142.65419 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2618994 log_pplx:4.2851281 loss:178.26132 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.49384\n",
      "I0710 08:35:09.340769 140295626643200 summary_utils.py:349] Steps/second: 0.177080, Examples/second: 25.052942\n",
      "I0710 08:35:09.341564 140295626643200 trainer.py:508] step:  5330, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:55.011814 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2628598 log_pplx:4.376451 loss:108.23509 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.49908\n",
      "I0710 08:35:12.431348 140295626643200 summary_utils.py:349] Steps/second: 0.177181, Examples/second: 25.092791\n",
      "I0710 08:35:12.432494 140295626643200 trainer.py:508] step:  5331, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:26.485104 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2627702 log_pplx:4.2887096 loss:66.106438 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.50452\n",
      "I0710 08:35:22.732987 140295626643200 summary_utils.py:349] Steps/second: 0.176998, Examples/second: 25.044145\n",
      "I0710 08:35:22.733810 140295626643200 trainer.py:508] step:  5332, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:318.51898 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2605956 log_pplx:3.6856632 loss:270.71198 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.50989\n",
      "I0710 08:35:29.026835 140295626643200 summary_utils.py:349] Steps/second: 0.176972, Examples/second: 25.026887\n",
      "I0710 08:35:29.027667 140295626643200 trainer.py:508] step:  5333, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:267.42697 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2620355 log_pplx:4.3761744 loss:183.30701 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.51538\n",
      "I0710 08:35:31.227575 140295626643200 summary_utils.py:349] Steps/second: 0.177108, Examples/second: 25.128423\n",
      "I0710 08:35:31.228342 140295626643200 trainer.py:508] step:  5334, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:11.797567 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2615184 log_pplx:4.248795 loss:30.78717 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:519.52026\n",
      "I0710 08:35:37.478811 140295626643200 summary_utils.py:349] Steps/second: 0.177084, Examples/second: 25.111317\n",
      "I0710 08:35:37.479642 140295626643200 trainer.py:508] step:  5335, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:96.841003 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2617375 log_pplx:4.1864476 loss:168.71384 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.5249\n",
      "I0710 08:35:41.367068 140295626643200 summary_utils.py:349] Steps/second: 0.177153, Examples/second: 25.125144\n",
      "I0710 08:35:41.367835 140295626643200 trainer.py:508] step:  5336, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:37.997871 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2634536 log_pplx:4.3848715 loss:110.1699 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.52985\n",
      "I0710 08:35:47.824003 140295626643200 summary_utils.py:349] Steps/second: 0.177121, Examples/second: 25.106938\n",
      "I0710 08:35:47.824977 140295626643200 base_runner.py:111] step:  5337, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:127.27912 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2621994 log_pplx:4.312355 loss:183.86801 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.53491\n",
      "I0710 08:35:50.874928 140295626643200 summary_utils.py:349] Steps/second: 0.177223, Examples/second: 25.146631\n",
      "I0710 08:35:50.875729 140295626643200 trainer.py:508] step:  5338, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:22.194462 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2628306 log_pplx:4.2859478 loss:66.214546 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.54047\n",
      "I0710 08:35:57.080794 140295626643200 summary_utils.py:349] Steps/second: 0.177201, Examples/second: 25.129826\n",
      "I0710 08:35:57.081697 140295626643200 trainer.py:508] step:  5339, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:82.624329 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2619923 log_pplx:4.2568212 loss:168.94258 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.54565\n",
      "I0710 08:36:00.933423 140295626643200 summary_utils.py:349] Steps/second: 0.177271, Examples/second: 25.143775\n",
      "I0710 08:36:00.934197 140295626643200 trainer.py:508] step:  5340, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:45.020939 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2638305 log_pplx:4.4306331 loss:111.92888 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.55096\n",
      "I0710 08:36:10.570469 140295626643200 summary_utils.py:349] Steps/second: 0.177115, Examples/second: 25.099207\n",
      "I0710 08:36:10.571250 140295626643200 trainer.py:508] step:  5341, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:264.42303 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2612911 log_pplx:3.6753476 loss:263.43054 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.55591\n",
      "I0710 08:36:17.063296 140295626643200 summary_utils.py:349] Steps/second: 0.177082, Examples/second: 25.080957\n",
      "I0710 08:36:17.064338 140295626643200 trainer.py:508] step:  5342, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:78.039391 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.261765 log_pplx:4.2783084 loss:170.65103 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.56085\n",
      "I0710 08:36:20.140075 140295626643200 summary_utils.py:349] Steps/second: 0.177182, Examples/second: 25.120269\n",
      "I0710 08:36:20.140872 140295626643200 trainer.py:508] step:  5343, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:24.939455 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2633489 log_pplx:4.3460212 loss:67.736832 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.56573\n",
      "I0710 08:36:23.133113 140295635035904 trainer.py:345] Write summary @5343\n",
      "I0710 08:36:26.984849 140295626643200 summary_utils.py:349] Steps/second: 0.177135, Examples/second: 25.117648\n",
      "I0710 08:36:26.985935 140295626643200 trainer.py:508] step:  5344, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:45.36697 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2635785 log_pplx:4.4207182 loss:110.79425 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.5705\n",
      "I0710 08:36:35.928982 140295626643200 summary_utils.py:349] Steps/second: 0.177007, Examples/second: 25.085958\n",
      "I0710 08:36:35.930165 140295626643200 trainer.py:508] step:  5345, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:102.06096 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2622844 log_pplx:4.3939099 loss:174.71281 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.57489\n",
      "I0710 08:36:45.607758 140295626643200 summary_utils.py:349] Steps/second: 0.176851, Examples/second: 25.050369\n",
      "I0710 08:36:45.609134 140295626643200 trainer.py:508] step:  5346, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:86.890121 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2617092 log_pplx:4.2393765 loss:166.28957 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.57971\n",
      "I0710 08:36:54.991159 140295626643200 summary_utils.py:349] Steps/second: 0.176707, Examples/second: 25.016542\n",
      "I0710 08:36:54.992619 140295626643200 trainer.py:508] step:  5347, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:93.710617 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2620568 log_pplx:4.1134062 loss:168.23831 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.58484\n",
      "I0710 08:37:00.576868 140295626643200 summary_utils.py:349] Steps/second: 0.176710, Examples/second: 25.020958\n",
      "I0710 08:37:00.578242 140295626643200 trainer.py:508] step:  5348, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:44.245857 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2637985 log_pplx:4.4154925 loss:110.85645 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.59033\n",
      "I0710 08:37:05.228262 140295626643200 summary_utils.py:349] Steps/second: 0.176749, Examples/second: 25.051348\n",
      "I0710 08:37:05.229677 140295626643200 trainer.py:508] step:  5349, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:25.972961 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2636141 log_pplx:4.3694243 loss:67.760216 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.59552\n",
      "I0710 08:37:21.232639 140295635035904 trainer.py:354] Write summary done: step 5343\n",
      "I0710 08:37:21.600742 140295626643200 summary_utils.py:349] Steps/second: 0.176338, Examples/second: 24.971063\n",
      "I0710 08:37:21.601813 140295626643200 trainer.py:508] step:  5350, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:207.34891 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2607659 log_pplx:3.4968195 loss:240.23152 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.60052\n",
      "I0710 08:37:27.972302 140295626643200 summary_utils.py:349] Steps/second: 0.176312, Examples/second: 24.953929\n",
      "I0710 08:37:27.973083 140295626643200 trainer.py:508] step:  5351, steps/sec: 0.18, examples/sec: 24.95 grad_norm/all/loss:113.65115 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2624596 log_pplx:4.302012 loss:172.61824 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.60724\n",
      "I0710 08:37:34.232032 140295626643200 summary_utils.py:349] Steps/second: 0.176289, Examples/second: 24.937445\n",
      "I0710 08:37:34.232785 140295626643200 trainer.py:508] step:  5352, steps/sec: 0.18, examples/sec: 24.94 grad_norm/all/loss:80.404922 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2621622 log_pplx:4.1781197 loss:165.92358 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.61377\n",
      "I0710 08:37:38.103515 140295626643200 summary_utils.py:349] Steps/second: 0.176358, Examples/second: 24.951159\n",
      "I0710 08:37:38.104334 140295626643200 trainer.py:508] step:  5353, steps/sec: 0.18, examples/sec: 24.95 grad_norm/all/loss:43.745255 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2640296 log_pplx:4.4357095 loss:112.58384 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.62036\n",
      "I0710 08:37:44.410792 140295626643200 summary_utils.py:349] Steps/second: 0.176333, Examples/second: 24.934460\n",
      "I0710 08:37:44.411607 140295626643200 trainer.py:508] step:  5354, steps/sec: 0.18, examples/sec: 24.93 grad_norm/all/loss:89.301865 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2622288 log_pplx:4.1939087 loss:176.72084 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.62677\n",
      "I0710 08:37:47.464869 140295626643200 summary_utils.py:349] Steps/second: 0.176433, Examples/second: 24.973251\n",
      "I0710 08:37:47.465804 140295626643200 trainer.py:508] step:  5355, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:23.460375 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.263562 log_pplx:4.2687354 loss:67.11586 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.63312\n",
      "I0710 08:37:49.647141 140295626643200 summary_utils.py:349] Steps/second: 0.176566, Examples/second: 25.071882\n",
      "I0710 08:37:49.647938 140295626643200 trainer.py:508] step:  5356, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:11.536228 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2620695 log_pplx:4.179522 loss:30.962749 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:519.63922\n",
      "I0710 08:37:56.271934 140295626643200 summary_utils.py:349] Steps/second: 0.176529, Examples/second: 25.053347\n",
      "I0710 08:37:56.272690 140295626643200 trainer.py:508] step:  5357, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:95.510254 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2623506 log_pplx:4.2022729 loss:166.41 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.64484\n",
      "I0710 08:38:00.096537 140295626643200 summary_utils.py:349] Steps/second: 0.176599, Examples/second: 25.067153\n",
      "I0710 08:38:00.097308 140295626643200 trainer.py:508] step:  5358, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:42.077824 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2639288 log_pplx:4.4441867 loss:112.07684 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.65002\n",
      "I0710 08:38:06.427182 140295626643200 summary_utils.py:349] Steps/second: 0.176574, Examples/second: 25.050249\n",
      "I0710 08:38:06.428015 140295626643200 trainer.py:508] step:  5359, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:105.6095 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2626302 log_pplx:4.3061914 loss:179.99881 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.65472\n",
      "I0710 08:38:13.035170 140295626643200 summary_utils.py:349] Steps/second: 0.176538, Examples/second: 25.031902\n",
      "I0710 08:38:13.035930 140295626643200 trainer.py:508] step:  5360, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:86.408516 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.262256 log_pplx:4.2673788 loss:169.14824 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.65961\n",
      "I0710 08:38:22.562520 140295626643200 summary_utils.py:349] Steps/second: 0.176392, Examples/second: 24.989413\n",
      "I0710 08:38:22.563388 140295626643200 trainer.py:508] step:  5361, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:250.18819 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2616915 log_pplx:3.6662767 loss:263.42194 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.66461\n",
      "I0710 08:38:25.583717 140295626643200 summary_utils.py:349] Steps/second: 0.176492, Examples/second: 25.028027\n",
      "I0710 08:38:25.584482 140295626643200 trainer.py:508] step:  5362, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:26.391693 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2637295 log_pplx:4.3335042 loss:67.253952 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.66986\n",
      "I0710 08:38:29.445311 140295626643200 summary_utils.py:349] Steps/second: 0.176560, Examples/second: 25.041569\n",
      "I0710 08:38:29.446123 140295626643200 trainer.py:508] step:  5363, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:97.877045 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2645537 log_pplx:4.4730635 loss:114.37063 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.67493\n",
      "I0710 08:38:35.663500 140295626643200 summary_utils.py:349] Steps/second: 0.176539, Examples/second: 25.025406\n",
      "I0710 08:38:35.664286 140295626643200 trainer.py:508] step:  5364, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:207.58115 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2626313 log_pplx:4.2374239 loss:170.66223 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.67938\n",
      "I0710 08:38:41.950051 140295626643200 summary_utils.py:349] Steps/second: 0.176516, Examples/second: 25.008922\n",
      "I0710 08:38:41.950792 140295626643200 trainer.py:508] step:  5365, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:118.68653 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2626641 log_pplx:4.2531772 loss:169.64859 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.68384\n",
      "I0710 08:38:45.762524 140295626643200 summary_utils.py:349] Steps/second: 0.176585, Examples/second: 25.022694\n",
      "I0710 08:38:45.763308 140295626643200 trainer.py:508] step:  5366, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:123.81981 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2646155 log_pplx:4.5424495 loss:114.32778 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.68848\n",
      "I0710 08:38:52.207633 140295626643200 summary_utils.py:349] Steps/second: 0.176556, Examples/second: 25.005406\n",
      "I0710 08:38:52.208381 140295626643200 trainer.py:508] step:  5367, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:119.71886 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2627575 log_pplx:4.221827 loss:174.41422 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.69379\n",
      "I0710 08:38:55.244561 140295626643200 summary_utils.py:349] Steps/second: 0.176654, Examples/second: 25.043683\n",
      "I0710 08:38:55.245372 140295626643200 trainer.py:508] step:  5368, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:33.617271 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2637513 log_pplx:4.2987194 loss:65.958473 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.69958\n",
      "I0710 08:39:01.654390 140295626643200 summary_utils.py:349] Steps/second: 0.176626, Examples/second: 25.026588\n",
      "I0710 08:39:01.655411 140295626643200 trainer.py:508] step:  5369, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:103.20597 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2625353 log_pplx:4.1609292 loss:166.80124 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.70526\n",
      "I0710 08:39:10.906436 140295626643200 summary_utils.py:349] Steps/second: 0.176492, Examples/second: 24.986004\n",
      "I0710 08:39:10.907244 140295626643200 trainer.py:508] step:  5370, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:362.78195 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2616817 log_pplx:3.7101285 loss:261.74957 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.71082\n",
      "I0710 08:39:17.304827 140295626643200 summary_utils.py:349] Steps/second: 0.176465, Examples/second: 24.969105\n",
      "I0710 08:39:17.305603 140295626643200 trainer.py:508] step:  5371, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:82.291489 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2626299 log_pplx:4.0890441 loss:163.10176 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.71606\n",
      "I0710 08:39:21.098400 140295626643200 summary_utils.py:349] Steps/second: 0.176534, Examples/second: 24.982906\n",
      "I0710 08:39:21.099173 140295626643200 trainer.py:508] step:  5372, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:65.169312 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2644321 log_pplx:4.4737325 loss:112.37457 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.72119\n",
      "I0710 08:39:27.345206 140295626643200 summary_utils.py:349] Steps/second: 0.176513, Examples/second: 24.966846\n",
      "I0710 08:39:27.346060 140295626643200 trainer.py:508] step:  5373, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:166.05186 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.262969 log_pplx:4.2905536 loss:173.39198 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.72595\n",
      "I0710 08:39:30.423395 140295626643200 summary_utils.py:349] Steps/second: 0.176609, Examples/second: 25.004646\n",
      "I0710 08:39:30.424239 140295626643200 trainer.py:508] step:  5374, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:58.302597 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.263499 log_pplx:4.3799043 loss:65.578804 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.73157\n",
      "I0710 08:39:36.879417 140295626643200 summary_utils.py:349] Steps/second: 0.176580, Examples/second: 24.987487\n",
      "I0710 08:39:36.880226 140295626643200 trainer.py:508] step:  5375, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:125.66409 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2628781 log_pplx:4.3070264 loss:170.82742 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.73663\n",
      "I0710 08:39:40.669695 140295626643200 summary_utils.py:349] Steps/second: 0.176649, Examples/second: 25.001234\n",
      "I0710 08:39:40.670624 140295626643200 trainer.py:508] step:  5376, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:63.176918 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2645011 log_pplx:4.3845429 loss:111.20296 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.74231\n",
      "I0710 08:39:46.702548 140295626643200 summary_utils.py:349] Steps/second: 0.176635, Examples/second: 24.986340\n",
      "I0710 08:39:46.703386 140295626643200 trainer.py:508] step:  5377, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:94.666611 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2631119 log_pplx:4.2477984 loss:167.78804 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.74756\n",
      "I0710 08:39:48.887494 140295626643200 summary_utils.py:349] Steps/second: 0.176764, Examples/second: 25.082480\n",
      "I0710 08:39:48.888294 140295626643200 trainer.py:508] step:  5378, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:11.349066 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2626259 log_pplx:4.1385217 loss:30.13361 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:519.75323\n",
      "I0710 08:39:55.260017 140295626643200 summary_utils.py:349] Steps/second: 0.176738, Examples/second: 25.065721\n",
      "I0710 08:39:55.260864 140295626643200 trainer.py:508] step:  5379, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:87.481293 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2629498 log_pplx:4.2502041 loss:171.60199 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.75854\n",
      "I0710 08:39:58.347634 140295626643200 summary_utils.py:349] Steps/second: 0.176833, Examples/second: 25.103187\n",
      "I0710 08:39:58.348427 140295626643200 trainer.py:508] step:  5380, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:24.436226 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2640806 log_pplx:4.3373213 loss:66.940453 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.76379\n",
      "I0710 08:40:02.268906 140295626643200 summary_utils.py:349] Steps/second: 0.176897, Examples/second: 25.116090\n",
      "I0710 08:40:02.269691 140295626643200 trainer.py:508] step:  5381, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:44.585354 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2647234 log_pplx:4.3492265 loss:111.39457 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.7688\n",
      "I0710 08:40:12.480217 140295626643200 summary_utils.py:349] Steps/second: 0.176729, Examples/second: 25.070817\n",
      "I0710 08:40:12.481042 140295626643200 trainer.py:508] step:  5382, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:427.88028 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2619364 log_pplx:3.7676518 loss:266.37292 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.77368\n",
      "I0710 08:40:18.895942 140295626643200 summary_utils.py:349] Steps/second: 0.176701, Examples/second: 25.053930\n",
      "I0710 08:40:18.896712 140295626643200 trainer.py:508] step:  5383, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:265.73502 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2632636 log_pplx:4.4212503 loss:182.59767 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.78064\n",
      "I0710 08:40:25.192545 140295626643200 summary_utils.py:349] Steps/second: 0.176677, Examples/second: 25.037711\n",
      "I0710 08:40:25.193319 140295626643200 trainer.py:508] step:  5384, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:207.20329 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2629086 log_pplx:4.2649989 loss:172.19933 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.78668\n",
      "I0710 08:40:29.101637 140295626643200 summary_utils.py:349] Steps/second: 0.176742, Examples/second: 25.050660\n",
      "I0710 08:40:29.102458 140295626643200 trainer.py:508] step:  5385, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:67.024292 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2647142 log_pplx:4.4524627 loss:111.81248 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.79199\n",
      "I0710 08:40:32.190828 140295626643200 summary_utils.py:349] Steps/second: 0.176836, Examples/second: 25.087863\n",
      "I0710 08:40:32.191860 140295626643200 trainer.py:508] step:  5386, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:34.651714 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2641058 log_pplx:4.2186456 loss:63.90588 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.79755\n",
      "I0710 08:40:38.500804 140295626643200 summary_utils.py:349] Steps/second: 0.176812, Examples/second: 25.071575\n",
      "I0710 08:40:38.501682 140295626643200 trainer.py:508] step:  5387, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:126.85411 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2637749 log_pplx:4.2748365 loss:183.76451 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.8031\n",
      "I0710 08:40:44.855690 140295626643200 summary_utils.py:349] Steps/second: 0.176787, Examples/second: 25.055095\n",
      "I0710 08:40:44.856548 140295626643200 trainer.py:508] step:  5388, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:130.96103 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2632192 log_pplx:4.3099146 loss:175.79065 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.80896\n",
      "I0710 08:40:48.740475 140295626643200 summary_utils.py:349] Steps/second: 0.176852, Examples/second: 25.068103\n",
      "I0710 08:40:48.741263 140295626643200 trainer.py:508] step:  5389, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:66.895645 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.265049 log_pplx:4.5405078 loss:115.86809 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.81537\n",
      "I0710 08:40:55.211522 140295626643200 summary_utils.py:349] Steps/second: 0.176822, Examples/second: 25.051060\n",
      "I0710 08:40:55.212339 140295626643200 trainer.py:508] step:  5390, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:93.945206 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2630829 log_pplx:4.218328 loss:167.09851 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.82196\n",
      "I0710 08:41:06.106458 140295626643200 summary_utils.py:349] Steps/second: 0.176630, Examples/second: 25.002884\n",
      "I0710 08:41:06.107248 140295626643200 trainer.py:508] step:  5391, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:314.44495 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.262483 log_pplx:3.6765945 loss:266.73697 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.82831\n",
      "I0710 08:41:09.200869 140295626643200 summary_utils.py:349] Steps/second: 0.176724, Examples/second: 25.039806\n",
      "I0710 08:41:09.201679 140295626643200 trainer.py:508] step:  5392, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:72.226784 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2646892 log_pplx:4.4735055 loss:70.545074 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.8338\n",
      "I0710 08:41:11.441101 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 08:41:15.734270 140295626643200 summary_utils.py:349] Steps/second: 0.176692, Examples/second: 25.022550\n",
      "I0710 08:41:15.735133 140295626643200 trainer.py:508] step:  5393, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:78.389305 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2631853 log_pplx:4.1968174 loss:169.55144 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.83875\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 08:41:16.677757 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 08:41:17.193995 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00005392\n",
      "I0710 08:41:22.210350 140295626643200 summary_utils.py:349] Steps/second: 0.176662, Examples/second: 25.005637\n",
      "I0710 08:41:22.211167 140295626643200 trainer.py:508] step:  5394, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:126.36889 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2636583 log_pplx:4.2795095 loss:179.09749 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.84351\n",
      "I0710 08:41:26.045006 140295626643200 summary_utils.py:349] Steps/second: 0.176729, Examples/second: 25.018843\n",
      "I0710 08:41:26.045778 140295626643200 trainer.py:508] step:  5395, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:74.925835 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.264612 log_pplx:4.4432507 loss:110.97019 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.84869\n",
      "I0710 08:41:32.333073 140295626643200 summary_utils.py:349] Steps/second: 0.176706, Examples/second: 25.002937\n",
      "I0710 08:41:32.334163 140295626643200 trainer.py:508] step:  5396, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:110.50411 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2632898 log_pplx:4.3119678 loss:169.08304 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.85333\n",
      "I0710 08:41:38.785442 140295626643200 summary_utils.py:349] Steps/second: 0.176677, Examples/second: 24.986229\n",
      "I0710 08:41:38.786234 140295626643200 trainer.py:508] step:  5397, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:91.357895 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2635695 log_pplx:4.2642379 loss:174.62056 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.8584\n",
      "I0710 08:41:41.847397 140295626643200 summary_utils.py:349] Steps/second: 0.176771, Examples/second: 25.023081\n",
      "I0710 08:41:41.848270 140295626643200 trainer.py:508] step:  5398, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:25.569286 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2646322 log_pplx:4.2646623 loss:67.001846 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.86359\n",
      "I0710 08:41:45.738394 140295626643200 summary_utils.py:349] Steps/second: 0.176835, Examples/second: 25.035930\n",
      "I0710 08:41:45.739184 140295626643200 trainer.py:508] step:  5399, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:68.347176 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2652324 log_pplx:4.4344134 loss:111.9135 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.86853\n",
      "I0710 08:41:52.052577 140295626643200 summary_utils.py:349] Steps/second: 0.176811, Examples/second: 25.019933\n",
      "I0710 08:41:52.053392 140295626643200 trainer.py:508] step:  5400, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:94.81208 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2637619 log_pplx:4.2072868 loss:169.76404 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.87292\n",
      "I0710 08:41:58.568588 140295626643200 summary_utils.py:349] Steps/second: 0.176780, Examples/second: 25.002944\n",
      "I0710 08:41:58.569337 140295626643200 trainer.py:508] step:  5401, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:87.382454 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2640744 log_pplx:4.2895899 loss:182.6293 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.87769\n",
      "I0710 08:42:09.018486 140295626643200 summary_utils.py:349] Steps/second: 0.176607, Examples/second: 24.957766\n",
      "I0710 08:42:09.019422 140295626643200 trainer.py:508] step:  5402, steps/sec: 0.18, examples/sec: 24.96 grad_norm/all/loss:409.60791 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2626736 log_pplx:3.6346114 loss:263.60019 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.88269\n",
      "I0710 08:42:11.335026 140295626643200 summary_utils.py:349] Steps/second: 0.176728, Examples/second: 25.050453\n",
      "I0710 08:42:11.335876 140295626643200 trainer.py:508] step:  5403, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:10.324944 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2632729 log_pplx:4.1121507 loss:30.238766 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:519.88898\n",
      "I0710 08:42:15.490215 140295626643200 summary_utils.py:349] Steps/second: 0.176782, Examples/second: 25.061853\n",
      "I0710 08:42:15.491074 140295626643200 trainer.py:508] step:  5404, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:72.114838 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2654196 log_pplx:4.5255265 loss:114.43925 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.89508\n",
      "I0710 08:42:18.688805 140295626643200 summary_utils.py:349] Steps/second: 0.176871, Examples/second: 25.097683\n",
      "I0710 08:42:18.689690 140295626643200 trainer.py:508] step:  5405, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:40.616795 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2650334 log_pplx:4.3989706 loss:69.283791 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.90063\n",
      "I0710 08:42:25.271282 140295626643200 summary_utils.py:349] Steps/second: 0.176837, Examples/second: 25.080344\n",
      "I0710 08:42:25.272040 140295626643200 trainer.py:508] step:  5406, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:801.37457 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2634009 log_pplx:5.1842561 loss:210.02715 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.90619\n",
      "I0710 08:42:31.821529 140295626643200 summary_utils.py:349] Steps/second: 0.176805, Examples/second: 25.063216\n",
      "I0710 08:42:31.822319 140295626643200 trainer.py:508] step:  5407, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:284.39206 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2639723 log_pplx:4.4963231 loss:185.07991 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.91077\n",
      "I0710 08:42:35.651677 140295626643200 summary_utils.py:349] Steps/second: 0.176870, Examples/second: 25.076213\n",
      "I0710 08:42:35.652502 140295626643200 trainer.py:508] step:  5408, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:193.87531 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2650918 log_pplx:4.7436504 loss:118.79881 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.9151\n",
      "I0710 08:42:41.821862 140295626643200 summary_utils.py:349] Steps/second: 0.176852, Examples/second: 25.061058\n",
      "I0710 08:42:41.822647 140295626643200 trainer.py:508] step:  5409, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:352.40939 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2638237 log_pplx:4.5884771 loss:185.43184 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.92065\n",
      "I0710 08:42:47.772241 140295626643200 summary_utils.py:349] Steps/second: 0.176841, Examples/second: 25.047056\n",
      "I0710 08:42:47.773013 140295626643200 trainer.py:508] step:  5410, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:341.39807 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2632644 log_pplx:4.5878153 loss:176.68826 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.92737\n",
      "I0710 08:42:50.858793 140295626643200 summary_utils.py:349] Steps/second: 0.176933, Examples/second: 25.083230\n",
      "I0710 08:42:50.859776 140295626643200 trainer.py:508] step:  5411, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:27.087814 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.265047 log_pplx:4.339273 loss:69.140205 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.935\n",
      "I0710 08:43:00.837333 140295626643200 summary_utils.py:349] Steps/second: 0.176779, Examples/second: 25.040744\n",
      "I0710 08:43:00.838158 140295626643200 trainer.py:508] step:  5412, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:267.90436 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2628896 log_pplx:3.8585055 loss:283.31076 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.9422\n",
      "I0710 08:43:04.755978 140295626643200 summary_utils.py:349] Steps/second: 0.176841, Examples/second: 25.053235\n",
      "I0710 08:43:04.756766 140295626643200 trainer.py:508] step:  5413, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:117.42164 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2654914 log_pplx:4.5236325 loss:113.45836 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.94867\n",
      "I0710 08:43:11.080724 140295626643200 summary_utils.py:349] Steps/second: 0.176817, Examples/second: 25.037417\n",
      "I0710 08:43:11.081547 140295626643200 trainer.py:508] step:  5414, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:138.96094 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2635669 log_pplx:4.292213 loss:170.07895 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.95459\n",
      "I0710 08:43:17.317261 140295626643200 summary_utils.py:349] Steps/second: 0.176796, Examples/second: 25.022084\n",
      "I0710 08:43:17.318173 140295626643200 trainer.py:508] step:  5415, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:123.42595 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2638812 log_pplx:4.4365573 loss:179.9024 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.96033\n",
      "I0710 08:43:23.993772 140295626643200 summary_utils.py:349] Steps/second: 0.176760, Examples/second: 25.004577\n",
      "I0710 08:43:23.994573 140295626643200 trainer.py:508] step:  5416, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:130.93356 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2637926 log_pplx:4.4227209 loss:173.70236 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.96576\n",
      "I0710 08:43:27.081310 140295626643200 summary_utils.py:349] Steps/second: 0.176851, Examples/second: 25.040509\n",
      "I0710 08:43:27.082096 140295626643200 trainer.py:508] step:  5417, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:81.683632 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2650284 log_pplx:4.6385808 loss:72.60466 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.97089\n",
      "I0710 08:43:30.942993 140295626643200 summary_utils.py:349] Steps/second: 0.176915, Examples/second: 25.053221\n",
      "I0710 08:43:30.944004 140295626643200 trainer.py:508] step:  5418, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:104.11626 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2652485 log_pplx:4.6232781 loss:114.83067 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.97546\n",
      "I0710 08:43:37.393453 140295626643200 summary_utils.py:349] Steps/second: 0.176886, Examples/second: 25.036855\n",
      "I0710 08:43:37.394366 140295626643200 trainer.py:508] step:  5419, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:146.18739 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2641385 log_pplx:4.4328647 loss:177.42542 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.97949\n",
      "I0710 08:43:43.595082 140295626643200 summary_utils.py:349] Steps/second: 0.176867, Examples/second: 25.021778\n",
      "I0710 08:43:43.595858 140295626643200 trainer.py:508] step:  5420, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:210.79871 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2641078 log_pplx:4.5025873 loss:182.46736 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:519.98413\n",
      "I0710 08:43:47.449366 140295626643200 summary_utils.py:349] Steps/second: 0.176931, Examples/second: 25.034500\n",
      "I0710 08:43:47.450150 140295626643200 trainer.py:508] step:  5421, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:58.492607 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2656882 log_pplx:4.5323515 loss:114.07362 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:519.98932\n",
      "I0710 08:43:50.482984 140295626643200 summary_utils.py:349] Steps/second: 0.177023, Examples/second: 25.070515\n",
      "I0710 08:43:50.483780 140295626643200 trainer.py:508] step:  5422, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:27.909349 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2652234 log_pplx:4.3471875 loss:68.281403 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:519.9942\n",
      "I0710 08:44:00.977718 140295626643200 summary_utils.py:349] Steps/second: 0.176852, Examples/second: 25.025981\n",
      "I0710 08:44:00.978759 140295626643200 trainer.py:508] step:  5423, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:371.01917 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2633344 log_pplx:3.8314664 loss:288.98834 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:519.9989\n",
      "I0710 08:44:07.328282 140295626643200 summary_utils.py:349] Steps/second: 0.176828, Examples/second: 25.010246\n",
      "I0710 08:44:07.329051 140295626643200 trainer.py:508] step:  5424, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:91.841751 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2641007 log_pplx:4.3581843 loss:177.10574 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.00494\n",
      "I0710 08:44:13.622128 140295626643200 summary_utils.py:349] Steps/second: 0.176805, Examples/second: 24.994833\n",
      "I0710 08:44:13.622949 140295626643200 trainer.py:508] step:  5425, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:95.569496 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2644188 log_pplx:4.2646103 loss:177.08797 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.01099\n",
      "I0710 08:44:15.833776 140295626643200 summary_utils.py:349] Steps/second: 0.176926, Examples/second: 25.085729\n",
      "I0710 08:44:15.834567 140295626643200 trainer.py:508] step:  5426, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:12.709465 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2641506 log_pplx:4.2741442 loss:30.83728 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:520.01703\n",
      "I0710 08:44:19.693418 140295626643200 summary_utils.py:349] Steps/second: 0.176989, Examples/second: 25.098293\n",
      "I0710 08:44:19.694407 140295626643200 trainer.py:508] step:  5427, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:78.562126 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2656398 log_pplx:4.5148463 loss:112.47611 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.02277\n",
      "I0710 08:44:26.042364 140295626643200 summary_utils.py:349] Steps/second: 0.176965, Examples/second: 25.082533\n",
      "I0710 08:44:26.043223 140295626643200 trainer.py:508] step:  5428, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:158.65401 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2643535 log_pplx:4.3927245 loss:179.00352 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.02789\n",
      "I0710 08:44:29.130361 140295626643200 summary_utils.py:349] Steps/second: 0.177055, Examples/second: 25.117973\n",
      "I0710 08:44:29.131185 140295626643200 trainer.py:508] step:  5429, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:33.776737 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2655526 log_pplx:4.441103 loss:68.490135 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.03271\n",
      "I0710 08:44:35.688281 140295626643200 summary_utils.py:349] Steps/second: 0.177023, Examples/second: 25.101178\n",
      "I0710 08:44:35.689028 140295626643200 trainer.py:508] step:  5430, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:99.083191 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2642053 log_pplx:4.3247256 loss:174.82703 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.03729\n",
      "I0710 08:44:39.594764 140295626643200 summary_utils.py:349] Steps/second: 0.177084, Examples/second: 25.113448\n",
      "I0710 08:44:39.595539 140295626643200 trainer.py:508] step:  5431, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:46.749847 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2660809 log_pplx:4.5172806 loss:112.67792 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.04211\n",
      "I0710 08:44:45.816856 140295626643200 summary_utils.py:349] Steps/second: 0.177064, Examples/second: 25.098361\n",
      "I0710 08:44:45.817662 140295626643200 trainer.py:508] step:  5432, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:85.732346 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2645644 log_pplx:4.3183217 loss:176.78128 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.04675\n",
      "I0710 08:44:54.461431 140295626643200 summary_utils.py:349] Steps/second: 0.176959, Examples/second: 25.063411\n",
      "I0710 08:44:54.462281 140295626643200 trainer.py:508] step:  5433, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:395.49164 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2633201 log_pplx:3.9046226 loss:278.3996 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:520.05164\n",
      "I0710 08:45:01.145665 140295626643200 summary_utils.py:349] Steps/second: 0.176923, Examples/second: 25.046146\n",
      "I0710 08:45:01.146614 140295626643200 trainer.py:508] step:  5434, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:92.190025 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2644492 log_pplx:4.3048444 loss:175.42241 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.05627\n",
      "I0710 08:45:04.226319 140295626643200 summary_utils.py:349] Steps/second: 0.177013, Examples/second: 25.081399\n",
      "I0710 08:45:04.227105 140295626643200 trainer.py:508] step:  5435, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:44.960182 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2657263 log_pplx:4.3589587 loss:68.381165 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.06134\n",
      "I0710 08:45:08.053580 140295626643200 summary_utils.py:349] Steps/second: 0.177076, Examples/second: 25.094007\n",
      "I0710 08:45:08.054496 140295626643200 trainer.py:508] step:  5436, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:59.319744 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2660795 log_pplx:4.4726682 loss:110.72648 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.06635\n",
      "I0710 08:45:14.370916 140295626643200 summary_utils.py:349] Steps/second: 0.177053, Examples/second: 25.078559\n",
      "I0710 08:45:14.371707 140295626643200 base_runner.py:111] step:  5437, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:129.64272 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2643801 log_pplx:4.2916136 loss:173.27393 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.07141\n",
      "I0710 08:45:20.983504 140295626643200 summary_utils.py:349] Steps/second: 0.177019, Examples/second: 25.061695\n",
      "I0710 08:45:20.984313 140295626643200 trainer.py:508] step:  5438, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:143.50067 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2643877 log_pplx:4.3321381 loss:173.50212 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.07666\n",
      "I0710 08:45:24.078624 140295626643200 summary_utils.py:349] Steps/second: 0.177108, Examples/second: 25.096730\n",
      "I0710 08:45:24.079414 140295626643200 trainer.py:508] step:  5439, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:30.965544 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2655892 log_pplx:4.3058548 loss:67.514458 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.08203\n",
      "I0710 08:45:27.931641 140295626643200 summary_utils.py:349] Steps/second: 0.177170, Examples/second: 25.109150\n",
      "I0710 08:45:27.932397 140295626643200 trainer.py:508] step:  5440, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:67.046013 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2662927 log_pplx:4.4810748 loss:112.64301 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.08698\n",
      "I0710 08:45:34.163621 140295626643200 summary_utils.py:349] Steps/second: 0.177150, Examples/second: 25.094163\n",
      "I0710 08:45:34.164422 140295626643200 trainer.py:508] step:  5441, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:80.421204 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2642704 log_pplx:4.3460608 loss:166.3998 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.09137\n",
      "I0710 08:45:40.567798 140295626643200 summary_utils.py:349] Steps/second: 0.177124, Examples/second: 25.078367\n",
      "I0710 08:45:40.568624 140295626643200 trainer.py:508] step:  5442, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:86.48967 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2649661 log_pplx:4.3105931 loss:181.63762 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.09564\n",
      "I0710 08:45:50.354306 140295626643200 summary_utils.py:349] Steps/second: 0.176980, Examples/second: 25.038238\n",
      "I0710 08:45:50.355110 140295626643200 trainer.py:508] step:  5443, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:436.41394 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.263916 log_pplx:3.8135872 loss:275.91306 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:520.10004\n",
      "I0710 08:45:51.688636 140295635035904 trainer.py:345] Write summary @5443\n",
      "I0710 08:46:02.178426 140295626643200 summary_utils.py:349] Steps/second: 0.176767, Examples/second: 24.996130\n",
      "I0710 08:46:02.179543 140295626643200 trainer.py:508] step:  5444, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:100.35033 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2648544 log_pplx:4.3185649 loss:174.524 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.10577\n",
      "I0710 08:46:07.584397 140295626643200 summary_utils.py:349] Steps/second: 0.176776, Examples/second: 25.000972\n",
      "I0710 08:46:07.585553 140295626643200 trainer.py:508] step:  5445, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:87.958839 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2665497 log_pplx:4.5433311 loss:116.7636 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.11157\n",
      "I0710 08:46:16.413518 140295626643200 summary_utils.py:349] Steps/second: 0.176667, Examples/second: 24.973620\n",
      "I0710 08:46:16.414909 140295626643200 trainer.py:508] step:  5446, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:116.50651 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2647378 log_pplx:4.2199712 loss:171.91109 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.11676\n",
      "I0710 08:46:20.355588 140295626643200 summary_utils.py:349] Steps/second: 0.176726, Examples/second: 25.004217\n",
      "I0710 08:46:20.356381 140295626643200 trainer.py:508] step:  5447, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:35.957024 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2663659 log_pplx:4.4607873 loss:70.187698 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.12164\n",
      "I0710 08:46:30.021858 140295626643200 summary_utils.py:349] Steps/second: 0.176589, Examples/second: 24.972874\n",
      "I0710 08:46:30.023192 140295626643200 trainer.py:508] step:  5448, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:99.263733 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2648051 log_pplx:4.2144012 loss:171.63148 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.12622\n",
      "I0710 08:46:36.056575 140295626643200 summary_utils.py:349] Steps/second: 0.176576, Examples/second: 24.974674\n",
      "I0710 08:46:36.061341 140295626643200 trainer.py:508] step:  5449, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:53.337425 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.266502 log_pplx:4.460815 loss:112.4683 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.1308\n",
      "I0710 08:46:46.180176 140295626643200 summary_utils.py:349] Steps/second: 0.176424, Examples/second: 24.941276\n",
      "I0710 08:46:46.181580 140295626643200 trainer.py:508] step:  5450, steps/sec: 0.18, examples/sec: 24.94 grad_norm/all/loss:91.514206 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2648144 log_pplx:4.2434659 loss:170.85255 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.13556\n",
      "I0710 08:46:49.495443 140295635035904 trainer.py:354] Write summary done: step 5443\n",
      "I0710 08:46:49.872384 140295626643200 summary_utils.py:349] Steps/second: 0.176491, Examples/second: 25.022360\n",
      "I0710 08:46:49.873252 140295626643200 trainer.py:508] step:  5451, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:12.808141 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2647703 log_pplx:4.1957774 loss:30.894691 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:520.14044\n",
      "I0710 08:46:52.967051 140295626643200 summary_utils.py:349] Steps/second: 0.176579, Examples/second: 25.056819\n",
      "I0710 08:46:52.967795 140295626643200 trainer.py:508] step:  5452, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:30.187347 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2656813 log_pplx:4.2622647 loss:66.198288 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.14502\n",
      "I0710 08:46:59.418529 140295626643200 summary_utils.py:349] Steps/second: 0.176552, Examples/second: 25.041081\n",
      "I0710 08:46:59.419556 140295626643200 trainer.py:508] step:  5453, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:91.648628 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2650218 log_pplx:4.3171363 loss:177.16449 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.14954\n",
      "I0710 08:47:10.423631 140295626643200 summary_utils.py:349] Steps/second: 0.176371, Examples/second: 24.995771\n",
      "I0710 08:47:10.424429 140295626643200 trainer.py:508] step:  5454, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:370.80838 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2638581 log_pplx:3.7766159 loss:276.73154 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:520.15417\n",
      "I0710 08:47:14.213029 140295626643200 summary_utils.py:349] Steps/second: 0.176434, Examples/second: 25.008318\n",
      "I0710 08:47:14.213775 140295626643200 trainer.py:508] step:  5455, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:57.820644 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2664503 log_pplx:4.4244328 loss:111.82754 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.15857\n",
      "I0710 08:47:20.839504 140295626643200 summary_utils.py:349] Steps/second: 0.176402, Examples/second: 24.991865\n",
      "I0710 08:47:20.840333 140295626643200 trainer.py:508] step:  5456, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:96.260399 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2646704 log_pplx:4.2313457 loss:164.01753 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.16315\n",
      "I0710 08:47:27.777065 140295626643200 summary_utils.py:349] Steps/second: 0.176359, Examples/second: 24.973964\n",
      "I0710 08:47:27.777847 140295626643200 trainer.py:508] step:  5457, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:82.227615 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2650406 log_pplx:4.240459 loss:172.37466 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.16821\n",
      "I0710 08:47:31.719745 140295626643200 summary_utils.py:349] Steps/second: 0.176417, Examples/second: 24.985751\n",
      "I0710 08:47:31.720485 140295626643200 trainer.py:508] step:  5458, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:39.68576 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.266649 log_pplx:4.4492455 loss:111.17554 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.17346\n",
      "I0710 08:47:34.748530 140295626643200 summary_utils.py:349] Steps/second: 0.176506, Examples/second: 25.020272\n",
      "I0710 08:47:34.749288 140295626643200 trainer.py:508] step:  5459, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:28.447094 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.266323 log_pplx:4.2984123 loss:68.925713 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.17853\n",
      "I0710 08:47:40.884100 140295626643200 summary_utils.py:349] Steps/second: 0.176491, Examples/second: 25.006215\n",
      "I0710 08:47:40.884942 140295626643200 trainer.py:508] step:  5460, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:78.165199 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2646708 log_pplx:4.2318535 loss:166.9995 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.18323\n",
      "I0710 08:47:46.834023 140295626643200 summary_utils.py:349] Steps/second: 0.176481, Examples/second: 24.993077\n",
      "I0710 08:47:46.834732 140295626643200 trainer.py:508] step:  5461, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:82.254761 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2649878 log_pplx:4.2165751 loss:169.45361 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.18805\n",
      "I0710 08:47:50.629601 140295626643200 summary_utils.py:349] Steps/second: 0.176544, Examples/second: 25.005509\n",
      "I0710 08:47:50.630464 140295626643200 trainer.py:508] step:  5462, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:86.163918 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2669262 log_pplx:4.5522084 loss:114.0897 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.19287\n",
      "I0710 08:47:59.970882 140295626643200 summary_utils.py:349] Steps/second: 0.176420, Examples/second: 24.968628\n",
      "I0710 08:47:59.971689 140295626643200 trainer.py:508] step:  5463, steps/sec: 0.18, examples/sec: 24.97 grad_norm/all/loss:326.88211 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.264003 log_pplx:3.7141995 loss:265.37958 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:520.19696\n",
      "I0710 08:48:06.088609 140295626643200 summary_utils.py:349] Steps/second: 0.176405, Examples/second: 24.954783\n",
      "I0710 08:48:06.089390 140295626643200 trainer.py:508] step:  5464, steps/sec: 0.18, examples/sec: 24.95 grad_norm/all/loss:78.452477 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2648853 log_pplx:4.2524085 loss:169.45847 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.20251\n",
      "I0710 08:48:09.046600 140295626643200 summary_utils.py:349] Steps/second: 0.176496, Examples/second: 24.989433\n",
      "I0710 08:48:09.047567 140295626643200 trainer.py:508] step:  5465, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:35.292576 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2665302 log_pplx:4.3747668 loss:68.765862 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.20776\n",
      "I0710 08:48:14.886102 140295626643200 summary_utils.py:349] Steps/second: 0.176490, Examples/second: 24.976909\n",
      "I0710 08:48:14.887039 140295626643200 trainer.py:508] step:  5466, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:102.63242 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2653062 log_pplx:4.257021 loss:170.91939 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.21265\n",
      "I0710 08:48:18.756831 140295626643200 summary_utils.py:349] Steps/second: 0.176551, Examples/second: 24.988929\n",
      "I0710 08:48:18.757674 140295626643200 trainer.py:508] step:  5467, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:54.472775 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2668451 log_pplx:4.4311533 loss:111.30503 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.21686\n",
      "I0710 08:48:24.865112 140295626643200 summary_utils.py:349] Steps/second: 0.176536, Examples/second: 24.975155\n",
      "I0710 08:48:24.865871 140295626643200 trainer.py:508] step:  5468, steps/sec: 0.18, examples/sec: 24.98 grad_norm/all/loss:92.019623 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2653477 log_pplx:4.2929034 loss:172.25276 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.22083\n",
      "I0710 08:48:31.016517 140295626643200 summary_utils.py:349] Steps/second: 0.176519, Examples/second: 24.961210\n",
      "I0710 08:48:31.017291 140295626643200 trainer.py:508] step:  5469, steps/sec: 0.18, examples/sec: 24.96 grad_norm/all/loss:77.157204 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2650658 log_pplx:4.1689234 loss:158.88809 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.22449\n",
      "I0710 08:48:34.007109 140295626643200 summary_utils.py:349] Steps/second: 0.176609, Examples/second: 24.995538\n",
      "I0710 08:48:34.007840 140295626643200 trainer.py:508] step:  5470, steps/sec: 0.18, examples/sec: 25.00 grad_norm/all/loss:40.097649 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2661283 log_pplx:4.3435311 loss:65.899506 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.22809\n",
      "I0710 08:48:37.665973 140295626643200 summary_utils.py:349] Steps/second: 0.176676, Examples/second: 25.008504\n",
      "I0710 08:48:37.666750 140295626643200 trainer.py:508] step:  5471, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:86.345284 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2667313 log_pplx:4.5013723 loss:113.43457 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.23181\n",
      "I0710 08:48:43.888522 140295626643200 summary_utils.py:349] Steps/second: 0.176657, Examples/second: 24.994217\n",
      "I0710 08:48:43.889365 140295626643200 trainer.py:508] step:  5472, steps/sec: 0.18, examples/sec: 24.99 grad_norm/all/loss:89.240372 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2651039 log_pplx:4.0788045 loss:165.44652 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.23602\n",
      "I0710 08:48:54.433032 140295626643200 summary_utils.py:349] Steps/second: 0.176494, Examples/second: 24.952063\n",
      "I0710 08:48:54.433838 140295626643200 trainer.py:508] step:  5473, steps/sec: 0.18, examples/sec: 24.95 grad_norm/all/loss:211.80585 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2645077 log_pplx:3.7858696 loss:275.8952 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:520.24048\n",
      "I0710 08:49:00.665704 140295626643200 summary_utils.py:349] Steps/second: 0.176475, Examples/second: 24.937840\n",
      "I0710 08:49:00.666692 140295626643200 trainer.py:508] step:  5474, steps/sec: 0.18, examples/sec: 24.94 grad_norm/all/loss:85.061935 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2654881 log_pplx:4.3064957 loss:174.89755 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.24536\n",
      "I0710 08:49:02.923471 140295626643200 summary_utils.py:349] Steps/second: 0.176589, Examples/second: 25.023630\n",
      "I0710 08:49:02.924317 140295626643200 trainer.py:508] step:  5475, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:10.998536 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2650715 log_pplx:4.1725121 loss:30.576691 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:520.25067\n",
      "I0710 08:49:06.792059 140295626643200 summary_utils.py:349] Steps/second: 0.176648, Examples/second: 25.035518\n",
      "I0710 08:49:06.792830 140295626643200 trainer.py:508] step:  5476, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:38.001019 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2670568 log_pplx:4.4106855 loss:112.38979 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.25574\n",
      "I0710 08:49:12.946685 140295626643200 summary_utils.py:349] Steps/second: 0.176632, Examples/second: 25.021597\n",
      "I0710 08:49:12.947695 140295626643200 trainer.py:508] step:  5477, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:83.423134 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2652612 log_pplx:4.2542586 loss:167.93686 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.26062\n",
      "I0710 08:49:16.015788 140295626643200 summary_utils.py:349] Steps/second: 0.176718, Examples/second: 25.055252\n",
      "I0710 08:49:16.016851 140295626643200 trainer.py:508] step:  5478, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:38.564968 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2668815 log_pplx:4.3922362 loss:67.41053 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.26556\n",
      "I0710 08:49:19.843962 140295626643200 summary_utils.py:349] Steps/second: 0.176779, Examples/second: 25.067278\n",
      "I0710 08:49:19.844992 140295626643200 trainer.py:508] step:  5479, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:49.844704 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2673481 log_pplx:4.4582362 loss:113.93579 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.2702\n",
      "I0710 08:49:26.020666 140295626643200 summary_utils.py:349] Steps/second: 0.176761, Examples/second: 25.053251\n",
      "I0710 08:49:26.021454 140295626643200 trainer.py:508] step:  5480, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:84.153595 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.265738 log_pplx:4.3169665 loss:175.10693 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.27454\n",
      "I0710 08:49:32.514100 140295626643200 summary_utils.py:349] Steps/second: 0.176734, Examples/second: 25.037772\n",
      "I0710 08:49:32.514815 140295626643200 trainer.py:508] step:  5481, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:79.98658 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2657396 log_pplx:4.2723026 loss:174.41676 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.27905\n",
      "I0710 08:49:36.371278 140295626643200 summary_utils.py:349] Steps/second: 0.176793, Examples/second: 25.049637\n",
      "I0710 08:49:36.372064 140295626643200 trainer.py:508] step:  5482, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:38.223091 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2672659 log_pplx:4.382741 loss:109.70548 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.28375\n",
      "I0710 08:49:39.430676 140295626643200 summary_utils.py:349] Steps/second: 0.176879, Examples/second: 25.083174\n",
      "I0710 08:49:39.431434 140295626643200 trainer.py:508] step:  5483, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:23.206823 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.266831 log_pplx:4.3114562 loss:67.484406 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.28815\n",
      "I0710 08:49:45.757641 140295626643200 summary_utils.py:349] Steps/second: 0.176857, Examples/second: 25.068476\n",
      "I0710 08:49:45.758754 140295626643200 trainer.py:508] step:  5484, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:78.272186 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.265113 log_pplx:4.1107693 loss:157.90494 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.29248\n",
      "I0710 08:49:52.298069 140295626643200 summary_utils.py:349] Steps/second: 0.176828, Examples/second: 25.052816\n",
      "I0710 08:49:52.299102 140295626643200 trainer.py:508] step:  5485, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:92.177643 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2658145 log_pplx:4.1693287 loss:176.77954 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.297\n",
      "I0710 08:49:58.521539 140295626643200 summary_utils.py:349] Steps/second: 0.176809, Examples/second: 25.038672\n",
      "I0710 08:49:58.522722 140295626643200 trainer.py:508] step:  5486, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:79.818748 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.265334 log_pplx:4.1846528 loss:170.47229 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.30182\n",
      "I0710 08:50:02.420458 140295626643200 summary_utils.py:349] Steps/second: 0.176867, Examples/second: 25.050285\n",
      "I0710 08:50:02.421273 140295626643200 trainer.py:508] step:  5487, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:47.385227 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2674723 log_pplx:4.4392552 loss:112.2854 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.30658\n",
      "I0710 08:50:13.215379 140295626643200 summary_utils.py:349] Steps/second: 0.176698, Examples/second: 25.007473\n",
      "I0710 08:50:13.216176 140295626643200 trainer.py:508] step:  5488, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:193.95824 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2647967 log_pplx:3.6575837 loss:260.32849 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:520.31079\n",
      "I0710 08:50:16.279200 140295626643200 summary_utils.py:349] Steps/second: 0.176783, Examples/second: 25.040784\n",
      "I0710 08:50:16.280281 140295626643200 trainer.py:508] step:  5489, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:21.519625 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2666245 log_pplx:4.2521605 loss:65.509842 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.31512\n",
      "I0710 08:50:22.372348 140295626643200 summary_utils.py:349] Steps/second: 0.176769, Examples/second: 25.027321\n",
      "I0710 08:50:22.373248 140295626643200 trainer.py:508] step:  5490, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:184.00349 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2659801 log_pplx:4.2749376 loss:173.40215 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.31934\n",
      "I0710 08:50:26.323535 140295626643200 summary_utils.py:349] Steps/second: 0.176825, Examples/second: 25.038649\n",
      "I0710 08:50:26.324328 140295626643200 trainer.py:508] step:  5491, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:41.80257 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2674165 log_pplx:4.3696346 loss:111.09798 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.32318\n",
      "I0710 08:50:32.730640 140295626643200 summary_utils.py:349] Steps/second: 0.176800, Examples/second: 25.023759\n",
      "I0710 08:50:32.731428 140295626643200 trainer.py:508] step:  5492, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:84.896301 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.265825 log_pplx:4.3205791 loss:172.98518 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.32697\n",
      "I0710 08:50:39.204221 140295626643200 summary_utils.py:349] Steps/second: 0.176773, Examples/second: 25.008598\n",
      "I0710 08:50:39.205044 140295626643200 trainer.py:508] step:  5493, steps/sec: 0.18, examples/sec: 25.01 grad_norm/all/loss:72.037857 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2661219 log_pplx:4.234302 loss:176.67625 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.33063\n",
      "I0710 08:50:42.271847 140295626643200 summary_utils.py:349] Steps/second: 0.176858, Examples/second: 25.041730\n",
      "I0710 08:50:42.272741 140295626643200 trainer.py:508] step:  5494, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:42.838306 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2672788 log_pplx:4.3712077 loss:69.000191 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.33453\n",
      "I0710 08:50:46.110361 140295626643200 summary_utils.py:349] Steps/second: 0.176917, Examples/second: 25.053527\n",
      "I0710 08:50:46.111131 140295626643200 trainer.py:508] step:  5495, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:93.188446 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2677108 log_pplx:4.5021205 loss:113.45345 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.33856\n",
      "I0710 08:50:52.578308 140295626643200 summary_utils.py:349] Steps/second: 0.176890, Examples/second: 25.038393\n",
      "I0710 08:50:52.579105 140295626643200 trainer.py:508] step:  5496, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:107.98325 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2658643 log_pplx:4.3029876 loss:177.33687 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.3432\n",
      "I0710 08:50:54.771068 140295626643200 summary_utils.py:349] Steps/second: 0.177003, Examples/second: 25.122672\n",
      "I0710 08:50:54.771821 140295626643200 trainer.py:508] step:  5497, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:11.107016 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2657974 log_pplx:4.2072601 loss:31.118933 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:520.34833\n",
      "I0710 08:51:00.898451 140295626643200 summary_utils.py:349] Steps/second: 0.176988, Examples/second: 25.109055\n",
      "I0710 08:51:00.899232 140295626643200 trainer.py:508] step:  5498, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:66.000465 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2655643 log_pplx:4.2127914 loss:168.24835 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.35303\n",
      "I0710 08:51:10.983337 140295626643200 summary_utils.py:349] Steps/second: 0.176843, Examples/second: 25.069859\n",
      "I0710 08:51:10.984095 140295626643200 trainer.py:508] step:  5499, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:209.33284 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2650824 log_pplx:3.6796961 loss:262.54633 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:520.35754\n",
      "I0710 08:51:17.442456 140295626643200 summary_utils.py:349] Steps/second: 0.176817, Examples/second: 25.054816\n",
      "I0710 08:51:17.443207 140295626643200 trainer.py:508] step:  5500, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:118.7526 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2659564 log_pplx:4.1917515 loss:170.34229 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.36316\n",
      "I0710 08:51:19.755354 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 08:51:20.667045 140295626643200 summary_utils.py:349] Steps/second: 0.176896, Examples/second: 25.086967\n",
      "I0710 08:51:20.667816 140295626643200 trainer.py:508] step:  5501, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:95.830627 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2670488 log_pplx:4.4245939 loss:70.205856 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.36835\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 08:51:28.083606 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 08:51:28.448652 140295626643200 summary_utils.py:349] Steps/second: 0.176827, Examples/second: 25.080514\n",
      "I0710 08:51:28.644434 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00005500\n",
      "I0710 08:51:28.644791 140295626643200 trainer.py:508] step:  5502, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:84.806671 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2677391 log_pplx:4.4567223 loss:112.08656 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.37286\n",
      "I0710 08:51:34.549704 140295626643200 summary_utils.py:349] Steps/second: 0.176813, Examples/second: 25.067149\n",
      "I0710 08:51:34.550581 140295626643200 trainer.py:508] step:  5503, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:88.350708 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2658814 log_pplx:4.1787791 loss:166.47212 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.37659\n",
      "I0710 08:51:40.797774 140295626643200 summary_utils.py:349] Steps/second: 0.176794, Examples/second: 25.053140\n",
      "I0710 08:51:40.798725 140295626643200 trainer.py:508] step:  5504, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:93.824333 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.265597 log_pplx:4.2467823 loss:169.44661 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.38\n",
      "I0710 08:51:44.474751 140295626643200 summary_utils.py:349] Steps/second: 0.176858, Examples/second: 25.065542\n",
      "I0710 08:51:44.475513 140295626643200 trainer.py:508] step:  5505, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:57.361084 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2677212 log_pplx:4.3813076 loss:108.6838 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.38391\n",
      "I0710 08:51:47.522555 140295626643200 summary_utils.py:349] Steps/second: 0.176942, Examples/second: 25.098339\n",
      "I0710 08:51:47.523629 140295626643200 trainer.py:508] step:  5506, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:47.930286 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2673422 log_pplx:4.363308 loss:69.267517 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.38794\n",
      "I0710 08:51:53.840095 140295626643200 summary_utils.py:349] Steps/second: 0.176920, Examples/second: 25.084010\n",
      "I0710 08:51:53.840874 140295626643200 trainer.py:508] step:  5507, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:112.66689 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2656534 log_pplx:4.1639175 loss:164.63086 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.39209\n",
      "I0710 08:51:57.763215 140295626643200 summary_utils.py:349] Steps/second: 0.176976, Examples/second: 25.095236\n",
      "I0710 08:51:57.764123 140295626643200 trainer.py:508] step:  5508, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:38.922604 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2676586 log_pplx:4.3545275 loss:109.9246 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.39673\n",
      "I0710 08:52:04.140070 140295626643200 summary_utils.py:349] Steps/second: 0.176953, Examples/second: 25.080667\n",
      "I0710 08:52:04.140835 140295626643200 trainer.py:508] step:  5509, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:74.340988 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2658098 log_pplx:4.2006774 loss:164.19397 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.40125\n",
      "I0710 08:52:10.516968 140295626643200 summary_utils.py:349] Steps/second: 0.176929, Examples/second: 25.066131\n",
      "I0710 08:52:10.517850 140295626643200 trainer.py:508] step:  5510, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:108.96306 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2660741 log_pplx:4.2643256 loss:171.15936 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.4057\n",
      "I0710 08:52:13.627380 140295626643200 summary_utils.py:349] Steps/second: 0.177011, Examples/second: 25.098486\n",
      "I0710 08:52:13.628137 140295626643200 trainer.py:508] step:  5511, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:21.399395 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2672242 log_pplx:4.2916737 loss:66.705353 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.4101\n",
      "I0710 08:52:17.510318 140295626643200 summary_utils.py:349] Steps/second: 0.177068, Examples/second: 25.109845\n",
      "I0710 08:52:17.511321 140295626643200 trainer.py:508] step:  5512, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:52.404121 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2677524 log_pplx:4.4268885 loss:111.0319 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.41412\n",
      "I0710 08:52:23.860356 140295626643200 summary_utils.py:349] Steps/second: 0.177045, Examples/second: 25.095433\n",
      "I0710 08:52:23.861129 140295626643200 trainer.py:508] step:  5513, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:95.181313 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2660804 log_pplx:4.1808476 loss:173.29617 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.41772\n",
      "I0710 08:52:35.238499 140295626643200 summary_utils.py:349] Steps/second: 0.176862, Examples/second: 25.050984\n",
      "I0710 08:52:35.239286 140295626643200 trainer.py:508] step:  5514, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:235.99324 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2653962 log_pplx:3.8161812 loss:290.69763 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:520.4212\n",
      "I0710 08:52:41.649070 140295626643200 summary_utils.py:349] Steps/second: 0.176837, Examples/second: 25.036412\n",
      "I0710 08:52:41.649830 140295626643200 trainer.py:508] step:  5515, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:94.631195 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2662255 log_pplx:4.2829776 loss:174.79903 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.4248\n",
      "I0710 08:52:45.372117 140295626643200 summary_utils.py:349] Steps/second: 0.176899, Examples/second: 25.048480\n",
      "I0710 08:52:45.372878 140295626643200 trainer.py:508] step:  5516, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:62.50959 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2677667 log_pplx:4.3389397 loss:108.93451 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.42926\n",
      "I0710 08:52:51.594350 140295626643200 summary_utils.py:349] Steps/second: 0.176881, Examples/second: 25.034789\n",
      "I0710 08:52:51.595260 140295626643200 trainer.py:508] step:  5517, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:119.49933 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.266204 log_pplx:4.2921543 loss:168.46704 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.43414\n",
      "I0710 08:52:54.573003 140295626643200 summary_utils.py:349] Steps/second: 0.176966, Examples/second: 25.067518\n",
      "I0710 08:52:54.573767 140295626643200 trainer.py:508] step:  5518, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:22.889725 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2678587 log_pplx:4.3210545 loss:68.12413 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.43988\n",
      "I0710 08:53:00.877126 140295626643200 summary_utils.py:349] Steps/second: 0.176945, Examples/second: 25.053458\n",
      "I0710 08:53:00.878010 140295626643200 trainer.py:508] step:  5519, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:93.730865 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2665343 log_pplx:4.2559328 loss:174.54642 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.44531\n",
      "I0710 08:53:03.055243 140295626643200 summary_utils.py:349] Steps/second: 0.177056, Examples/second: 25.135849\n",
      "I0710 08:53:03.056001 140295626643200 trainer.py:508] step:  5520, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:12.81085 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2663192 log_pplx:4.2192521 loss:31.108742 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:520.45099\n",
      "2020-07-10 08:53:03.523903: I lingvo/core/ops/record_yielder.cc:532] Epoch 4: total records 46838\n",
      "2020-07-10 08:53:03.523956: I lingvo/core/ops/record_yielder.cc:485] Epoch 4 /tmp/punctuator_data/train.txt\n",
      "I0710 08:53:06.890137 140295626643200 summary_utils.py:349] Steps/second: 0.177114, Examples/second: 25.147298\n",
      "I0710 08:53:06.890953 140295626643200 trainer.py:508] step:  5521, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:95.723991 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2680142 log_pplx:4.4652801 loss:111.82737 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.45636\n",
      "I0710 08:53:12.858329 140295626643200 summary_utils.py:349] Steps/second: 0.177104, Examples/second: 25.134697\n",
      "I0710 08:53:12.859235 140295626643200 trainer.py:508] step:  5522, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:85.470291 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2663099 log_pplx:4.3151746 loss:168.88516 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.46082\n",
      "I0710 08:53:19.191940 140295626643200 summary_utils.py:349] Steps/second: 0.177082, Examples/second: 25.120474\n",
      "I0710 08:53:19.192676 140295626643200 trainer.py:508] step:  5523, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:70.945114 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2662423 log_pplx:4.1708355 loss:170.17007 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.4649\n",
      "I0710 08:53:30.231832 140295626643200 summary_utils.py:349] Steps/second: 0.176911, Examples/second: 25.077935\n",
      "I0710 08:53:30.232589 140295626643200 trainer.py:508] step:  5524, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:398.84836 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2655022 log_pplx:3.7462394 loss:267.01324 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:520.46881\n",
      "I0710 08:53:33.271969 140295626643200 summary_utils.py:349] Steps/second: 0.176993, Examples/second: 25.110138\n",
      "I0710 08:53:33.272793 140295626643200 trainer.py:508] step:  5525, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:22.299683 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2678783 log_pplx:4.2673025 loss:66.159859 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.47437\n",
      "I0710 08:53:39.428474 140295626643200 summary_utils.py:349] Steps/second: 0.176977, Examples/second: 25.096790\n",
      "I0710 08:53:39.429340 140295626643200 trainer.py:508] step:  5526, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:215.78963 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2663085 log_pplx:4.1616278 loss:165.8929 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.47986\n",
      "I0710 08:53:43.232728 140295626643200 summary_utils.py:349] Steps/second: 0.177036, Examples/second: 25.108329\n",
      "I0710 08:53:43.233536 140295626643200 trainer.py:508] step:  5527, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:46.300182 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.268029 log_pplx:4.339735 loss:108.87311 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.48438\n",
      "I0710 08:53:49.456522 140295626643200 summary_utils.py:349] Steps/second: 0.177018, Examples/second: 25.094706\n",
      "I0710 08:53:49.457341 140295626643200 trainer.py:508] step:  5528, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:125.81959 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.266382 log_pplx:3.9738834 loss:158.95532 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.48865\n",
      "I0710 08:53:55.928336 140295626643200 summary_utils.py:349] Steps/second: 0.176992, Examples/second: 25.080002\n",
      "I0710 08:53:55.929113 140295626643200 trainer.py:508] step:  5529, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:83.300285 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2668691 log_pplx:4.0463324 loss:168.78264 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.49268\n",
      "I0710 08:53:58.918092 140295626643200 summary_utils.py:349] Steps/second: 0.177076, Examples/second: 25.112282\n",
      "I0710 08:53:58.918857 140295626643200 trainer.py:508] step:  5530, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:37.607071 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2675394 log_pplx:4.2062235 loss:64.424217 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.49731\n",
      "I0710 08:54:02.708985 140295626643200 summary_utils.py:349] Steps/second: 0.177134, Examples/second: 25.123830\n",
      "I0710 08:54:02.709798 140295626643200 trainer.py:508] step:  5531, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:113.68383 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2686007 log_pplx:4.4830546 loss:113.98167 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.50201\n",
      "I0710 08:54:08.705040 140295626643200 summary_utils.py:349] Steps/second: 0.177123, Examples/second: 25.111255\n",
      "I0710 08:54:08.705791 140295626643200 trainer.py:508] step:  5532, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:132.49448 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2665873 log_pplx:4.0921636 loss:160.87317 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.50745\n",
      "I0710 08:54:15.032541 140295626643200 summary_utils.py:349] Steps/second: 0.177102, Examples/second: 25.097226\n",
      "I0710 08:54:15.033497 140295626643200 trainer.py:508] step:  5533, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:96.60701 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2665737 log_pplx:4.0726376 loss:160.30923 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.51331\n",
      "I0710 08:54:24.229120 140295626643200 summary_utils.py:349] Steps/second: 0.176990, Examples/second: 25.063354\n",
      "I0710 08:54:24.229982 140295626643200 trainer.py:508] step:  5534, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:524.71637 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2654201 log_pplx:3.4289153 loss:238.48105 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:520.51923\n",
      "I0710 08:54:28.134872 140295626643200 summary_utils.py:349] Steps/second: 0.177045, Examples/second: 25.074372\n",
      "I0710 08:54:28.135651 140295626643200 trainer.py:508] step:  5535, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:57.075439 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2686666 log_pplx:4.3512506 loss:110.16823 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.52478\n",
      "I0710 08:54:34.197618 140295626643200 summary_utils.py:349] Steps/second: 0.177032, Examples/second: 25.061609\n",
      "I0710 08:54:34.198398 140295626643200 trainer.py:508] step:  5536, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:131.5255 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.266818 log_pplx:4.0486121 loss:162.75421 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.52997\n",
      "I0710 08:54:37.260802 140295626643200 summary_utils.py:349] Steps/second: 0.177113, Examples/second: 25.093353\n",
      "I0710 08:54:37.261811 140295626643200 base_runner.py:111] step:  5537, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:54.436241 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2678083 log_pplx:4.2465391 loss:65.804779 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.53558\n",
      "I0710 08:54:43.526290 140295626643200 summary_utils.py:349] Steps/second: 0.177093, Examples/second: 25.079692\n",
      "I0710 08:54:43.527127 140295626643200 trainer.py:508] step:  5538, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:225.50513 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2669425 log_pplx:4.2122149 loss:171.80571 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.54065\n",
      "I0710 08:54:47.333543 140295626643200 summary_utils.py:349] Steps/second: 0.177151, Examples/second: 25.091098\n",
      "I0710 08:54:47.334359 140295626643200 trainer.py:508] step:  5539, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:63.331432 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2682139 log_pplx:4.3343925 loss:107.38458 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.54657\n",
      "I0710 08:54:53.655517 140295626643200 summary_utils.py:349] Steps/second: 0.177130, Examples/second: 25.077212\n",
      "I0710 08:54:53.656257 140295626643200 trainer.py:508] step:  5540, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:102.70148 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2668378 log_pplx:4.0657487 loss:164.71365 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.552\n",
      "I0710 08:54:56.688297 140295626643200 summary_utils.py:349] Steps/second: 0.177212, Examples/second: 25.108974\n",
      "I0710 08:54:56.689118 140295626643200 trainer.py:508] step:  5541, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:41.837643 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2679182 log_pplx:4.1806178 loss:64.032043 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.5578\n",
      "I0710 08:55:03.026298 140295626643200 summary_utils.py:349] Steps/second: 0.177190, Examples/second: 25.095021\n",
      "I0710 08:55:03.027123 140295626643200 trainer.py:508] step:  5542, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:97.069786 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2670541 log_pplx:4.0308094 loss:164.20509 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.56317\n",
      "I0710 08:55:06.811493 140295626643200 summary_utils.py:349] Steps/second: 0.177248, Examples/second: 25.106475\n",
      "I0710 08:55:06.812379 140295626643200 trainer.py:508] step:  5543, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:67.013771 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2685566 log_pplx:4.258213 loss:106.26903 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.56879\n",
      "I0710 08:55:09.959437 140295635035904 trainer.py:345] Write summary @5543\n",
      "2020-07-10 08:55:12.003629: I lingvo/core/ops/record_batcher.cc:394] 5656 total seconds passed. Total records yielded: 2300. Total records skipped: 0\n",
      "I0710 08:55:17.437413 140295626643200 summary_utils.py:349] Steps/second: 0.177093, Examples/second: 25.073628\n",
      "I0710 08:55:17.439021 140295626643200 trainer.py:508] step:  5544, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:90.000771 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2668308 log_pplx:4.0313783 loss:164.53064 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.57404\n",
      "I0710 08:55:20.189852 140295626643200 summary_utils.py:349] Steps/second: 0.177183, Examples/second: 25.151494\n",
      "I0710 08:55:20.191004 140295626643200 trainer.py:508] step:  5545, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:23.004438 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2668713 log_pplx:4.1660042 loss:30.960247 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:520.57935\n",
      "I0710 08:55:33.735225 140295626643200 summary_utils.py:349] Steps/second: 0.176937, Examples/second: 25.098766\n",
      "I0710 08:55:33.736454 140295626643200 trainer.py:508] step:  5546, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:532.29736 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2657171 log_pplx:3.5438488 loss:247.27206 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:520.58453\n",
      "I0710 08:55:43.285453 140295626643200 summary_utils.py:349] Steps/second: 0.176816, Examples/second: 25.070809\n",
      "I0710 08:55:43.287044 140295626643200 trainer.py:508] step:  5547, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:144.80937 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2669928 log_pplx:4.0378184 loss:163.68307 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.59186\n",
      "I0710 08:55:49.077266 140295626643200 summary_utils.py:349] Steps/second: 0.176812, Examples/second: 25.073396\n",
      "I0710 08:55:49.078530 140295626643200 trainer.py:508] step:  5548, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:43.573433 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2689995 log_pplx:4.341198 loss:109.01835 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.59882\n",
      "I0710 08:55:53.562714 140295626643200 summary_utils.py:349] Steps/second: 0.176848, Examples/second: 25.098479\n",
      "I0710 08:55:53.564056 140295626643200 trainer.py:508] step:  5549, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:39.924385 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2685552 log_pplx:4.2070246 loss:65.685455 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.60559\n",
      "I0710 08:55:57.133416 140295635035904 trainer.py:354] Write summary done: step 5543\n",
      "I0710 08:56:01.605508 140295626643200 summary_utils.py:349] Steps/second: 0.176775, Examples/second: 25.077213\n",
      "I0710 08:56:01.606285 140295626643200 trainer.py:508] step:  5550, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:389.00473 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2669878 log_pplx:4.3394513 loss:172.16772 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.61212\n",
      "I0710 08:56:07.698537 140295626643200 summary_utils.py:349] Steps/second: 0.176761, Examples/second: 25.064523\n",
      "I0710 08:56:07.699419 140295626643200 trainer.py:508] step:  5551, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:205.42973 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2674036 log_pplx:4.1816182 loss:167.42154 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.61737\n",
      "I0710 08:56:11.538386 140295626643200 summary_utils.py:349] Steps/second: 0.176817, Examples/second: 25.075630\n",
      "I0710 08:56:11.539129 140295626643200 trainer.py:508] step:  5552, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:98.743332 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2687676 log_pplx:4.3697705 loss:109.89973 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.62231\n",
      "I0710 08:56:17.978292 140295626643200 summary_utils.py:349] Steps/second: 0.176793, Examples/second: 25.061451\n",
      "I0710 08:56:17.979104 140295626643200 trainer.py:508] step:  5553, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:108.13686 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2671956 log_pplx:3.9969141 loss:161.22554 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.62762\n",
      "I0710 08:56:21.034235 140295626643200 summary_utils.py:349] Steps/second: 0.176873, Examples/second: 25.092654\n",
      "I0710 08:56:21.035046 140295626643200 trainer.py:508] step:  5554, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:40.908993 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2685912 log_pplx:4.3423338 loss:67.713264 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.63312\n",
      "I0710 08:56:27.011439 140295626643200 summary_utils.py:349] Steps/second: 0.176863, Examples/second: 25.080495\n",
      "I0710 08:56:27.012302 140295626643200 trainer.py:508] step:  5555, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:191.85841 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2671794 log_pplx:4.2057457 loss:165.07553 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.63843\n",
      "I0710 08:56:33.311762 140295626643200 summary_utils.py:349] Steps/second: 0.176843, Examples/second: 25.066956\n",
      "I0710 08:56:33.312566 140295626643200 trainer.py:508] step:  5556, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:122.80399 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2673455 log_pplx:4.1042085 loss:170.94028 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.64435\n",
      "I0710 08:56:44.018302 140295626643200 summary_utils.py:349] Steps/second: 0.176688, Examples/second: 25.027395\n",
      "I0710 08:56:44.019110 140295626643200 trainer.py:508] step:  5557, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:217.43608 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2667918 log_pplx:3.486737 loss:259.1517 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:520.65033\n",
      "I0710 08:56:47.873775 140295626643200 summary_utils.py:349] Steps/second: 0.176744, Examples/second: 25.038388\n",
      "I0710 08:56:47.874699 140295626643200 trainer.py:508] step:  5558, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:62.396942 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2690879 log_pplx:4.370626 loss:111.01389 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.65668\n",
      "I0710 08:56:54.059288 140295626643200 summary_utils.py:349] Steps/second: 0.176727, Examples/second: 25.025441\n",
      "I0710 08:56:54.060075 140295626643200 trainer.py:508] step:  5559, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:102.1727 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2671564 log_pplx:4.096086 loss:158.36491 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.66272\n",
      "I0710 08:56:57.128628 140295626643200 summary_utils.py:349] Steps/second: 0.176807, Examples/second: 25.056410\n",
      "I0710 08:56:57.129389 140295626643200 trainer.py:508] step:  5560, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:76.267326 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2686695 log_pplx:4.3510346 loss:68.375832 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.6684\n",
      "I0710 08:57:03.402209 140295626643200 summary_utils.py:349] Steps/second: 0.176788, Examples/second: 25.043083\n",
      "I0710 08:57:03.403078 140295626643200 trainer.py:508] step:  5561, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:94.938751 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2671129 log_pplx:4.0483036 loss:158.03563 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.67352\n",
      "I0710 08:57:09.844202 140295626643200 summary_utils.py:349] Steps/second: 0.176764, Examples/second: 25.029058\n",
      "I0710 08:57:09.845005 140295626643200 trainer.py:508] step:  5562, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:124.66911 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2676065 log_pplx:4.0985589 loss:167.22121 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.67828\n",
      "I0710 08:57:13.658734 140295626643200 summary_utils.py:349] Steps/second: 0.176820, Examples/second: 25.040178\n",
      "I0710 08:57:13.659555 140295626643200 trainer.py:508] step:  5563, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:45.443928 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2691237 log_pplx:4.251492 loss:106.6593 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.68292\n",
      "I0710 08:57:19.994164 140295626643200 summary_utils.py:349] Steps/second: 0.176799, Examples/second: 25.026640\n",
      "I0710 08:57:19.994928 140295626643200 trainer.py:508] step:  5564, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:87.952652 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2680246 log_pplx:4.0899062 loss:168.4019 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.68738\n",
      "I0710 08:57:23.007806 140295626643200 summary_utils.py:349] Steps/second: 0.176880, Examples/second: 25.057711\n",
      "I0710 08:57:23.008573 140295626643200 trainer.py:508] step:  5565, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:25.942184 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2687573 log_pplx:4.2949672 loss:67.041756 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.69238\n",
      "I0710 08:57:29.206952 140295626643200 summary_utils.py:349] Steps/second: 0.176863, Examples/second: 25.044762\n",
      "I0710 08:57:29.207833 140295626643200 trainer.py:508] step:  5566, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:104.68124 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2676173 log_pplx:4.1285152 loss:165.9147 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.69708\n",
      "I0710 08:57:33.018285 140295626643200 summary_utils.py:349] Steps/second: 0.176919, Examples/second: 25.055849\n",
      "I0710 08:57:33.019023 140295626643200 trainer.py:508] step:  5567, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:46.756424 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2693059 log_pplx:4.3087878 loss:108.6084 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.70245\n",
      "I0710 08:57:42.674851 140295626643200 summary_utils.py:349] Steps/second: 0.176797, Examples/second: 25.021218\n",
      "I0710 08:57:42.675734 140295626643200 trainer.py:508] step:  5568, steps/sec: 0.18, examples/sec: 25.02 grad_norm/all/loss:177.9919 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2667521 log_pplx:3.4052677 loss:246.28598 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:520.70782\n",
      "I0710 08:57:44.957833 140295626643200 summary_utils.py:349] Steps/second: 0.176900, Examples/second: 25.099190\n",
      "I0710 08:57:44.958794 140295626643200 trainer.py:508] step:  5569, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:22.186642 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2674803 log_pplx:4.1015487 loss:30.993925 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:520.71301\n",
      "I0710 08:57:51.453438 140295626643200 summary_utils.py:349] Steps/second: 0.176874, Examples/second: 25.084973\n",
      "I0710 08:57:51.454277 140295626643200 trainer.py:508] step:  5570, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:95.856575 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2673569 log_pplx:3.9609308 loss:159.08089 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.71783\n",
      "I0710 08:57:55.298498 140295626643200 summary_utils.py:349] Steps/second: 0.176929, Examples/second: 25.095846\n",
      "I0710 08:57:55.299554 140295626643200 trainer.py:508] step:  5571, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:47.323254 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2693934 log_pplx:4.2846723 loss:107.03648 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.7229\n",
      "I0710 08:58:01.523759 140295626643200 summary_utils.py:349] Steps/second: 0.176912, Examples/second: 25.082817\n",
      "I0710 08:58:01.524562 140295626643200 trainer.py:508] step:  5572, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:81.486679 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2675143 log_pplx:4.0032902 loss:157.17918 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.72784\n",
      "I0710 08:58:04.578274 140295626643200 summary_utils.py:349] Steps/second: 0.176990, Examples/second: 25.113463\n",
      "I0710 08:58:04.579237 140295626643200 trainer.py:508] step:  5573, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:27.871567 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2691901 log_pplx:4.2713213 loss:67.373428 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.73303\n",
      "I0710 08:58:10.808962 140295626643200 summary_utils.py:349] Steps/second: 0.176973, Examples/second: 25.100412\n",
      "I0710 08:58:10.809708 140295626643200 trainer.py:508] step:  5574, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:74.791092 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2678572 log_pplx:3.9857781 loss:164.01477 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.73798\n",
      "I0710 08:58:14.572173 140295626643200 summary_utils.py:349] Steps/second: 0.177030, Examples/second: 25.111590\n",
      "I0710 08:58:14.572903 140295626643200 trainer.py:508] step:  5575, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:48.687546 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2694286 log_pplx:4.3113656 loss:108.29612 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.74298\n",
      "I0710 08:58:21.045218 140295626643200 summary_utils.py:349] Steps/second: 0.177005, Examples/second: 25.097527\n",
      "I0710 08:58:21.046008 140295626643200 trainer.py:508] step:  5576, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:87.856987 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2677318 log_pplx:4.0364981 loss:164.4873 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.74774\n",
      "I0710 08:58:31.279578 140295626643200 summary_utils.py:349] Steps/second: 0.176867, Examples/second: 25.060647\n",
      "I0710 08:58:31.280440 140295626643200 trainer.py:508] step:  5577, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:228.9816 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2670106 log_pplx:3.372906 loss:242.93355 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:520.7525\n",
      "I0710 08:58:34.338860 140295626643200 summary_utils.py:349] Steps/second: 0.176945, Examples/second: 25.091130\n",
      "I0710 08:58:34.339700 140295626643200 trainer.py:508] step:  5578, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:33.730434 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2689116 log_pplx:4.1604495 loss:64.178185 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.75842\n",
      "I0710 08:58:40.473641 140295626643200 summary_utils.py:349] Steps/second: 0.176930, Examples/second: 25.078577\n",
      "I0710 08:58:40.474588 140295626643200 trainer.py:508] step:  5579, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:70.771591 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2679106 log_pplx:3.9466174 loss:155.84206 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.76422\n",
      "I0710 08:58:46.897039 140295626643200 summary_utils.py:349] Steps/second: 0.176907, Examples/second: 25.064823\n",
      "I0710 08:58:46.897775 140295626643200 trainer.py:508] step:  5580, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:88.924797 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2682439 log_pplx:4.0720119 loss:171.58443 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.76978\n",
      "I0710 08:58:50.756828 140295626643200 summary_utils.py:349] Steps/second: 0.176961, Examples/second: 25.075545\n",
      "I0710 08:58:50.757627 140295626643200 trainer.py:508] step:  5581, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:38.976192 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2694585 log_pplx:4.2185531 loss:106.20207 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.77521\n",
      "I0710 08:58:56.886558 140295626643200 summary_utils.py:349] Steps/second: 0.176946, Examples/second: 25.063064\n",
      "I0710 08:58:56.887394 140295626643200 trainer.py:508] step:  5582, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:72.688065 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2677768 log_pplx:3.9363086 loss:155.87781 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.78033\n",
      "I0710 08:58:59.892783 140295626643200 summary_utils.py:349] Steps/second: 0.177026, Examples/second: 25.093638\n",
      "I0710 08:58:59.893613 140295626643200 trainer.py:508] step:  5583, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:21.287083 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2691373 log_pplx:4.2114987 loss:65.590813 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.78552\n",
      "I0710 08:59:06.386379 140295626643200 summary_utils.py:349] Steps/second: 0.177000, Examples/second: 25.079615\n",
      "I0710 08:59:06.387146 140295626643200 trainer.py:508] step:  5584, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:82.61969 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2680303 log_pplx:3.9703372 loss:161.54312 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.79041\n",
      "I0710 08:59:15.831174 140295626643200 summary_utils.py:349] Steps/second: 0.176887, Examples/second: 25.046386\n",
      "I0710 08:59:15.831940 140295626643200 trainer.py:508] step:  5585, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:330.13126 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2672937 log_pplx:3.4337106 loss:249.20157 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:520.79572\n",
      "I0710 08:59:19.729178 140295626643200 summary_utils.py:349] Steps/second: 0.176939, Examples/second: 25.056906\n",
      "I0710 08:59:19.730034 140295626643200 trainer.py:508] step:  5586, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:43.485825 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2695128 log_pplx:4.2673869 loss:107.19142 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.80139\n",
      "I0710 08:59:26.147123 140295626643200 summary_utils.py:349] Steps/second: 0.176917, Examples/second: 25.043290\n",
      "I0710 08:59:26.148060 140295626643200 trainer.py:508] step:  5587, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:92.383949 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2681674 log_pplx:4.0277252 loss:165.94229 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.80701\n",
      "I0710 08:59:32.456565 140295626643200 summary_utils.py:349] Steps/second: 0.176897, Examples/second: 25.030160\n",
      "I0710 08:59:32.457558 140295626643200 trainer.py:508] step:  5588, steps/sec: 0.18, examples/sec: 25.03 grad_norm/all/loss:100.22533 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2680303 log_pplx:3.9940553 loss:160.36133 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.81299\n",
      "I0710 08:59:35.543277 140295626643200 summary_utils.py:349] Steps/second: 0.176973, Examples/second: 25.060229\n",
      "I0710 08:59:35.544247 140295626643200 trainer.py:508] step:  5589, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:33.539925 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2693069 log_pplx:4.2342958 loss:66.772865 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.81921\n",
      "I0710 08:59:39.396306 140295626643200 summary_utils.py:349] Steps/second: 0.177027, Examples/second: 25.070894\n",
      "I0710 08:59:39.397153 140295626643200 trainer.py:508] step:  5590, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:49.159794 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2696773 log_pplx:4.3203716 loss:107.84729 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.82495\n",
      "I0710 08:59:41.583929 140295626643200 summary_utils.py:349] Steps/second: 0.177130, Examples/second: 25.147721\n",
      "I0710 08:59:41.584711 140295626643200 trainer.py:508] step:  5591, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:14.480779 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2678472 log_pplx:4.1232052 loss:29.699963 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:520.83026\n",
      "I0710 08:59:47.940847 140295626643200 summary_utils.py:349] Steps/second: 0.177109, Examples/second: 25.134314\n",
      "I0710 08:59:47.941751 140295626643200 trainer.py:508] step:  5592, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:93.872559 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2684242 log_pplx:4.0540724 loss:171.48724 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.83521\n",
      "I0710 08:59:54.410185 140295626643200 summary_utils.py:349] Steps/second: 0.177084, Examples/second: 25.120462\n",
      "I0710 08:59:54.410961 140295626643200 trainer.py:508] step:  5593, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:84.874969 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.268178 log_pplx:3.9889462 loss:162.99832 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.8396\n",
      "I0710 08:59:58.258760 140295626643200 summary_utils.py:349] Steps/second: 0.177138, Examples/second: 25.131074\n",
      "I0710 08:59:58.259622 140295626643200 trainer.py:508] step:  5594, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:37.312321 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2700469 log_pplx:4.2441592 loss:107.64249 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.84412\n",
      "I0710 09:00:01.273677 140295626643200 summary_utils.py:349] Steps/second: 0.177216, Examples/second: 25.161263\n",
      "I0710 09:00:01.274522 140295626643200 trainer.py:508] step:  5595, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:20.610352 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2694761 log_pplx:4.1314354 loss:64.537544 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.84821\n",
      "I0710 09:00:07.529220 140295626643200 summary_utils.py:349] Steps/second: 0.177197, Examples/second: 25.148313\n",
      "I0710 09:00:07.529978 140295626643200 trainer.py:508] step:  5596, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:80.801537 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2683575 log_pplx:4.014564 loss:159.47855 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.85229\n",
      "I0710 09:00:18.358390 140295626643200 summary_utils.py:349] Steps/second: 0.177044, Examples/second: 25.109507\n",
      "I0710 09:00:18.359150 140295626643200 trainer.py:508] step:  5597, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:393.79883 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2676969 log_pplx:3.585999 loss:269.9361 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:520.85657\n",
      "I0710 09:00:24.759240 140295626643200 summary_utils.py:349] Steps/second: 0.177022, Examples/second: 25.096039\n",
      "I0710 09:00:24.759978 140295626643200 trainer.py:508] step:  5598, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:85.146584 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2688737 log_pplx:4.0713987 loss:173.95049 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.86261\n",
      "I0710 09:00:28.639549 140295626643200 summary_utils.py:349] Steps/second: 0.177074, Examples/second: 25.106480\n",
      "I0710 09:00:28.640311 140295626643200 trainer.py:508] step:  5599, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:66.721619 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2701604 log_pplx:4.3436322 loss:110.27395 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.86884\n",
      "I0710 09:00:35.182205 140295626643200 summary_utils.py:349] Steps/second: 0.177048, Examples/second: 25.092446\n",
      "I0710 09:00:35.183013 140295626643200 trainer.py:508] step:  5600, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:214.39856 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2691067 log_pplx:4.2186198 loss:181.34793 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.87476\n",
      "I0710 09:00:38.228151 140295626643200 summary_utils.py:349] Steps/second: 0.177124, Examples/second: 25.122339\n",
      "I0710 09:00:38.228931 140295626643200 trainer.py:508] step:  5601, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:27.593628 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2694488 log_pplx:4.1264005 loss:63.604599 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.88\n",
      "I0710 09:00:44.288617 140295626643200 summary_utils.py:349] Steps/second: 0.177112, Examples/second: 25.110323\n",
      "I0710 09:00:44.289394 140295626643200 trainer.py:508] step:  5602, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:88.99295 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2683042 log_pplx:3.9421616 loss:152.70949 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.88519\n",
      "I0710 09:00:48.142711 140295626643200 summary_utils.py:349] Steps/second: 0.177165, Examples/second: 25.120830\n",
      "I0710 09:00:48.143464 140295626643200 trainer.py:508] step:  5603, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:97.078484 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2701818 log_pplx:4.339179 loss:110.05244 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.8902\n",
      "I0710 09:00:54.596678 140295626643200 summary_utils.py:349] Steps/second: 0.177141, Examples/second: 25.107196\n",
      "I0710 09:00:54.597576 140295626643200 trainer.py:508] step:  5604, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:83.472221 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2686024 log_pplx:3.9643276 loss:161.94279 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.89575\n",
      "I0710 09:01:00.820813 140295626643200 summary_utils.py:349] Steps/second: 0.177124, Examples/second: 25.094546\n",
      "I0710 09:01:00.821638 140295626643200 trainer.py:508] step:  5605, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:78.223007 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2685988 log_pplx:4.0265312 loss:161.91687 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.90143\n",
      "I0710 09:01:10.877947 140295626643200 summary_utils.py:349] Steps/second: 0.176995, Examples/second: 25.059383\n",
      "I0710 09:01:10.878765 140295626643200 trainer.py:508] step:  5606, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:398.38086 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2676834 log_pplx:3.432528 loss:253.49222 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:520.90698\n",
      "I0710 09:01:14.713175 140295626643200 summary_utils.py:349] Steps/second: 0.177048, Examples/second: 25.069954\n",
      "I0710 09:01:14.714124 140295626643200 trainer.py:508] step:  5607, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:44.048683 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2702241 log_pplx:4.3278966 loss:110.33433 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.9115\n",
      "I0710 09:01:17.792344 140295626643200 summary_utils.py:349] Steps/second: 0.177123, Examples/second: 25.099525\n",
      "I0710 09:01:17.793154 140295626643200 trainer.py:508] step:  5608, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:25.335468 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2698891 log_pplx:4.2140684 loss:65.927124 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.91632\n",
      "I0710 09:01:24.319936 140295626643200 summary_utils.py:349] Steps/second: 0.177097, Examples/second: 25.085675\n",
      "I0710 09:01:24.320764 140295626643200 trainer.py:508] step:  5609, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:191.71217 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2685758 log_pplx:4.0543647 loss:167.29323 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.92102\n",
      "I0710 09:01:27.434676 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 09:01:30.990645 140295626643200 summary_utils.py:349] Steps/second: 0.177067, Examples/second: 25.071262\n",
      "I0710 09:01:30.991456 140295626643200 trainer.py:508] step:  5610, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:159.35385 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2686976 log_pplx:4.0146885 loss:165.80664 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.92706\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 09:01:32.712739 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 09:01:33.225359 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00005609\n",
      "I0710 09:01:34.893496 140295626643200 summary_utils.py:349] Steps/second: 0.177118, Examples/second: 25.081512\n",
      "I0710 09:01:34.894307 140295626643200 trainer.py:508] step:  5611, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:66.766403 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2703054 log_pplx:4.3315153 loss:108.2608 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.93414\n",
      "I0710 09:01:41.172728 140295626643200 summary_utils.py:349] Steps/second: 0.177100, Examples/second: 25.068745\n",
      "I0710 09:01:41.173522 140295626643200 trainer.py:508] step:  5612, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:88.15976 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.269189 log_pplx:4.0573878 loss:163.51273 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.94067\n",
      "I0710 09:01:44.211864 140295626643200 summary_utils.py:349] Steps/second: 0.177176, Examples/second: 25.098354\n",
      "I0710 09:01:44.212864 140295626643200 trainer.py:508] step:  5613, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:61.393456 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2700793 log_pplx:4.3263459 loss:67.565353 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.94702\n",
      "I0710 09:01:46.402465 140295626643200 summary_utils.py:349] Steps/second: 0.177276, Examples/second: 25.173580\n",
      "I0710 09:01:46.403414 140295626643200 trainer.py:508] step:  5614, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:14.153632 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2684 log_pplx:4.0299973 loss:29.524456 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:520.95276\n",
      "I0710 09:01:52.887707 140295626643200 summary_utils.py:349] Steps/second: 0.177252, Examples/second: 25.159890\n",
      "I0710 09:01:52.888492 140295626643200 trainer.py:508] step:  5615, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:104.14394 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2688898 log_pplx:4.073699 loss:164.42468 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.95801\n",
      "I0710 09:01:56.697478 140295626643200 summary_utils.py:349] Steps/second: 0.177305, Examples/second: 25.170431\n",
      "I0710 09:01:56.698254 140295626643200 trainer.py:508] step:  5616, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:64.882858 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2703919 log_pplx:4.29216 loss:106.92844 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.96283\n",
      "I0710 09:02:02.983728 140295626643200 summary_utils.py:349] Steps/second: 0.177286, Examples/second: 25.157590\n",
      "I0710 09:02:02.984606 140295626643200 trainer.py:508] step:  5617, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:99.907249 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2694935 log_pplx:4.1146817 loss:174.20535 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.96698\n",
      "I0710 09:02:09.452062 140295626643200 summary_utils.py:349] Steps/second: 0.177262, Examples/second: 25.144024\n",
      "I0710 09:02:09.452896 140295626643200 trainer.py:508] step:  5618, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:85.992409 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.269053 log_pplx:4.0095239 loss:163.63867 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.97125\n",
      "I0710 09:02:19.238373 140295626643200 summary_utils.py:349] Steps/second: 0.177142, Examples/second: 25.110287\n",
      "I0710 09:02:19.239184 140295626643200 trainer.py:508] step:  5619, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:296.20874 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2678474 log_pplx:3.4456594 loss:242.7467 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:520.97577\n",
      "I0710 09:02:22.332289 140295626643200 summary_utils.py:349] Steps/second: 0.177216, Examples/second: 25.139465\n",
      "I0710 09:02:22.333066 140295626643200 trainer.py:508] step:  5620, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:51.087166 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2703851 log_pplx:4.2917166 loss:66.957474 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:520.98193\n",
      "I0710 09:02:26.283622 140295626643200 summary_utils.py:349] Steps/second: 0.177265, Examples/second: 25.149386\n",
      "I0710 09:02:26.284391 140295626643200 trainer.py:508] step:  5621, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:42.264343 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2704543 log_pplx:4.2532525 loss:106.8098 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:520.98798\n",
      "I0710 09:02:32.699486 140295626643200 summary_utils.py:349] Steps/second: 0.177243, Examples/second: 25.136096\n",
      "I0710 09:02:32.700291 140295626643200 trainer.py:508] step:  5622, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:120.88615 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2686231 log_pplx:3.9795191 loss:156.4946 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.99365\n",
      "I0710 09:02:39.107091 140295626643200 summary_utils.py:349] Steps/second: 0.177220, Examples/second: 25.122867\n",
      "I0710 09:02:39.107894 140295626643200 trainer.py:508] step:  5623, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:141.20845 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2691182 log_pplx:4.0434613 loss:169.67375 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:520.99884\n",
      "I0710 09:02:45.628002 140295626643200 summary_utils.py:349] Steps/second: 0.177195, Examples/second: 25.109203\n",
      "I0710 09:02:45.628786 140295626643200 trainer.py:508] step:  5624, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:90.393814 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.268967 log_pplx:4.046236 loss:163.41736 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.00342\n",
      "I0710 09:02:49.420340 140295626643200 summary_utils.py:349] Steps/second: 0.177248, Examples/second: 25.119756\n",
      "I0710 09:02:49.421120 140295626643200 trainer.py:508] step:  5625, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:66.922043 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2705373 log_pplx:4.3261123 loss:108.31504 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.00787\n",
      "I0710 09:02:52.489598 140295626643200 summary_utils.py:349] Steps/second: 0.177323, Examples/second: 25.148887\n",
      "I0710 09:02:52.490431 140295626643200 trainer.py:508] step:  5626, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:34.512714 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2700588 log_pplx:4.1734324 loss:63.726353 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.0127\n",
      "I0710 09:02:59.368098 140295626643200 summary_utils.py:349] Steps/second: 0.177287, Examples/second: 25.133763\n",
      "I0710 09:02:59.368878 140295626643200 trainer.py:508] step:  5627, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:96.231461 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2691905 log_pplx:4.0270739 loss:165.61343 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.01758\n",
      "I0710 09:03:05.926037 140295626643200 summary_utils.py:349] Steps/second: 0.177261, Examples/second: 25.119981\n",
      "I0710 09:03:05.926849 140295626643200 trainer.py:508] step:  5628, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:79.161743 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.269052 log_pplx:3.9708908 loss:162.26053 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.02295\n",
      "I0710 09:03:09.829625 140295626643200 summary_utils.py:349] Steps/second: 0.177311, Examples/second: 25.130038\n",
      "I0710 09:03:09.830394 140295626643200 trainer.py:508] step:  5629, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:40.072514 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2708663 log_pplx:4.315258 loss:107.88145 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.02863\n",
      "I0710 09:03:16.316930 140295626643200 summary_utils.py:349] Steps/second: 0.177286, Examples/second: 25.116571\n",
      "I0710 09:03:16.317731 140295626643200 trainer.py:508] step:  5630, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:70.557518 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2692119 log_pplx:3.9148195 loss:156.98428 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.03418\n",
      "I0710 09:03:25.728136 140295626643200 summary_utils.py:349] Steps/second: 0.177178, Examples/second: 25.084767\n",
      "I0710 09:03:25.728895 140295626643200 trainer.py:508] step:  5631, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:201.71179 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2681652 log_pplx:3.4555221 loss:245.774 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:521.03955\n",
      "I0710 09:03:28.803458 140295626643200 summary_utils.py:349] Steps/second: 0.177252, Examples/second: 25.113719\n",
      "I0710 09:03:28.804337 140295626643200 trainer.py:508] step:  5632, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:73.23243 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2704062 log_pplx:4.294939 loss:67.913727 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.04523\n",
      "I0710 09:03:35.257208 140295626643200 summary_utils.py:349] Steps/second: 0.177228, Examples/second: 25.100447\n",
      "I0710 09:03:35.257992 140295626643200 trainer.py:508] step:  5633, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:82.859573 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2694693 log_pplx:3.9760776 loss:165.50424 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.05035\n",
      "I0710 09:03:39.080300 140295626643200 summary_utils.py:349] Steps/second: 0.177280, Examples/second: 25.110796\n",
      "I0710 09:03:39.081065 140295626643200 trainer.py:508] step:  5634, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:60.548851 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2709601 log_pplx:4.3128963 loss:108.2537 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.0556\n",
      "I0710 09:03:45.390851 140295626643200 summary_utils.py:349] Steps/second: 0.177261, Examples/second: 25.098129\n",
      "I0710 09:03:45.391624 140295626643200 trainer.py:508] step:  5635, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:91.358376 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2696259 log_pplx:4.0812159 loss:165.64635 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.06036\n",
      "I0710 09:03:51.385871 140295626643200 summary_utils.py:349] Steps/second: 0.177251, Examples/second: 25.086764\n",
      "I0710 09:03:51.386781 140295626643200 trainer.py:508] step:  5636, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:68.988045 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2690359 log_pplx:3.9566243 loss:150.94522 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.06543\n",
      "I0710 09:03:53.586207 140295626643200 summary_utils.py:349] Steps/second: 0.177350, Examples/second: 25.160412\n",
      "I0710 09:03:53.586977 140295626643200 base_runner.py:111] step:  5637, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:11.578045 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2694557 log_pplx:4.1724157 loss:30.828613 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:521.07043\n",
      "I0710 09:03:57.368137 140295626643200 summary_utils.py:349] Steps/second: 0.177403, Examples/second: 25.170861\n",
      "I0710 09:03:57.368938 140295626643200 trainer.py:508] step:  5638, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:45.903534 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2710255 log_pplx:4.2712908 loss:106.99585 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.07501\n",
      "I0710 09:04:00.444872 140295626643200 summary_utils.py:349] Steps/second: 0.177476, Examples/second: 25.199617\n",
      "I0710 09:04:00.445672 140295626643200 trainer.py:508] step:  5639, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:28.356445 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2707841 log_pplx:4.2255635 loss:66.040932 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.07965\n",
      "I0710 09:04:06.977412 140295626643200 summary_utils.py:349] Steps/second: 0.177450, Examples/second: 25.186004\n",
      "I0710 09:04:06.978412 140295626643200 trainer.py:508] step:  5640, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:88.279221 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2695549 log_pplx:4.0329361 loss:164.69502 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.08405\n",
      "I0710 09:04:13.351716 140295626643200 summary_utils.py:349] Steps/second: 0.177429, Examples/second: 25.173059\n",
      "I0710 09:04:13.352509 140295626643200 trainer.py:508] step:  5641, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:86.074921 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2696389 log_pplx:3.9956553 loss:167.96736 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.08881\n",
      "I0710 09:04:24.374738 140295626643200 summary_utils.py:349] Steps/second: 0.177276, Examples/second: 25.134960\n",
      "I0710 09:04:24.375553 140295626643200 trainer.py:508] step:  5642, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:209.25536 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2688385 log_pplx:3.4733238 loss:258.76263 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:521.09357\n",
      "I0710 09:04:30.845364 140295626643200 summary_utils.py:349] Steps/second: 0.177252, Examples/second: 25.121716\n",
      "I0710 09:04:30.846151 140295626643200 trainer.py:508] step:  5643, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:103.881 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2696959 log_pplx:4.008358 loss:164.84372 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.09924\n",
      "I0710 09:04:34.652991 140295626643200 summary_utils.py:349] Steps/second: 0.177304, Examples/second: 25.132022\n",
      "I0710 09:04:34.653753 140295626643200 trainer.py:508] step:  5644, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:53.153015 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2711153 log_pplx:4.2334976 loss:105.467 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.10474\n",
      "I0710 09:04:37.629009 140295635035904 trainer.py:345] Write summary @5645\n",
      "I0710 09:04:37.629271 140295626643200 summary_utils.py:349] Steps/second: 0.177380, Examples/second: 25.161030\n",
      "I0710 09:04:37.630415 140295626643200 trainer.py:508] step:  5645, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:22.121407 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2707292 log_pplx:4.114501 loss:63.565823 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.10992\n",
      "I0710 09:04:48.795353 140295626643200 summary_utils.py:349] Steps/second: 0.177223, Examples/second: 25.128918\n",
      "I0710 09:04:48.797988 140295626643200 trainer.py:508] step:  5646, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:86.893036 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2695144 log_pplx:4.009553 loss:162.88809 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.11481\n",
      "I0710 09:04:58.428785 140295626643200 summary_utils.py:349] Steps/second: 0.177110, Examples/second: 25.103057\n",
      "I0710 09:04:58.430175 140295626643200 trainer.py:508] step:  5647, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:87.397087 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2695216 log_pplx:3.9716983 loss:161.25095 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.11969\n",
      "I0710 09:05:03.883133 140295626643200 summary_utils.py:349] Steps/second: 0.177116, Examples/second: 25.106736\n",
      "I0710 09:05:03.884473 140295626643200 trainer.py:508] step:  5648, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:39.992386 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2710911 log_pplx:4.2744617 loss:107.28899 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.12469\n",
      "I0710 09:05:13.709383 140295626643200 summary_utils.py:349] Steps/second: 0.176998, Examples/second: 25.080201\n",
      "I0710 09:05:13.710683 140295626643200 trainer.py:508] step:  5649, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:74.782295 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2696065 log_pplx:3.9285171 loss:162.49329 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.12976\n",
      "I0710 09:05:18.146247 140295626643200 summary_utils.py:349] Steps/second: 0.177032, Examples/second: 25.103221\n",
      "I0710 09:05:18.147641 140295626643200 trainer.py:508] step:  5650, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:27.547485 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2712798 log_pplx:4.2827368 loss:66.71701 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.13507\n",
      "I0710 09:05:24.101579 140295635035904 trainer.py:354] Write summary done: step 5645\n",
      "I0710 09:05:24.698440 140295626643200 summary_utils.py:349] Steps/second: 0.177006, Examples/second: 25.102509\n",
      "I0710 09:05:24.699226 140295626643200 trainer.py:508] step:  5651, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:43.241062 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2714537 log_pplx:4.2590599 loss:108.12688 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.14026\n",
      "I0710 09:05:30.897167 140295626643200 summary_utils.py:349] Steps/second: 0.176991, Examples/second: 25.090508\n",
      "I0710 09:05:30.898041 140295626643200 trainer.py:508] step:  5652, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:77.79911 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2696413 log_pplx:3.9681666 loss:156.9906 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.14526\n",
      "I0710 09:05:41.098837 140295626643200 summary_utils.py:349] Steps/second: 0.176863, Examples/second: 25.056281\n",
      "I0710 09:05:41.099643 140295626643200 trainer.py:508] step:  5653, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:207.52678 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2689645 log_pplx:3.4435906 loss:252.24301 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:521.1507\n",
      "I0710 09:05:47.600691 140295626643200 summary_utils.py:349] Steps/second: 0.176840, Examples/second: 25.043155\n",
      "I0710 09:05:47.601447 140295626643200 trainer.py:508] step:  5654, steps/sec: 0.18, examples/sec: 25.04 grad_norm/all/loss:71.498322 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2696661 log_pplx:3.9100914 loss:156.79465 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.15607\n",
      "I0710 09:05:50.675656 140295626643200 summary_utils.py:349] Steps/second: 0.176912, Examples/second: 25.071479\n",
      "I0710 09:05:50.676484 140295626643200 trainer.py:508] step:  5655, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:28.36727 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.270985 log_pplx:4.1798 loss:65.946136 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.16193\n",
      "I0710 09:05:56.792621 140295626643200 summary_utils.py:349] Steps/second: 0.176899, Examples/second: 25.059881\n",
      "I0710 09:05:56.793418 140295626643200 trainer.py:508] step:  5656, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:79.797081 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.27001 log_pplx:4.0057487 loss:159.9796 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.16742\n",
      "I0710 09:06:00.558135 140295626643200 summary_utils.py:349] Steps/second: 0.176952, Examples/second: 25.070252\n",
      "I0710 09:06:00.558864 140295626643200 trainer.py:508] step:  5657, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:48.94421 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2713678 log_pplx:4.1797528 loss:105.64325 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.1731\n",
      "I0710 09:06:06.955217 140295626643200 summary_utils.py:349] Steps/second: 0.176931, Examples/second: 25.057565\n",
      "I0710 09:06:06.955943 140295626643200 trainer.py:508] step:  5658, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:79.223724 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2696044 log_pplx:3.9631648 loss:156.8918 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.17834\n",
      "I0710 09:06:09.138032 140295626643200 summary_utils.py:349] Steps/second: 0.177028, Examples/second: 25.129718\n",
      "I0710 09:06:09.138909 140295626643200 trainer.py:508] step:  5659, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:9.8044806 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2698225 log_pplx:4.1038699 loss:29.961458 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:521.18341\n",
      "I0710 09:06:15.551933 140295626643200 summary_utils.py:349] Steps/second: 0.177006, Examples/second: 25.116923\n",
      "I0710 09:06:15.552752 140295626643200 trainer.py:508] step:  5660, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:72.006706 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2701297 log_pplx:3.9801092 loss:161.64218 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.18817\n",
      "I0710 09:06:19.311465 140295626643200 summary_utils.py:349] Steps/second: 0.177059, Examples/second: 25.127252\n",
      "I0710 09:06:19.312273 140295626643200 trainer.py:508] step:  5661, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:39.462234 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2716627 log_pplx:4.1253934 loss:103.36687 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.19305\n",
      "I0710 09:06:22.357297 140295626643200 summary_utils.py:349] Steps/second: 0.177132, Examples/second: 25.155509\n",
      "I0710 09:06:22.358139 140295626643200 trainer.py:508] step:  5662, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:25.626102 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2712257 log_pplx:4.1345692 loss:63.6982 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.19788\n",
      "I0710 09:06:28.922686 140295626643200 summary_utils.py:349] Steps/second: 0.177106, Examples/second: 25.142115\n",
      "I0710 09:06:28.923625 140295626643200 trainer.py:508] step:  5663, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:88.008133 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2700862 log_pplx:3.8730052 loss:158.84163 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.20245\n",
      "I0710 09:06:38.916721 140295626643200 summary_utils.py:349] Steps/second: 0.176985, Examples/second: 25.108937\n",
      "I0710 09:06:38.917498 140295626643200 trainer.py:508] step:  5664, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:212.55316 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2690439 log_pplx:3.3029447 loss:237.39914 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:521.20734\n",
      "I0710 09:06:42.733122 140295626643200 summary_utils.py:349] Steps/second: 0.177036, Examples/second: 25.119009\n",
      "I0710 09:06:42.733963 140295626643200 trainer.py:508] step:  5665, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:57.086788 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2720906 log_pplx:4.3386416 loss:110.44556 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.2132\n",
      "I0710 09:06:49.124877 140295626643200 summary_utils.py:349] Steps/second: 0.177015, Examples/second: 25.106379\n",
      "I0710 09:06:49.125598 140295626643200 trainer.py:508] step:  5666, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:107.83208 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2703804 log_pplx:4.0240541 loss:165.53952 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.21881\n",
      "I0710 09:06:55.455682 140295626643200 summary_utils.py:349] Steps/second: 0.176996, Examples/second: 25.094014\n",
      "I0710 09:06:55.456500 140295626643200 trainer.py:508] step:  5667, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:85.756927 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2703004 log_pplx:4.0239224 loss:164.02515 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.22449\n",
      "I0710 09:06:58.486675 140295626643200 summary_utils.py:349] Steps/second: 0.177069, Examples/second: 25.122185\n",
      "I0710 09:06:58.487579 140295626643200 trainer.py:508] step:  5668, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:19.614061 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2711933 log_pplx:4.1292372 loss:64.245132 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.23029\n",
      "I0710 09:07:02.286192 140295626643200 summary_utils.py:349] Steps/second: 0.177120, Examples/second: 25.132285\n",
      "I0710 09:07:02.287028 140295626643200 trainer.py:508] step:  5669, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:48.161598 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2719536 log_pplx:4.253571 loss:108.99776 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.2359\n",
      "I0710 09:07:08.351170 140295626643200 summary_utils.py:349] Steps/second: 0.177109, Examples/second: 25.120963\n",
      "I0710 09:07:08.352005 140295626643200 trainer.py:508] step:  5670, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:105.44435 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2702787 log_pplx:4.0150003 loss:160.09814 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.24158\n",
      "I0710 09:07:14.615584 140295626643200 summary_utils.py:349] Steps/second: 0.177091, Examples/second: 25.108881\n",
      "I0710 09:07:14.616439 140295626643200 trainer.py:508] step:  5671, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:74.753181 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2701582 log_pplx:3.9379573 loss:154.51561 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.24762\n",
      "I0710 09:07:24.317481 140295626643200 summary_utils.py:349] Steps/second: 0.176979, Examples/second: 25.077133\n",
      "I0710 09:07:24.318218 140295626643200 trainer.py:508] step:  5672, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:223.39479 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2692484 log_pplx:3.3593149 loss:239.18326 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:521.25378\n",
      "I0710 09:07:28.134241 140295626643200 summary_utils.py:349] Steps/second: 0.177030, Examples/second: 25.087151\n",
      "I0710 09:07:28.134968 140295626643200 trainer.py:508] step:  5673, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:84.988068 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2718809 log_pplx:4.2482848 loss:107.90642 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.26044\n",
      "I0710 09:07:34.222168 140295626643200 summary_utils.py:349] Steps/second: 0.177018, Examples/second: 25.075828\n",
      "I0710 09:07:34.223352 140295626643200 trainer.py:508] step:  5674, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:163.20135 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2707045 log_pplx:4.1097078 loss:165.31299 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.2663\n",
      "I0710 09:07:37.272084 140295626643200 summary_utils.py:349] Steps/second: 0.177090, Examples/second: 25.103764\n",
      "I0710 09:07:37.272824 140295626643200 trainer.py:508] step:  5675, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:31.957043 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2718529 log_pplx:4.2427969 loss:66.890343 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.27118\n",
      "I0710 09:07:43.431766 140295626643200 summary_utils.py:349] Steps/second: 0.177075, Examples/second: 25.092162\n",
      "I0710 09:07:43.432495 140295626643200 trainer.py:508] step:  5676, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:82.95536 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2703913 log_pplx:3.9124436 loss:155.71527 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.27563\n",
      "I0710 09:07:49.748651 140295626643200 summary_utils.py:349] Steps/second: 0.177057, Examples/second: 25.079969\n",
      "I0710 09:07:49.749411 140295626643200 trainer.py:508] step:  5677, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:88.617722 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.270658 log_pplx:3.9724746 loss:164.90735 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.28003\n",
      "I0710 09:07:53.448159 140295626643200 summary_utils.py:349] Steps/second: 0.177111, Examples/second: 25.090402\n",
      "I0710 09:07:53.449175 140295626643200 trainer.py:508] step:  5678, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:110.69283 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2720809 log_pplx:4.3040576 loss:108.46227 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.28485\n",
      "I0710 09:07:59.671627 140295626643200 summary_utils.py:349] Steps/second: 0.177095, Examples/second: 25.078594\n",
      "I0710 09:07:59.672374 140295626643200 trainer.py:508] step:  5679, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:81.710197 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2703503 log_pplx:3.8599875 loss:154.30301 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.29041\n",
      "I0710 09:08:02.723354 140295626643200 summary_utils.py:349] Steps/second: 0.177166, Examples/second: 25.106411\n",
      "I0710 09:08:02.724212 140295626643200 trainer.py:508] step:  5680, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:23.704479 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2720146 log_pplx:4.2234168 loss:65.594933 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.29608\n",
      "I0710 09:08:09.323248 140295626643200 summary_utils.py:349] Steps/second: 0.177140, Examples/second: 25.093141\n",
      "I0710 09:08:09.324120 140295626643200 trainer.py:508] step:  5681, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:141.70396 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2700182 log_pplx:4.0008817 loss:156.28444 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.30157\n",
      "I0710 09:08:13.337090 140295626643200 summary_utils.py:349] Steps/second: 0.177184, Examples/second: 25.102315\n",
      "I0710 09:08:13.337801 140295626643200 trainer.py:508] step:  5682, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:70.292297 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2723012 log_pplx:4.3185816 loss:108.74728 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.30682\n",
      "I0710 09:08:24.019879 140295626643200 summary_utils.py:349] Steps/second: 0.177047, Examples/second: 25.067066\n",
      "I0710 09:08:24.020654 140295626643200 trainer.py:508] step:  5683, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:216.32051 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2695059 log_pplx:3.3586025 loss:251.64326 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:521.31158\n",
      "I0710 09:08:26.182367 140295626643200 summary_utils.py:349] Steps/second: 0.177142, Examples/second: 25.137767\n",
      "I0710 09:08:26.183213 140295626643200 trainer.py:508] step:  5684, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:13.889545 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2703972 log_pplx:4.0381446 loss:29.623575 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:521.31708\n",
      "I0710 09:08:32.489728 140295626643200 summary_utils.py:349] Steps/second: 0.177124, Examples/second: 25.125647\n",
      "I0710 09:08:32.490529 140295626643200 trainer.py:508] step:  5685, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:134.27907 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2705139 log_pplx:3.8977075 loss:155.51852 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.32239\n",
      "I0710 09:08:39.000041 140295626643200 summary_utils.py:349] Steps/second: 0.177100, Examples/second: 25.112765\n",
      "I0710 09:08:39.000782 140295626643200 trainer.py:508] step:  5686, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:129.98409 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2710831 log_pplx:3.9674752 loss:167.37785 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.32837\n",
      "I0710 09:08:42.831144 140295626643200 summary_utils.py:349] Steps/second: 0.177150, Examples/second: 25.122591\n",
      "I0710 09:08:42.831950 140295626643200 trainer.py:508] step:  5687, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:55.141762 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2724239 log_pplx:4.2906928 loss:107.58913 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.33459\n",
      "I0710 09:08:45.889067 140295626643200 summary_utils.py:349] Steps/second: 0.177220, Examples/second: 25.150180\n",
      "I0710 09:08:45.889854 140295626643200 trainer.py:508] step:  5688, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:33.802742 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2718781 log_pplx:4.1599779 loss:65.340904 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.34039\n",
      "I0710 09:08:52.129282 140295626643200 summary_utils.py:349] Steps/second: 0.177204, Examples/second: 25.138344\n",
      "I0710 09:08:52.130067 140295626643200 trainer.py:508] step:  5689, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:97.732086 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2704494 log_pplx:3.9478142 loss:152.78038 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.34576\n",
      "I0710 09:08:55.901795 140295626643200 summary_utils.py:349] Steps/second: 0.177255, Examples/second: 25.148362\n",
      "I0710 09:08:55.902723 140295626643200 trainer.py:508] step:  5690, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:46.625885 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2722987 log_pplx:4.2566748 loss:105.59214 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.35083\n",
      "I0710 09:09:02.101801 140295626643200 summary_utils.py:349] Steps/second: 0.177240, Examples/second: 25.136702\n",
      "I0710 09:09:02.102758 140295626643200 trainer.py:508] step:  5691, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:85.528336 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2705755 log_pplx:3.9380863 loss:157.67111 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.35559\n",
      "I0710 09:09:08.449770 140295626643200 summary_utils.py:349] Steps/second: 0.177220, Examples/second: 25.124493\n",
      "I0710 09:09:08.450567 140295626643200 trainer.py:508] step:  5692, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:97.805908 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2710942 log_pplx:3.9638746 loss:163.55939 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.36041\n",
      "I0710 09:09:11.508779 140295626643200 summary_utils.py:349] Steps/second: 0.177291, Examples/second: 25.151968\n",
      "I0710 09:09:11.509610 140295626643200 trainer.py:508] step:  5693, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:47.211609 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.271957 log_pplx:4.2588472 loss:66.511215 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.36554\n",
      "I0710 09:09:20.830100 140295626643200 summary_utils.py:349] Steps/second: 0.177191, Examples/second: 25.122188\n",
      "I0710 09:09:20.831061 140295626643200 trainer.py:508] step:  5694, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:213.09633 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2698956 log_pplx:3.2771726 loss:232.35152 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:521.37085\n",
      "I0710 09:09:26.828837 140295626643200 summary_utils.py:349] Steps/second: 0.177181, Examples/second: 25.111371\n",
      "I0710 09:09:26.829608 140295626643200 trainer.py:508] step:  5695, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:89.911613 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2709141 log_pplx:3.9705782 loss:162.84334 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.37585\n",
      "I0710 09:09:30.683753 140295626643200 summary_utils.py:349] Steps/second: 0.177230, Examples/second: 25.121035\n",
      "I0710 09:09:30.684524 140295626643200 trainer.py:508] step:  5696, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:48.621384 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2727135 log_pplx:4.2247038 loss:106.35692 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.38159\n",
      "I0710 09:09:36.678312 140295626643200 summary_utils.py:349] Steps/second: 0.177220, Examples/second: 25.110250\n",
      "I0710 09:09:36.679152 140295626643200 trainer.py:508] step:  5697, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:145.63853 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.271163 log_pplx:3.981775 loss:164.19844 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.38745\n",
      "I0710 09:09:43.084285 140295626643200 summary_utils.py:349] Steps/second: 0.177199, Examples/second: 25.097909\n",
      "I0710 09:09:43.085090 140295626643200 trainer.py:508] step:  5698, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:115.53384 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2709095 log_pplx:4.0477033 loss:160.74442 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.39307\n",
      "I0710 09:09:46.181834 140295626643200 summary_utils.py:349] Steps/second: 0.177268, Examples/second: 25.125104\n",
      "I0710 09:09:46.182766 140295626643200 trainer.py:508] step:  5699, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:24.004246 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2723693 log_pplx:4.1097002 loss:65.161217 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.39868\n",
      "I0710 09:09:50.090781 140295626643200 summary_utils.py:349] Steps/second: 0.177315, Examples/second: 25.134524\n",
      "I0710 09:09:50.091585 140295626643200 trainer.py:508] step:  5700, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:62.02562 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2727622 log_pplx:4.2331328 loss:105.80186 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.40399\n",
      "I0710 09:09:56.517839 140295626643200 summary_utils.py:349] Steps/second: 0.177294, Examples/second: 25.122104\n",
      "I0710 09:09:56.518662 140295626643200 trainer.py:508] step:  5701, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:102.96316 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2709142 log_pplx:3.9954338 loss:159.61758 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.40936\n",
      "I0710 09:10:02.822642 140295626643200 summary_utils.py:349] Steps/second: 0.177276, Examples/second: 25.110176\n",
      "I0710 09:10:02.823450 140295626643200 trainer.py:508] step:  5702, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:93.610306 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2710289 log_pplx:3.9405046 loss:160.42781 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.41504\n",
      "I0710 09:10:06.673212 140295626643200 summary_utils.py:349] Steps/second: 0.177324, Examples/second: 25.119804\n",
      "I0710 09:10:06.674198 140295626643200 trainer.py:508] step:  5703, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:42.930523 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.272947 log_pplx:4.305275 loss:108.14311 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.4209\n",
      "I0710 09:10:09.748891 140295626643200 summary_utils.py:349] Steps/second: 0.177393, Examples/second: 25.146974\n",
      "I0710 09:10:09.749737 140295626643200 trainer.py:508] step:  5704, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:41.54977 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2721845 log_pplx:4.2049112 loss:64.781921 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.42633\n",
      "I0710 09:10:16.159713 140295626643200 summary_utils.py:349] Steps/second: 0.177372, Examples/second: 25.134641\n",
      "I0710 09:10:16.160518 140295626643200 trainer.py:508] step:  5705, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:102.42886 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2711831 log_pplx:4.0743251 loss:165.92688 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.43115\n",
      "I0710 09:10:26.004483 140295626643200 summary_utils.py:349] Steps/second: 0.177259, Examples/second: 25.103185\n",
      "I0710 09:10:26.005305 140295626643200 trainer.py:508] step:  5706, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:227.91348 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2701043 log_pplx:3.4213552 loss:251.46964 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:521.43628\n",
      "I0710 09:10:32.365231 140295626643200 summary_utils.py:349] Steps/second: 0.177240, Examples/second: 25.091115\n",
      "I0710 09:10:32.366150 140295626643200 trainer.py:508] step:  5707, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:81.039253 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2713753 log_pplx:4.0238523 loss:161.80916 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.44263\n",
      "I0710 09:10:36.170359 140295626643200 summary_utils.py:349] Steps/second: 0.177289, Examples/second: 25.100885\n",
      "I0710 09:10:36.171182 140295626643200 trainer.py:508] step:  5708, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:69.564911 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2728502 log_pplx:4.2423272 loss:108.33844 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.44904\n",
      "I0710 09:10:38.379486 140295626643200 summary_utils.py:349] Steps/second: 0.177381, Examples/second: 25.169983\n",
      "I0710 09:10:38.380303 140295626643200 trainer.py:508] step:  5709, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:16.836271 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2713948 log_pplx:4.1191182 loss:30.571581 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:521.45465\n",
      "I0710 09:10:44.701703 140295626643200 summary_utils.py:349] Steps/second: 0.177363, Examples/second: 25.158018\n",
      "I0710 09:10:44.702665 140295626643200 trainer.py:508] step:  5710, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:124.84388 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2712855 log_pplx:4.0029855 loss:159.86923 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.46014\n",
      "I0710 09:10:51.110625 140295626643200 summary_utils.py:349] Steps/second: 0.177342, Examples/second: 25.145747\n",
      "I0710 09:10:51.111395 140295626643200 trainer.py:508] step:  5711, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:124.67319 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2715266 log_pplx:4.1125627 loss:163.98842 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.46588\n",
      "I0710 09:10:54.132721 140295626643200 summary_utils.py:349] Steps/second: 0.177412, Examples/second: 25.172926\n",
      "I0710 09:10:54.133564 140295626643200 trainer.py:508] step:  5712, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:23.040508 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2720335 log_pplx:4.0562739 loss:61.446217 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.4718\n",
      "I0710 09:10:58.008838 140295626643200 summary_utils.py:349] Steps/second: 0.177459, Examples/second: 25.182346\n",
      "I0710 09:10:58.009643 140295626643200 trainer.py:508] step:  5713, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:39.997879 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2730143 log_pplx:4.2387018 loss:107.769 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.47742\n",
      "I0710 09:11:04.310943 140295626643200 summary_utils.py:349] Steps/second: 0.177442, Examples/second: 25.170481\n",
      "I0710 09:11:04.311706 140295626643200 trainer.py:508] step:  5714, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:84.948677 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2715465 log_pplx:4.0388632 loss:162.86716 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.48267\n",
      "I0710 09:11:10.506846 140295626643200 summary_utils.py:349] Steps/second: 0.177427, Examples/second: 25.159041\n",
      "I0710 09:11:10.507725 140295626643200 trainer.py:508] step:  5715, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:88.674118 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.271118 log_pplx:4.0152016 loss:157.49629 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.48755\n",
      "I0710 09:11:20.963565 140295626643200 summary_utils.py:349] Steps/second: 0.177298, Examples/second: 25.125494\n",
      "I0710 09:11:20.964343 140295626643200 trainer.py:508] step:  5716, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:270.04761 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2706598 log_pplx:3.351486 loss:241.80972 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:521.49219\n",
      "I0710 09:11:27.112502 140295626643200 summary_utils.py:349] Steps/second: 0.177284, Examples/second: 25.114302\n",
      "I0710 09:11:27.113295 140295626643200 trainer.py:508] step:  5717, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:90.541855 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2717642 log_pplx:3.9712503 loss:163.4666 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.4978\n",
      "I0710 09:11:30.932543 140295626643200 summary_utils.py:349] Steps/second: 0.177333, Examples/second: 25.123921\n",
      "I0710 09:11:30.933332 140295626643200 trainer.py:508] step:  5718, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:53.449291 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2730112 log_pplx:4.145071 loss:104.66305 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.50323\n",
      "I0710 09:11:33.918975 140295626643200 summary_utils.py:349] Steps/second: 0.177404, Examples/second: 25.151083\n",
      "I0710 09:11:33.919837 140295626643200 trainer.py:508] step:  5719, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:24.141205 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2730466 log_pplx:4.2128668 loss:65.64502 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.50842\n",
      "I0710 09:11:34.416118 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 09:11:39.321123 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 09:11:39.850219 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00005719\n",
      "I0710 09:11:40.173376 140295626643200 summary_utils.py:349] Steps/second: 0.177387, Examples/second: 25.139491\n",
      "I0710 09:11:40.174134 140295626643200 trainer.py:508] step:  5720, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:120.86086 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2715254 log_pplx:4.079052 loss:166.11938 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.51337\n",
      "I0710 09:11:46.406713 140295626643200 summary_utils.py:349] Steps/second: 0.177371, Examples/second: 25.128000\n",
      "I0710 09:11:46.407460 140295626643200 trainer.py:508] step:  5721, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:131.43681 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2717195 log_pplx:3.9805338 loss:165.1424 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.51807\n",
      "I0710 09:11:50.227964 140295626643200 summary_utils.py:349] Steps/second: 0.177420, Examples/second: 25.137580\n",
      "I0710 09:11:50.228793 140295626643200 trainer.py:508] step:  5722, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:74.935432 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2730317 log_pplx:4.2754703 loss:106.75315 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.52258\n",
      "I0710 09:11:56.652955 140295626643200 summary_utils.py:349] Steps/second: 0.177399, Examples/second: 25.125388\n",
      "I0710 09:11:56.653935 140295626643200 trainer.py:508] step:  5723, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:133.36633 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2720325 log_pplx:3.9765658 loss:169.05376 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.52771\n",
      "I0710 09:11:59.615998 140295626643200 summary_utils.py:349] Steps/second: 0.177470, Examples/second: 25.152532\n",
      "I0710 09:11:59.616745 140295626643200 trainer.py:508] step:  5724, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:26.616079 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2730066 log_pplx:4.1380768 loss:64.431152 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.53394\n",
      "I0710 09:12:05.906173 140295626643200 summary_utils.py:349] Steps/second: 0.177452, Examples/second: 25.140849\n",
      "I0710 09:12:05.907006 140295626643200 trainer.py:508] step:  5725, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:80.183426 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.271637 log_pplx:3.9795036 loss:161.76682 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.53992\n",
      "I0710 09:12:09.753539 140295626643200 summary_utils.py:349] Steps/second: 0.177500, Examples/second: 25.150295\n",
      "I0710 09:12:09.754375 140295626643200 trainer.py:508] step:  5726, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:63.461376 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2733424 log_pplx:4.2647052 loss:108.35017 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.54602\n",
      "I0710 09:12:16.416954 140295626643200 summary_utils.py:349] Steps/second: 0.177472, Examples/second: 25.137232\n",
      "I0710 09:12:16.417727 140295626643200 trainer.py:508] step:  5727, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:92.446716 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2720207 log_pplx:3.9844759 loss:164.60867 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.55182\n",
      "I0710 09:12:27.521709 140295626643200 summary_utils.py:349] Steps/second: 0.177328, Examples/second: 25.101626\n",
      "I0710 09:12:27.522568 140295626643200 trainer.py:508] step:  5728, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:236.68297 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2708744 log_pplx:3.3352168 loss:241.21953 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:521.55719\n",
      "I0710 09:12:30.557369 140295626643200 summary_utils.py:349] Steps/second: 0.177396, Examples/second: 25.128385\n",
      "I0710 09:12:30.558180 140295626643200 trainer.py:508] step:  5729, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:37.042969 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2730314 log_pplx:4.167819 loss:65.301254 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.56232\n",
      "I0710 09:12:37.030093 140295626643200 summary_utils.py:349] Steps/second: 0.177374, Examples/second: 25.116097\n",
      "I0710 09:12:37.030867 140295626643200 trainer.py:508] step:  5730, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:114.17974 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2717986 log_pplx:3.9132993 loss:154.96666 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.5672\n",
      "I0710 09:12:40.843474 140295626643200 summary_utils.py:349] Steps/second: 0.177422, Examples/second: 25.125640\n",
      "I0710 09:12:40.844278 140295626643200 trainer.py:508] step:  5731, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:41.894733 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2734125 log_pplx:4.2038789 loss:105.51737 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.57275\n",
      "I0710 09:12:47.123626 140295626643200 summary_utils.py:349] Steps/second: 0.177406, Examples/second: 25.114091\n",
      "I0710 09:12:47.124432 140295626643200 trainer.py:508] step:  5732, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:113.40884 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.271644 log_pplx:3.8886051 loss:155.49559 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.57825\n",
      "I0710 09:12:49.317887 140295626643200 summary_utils.py:349] Steps/second: 0.177496, Examples/second: 25.181898\n",
      "I0710 09:12:49.318684 140295626643200 trainer.py:508] step:  5733, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:27.329048 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2715918 log_pplx:4.111165 loss:29.581116 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:521.58423\n",
      "I0710 09:12:55.711853 140295626643200 summary_utils.py:349] Steps/second: 0.177476, Examples/second: 25.169886\n",
      "I0710 09:12:55.712625 140295626643200 trainer.py:508] step:  5734, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:75.840233 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2720284 log_pplx:3.9447806 loss:165.38493 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.58972\n",
      "I0710 09:12:58.732896 140295626643200 summary_utils.py:349] Steps/second: 0.177545, Examples/second: 25.196556\n",
      "I0710 09:12:58.733640 140295626643200 trainer.py:508] step:  5735, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:24.300724 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2729821 log_pplx:4.1575541 loss:64.019829 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.5954\n",
      "I0710 09:13:02.543187 140295626643200 summary_utils.py:349] Steps/second: 0.177593, Examples/second: 25.206035\n",
      "I0710 09:13:02.544007 140295626643200 trainer.py:508] step:  5736, steps/sec: 0.18, examples/sec: 25.21 grad_norm/all/loss:76.604904 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2735765 log_pplx:4.3412232 loss:108.20499 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.60071\n",
      "I0710 09:13:08.558774 140295626643200 summary_utils.py:349] Steps/second: 0.177583, Examples/second: 25.195435\n",
      "I0710 09:13:08.559677 140295626643200 base_runner.py:111] step:  5737, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:87.22403 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2718568 log_pplx:3.9941945 loss:157.67082 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.60516\n",
      "I0710 09:13:20.138808 140295626643200 summary_utils.py:349] Steps/second: 0.177427, Examples/second: 25.158241\n",
      "I0710 09:13:20.139769 140295626643200 trainer.py:508] step:  5738, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:407.34088 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2708422 log_pplx:3.4515193 loss:238.15483 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:521.60938\n",
      "I0710 09:13:26.227808 140295626643200 summary_utils.py:349] Steps/second: 0.177415, Examples/second: 25.147439\n",
      "I0710 09:13:26.228735 140295626643200 trainer.py:508] step:  5739, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:132.19937 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.271906 log_pplx:3.9929485 loss:155.32571 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.61505\n",
      "I0710 09:13:30.037192 140295626643200 summary_utils.py:349] Steps/second: 0.177463, Examples/second: 25.156910\n",
      "I0710 09:13:30.037972 140295626643200 trainer.py:508] step:  5740, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:39.94854 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2738296 log_pplx:4.2401681 loss:106.48125 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.62054\n",
      "I0710 09:13:36.581783 140295626643200 summary_utils.py:349] Steps/second: 0.177439, Examples/second: 25.144436\n",
      "I0710 09:13:36.582622 140295626643200 trainer.py:508] step:  5741, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:142.80711 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2722026 log_pplx:4.0247998 loss:160.84106 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.62579\n",
      "I0710 09:13:39.647271 140295626643200 summary_utils.py:349] Steps/second: 0.177506, Examples/second: 25.170792\n",
      "I0710 09:13:39.648077 140295626643200 trainer.py:508] step:  5742, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:61.518597 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2730923 log_pplx:4.2151351 loss:64.659515 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.63074\n",
      "I0710 09:13:46.292846 140295626643200 summary_utils.py:349] Steps/second: 0.177480, Examples/second: 25.157948\n",
      "I0710 09:13:46.293683 140295626643200 trainer.py:508] step:  5743, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:172.49548 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2724881 log_pplx:4.085279 loss:170.81575 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.63593\n",
      "I0710 09:13:50.139876 140295626643200 summary_utils.py:349] Steps/second: 0.177526, Examples/second: 25.167247\n",
      "I0710 09:13:50.140668 140295626643200 trainer.py:508] step:  5744, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:115.16008 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2736341 log_pplx:4.3318052 loss:107.80779 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.64136\n",
      "I0710 09:13:56.568691 140295626643200 summary_utils.py:349] Steps/second: 0.177505, Examples/second: 25.155227\n",
      "I0710 09:13:56.569794 140295626643200 trainer.py:508] step:  5745, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:97.120522 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.27238 log_pplx:4.0631404 loss:163.23666 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.64734\n",
      "I0710 09:14:02.863608 140295626643200 summary_utils.py:349] Steps/second: 0.177488, Examples/second: 25.143725\n",
      "I0710 09:14:02.864522 140295626643200 trainer.py:508] step:  5746, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:89.312805 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2723521 log_pplx:3.9695706 loss:164.38985 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.65375\n",
      "I0710 09:14:04.528730 140295635035904 trainer.py:345] Write summary @5746\n",
      "I0710 09:14:09.130420 140295626643200 summary_utils.py:349] Steps/second: 0.177472, Examples/second: 25.158162\n",
      "I0710 09:14:09.131524 140295626643200 trainer.py:508] step:  5747, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:51.763565 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.273434 log_pplx:4.1632061 loss:64.903732 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.65991\n",
      "I0710 09:14:26.375874 140295626643200 summary_utils.py:349] Steps/second: 0.177170, Examples/second: 25.100539\n",
      "I0710 09:14:26.377288 140295626643200 trainer.py:508] step:  5748, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:243.72365 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2716589 log_pplx:3.3463614 loss:250.22415 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:521.66559\n",
      "I0710 09:14:31.690022 140295626643200 summary_utils.py:349] Steps/second: 0.177179, Examples/second: 25.104429\n",
      "I0710 09:14:31.691314 140295626643200 trainer.py:508] step:  5749, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:87.19075 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2739195 log_pplx:4.2857623 loss:106.98334 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.67169\n",
      "I0710 09:14:40.999166 140295626643200 summary_utils.py:349] Steps/second: 0.177084, Examples/second: 25.081991\n",
      "I0710 09:14:41.000626 140295626643200 trainer.py:508] step:  5750, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:94.443192 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.272285 log_pplx:4.0018334 loss:158.87276 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.67719\n",
      "I0710 09:14:51.353062 140295626643200 summary_utils.py:349] Steps/second: 0.176963, Examples/second: 25.055797\n",
      "I0710 09:14:51.354249 140295626643200 trainer.py:508] step:  5751, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:95.958214 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2723205 log_pplx:3.9195547 loss:156.97816 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.68268\n",
      "I0710 09:14:55.786535 140295626643200 summary_utils.py:349] Steps/second: 0.176994, Examples/second: 25.076907\n",
      "I0710 09:14:55.787606 140295626643200 trainer.py:508] step:  5752, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:38.706928 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2737308 log_pplx:4.2259083 loss:65.798706 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.68817\n",
      "I0710 09:15:02.331667 140295635035904 trainer.py:354] Write summary done: step 5746\n",
      "I0710 09:15:02.554493 140295626643200 summary_utils.py:349] Steps/second: 0.176965, Examples/second: 25.075493\n",
      "I0710 09:15:02.555219 140295626643200 trainer.py:508] step:  5753, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:40.511559 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2742505 log_pplx:4.1645989 loss:104.89584 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.69318\n",
      "I0710 09:15:08.973337 140295626643200 summary_utils.py:349] Steps/second: 0.176945, Examples/second: 25.063722\n",
      "I0710 09:15:08.974478 140295626643200 trainer.py:508] step:  5754, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:115.77481 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2728629 log_pplx:4.0526481 loss:169.19806 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.69794\n",
      "I0710 09:15:11.193322 140295626643200 summary_utils.py:349] Steps/second: 0.177034, Examples/second: 25.130053\n",
      "I0710 09:15:11.194148 140295626643200 trainer.py:508] step:  5755, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:11.333708 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2725139 log_pplx:4.0554986 loss:30.202377 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:521.70319\n",
      "I0710 09:15:17.464304 140295626643200 summary_utils.py:349] Steps/second: 0.177018, Examples/second: 25.118786\n",
      "I0710 09:15:17.465107 140295626643200 trainer.py:508] step:  5756, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:86.478813 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.27276 log_pplx:4.0773454 loss:163.90929 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.70825\n",
      "I0710 09:15:21.243028 140295626643200 summary_utils.py:349] Steps/second: 0.177066, Examples/second: 25.128232\n",
      "I0710 09:15:21.243832 140295626643200 trainer.py:508] step:  5757, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:51.456516 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.274218 log_pplx:4.2108669 loss:106.53493 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.71375\n",
      "I0710 09:15:27.110917 140295626643200 summary_utils.py:349] Steps/second: 0.177060, Examples/second: 25.118451\n",
      "I0710 09:15:27.111863 140295626643200 trainer.py:508] step:  5758, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:80.389526 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2727503 log_pplx:4.0116529 loss:164.22705 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.71906\n",
      "I0710 09:15:37.295040 140295626643200 summary_utils.py:349] Steps/second: 0.176944, Examples/second: 25.087202\n",
      "I0710 09:15:37.295851 140295626643200 trainer.py:508] step:  5759, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:358.41937 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2718008 log_pplx:3.3911085 loss:249.83992 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:521.72418\n",
      "I0710 09:15:40.310364 140295626643200 summary_utils.py:349] Steps/second: 0.177011, Examples/second: 25.113305\n",
      "I0710 09:15:40.311336 140295626643200 trainer.py:508] step:  5760, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:32.93198 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.273579 log_pplx:4.1547747 loss:64.512604 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.73065\n",
      "I0710 09:15:46.651517 140295626643200 summary_utils.py:349] Steps/second: 0.176994, Examples/second: 25.101846\n",
      "I0710 09:15:46.652315 140295626643200 trainer.py:508] step:  5761, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:248.2227 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2725569 log_pplx:4.1377316 loss:163.9576 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.73712\n",
      "I0710 09:15:50.609557 140295626643200 summary_utils.py:349] Steps/second: 0.177037, Examples/second: 25.110610\n",
      "I0710 09:15:50.610450 140295626643200 trainer.py:508] step:  5762, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:41.521683 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2745428 log_pplx:4.2462935 loss:107.1393 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.74286\n",
      "I0710 09:15:57.108507 140295626643200 summary_utils.py:349] Steps/second: 0.177015, Examples/second: 25.098599\n",
      "I0710 09:15:57.109325 140295626643200 trainer.py:508] step:  5763, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:294.14423 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2728498 log_pplx:3.9652481 loss:162.47604 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.74841\n",
      "I0710 09:16:03.445066 140295626643200 summary_utils.py:349] Steps/second: 0.176998, Examples/second: 25.087197\n",
      "I0710 09:16:03.445864 140295626643200 trainer.py:508] step:  5764, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:94.02784 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2731357 log_pplx:4.0152583 loss:164.47502 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.75317\n",
      "I0710 09:16:07.277593 140295626643200 summary_utils.py:349] Steps/second: 0.177044, Examples/second: 25.096403\n",
      "I0710 09:16:07.278611 140295626643200 trainer.py:508] step:  5765, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:121.63461 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2742684 log_pplx:4.3320971 loss:108.41073 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.75812\n",
      "I0710 09:16:10.362201 140295626643200 summary_utils.py:349] Steps/second: 0.177109, Examples/second: 25.122139\n",
      "I0710 09:16:10.363000 140295626643200 trainer.py:508] step:  5766, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:40.83997 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2740247 log_pplx:4.1198373 loss:64.452919 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.76361\n",
      "I0710 09:16:16.938665 140295626643200 summary_utils.py:349] Steps/second: 0.177086, Examples/second: 25.109870\n",
      "I0710 09:16:16.939518 140295626643200 trainer.py:508] step:  5767, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:171.35094 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2730408 log_pplx:4.077148 loss:166.34763 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.7691\n",
      "I0710 09:16:23.290680 140295626643200 summary_utils.py:349] Steps/second: 0.177068, Examples/second: 25.098435\n",
      "I0710 09:16:23.291591 140295626643200 trainer.py:508] step:  5768, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:106.09412 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2725657 log_pplx:3.974633 loss:158.53816 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.77533\n",
      "I0710 09:16:33.243322 140295626643200 summary_utils.py:349] Steps/second: 0.176958, Examples/second: 25.068301\n",
      "I0710 09:16:33.244183 140295626643200 trainer.py:508] step:  5769, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:255.4015 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2719179 log_pplx:3.435385 loss:245.20061 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:521.78162\n",
      "I0710 09:16:37.053723 140295626643200 summary_utils.py:349] Steps/second: 0.177005, Examples/second: 25.077558\n",
      "I0710 09:16:37.054563 140295626643200 trainer.py:508] step:  5770, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:95.485992 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2742308 log_pplx:4.2604218 loss:107.14961 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.78821\n",
      "I0710 09:16:43.118077 140295626643200 summary_utils.py:349] Steps/second: 0.176994, Examples/second: 25.067219\n",
      "I0710 09:16:43.118873 140295626643200 trainer.py:508] step:  5771, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:94.917473 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2730502 log_pplx:3.9799523 loss:158.8996 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.79388\n",
      "I0710 09:16:46.211391 140295626643200 summary_utils.py:349] Steps/second: 0.177059, Examples/second: 25.092805\n",
      "I0710 09:16:46.212256 140295626643200 trainer.py:508] step:  5772, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:89.147942 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2746406 log_pplx:4.4149547 loss:70.121895 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.79926\n",
      "I0710 09:16:52.552205 140295626643200 summary_utils.py:349] Steps/second: 0.177042, Examples/second: 25.081473\n",
      "I0710 09:16:52.552963 140295626643200 trainer.py:508] step:  5773, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:103.89136 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2727904 log_pplx:4.0034962 loss:160.34001 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.80396\n",
      "I0710 09:16:56.365134 140295626643200 summary_utils.py:349] Steps/second: 0.177088, Examples/second: 25.090688\n",
      "I0710 09:16:56.365950 140295626643200 trainer.py:508] step:  5774, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:56.031437 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2749081 log_pplx:4.2828989 loss:109.05331 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.80902\n",
      "I0710 09:17:02.744968 140295626643200 summary_utils.py:349] Steps/second: 0.177070, Examples/second: 25.079235\n",
      "I0710 09:17:02.745826 140295626643200 trainer.py:508] step:  5775, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:111.42973 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2725668 log_pplx:3.9757586 loss:158.28488 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.81384\n",
      "I0710 09:17:09.206458 140295626643200 summary_utils.py:349] Steps/second: 0.177049, Examples/second: 25.067511\n",
      "I0710 09:17:09.207242 140295626643200 trainer.py:508] step:  5776, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:117.83803 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2729143 log_pplx:4.0413938 loss:163.42387 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.81934\n",
      "I0710 09:17:11.418117 140295626643200 summary_utils.py:349] Steps/second: 0.177136, Examples/second: 25.132731\n",
      "I0710 09:17:11.418914 140295626643200 trainer.py:508] step:  5777, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:11.741589 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2728869 log_pplx:4.0947328 loss:29.998716 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:521.82544\n",
      "I0710 09:17:14.531194 140295626643200 summary_utils.py:349] Steps/second: 0.177200, Examples/second: 25.158113\n",
      "I0710 09:17:14.531986 140295626643200 trainer.py:508] step:  5778, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:24.116562 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2739769 log_pplx:4.1144819 loss:63.051224 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.83118\n",
      "I0710 09:17:20.456832 140295626643200 summary_utils.py:349] Steps/second: 0.177193, Examples/second: 25.148258\n",
      "I0710 09:17:20.457830 140295626643200 trainer.py:508] step:  5779, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:86.996017 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2728184 log_pplx:4.0104938 loss:154.95544 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.83649\n",
      "I0710 09:17:24.317543 140295626643200 summary_utils.py:349] Steps/second: 0.177238, Examples/second: 25.157228\n",
      "I0710 09:17:24.318430 140295626643200 trainer.py:508] step:  5780, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:60.857132 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2749552 log_pplx:4.2925267 loss:109.59358 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.84149\n",
      "I0710 09:17:33.864786 140295626643200 summary_utils.py:349] Steps/second: 0.177139, Examples/second: 25.128723\n",
      "I0710 09:17:33.865556 140295626643200 trainer.py:508] step:  5781, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:216.13959 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.272113 log_pplx:3.3087058 loss:236.3243 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:521.84613\n",
      "I0710 09:17:40.114146 140295626643200 summary_utils.py:349] Steps/second: 0.177124, Examples/second: 25.117763\n",
      "I0710 09:17:40.114937 140295626643200 trainer.py:508] step:  5782, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:119.85082 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2732909 log_pplx:3.9443696 loss:161.37405 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.85052\n",
      "I0710 09:17:46.794716 140295626643200 summary_utils.py:349] Steps/second: 0.177098, Examples/second: 25.105283\n",
      "I0710 09:17:46.795657 140295626643200 trainer.py:508] step:  5783, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:94.953163 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2730702 log_pplx:3.9625366 loss:160.08647 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.85486\n",
      "I0710 09:17:50.700118 140295626643200 summary_utils.py:349] Steps/second: 0.177142, Examples/second: 25.114084\n",
      "I0710 09:17:50.700933 140295626643200 trainer.py:508] step:  5784, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:50.071461 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.275219 log_pplx:4.2468505 loss:106.72866 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.85919\n",
      "I0710 09:17:53.789504 140295626643200 summary_utils.py:349] Steps/second: 0.177206, Examples/second: 25.139417\n",
      "I0710 09:17:53.790390 140295626643200 trainer.py:508] step:  5785, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:51.109539 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.274508 log_pplx:4.2605643 loss:67.519958 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.86383\n",
      "I0710 09:18:00.210243 140295626643200 summary_utils.py:349] Steps/second: 0.177187, Examples/second: 25.127868\n",
      "I0710 09:18:00.211065 140295626643200 trainer.py:508] step:  5786, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:92.337097 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2736738 log_pplx:4.1690931 loss:170.82858 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.86847\n",
      "I0710 09:18:06.332341 140295626643200 summary_utils.py:349] Steps/second: 0.177175, Examples/second: 25.117402\n",
      "I0710 09:18:06.333350 140295626643200 trainer.py:508] step:  5787, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:82.278511 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2733158 log_pplx:4.01297 loss:160.61911 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.87372\n",
      "I0710 09:18:10.170653 140295626643200 summary_utils.py:349] Steps/second: 0.177220, Examples/second: 25.126411\n",
      "I0710 09:18:10.171517 140295626643200 trainer.py:508] step:  5788, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:41.254353 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2749214 log_pplx:4.2325349 loss:105.7869 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.87921\n",
      "I0710 09:18:16.261073 140295626643200 summary_utils.py:349] Steps/second: 0.177209, Examples/second: 25.116074\n",
      "I0710 09:18:16.261870 140295626643200 trainer.py:508] step:  5789, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:76.566025 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2735362 log_pplx:4.0361376 loss:161.79868 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.8844\n",
      "I0710 09:18:26.405938 140295626643200 summary_utils.py:349] Steps/second: 0.177096, Examples/second: 25.085715\n",
      "I0710 09:18:26.406802 140295626643200 trainer.py:508] step:  5790, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:244.78932 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2726566 log_pplx:3.366704 loss:245.43271 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:521.88965\n",
      "I0710 09:18:29.689388 140295626643200 summary_utils.py:349] Steps/second: 0.177155, Examples/second: 25.110244\n",
      "I0710 09:18:29.690799 140295626643200 trainer.py:508] step:  5791, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:31.208368 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.274491 log_pplx:4.1428175 loss:65.200829 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.89636\n",
      "I0710 09:18:36.481076 140295626643200 summary_utils.py:349] Steps/second: 0.177126, Examples/second: 25.097466\n",
      "I0710 09:18:36.481885 140295626643200 trainer.py:508] step:  5792, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:126.38489 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2727717 log_pplx:3.9229407 loss:150.88614 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.90265\n",
      "I0710 09:18:42.877794 140295626643200 summary_utils.py:349] Steps/second: 0.177108, Examples/second: 25.086109\n",
      "I0710 09:18:42.878589 140295626643200 trainer.py:508] step:  5793, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:106.04682 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.273333 log_pplx:4.0334082 loss:161.48758 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.90802\n",
      "I0710 09:18:46.672533 140295626643200 summary_utils.py:349] Steps/second: 0.177154, Examples/second: 25.095242\n",
      "I0710 09:18:46.673516 140295626643200 trainer.py:508] step:  5794, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:44.331112 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2751073 log_pplx:4.1855779 loss:106.05208 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.91296\n",
      "I0710 09:18:53.017267 140295626643200 summary_utils.py:349] Steps/second: 0.177136, Examples/second: 25.084087\n",
      "I0710 09:18:53.018287 140295626643200 trainer.py:508] step:  5795, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:86.242264 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2736748 log_pplx:4.0765252 loss:162.90814 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.9176\n",
      "I0710 09:18:56.055722 140295626643200 summary_utils.py:349] Steps/second: 0.177201, Examples/second: 25.109391\n",
      "I0710 09:18:56.056500 140295626643200 trainer.py:508] step:  5796, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:22.08197 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2751051 log_pplx:4.1552224 loss:66.580948 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.92218\n",
      "I0710 09:18:59.963161 140295626643200 summary_utils.py:349] Steps/second: 0.177245, Examples/second: 25.118096\n",
      "I0710 09:18:59.964115 140295626643200 trainer.py:508] step:  5797, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:48.385307 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2751091 log_pplx:4.1599398 loss:104.70048 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.92651\n",
      "I0710 09:19:06.126808 140295626643200 summary_utils.py:349] Steps/second: 0.177232, Examples/second: 25.107581\n",
      "I0710 09:19:06.127728 140295626643200 trainer.py:508] step:  5798, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:110.99979 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2734575 log_pplx:4.0598903 loss:163.86734 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.93103\n",
      "I0710 09:19:12.196185 140295626643200 summary_utils.py:349] Steps/second: 0.177221, Examples/second: 25.097415\n",
      "I0710 09:19:12.197003 140295626643200 trainer.py:508] step:  5799, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:96.850075 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2735232 log_pplx:3.9277689 loss:159.81108 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.9361\n",
      "I0710 09:19:14.391186 140295626643200 summary_utils.py:349] Steps/second: 0.177307, Examples/second: 25.161559\n",
      "I0710 09:19:14.391979 140295626643200 trainer.py:508] step:  5800, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:14.620913 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2736478 log_pplx:4.1039629 loss:30.274738 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:521.94159\n",
      "I0710 09:19:25.362772 140295626643200 summary_utils.py:349] Steps/second: 0.177174, Examples/second: 25.128470\n",
      "I0710 09:19:25.363643 140295626643200 trainer.py:508] step:  5801, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:518.77557 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2729774 log_pplx:3.4987934 loss:252.17551 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:521.94666\n",
      "I0710 09:19:31.798448 140295626643200 summary_utils.py:349] Steps/second: 0.177155, Examples/second: 25.117026\n",
      "I0710 09:19:31.799178 140295626643200 trainer.py:508] step:  5802, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:84.007607 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2737885 log_pplx:3.9922729 loss:164.58145 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.95172\n",
      "I0710 09:19:34.884923 140295626643200 summary_utils.py:349] Steps/second: 0.177218, Examples/second: 25.142010\n",
      "I0710 09:19:34.885690 140295626643200 trainer.py:508] step:  5803, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:28.505095 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2746109 log_pplx:4.0710549 loss:62.131306 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.95679\n",
      "I0710 09:19:38.805182 140295626643200 summary_utils.py:349] Steps/second: 0.177261, Examples/second: 25.150605\n",
      "I0710 09:19:38.805975 140295626643200 trainer.py:508] step:  5804, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:49.056923 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2753546 log_pplx:4.1852884 loss:105.18153 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.96155\n",
      "I0710 09:19:44.829739 140295626643200 summary_utils.py:349] Steps/second: 0.177251, Examples/second: 25.140607\n",
      "I0710 09:19:44.830553 140295626643200 trainer.py:508] step:  5805, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:246.51231 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2736604 log_pplx:4.1963439 loss:165.02121 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.96631\n",
      "I0710 09:19:51.093716 140295626643200 summary_utils.py:349] Steps/second: 0.177236, Examples/second: 25.129786\n",
      "I0710 09:19:51.094521 140295626643200 trainer.py:508] step:  5806, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:181.23949 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2736595 log_pplx:4.0934696 loss:163.84113 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.97235\n",
      "I0710 09:19:54.948527 140295626643200 summary_utils.py:349] Steps/second: 0.177280, Examples/second: 25.138598\n",
      "I0710 09:19:54.949348 140295626643200 trainer.py:508] step:  5807, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:85.991585 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.275641 log_pplx:4.2644334 loss:110.04903 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:521.979\n",
      "I0710 09:20:01.244728 140295626643200 summary_utils.py:349] Steps/second: 0.177264, Examples/second: 25.127681\n",
      "I0710 09:20:01.245484 140295626643200 trainer.py:508] step:  5808, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:85.816559 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2740277 log_pplx:4.1395135 loss:168.01251 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.98486\n",
      "I0710 09:20:04.256791 140295626643200 summary_utils.py:349] Steps/second: 0.177329, Examples/second: 25.152819\n",
      "I0710 09:20:04.257559 140295626643200 trainer.py:508] step:  5809, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:40.472584 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2754638 log_pplx:4.1967144 loss:65.622841 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:521.99036\n",
      "I0710 09:20:10.796732 140295626643200 summary_utils.py:349] Steps/second: 0.177307, Examples/second: 25.141049\n",
      "I0710 09:20:10.797564 140295626643200 trainer.py:508] step:  5810, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:129.79399 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.273909 log_pplx:4.0690403 loss:166.93237 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:521.99542\n",
      "I0710 09:20:14.684462 140295626643200 summary_utils.py:349] Steps/second: 0.177350, Examples/second: 25.149716\n",
      "I0710 09:20:14.685302 140295626643200 trainer.py:508] step:  5811, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:84.15136 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.275509 log_pplx:4.2834449 loss:106.631 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522\n",
      "I0710 09:20:25.522970 140295626643200 summary_utils.py:349] Steps/second: 0.177222, Examples/second: 25.117388\n",
      "I0710 09:20:25.523772 140295626643200 trainer.py:508] step:  5812, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:799.59656 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2730864 log_pplx:3.948571 loss:296.43896 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.00409\n",
      "I0710 09:20:31.974092 140295626643200 summary_utils.py:349] Steps/second: 0.177202, Examples/second: 25.105995\n",
      "I0710 09:20:31.974857 140295626643200 trainer.py:508] step:  5813, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:127.66628 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2739331 log_pplx:4.1064539 loss:165.90073 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.0108\n",
      "I0710 09:20:35.038866 140295626643200 summary_utils.py:349] Steps/second: 0.177265, Examples/second: 25.130850\n",
      "I0710 09:20:35.039618 140295626643200 trainer.py:508] step:  5814, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:37.329044 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2753668 log_pplx:4.2188287 loss:66.133438 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.01727\n",
      "I0710 09:20:41.431815 140295626643200 summary_utils.py:349] Steps/second: 0.177247, Examples/second: 25.119664\n",
      "I0710 09:20:41.432663 140295626643200 trainer.py:508] step:  5815, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:351.17249 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2737753 log_pplx:4.3449459 loss:169.34427 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.0238\n",
      "I0710 09:20:45.251919 140295626643200 summary_utils.py:349] Steps/second: 0.177292, Examples/second: 25.128541\n",
      "I0710 09:20:45.252853 140295626643200 trainer.py:508] step:  5816, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:47.915237 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2760285 log_pplx:4.3212433 loss:109.00337 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.02966\n",
      "I0710 09:20:51.765438 140295626643200 summary_utils.py:349] Steps/second: 0.177270, Examples/second: 25.116952\n",
      "I0710 09:20:51.766270 140295626643200 trainer.py:508] step:  5817, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:140.02428 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2739981 log_pplx:4.0977488 loss:158.99266 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.03546\n",
      "I0710 09:20:55.611595 140295626643200 summary_utils.py:349] Steps/second: 0.177314, Examples/second: 25.125728\n",
      "I0710 09:20:55.612586 140295626643200 trainer.py:508] step:  5818, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:94.545799 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2760513 log_pplx:4.3666677 loss:109.54877 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.04126\n",
      "I0710 09:21:01.398889 140295626643200 summary_utils.py:349] Steps/second: 0.177310, Examples/second: 25.116681\n",
      "I0710 09:21:01.399798 140295626643200 trainer.py:508] step:  5819, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:85.757042 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2741724 log_pplx:4.1290712 loss:163.87253 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.0473\n",
      "I0710 09:21:11.199813 140295626643200 summary_utils.py:349] Steps/second: 0.177209, Examples/second: 25.088205\n",
      "I0710 09:21:11.200649 140295626643200 trainer.py:508] step:  5820, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:314.68454 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2733122 log_pplx:3.5950291 loss:258.8421 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.05334\n",
      "I0710 09:21:14.238684 140295626643200 summary_utils.py:349] Steps/second: 0.177272, Examples/second: 25.113023\n",
      "I0710 09:21:14.239497 140295626643200 trainer.py:508] step:  5821, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:47.221062 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2756536 log_pplx:4.2009974 loss:66.198532 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.05884\n",
      "I0710 09:21:20.631235 140295626643200 summary_utils.py:349] Steps/second: 0.177254, Examples/second: 25.101914\n",
      "I0710 09:21:20.631992 140295626643200 trainer.py:508] step:  5822, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:138.263 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2744637 log_pplx:4.1280279 loss:169.97157 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.06433\n",
      "I0710 09:21:27.117085 140295626643200 summary_utils.py:349] Steps/second: 0.177233, Examples/second: 25.090502\n",
      "I0710 09:21:27.117879 140295626643200 trainer.py:508] step:  5823, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:136.68239 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2744353 log_pplx:4.1903896 loss:174.47734 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.07013\n",
      "I0710 09:21:29.316455 140295626643200 summary_utils.py:349] Steps/second: 0.177317, Examples/second: 25.153440\n",
      "I0710 09:21:29.317236 140295626643200 trainer.py:508] step:  5824, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:13.018816 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2741548 log_pplx:4.0569563 loss:29.294079 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:522.07629\n",
      "I0710 09:21:33.179550 140295626643200 summary_utils.py:349] Steps/second: 0.177361, Examples/second: 25.162092\n",
      "I0710 09:21:33.180539 140295626643200 trainer.py:508] step:  5825, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:48.670288 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2762089 log_pplx:4.296102 loss:109.68484 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.08209\n",
      "I0710 09:21:39.662366 140295626643200 summary_utils.py:349] Steps/second: 0.177340, Examples/second: 25.150656\n",
      "I0710 09:21:39.663436 140295626643200 trainer.py:508] step:  5826, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:96.580574 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2746001 log_pplx:4.1117377 loss:168.42705 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.08783\n",
      "I0710 09:21:42.722319 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 09:21:42.722979 140295626643200 summary_utils.py:349] Steps/second: 0.177403, Examples/second: 25.175275\n",
      "I0710 09:21:42.724418 140295626643200 trainer.py:508] step:  5827, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:54.442814 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2753063 log_pplx:4.2585788 loss:66.007973 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.09332\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 09:21:47.818652 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 09:21:48.329299 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00005827\n",
      "I0710 09:21:49.340631 140295626643200 summary_utils.py:349] Steps/second: 0.177379, Examples/second: 25.163377\n",
      "I0710 09:21:49.341421 140295626643200 trainer.py:508] step:  5828, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:86.560585 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2743464 log_pplx:4.0964594 loss:164.67764 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.09814\n",
      "I0710 09:21:53.169432 140295626643200 summary_utils.py:349] Steps/second: 0.177423, Examples/second: 25.172118\n",
      "I0710 09:21:53.170209 140295626643200 trainer.py:508] step:  5829, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:67.568604 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2762789 log_pplx:4.3564568 loss:109.02031 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.10284\n",
      "I0710 09:21:59.434488 140295626643200 summary_utils.py:349] Steps/second: 0.177408, Examples/second: 25.161457\n",
      "I0710 09:21:59.435267 140295626643200 trainer.py:508] step:  5830, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:74.94355 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2744752 log_pplx:4.1161642 loss:170.15193 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.10712\n",
      "I0710 09:22:02.519695 140295626643200 summary_utils.py:349] Steps/second: 0.177470, Examples/second: 25.185920\n",
      "I0710 09:22:02.520498 140295626643200 trainer.py:508] step:  5831, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:33.334595 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2756023 log_pplx:4.1571088 loss:65.036018 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.11133\n",
      "I0710 09:22:06.450376 140295626643200 summary_utils.py:349] Steps/second: 0.177511, Examples/second: 25.194282\n",
      "I0710 09:22:06.451172 140295626643200 trainer.py:508] step:  5832, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:39.548092 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.276255 log_pplx:4.2290916 loss:107.15462 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.1153\n",
      "I0710 09:22:15.748823 140295626643200 summary_utils.py:349] Steps/second: 0.177422, Examples/second: 25.167687\n",
      "I0710 09:22:15.749614 140295626643200 trainer.py:508] step:  5833, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:219.03555 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2734976 log_pplx:3.5358343 loss:255.28723 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.11914\n",
      "I0710 09:22:22.175969 140295626643200 summary_utils.py:349] Steps/second: 0.177403, Examples/second: 25.156504\n",
      "I0710 09:22:22.935805 140295626643200 trainer.py:508] step:  5834, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:108.58308 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2743833 log_pplx:4.1098919 loss:163.52231 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.12457\n",
      "I0710 09:22:29.557772 140295626643200 summary_utils.py:349] Steps/second: 0.177361, Examples/second: 25.142061\n",
      "I0710 09:22:29.558560 140295626643200 trainer.py:508] step:  5835, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:87.647179 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.274388 log_pplx:4.04424 loss:159.03975 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.13037\n",
      "I0710 09:22:33.563809 140295626643200 summary_utils.py:349] Steps/second: 0.177400, Examples/second: 25.150157\n",
      "I0710 09:22:33.564665 140295626643200 trainer.py:508] step:  5836, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:37.460014 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2761759 log_pplx:4.1683207 loss:103.66093 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.13635\n",
      "I0710 09:22:40.229235 140295626643200 summary_utils.py:349] Steps/second: 0.177375, Examples/second: 25.138199\n",
      "I0710 09:22:40.230028 140295626643200 base_runner.py:111] step:  5837, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:80.279579 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2749844 log_pplx:4.0431538 loss:165.66823 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.14197\n",
      "I0710 09:22:43.276268 140295626643200 summary_utils.py:349] Steps/second: 0.177438, Examples/second: 25.162667\n",
      "I0710 09:22:43.277107 140295626643200 trainer.py:508] step:  5838, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:22.365448 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2759463 log_pplx:4.0959706 loss:64.079544 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.14758\n",
      "I0710 09:22:49.584758 140295626643200 summary_utils.py:349] Steps/second: 0.177422, Examples/second: 25.151938\n",
      "I0710 09:22:49.585749 140295626643200 trainer.py:508] step:  5839, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:88.489517 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.274626 log_pplx:3.9214215 loss:157.7392 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.15289\n",
      "I0710 09:22:55.817044 140295626643200 summary_utils.py:349] Steps/second: 0.177407, Examples/second: 25.141487\n",
      "I0710 09:22:55.817919 140295626643200 trainer.py:508] step:  5840, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:88.654152 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2748151 log_pplx:4.0656085 loss:167.50305 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.15765\n",
      "I0710 09:22:59.723232 140295626643200 summary_utils.py:349] Steps/second: 0.177449, Examples/second: 25.149896\n",
      "I0710 09:22:59.724085 140295626643200 trainer.py:508] step:  5841, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:46.393421 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2766515 log_pplx:4.2745495 loss:107.79879 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.16193\n",
      "I0710 09:23:10.200730 140295626643200 summary_utils.py:349] Steps/second: 0.177333, Examples/second: 25.119524\n",
      "I0710 09:23:10.201511 140295626643200 trainer.py:508] step:  5842, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:207.59052 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2735466 log_pplx:3.4416647 loss:241.86299 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.16589\n",
      "I0710 09:23:16.660051 140295626643200 summary_utils.py:349] Steps/second: 0.177313, Examples/second: 25.108357\n",
      "I0710 09:23:16.660808 140295626643200 trainer.py:508] step:  5843, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:78.5382 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2747445 log_pplx:4.0022879 loss:164.2439 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.16956\n",
      "I0710 09:23:19.755416 140295626643200 summary_utils.py:349] Steps/second: 0.177374, Examples/second: 25.132551\n",
      "I0710 09:23:19.756223 140295626643200 trainer.py:508] step:  5844, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:33.410061 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2762827 log_pplx:4.2196088 loss:66.458839 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.17365\n",
      "I0710 09:23:23.621543 140295626643200 summary_utils.py:349] Steps/second: 0.177417, Examples/second: 25.141074\n",
      "I0710 09:23:23.622308 140295626643200 trainer.py:508] step:  5845, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:46.005466 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2764304 log_pplx:4.1980562 loss:105.44993 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.17792\n",
      "I0710 09:23:29.944701 140295626643200 summary_utils.py:349] Steps/second: 0.177400, Examples/second: 25.130372\n",
      "I0710 09:23:29.945497 140295626643200 trainer.py:508] step:  5846, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:84.36203 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2747462 log_pplx:4.0384917 loss:163.10458 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.18243\n",
      "I0710 09:23:32.988876 140295635035904 trainer.py:345] Write summary @5846\n",
      "I0710 09:23:40.302556 140295626643200 summary_utils.py:349] Steps/second: 0.177287, Examples/second: 25.105972\n",
      "I0710 09:23:40.304289 140295626643200 trainer.py:508] step:  5847, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:85.968002 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2746156 log_pplx:3.9872036 loss:157.79358 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.18732\n",
      "I0710 09:23:43.009932 140295626643200 summary_utils.py:349] Steps/second: 0.177357, Examples/second: 25.166042\n",
      "I0710 09:23:43.011076 140295626643200 trainer.py:508] step:  5848, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:12.04613 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2746798 log_pplx:4.0203485 loss:29.304573 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:522.19275\n",
      "I0710 09:23:48.502329 140295626643200 summary_utils.py:349] Steps/second: 0.177361, Examples/second: 25.168986\n",
      "I0710 09:23:48.503598 140295626643200 trainer.py:508] step:  5849, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:54.349354 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2764192 log_pplx:4.2691708 loss:107.26292 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.19788\n",
      "I0710 09:23:58.430291 140295626643200 summary_utils.py:349] Steps/second: 0.177258, Examples/second: 25.146052\n",
      "I0710 09:23:58.431352 140295626643200 trainer.py:508] step:  5850, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:87.388321 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2750576 log_pplx:4.0530338 loss:166.07306 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.20264\n",
      "I0710 09:24:02.417033 140295626643200 summary_utils.py:349] Steps/second: 0.177297, Examples/second: 25.167067\n",
      "I0710 09:24:02.418209 140295626643200 trainer.py:508] step:  5851, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:30.041206 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2762493 log_pplx:4.1832428 loss:66.310921 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.20728\n",
      "2020-07-10 09:24:03.158268: I lingvo/core/ops/record_yielder.cc:532] Epoch 5: total records 46838\n",
      "2020-07-10 09:24:03.158348: I lingvo/core/ops/record_yielder.cc:485] Epoch 5 /tmp/punctuator_data/train.txt\n",
      "I0710 09:24:11.699084 140295626643200 summary_utils.py:349] Steps/second: 0.177210, Examples/second: 25.146368\n",
      "I0710 09:24:11.700327 140295626643200 trainer.py:508] step:  5852, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:78.862732 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2750915 log_pplx:3.9971836 loss:158.43839 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.21161\n",
      "I0710 09:24:17.610617 140295626643200 summary_utils.py:349] Steps/second: 0.177204, Examples/second: 25.147895\n",
      "I0710 09:24:17.611983 140295626643200 trainer.py:508] step:  5853, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:44.179817 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2765366 log_pplx:4.1533313 loss:103.31411 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.2157\n",
      "I0710 09:24:32.808955 140295635035904 trainer.py:354] Write summary done: step 5846\n",
      "I0710 09:24:33.883121 140295626643200 summary_utils.py:349] Steps/second: 0.176951, Examples/second: 25.098287\n",
      "I0710 09:24:33.883974 140295626643200 trainer.py:508] step:  5854, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:180.8828 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2739657 log_pplx:3.4400833 loss:243.21391 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.21942\n",
      "I0710 09:24:40.223893 140295626643200 summary_utils.py:349] Steps/second: 0.176935, Examples/second: 25.087663\n",
      "I0710 09:24:40.224693 140295626643200 trainer.py:508] step:  5855, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:78.672302 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2751551 log_pplx:3.9467149 loss:162.65398 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.22369\n",
      "I0710 09:24:46.546990 140295626643200 summary_utils.py:349] Steps/second: 0.176919, Examples/second: 25.077116\n",
      "I0710 09:24:46.547844 140295626643200 trainer.py:508] step:  5856, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:69.816322 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2745802 log_pplx:3.7517502 loss:147.86586 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.22833\n",
      "I0710 09:24:49.566859 140295626643200 summary_utils.py:349] Steps/second: 0.176981, Examples/second: 25.101286\n",
      "I0710 09:24:49.567629 140295626643200 trainer.py:508] step:  5857, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:21.511288 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2762076 log_pplx:4.0350962 loss:63.079903 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.23303\n",
      "I0710 09:24:55.840183 140295626643200 summary_utils.py:349] Steps/second: 0.176966, Examples/second: 25.090908\n",
      "I0710 09:24:55.840954 140295626643200 trainer.py:508] step:  5858, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:76.964836 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2752712 log_pplx:3.8181968 loss:157.45288 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.23755\n",
      "I0710 09:24:59.631741 140295626643200 summary_utils.py:349] Steps/second: 0.177010, Examples/second: 25.099593\n",
      "I0710 09:24:59.632514 140295626643200 trainer.py:508] step:  5859, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:39.702488 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2765182 log_pplx:4.0438743 loss:100.79356 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.24219\n",
      "I0710 09:25:05.850851 140295626643200 summary_utils.py:349] Steps/second: 0.176997, Examples/second: 25.089413\n",
      "I0710 09:25:05.851720 140295626643200 trainer.py:508] step:  5860, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:90.555832 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.275326 log_pplx:3.7793224 loss:155.3774 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.24689\n",
      "I0710 09:25:12.083679 140295626643200 summary_utils.py:349] Steps/second: 0.176983, Examples/second: 25.079203\n",
      "I0710 09:25:12.084459 140295626643200 trainer.py:508] step:  5861, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:77.028389 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2750896 log_pplx:3.7964706 loss:149.81822 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.25122\n",
      "I0710 09:25:15.936534 140295626643200 summary_utils.py:349] Steps/second: 0.177026, Examples/second: 25.087670\n",
      "I0710 09:25:15.937340 140295626643200 trainer.py:508] step:  5862, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:43.570038 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2770652 log_pplx:4.1470308 loss:104.84212 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.25543\n",
      "I0710 09:25:19.030948 140295626643200 summary_utils.py:349] Steps/second: 0.177086, Examples/second: 25.111491\n",
      "I0710 09:25:19.031733 140295626643200 trainer.py:508] step:  5863, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:20.944506 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2766081 log_pplx:4.0183825 loss:62.75584 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.25964\n",
      "I0710 09:25:25.307133 140295626643200 summary_utils.py:349] Steps/second: 0.177071, Examples/second: 25.101135\n",
      "I0710 09:25:25.307920 140295626643200 trainer.py:508] step:  5864, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:69.715614 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2755746 log_pplx:3.8578038 loss:155.56595 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.26355\n",
      "I0710 09:25:34.367274 140295626643200 summary_utils.py:349] Steps/second: 0.176991, Examples/second: 25.076157\n",
      "I0710 09:25:34.368238 140295626643200 trainer.py:508] step:  5865, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:174.68343 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2743937 log_pplx:3.0324874 loss:217.2019 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.26758\n",
      "I0710 09:25:40.610816 140295626643200 summary_utils.py:349] Steps/second: 0.176977, Examples/second: 25.065961\n",
      "I0710 09:25:40.611605 140295626643200 trainer.py:508] step:  5866, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:72.628693 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2753944 log_pplx:3.7796209 loss:153.92506 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.27209\n",
      "I0710 09:25:44.450725 140295626643200 summary_utils.py:349] Steps/second: 0.177019, Examples/second: 25.074446\n",
      "I0710 09:25:44.451485 140295626643200 trainer.py:508] step:  5867, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:42.351868 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.27672 log_pplx:4.0922632 loss:102.33217 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.27704\n",
      "I0710 09:25:50.878499 140295626643200 summary_utils.py:349] Steps/second: 0.177001, Examples/second: 25.063651\n",
      "I0710 09:25:50.879322 140295626643200 trainer.py:508] step:  5868, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:70.88121 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2751762 log_pplx:3.7374072 loss:153.46727 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.2818\n",
      "I0710 09:25:53.963271 140295626643200 summary_utils.py:349] Steps/second: 0.177061, Examples/second: 25.087404\n",
      "I0710 09:25:53.964065 140295626643200 trainer.py:508] step:  5869, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:26.382767 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2762706 log_pplx:4.0186357 loss:61.37838 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.28662\n",
      "I0710 09:26:00.441624 140295626643200 summary_utils.py:349] Steps/second: 0.177042, Examples/second: 25.076443\n",
      "I0710 09:26:00.442420 140295626643200 trainer.py:508] step:  5870, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:76.639351 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2755351 log_pplx:3.7366993 loss:152.59744 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.29126\n",
      "I0710 09:26:02.647295 140295626643200 summary_utils.py:349] Steps/second: 0.177123, Examples/second: 25.137080\n",
      "I0710 09:26:02.648197 140295626643200 trainer.py:508] step:  5871, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:11.799594 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2751968 log_pplx:3.9476898 loss:29.160477 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:522.2962\n",
      "I0710 09:26:06.520694 140295626643200 summary_utils.py:349] Steps/second: 0.177164, Examples/second: 25.145393\n",
      "I0710 09:26:06.521640 140295626643200 trainer.py:508] step:  5872, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:35.790215 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2769901 log_pplx:4.0445461 loss:100.2289 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.30084\n",
      "I0710 09:26:12.921414 140295626643200 summary_utils.py:349] Steps/second: 0.177147, Examples/second: 25.134659\n",
      "I0710 09:26:12.922249 140295626643200 trainer.py:508] step:  5873, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:74.758469 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2752916 log_pplx:3.7259355 loss:149.87576 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.30542\n",
      "I0710 09:26:22.125941 140295626643200 summary_utils.py:349] Steps/second: 0.177063, Examples/second: 25.109318\n",
      "I0710 09:26:22.126697 140295626643200 trainer.py:508] step:  5874, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:166.94781 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.274765 log_pplx:3.1288276 loss:221.28635 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.31\n",
      "I0710 09:26:28.525147 140295626643200 summary_utils.py:349] Steps/second: 0.177045, Examples/second: 25.098642\n",
      "I0710 09:26:28.526107 140295626643200 trainer.py:508] step:  5875, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:77.189262 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2756023 log_pplx:3.7319701 loss:154.59685 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.31476\n",
      "I0710 09:26:31.641266 140295626643200 summary_utils.py:349] Steps/second: 0.177105, Examples/second: 25.122158\n",
      "I0710 09:26:31.642370 140295626643200 trainer.py:508] step:  5876, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:27.539297 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.276816 log_pplx:4.0713515 loss:64.410057 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.31976\n",
      "I0710 09:26:35.508900 140295626643200 summary_utils.py:349] Steps/second: 0.177146, Examples/second: 25.130465\n",
      "I0710 09:26:35.509787 140295626643200 trainer.py:508] step:  5877, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:36.87706 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2769006 log_pplx:3.9999783 loss:99.77446 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.32458\n",
      "I0710 09:26:42.117817 140295626643200 summary_utils.py:349] Steps/second: 0.177124, Examples/second: 25.119094\n",
      "I0710 09:26:42.118638 140295626643200 trainer.py:508] step:  5878, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:80.927826 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2758167 log_pplx:3.8583179 loss:156.16542 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.32922\n",
      "I0710 09:26:48.532155 140295626643200 summary_utils.py:349] Steps/second: 0.177106, Examples/second: 25.108388\n",
      "I0710 09:26:48.532997 140295626643200 trainer.py:508] step:  5879, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:70.285378 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2753822 log_pplx:3.7404599 loss:147.74817 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.33411\n",
      "I0710 09:26:54.740762 140295626643200 summary_utils.py:349] Steps/second: 0.177093, Examples/second: 25.098381\n",
      "I0710 09:26:54.741729 140295626643200 trainer.py:508] step:  5880, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:73.824913 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2753342 log_pplx:3.6983738 loss:144.14413 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.33899\n",
      "I0710 09:26:58.537408 140295626643200 summary_utils.py:349] Steps/second: 0.177136, Examples/second: 25.106910\n",
      "I0710 09:26:58.538220 140295626643200 trainer.py:508] step:  5881, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:42.689037 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2773319 log_pplx:4.0460787 loss:101.45541 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.34375\n",
      "I0710 09:27:01.631990 140295626643200 summary_utils.py:349] Steps/second: 0.177195, Examples/second: 25.130401\n",
      "I0710 09:27:01.632760 140295626643200 trainer.py:508] step:  5882, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:22.386356 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2766942 log_pplx:4.0361238 loss:62.370724 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.34833\n",
      "I0710 09:27:08.067207 140295626643200 summary_utils.py:349] Steps/second: 0.177177, Examples/second: 25.119644\n",
      "I0710 09:27:08.067946 140295626643200 trainer.py:508] step:  5883, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:87.565689 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2755438 log_pplx:3.7012231 loss:150.77859 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.35272\n",
      "I0710 09:27:14.411975 140295626643200 summary_utils.py:349] Steps/second: 0.177161, Examples/second: 25.109204\n",
      "I0710 09:27:14.412721 140295626643200 trainer.py:508] step:  5884, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:81.308617 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2759483 log_pplx:3.6942072 loss:152.33986 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.35754\n",
      "I0710 09:27:18.193955 140295626643200 summary_utils.py:349] Steps/second: 0.177204, Examples/second: 25.117753\n",
      "I0710 09:27:18.194771 140295626643200 trainer.py:508] step:  5885, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:37.852436 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2774736 log_pplx:4.070044 loss:101.57304 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.36273\n",
      "I0710 09:27:28.976736 140295626643200 summary_utils.py:349] Steps/second: 0.177084, Examples/second: 25.087451\n",
      "I0710 09:27:28.977638 140295626643200 trainer.py:508] step:  5886, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:218.77081 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2751352 log_pplx:3.2140238 loss:237.2753 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.36792\n",
      "2020-07-10 09:27:33.021941: I lingvo/core/ops/record_batcher.cc:394] 7637 total seconds passed. Total records yielded: 192418. Total records skipped: 86\n",
      "2020-07-10 09:27:33.022002: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 09:27:33.022014: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 09:27:33.022031: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 09:27:33.022069: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 09:27:33.022091: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 121\n",
      "2020-07-10 09:27:33.022101: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 09:27:33.022111: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 09:27:33.022121: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 09:27:33.022133: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 09:27:33.022145: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "I0710 09:27:35.201928 140295626643200 summary_utils.py:349] Steps/second: 0.177071, Examples/second: 25.077460\n",
      "I0710 09:27:35.202752 140295626643200 trainer.py:508] step:  5887, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:73.756882 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2758476 log_pplx:3.7782948 loss:153.25708 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.37402\n",
      "I0710 09:27:38.311345 140295626643200 summary_utils.py:349] Steps/second: 0.177130, Examples/second: 25.100800\n",
      "I0710 09:27:38.312083 140295626643200 trainer.py:508] step:  5888, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:28.898703 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2770385 log_pplx:4.0259438 loss:62.700924 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.38025\n",
      "I0710 09:27:44.661253 140295626643200 summary_utils.py:349] Steps/second: 0.177114, Examples/second: 25.090400\n",
      "I0710 09:27:44.662122 140295626643200 trainer.py:508] step:  5889, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:99.492561 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2755595 log_pplx:3.7880683 loss:146.97707 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.38617\n",
      "I0710 09:27:48.528260 140295626643200 summary_utils.py:349] Steps/second: 0.177155, Examples/second: 25.098646\n",
      "I0710 09:27:48.529048 140295626643200 trainer.py:508] step:  5890, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:40.643055 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2777357 log_pplx:4.0491939 loss:101.81191 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.39154\n",
      "I0710 09:27:55.039717 140295626643200 summary_utils.py:349] Steps/second: 0.177135, Examples/second: 25.087732\n",
      "I0710 09:27:55.040510 140295626643200 trainer.py:508] step:  5891, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:92.866837 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2756681 log_pplx:3.7389369 loss:150.30527 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.39667\n",
      "I0710 09:28:01.059458 140295626643200 summary_utils.py:349] Steps/second: 0.177126, Examples/second: 25.078448\n",
      "I0710 09:28:01.060235 140295626643200 trainer.py:508] step:  5892, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:82.609886 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2763239 log_pplx:3.9196718 loss:161.14751 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.40137\n",
      "I0710 09:28:04.137723 140295626643200 summary_utils.py:349] Steps/second: 0.177186, Examples/second: 25.101811\n",
      "I0710 09:28:04.138697 140295626643200 trainer.py:508] step:  5893, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:45.430035 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2770809 log_pplx:4.039957 loss:62.177464 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.40643\n",
      "I0710 09:28:08.072926 140295626643200 summary_utils.py:349] Steps/second: 0.177225, Examples/second: 25.109807\n",
      "I0710 09:28:08.073807 140295626643200 trainer.py:508] step:  5894, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:71.459129 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2777716 log_pplx:4.0668192 loss:102.33134 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.4115\n",
      "I0710 09:28:10.301668 140295626643200 summary_utils.py:349] Steps/second: 0.177304, Examples/second: 25.169348\n",
      "I0710 09:28:10.302488 140295626643200 trainer.py:508] step:  5895, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:11.416172 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2756906 log_pplx:3.916996 loss:29.063803 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:522.41687\n",
      "I0710 09:28:16.637729 140295626643200 summary_utils.py:349] Steps/second: 0.177288, Examples/second: 25.158980\n",
      "I0710 09:28:16.638464 140295626643200 trainer.py:508] step:  5896, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:106.74284 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2763224 log_pplx:3.8763528 loss:157.0892 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.42188\n",
      "I0710 09:28:26.478853 140295626643200 summary_utils.py:349] Steps/second: 0.177191, Examples/second: 25.131933\n",
      "I0710 09:28:26.479668 140295626643200 trainer.py:508] step:  5897, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:177.50468 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2749218 log_pplx:3.040391 loss:218.68013 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.42737\n",
      "I0710 09:28:32.941031 140295626643200 summary_utils.py:349] Steps/second: 0.177172, Examples/second: 25.121206\n",
      "I0710 09:28:32.941860 140295626643200 trainer.py:508] step:  5898, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:85.283302 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2760487 log_pplx:3.7786453 loss:153.46024 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.43274\n",
      "I0710 09:28:39.600250 140295626643200 summary_utils.py:349] Steps/second: 0.177149, Examples/second: 25.109853\n",
      "I0710 09:28:39.601108 140295626643200 trainer.py:508] step:  5899, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:90.907776 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2763412 log_pplx:3.7368393 loss:153.67752 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.43817\n",
      "I0710 09:28:43.411257 140295626643200 summary_utils.py:349] Steps/second: 0.177191, Examples/second: 25.118213\n",
      "I0710 09:28:43.412048 140295626643200 trainer.py:508] step:  5900, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:108.6039 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2777331 log_pplx:4.1996589 loss:106.25137 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.44348\n",
      "I0710 09:28:46.518523 140295626643200 summary_utils.py:349] Steps/second: 0.177249, Examples/second: 25.141337\n",
      "I0710 09:28:46.519314 140295626643200 trainer.py:508] step:  5901, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:59.182941 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2771896 log_pplx:4.036025 loss:62.589912 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.44781\n",
      "I0710 09:28:53.313926 140295626643200 summary_utils.py:349] Steps/second: 0.177223, Examples/second: 25.129542\n",
      "I0710 09:28:53.314789 140295626643200 trainer.py:508] step:  5902, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:117.79166 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2758794 log_pplx:3.7045641 loss:147.25642 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.4516\n",
      "I0710 09:28:59.931874 140295626643200 summary_utils.py:349] Steps/second: 0.177200, Examples/second: 25.118346\n",
      "I0710 09:28:59.932694 140295626643200 trainer.py:508] step:  5903, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:143.37076 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.276417 log_pplx:3.7919538 loss:156.37068 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.45624\n",
      "I0710 09:29:03.928422 140295626643200 summary_utils.py:349] Steps/second: 0.177238, Examples/second: 25.126076\n",
      "I0710 09:29:03.929216 140295626643200 trainer.py:508] step:  5904, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:46.318043 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2781248 log_pplx:4.0799446 loss:103.52859 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.46161\n",
      "I0710 09:29:07.022099 140295626643200 summary_utils.py:349] Steps/second: 0.177297, Examples/second: 25.149180\n",
      "I0710 09:29:07.022854 140295626643200 trainer.py:508] step:  5905, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:24.970524 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2774333 log_pplx:4.048049 loss:63.772579 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.46716\n",
      "I0710 09:29:13.515898 140295626643200 summary_utils.py:349] Steps/second: 0.177277, Examples/second: 25.138391\n",
      "I0710 09:29:13.516681 140295626643200 trainer.py:508] step:  5906, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:126.04215 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2764245 log_pplx:3.7200651 loss:148.66312 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.47235\n",
      "I0710 09:29:23.995007 140295626643200 summary_utils.py:349] Steps/second: 0.177166, Examples/second: 25.109498\n",
      "I0710 09:29:23.995841 140295626643200 trainer.py:508] step:  5907, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:171.22449 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2751243 log_pplx:3.014972 loss:215.19363 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.47711\n",
      "I0710 09:29:30.478774 140295626643200 summary_utils.py:349] Steps/second: 0.177147, Examples/second: 25.098799\n",
      "I0710 09:29:30.479642 140295626643200 trainer.py:508] step:  5908, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:91.413727 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2762454 log_pplx:3.7123694 loss:153.87773 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.4823\n",
      "I0710 09:29:34.348047 140295626643200 summary_utils.py:349] Steps/second: 0.177188, Examples/second: 25.106920\n",
      "I0710 09:29:34.348808 140295626643200 trainer.py:508] step:  5909, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:50.041412 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2781239 log_pplx:4.1490746 loss:103.41567 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.48761\n",
      "I0710 09:29:40.770561 140295626643200 summary_utils.py:349] Steps/second: 0.177170, Examples/second: 25.096436\n",
      "I0710 09:29:40.771590 140295626643200 trainer.py:508] step:  5910, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:102.94177 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2761691 log_pplx:3.8168182 loss:147.56775 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.49304\n",
      "I0710 09:29:43.862666 140295626643200 summary_utils.py:349] Steps/second: 0.177228, Examples/second: 25.119447\n",
      "I0710 09:29:43.863613 140295626643200 trainer.py:508] step:  5911, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:41.402218 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2777088 log_pplx:4.0108519 loss:63.374588 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.49921\n",
      "I0710 09:29:50.119053 140295626643200 summary_utils.py:349] Steps/second: 0.177214, Examples/second: 25.109503\n",
      "I0710 09:29:50.119849 140295626643200 trainer.py:508] step:  5912, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:84.576248 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2761462 log_pplx:3.729979 loss:149.05928 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.50525\n",
      "I0710 09:29:53.918745 140295626643200 summary_utils.py:349] Steps/second: 0.177256, Examples/second: 25.117823\n",
      "I0710 09:29:53.919499 140295626643200 trainer.py:508] step:  5913, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:53.893215 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2784563 log_pplx:4.1135192 loss:103.66067 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.51141\n",
      "I0710 09:30:00.394506 140295626643200 summary_utils.py:349] Steps/second: 0.177237, Examples/second: 25.107184\n",
      "I0710 09:30:00.395305 140295626643200 trainer.py:508] step:  5914, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:85.843369 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2766594 log_pplx:3.7669179 loss:156.42126 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.51709\n",
      "I0710 09:30:09.446577 140295626643200 summary_utils.py:349] Steps/second: 0.177159, Examples/second: 25.083108\n",
      "I0710 09:30:09.447333 140295626643200 trainer.py:508] step:  5915, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:241.02428 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2756209 log_pplx:3.1411231 loss:226.00378 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.52277\n",
      "I0710 09:30:15.590165 140295626643200 summary_utils.py:349] Steps/second: 0.177148, Examples/second: 25.073589\n",
      "I0710 09:30:15.590933 140295626643200 trainer.py:508] step:  5916, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:86.70002 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2765366 log_pplx:3.6740575 loss:149.80969 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.52911\n",
      "I0710 09:30:19.438097 140295626643200 summary_utils.py:349] Steps/second: 0.177189, Examples/second: 25.081745\n",
      "I0710 09:30:19.438872 140295626643200 trainer.py:508] step:  5917, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:85.904388 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2782176 log_pplx:4.1003366 loss:102.94408 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.53528\n",
      "I0710 09:30:22.442963 140295626643200 summary_utils.py:349] Steps/second: 0.177249, Examples/second: 25.104929\n",
      "I0710 09:30:22.443753 140295626643200 trainer.py:508] step:  5918, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:43.283665 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2776021 log_pplx:4.0347261 loss:63.405094 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.54053\n",
      "I0710 09:30:28.898146 140295626643200 summary_utils.py:349] Steps/second: 0.177231, Examples/second: 25.094405\n",
      "I0710 09:30:28.898972 140295626643200 trainer.py:508] step:  5919, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:102.94398 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2764267 log_pplx:3.7755818 loss:150.59853 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.54541\n",
      "I0710 09:30:31.100959 140295626643200 summary_utils.py:349] Steps/second: 0.177309, Examples/second: 25.152960\n",
      "I0710 09:30:31.101712 140295626643200 trainer.py:508] step:  5920, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:11.705552 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2766864 log_pplx:3.9724879 loss:29.2971 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:522.55035\n",
      "I0710 09:30:37.538362 140295626643200 summary_utils.py:349] Steps/second: 0.177291, Examples/second: 25.142466\n",
      "I0710 09:30:37.539176 140295626643200 trainer.py:508] step:  5921, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:105.76333 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2768431 log_pplx:3.8477449 loss:158.95996 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.55499\n",
      "I0710 09:30:41.347003 140295626643200 summary_utils.py:349] Steps/second: 0.177332, Examples/second: 25.150691\n",
      "I0710 09:30:41.347819 140295626643200 trainer.py:508] step:  5922, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:83.406738 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2786268 log_pplx:4.2125483 loss:104.44487 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.5603\n",
      "I0710 09:30:47.798729 140295626643200 summary_utils.py:349] Steps/second: 0.177314, Examples/second: 25.140167\n",
      "I0710 09:30:47.799505 140295626643200 trainer.py:508] step:  5923, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:91.860039 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2772372 log_pplx:3.8097041 loss:159.48373 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.56616\n",
      "I0710 09:30:50.811721 140295626643200 summary_utils.py:349] Steps/second: 0.177373, Examples/second: 25.163218\n",
      "I0710 09:30:50.812447 140295626643200 trainer.py:508] step:  5924, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:33.404396 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.278139 log_pplx:4.05685 loss:62.84948 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.57178\n",
      "I0710 09:30:56.794776 140295626643200 summary_utils.py:349] Steps/second: 0.177365, Examples/second: 25.154202\n",
      "I0710 09:30:56.795777 140295626643200 trainer.py:508] step:  5925, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:90.636688 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2765052 log_pplx:3.759866 loss:148.23271 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.57715\n",
      "I0710 09:31:00.646032 140295626643200 summary_utils.py:349] Steps/second: 0.177406, Examples/second: 25.162265\n",
      "I0710 09:31:00.646827 140295626643200 trainer.py:508] step:  5926, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:39.921997 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2783998 log_pplx:4.0834613 loss:102.95427 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.58209\n",
      "I0710 09:31:06.830181 140295626643200 summary_utils.py:349] Steps/second: 0.177394, Examples/second: 25.152616\n",
      "I0710 09:31:06.830932 140295626643200 trainer.py:508] step:  5927, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:97.140305 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2768518 log_pplx:3.7562549 loss:153.44301 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.58661\n",
      "I0710 09:31:17.595622 140295626643200 summary_utils.py:349] Steps/second: 0.177278, Examples/second: 25.123204\n",
      "I0710 09:31:17.596614 140295626643200 trainer.py:508] step:  5928, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:557.21375 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2762009 log_pplx:3.2750449 loss:246.93839 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.59088\n",
      "I0710 09:31:23.768545 140295626643200 summary_utils.py:349] Steps/second: 0.177266, Examples/second: 25.113642\n",
      "I0710 09:31:23.769386 140295626643200 trainer.py:508] step:  5929, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:121.50215 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2768874 log_pplx:3.7981536 loss:152.30597 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.59656\n",
      "I0710 09:31:26.837354 140295626643200 summary_utils.py:349] Steps/second: 0.177324, Examples/second: 25.136420\n",
      "I0710 09:31:26.838355 140295626643200 trainer.py:508] step:  5930, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:25.377153 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2778305 log_pplx:3.9667616 loss:60.539597 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.60309\n",
      "I0710 09:31:30.691630 140295626643200 summary_utils.py:349] Steps/second: 0.177364, Examples/second: 25.144450\n",
      "I0710 09:31:30.692466 140295626643200 trainer.py:508] step:  5931, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:78.659241 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2784284 log_pplx:4.1129847 loss:102.41331 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.60944\n",
      "I0710 09:31:37.123510 140295626643200 summary_utils.py:349] Steps/second: 0.177346, Examples/second: 25.134060\n",
      "I0710 09:31:37.124269 140295626643200 trainer.py:508] step:  5932, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:347.00055 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2772434 log_pplx:4.1044731 loss:166.28247 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.61597\n",
      "I0710 09:31:43.163249 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 09:31:43.761885 140295626643200 summary_utils.py:349] Steps/second: 0.177323, Examples/second: 25.123029\n",
      "I0710 09:31:43.762670 140295626643200 trainer.py:508] step:  5933, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:180.3849 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2770792 log_pplx:3.9626625 loss:163.60841 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.62408\n",
      "I0710 09:31:46.930029 140295626643200 summary_utils.py:349] Steps/second: 0.177379, Examples/second: 25.145427\n",
      "I0710 09:31:46.930858 140295626643200 trainer.py:508] step:  5934, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:105.33772 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2780783 log_pplx:4.3368263 loss:66.339897 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.63251\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 09:31:48.680124 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 09:31:49.187808 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00005932\n",
      "I0710 09:31:50.806960 140295626643200 summary_utils.py:349] Steps/second: 0.177419, Examples/second: 25.153361\n",
      "I0710 09:31:50.807710 140295626643200 trainer.py:508] step:  5935, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:248.24292 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2788023 log_pplx:4.5091009 loss:112.92479 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.64001\n",
      "I0710 09:31:56.919101 140295626643200 summary_utils.py:349] Steps/second: 0.177408, Examples/second: 25.144010\n",
      "I0710 09:31:56.919891 140295626643200 trainer.py:508] step:  5936, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:211.19226 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.277211 log_pplx:3.9532642 loss:157.93291 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.64594\n",
      "I0710 09:32:03.036390 140295626643200 summary_utils.py:349] Steps/second: 0.177397, Examples/second: 25.134656\n",
      "I0710 09:32:03.037204 140295626643200 base_runner.py:111] step:  5937, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:105.28266 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2770804 log_pplx:3.7777224 loss:148.7478 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.65112\n",
      "I0710 09:32:12.295708 140295626643200 summary_utils.py:349] Steps/second: 0.177316, Examples/second: 25.110264\n",
      "I0710 09:32:12.296507 140295626643200 trainer.py:508] step:  5938, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:399.03497 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2761149 log_pplx:3.1650295 loss:233.89566 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.65619\n",
      "I0710 09:32:16.094035 140295626643200 summary_utils.py:349] Steps/second: 0.177357, Examples/second: 25.118439\n",
      "I0710 09:32:16.094791 140295626643200 trainer.py:508] step:  5939, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:99.774101 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2789496 log_pplx:4.3181653 loss:107.11751 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.66284\n",
      "I0710 09:32:22.234453 140295626643200 summary_utils.py:349] Steps/second: 0.177346, Examples/second: 25.109055\n",
      "I0710 09:32:22.235266 140295626643200 trainer.py:508] step:  5940, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:99.318893 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2773641 log_pplx:3.8208501 loss:157.94438 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.66998\n",
      "I0710 09:32:25.341636 140295626643200 summary_utils.py:349] Steps/second: 0.177403, Examples/second: 25.131544\n",
      "I0710 09:32:25.342473 140295626643200 trainer.py:508] step:  5941, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:71.750748 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2784833 log_pplx:4.230793 loss:66.651512 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.67743\n",
      "I0710 09:32:31.515670 140295626643200 summary_utils.py:349] Steps/second: 0.177391, Examples/second: 25.122054\n",
      "I0710 09:32:31.516476 140295626643200 trainer.py:508] step:  5942, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:96.915901 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2771647 log_pplx:3.7740645 loss:150.44365 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.68506\n",
      "I0710 09:32:33.702317 140295626643200 summary_utils.py:349] Steps/second: 0.177468, Examples/second: 25.179746\n",
      "I0710 09:32:33.703156 140295626643200 trainer.py:508] step:  5943, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:20.436235 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2774484 log_pplx:4.0905938 loss:30.240032 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:522.69226\n",
      "I0710 09:32:37.603581 140295626643200 summary_utils.py:349] Steps/second: 0.177507, Examples/second: 25.187538\n",
      "I0710 09:32:37.604439 140295626643200 trainer.py:508] step:  5944, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:44.502213 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2793896 log_pplx:4.1884489 loss:106.7531 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.6991\n",
      "I0710 09:32:44.202471 140295626643200 summary_utils.py:349] Steps/second: 0.177485, Examples/second: 25.176671\n",
      "I0710 09:32:44.203250 140295626643200 trainer.py:508] step:  5945, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:174.32298 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2773658 log_pplx:3.9213901 loss:158.62021 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.70563\n",
      "I0710 09:32:47.296439 140295626643200 summary_utils.py:349] Steps/second: 0.177542, Examples/second: 25.199113\n",
      "I0710 09:32:47.297235 140295626643200 trainer.py:508] step:  5946, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:27.45454 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2788714 log_pplx:4.0451136 loss:63.189095 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.71106\n",
      "I0710 09:32:53.220244 140295635035904 trainer.py:345] Write summary @5946\n",
      "I0710 09:32:56.342051 140295626643200 summary_utils.py:349] Steps/second: 0.177466, Examples/second: 25.180493\n",
      "I0710 09:32:56.343257 140295626643200 trainer.py:508] step:  5947, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:166.59541 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2772746 log_pplx:3.9636862 loss:159.58792 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.71625\n",
      "I0710 09:33:05.527617 140295626643200 summary_utils.py:349] Steps/second: 0.177386, Examples/second: 25.161473\n",
      "I0710 09:33:05.528764 140295626643200 trainer.py:508] step:  5948, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:84.420807 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2778851 log_pplx:3.8295691 loss:156.19856 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.72083\n",
      "I0710 09:33:11.082060 140295626643200 summary_utils.py:349] Steps/second: 0.177388, Examples/second: 25.164016\n",
      "I0710 09:33:11.083061 140295626643200 trainer.py:508] step:  5949, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:48.974823 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2795719 log_pplx:4.2357869 loss:106.1594 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.7254\n",
      "I0710 09:33:28.180046 140295626643200 summary_utils.py:349] Steps/second: 0.177133, Examples/second: 25.115093\n",
      "I0710 09:33:28.181459 140295626643200 trainer.py:508] step:  5950, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:349.56299 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2766311 log_pplx:3.1746845 loss:236.83147 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.72998\n",
      "I0710 09:33:38.273701 140295626643200 summary_utils.py:349] Steps/second: 0.177035, Examples/second: 25.093371\n",
      "I0710 09:33:38.275135 140295626643200 trainer.py:508] step:  5951, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:144.57399 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2775111 log_pplx:3.9782183 loss:156.64235 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.73407\n",
      "I0710 09:33:42.764352 140295626643200 summary_utils.py:349] Steps/second: 0.177060, Examples/second: 25.111304\n",
      "I0710 09:33:42.766089 140295626643200 trainer.py:508] step:  5952, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:29.039473 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2787232 log_pplx:4.0601711 loss:63.408451 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.73944\n",
      "I0710 09:33:48.118476 140295635035904 trainer.py:354] Write summary done: step 5946\n",
      "I0710 09:33:51.609113 140295626643200 summary_utils.py:349] Steps/second: 0.176990, Examples/second: 25.093542\n",
      "I0710 09:33:51.609930 140295626643200 trainer.py:508] step:  5953, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:272.1694 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2777185 log_pplx:4.0329418 loss:163.88869 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.74457\n",
      "I0710 09:33:55.477527 140295626643200 summary_utils.py:349] Steps/second: 0.177029, Examples/second: 25.101403\n",
      "I0710 09:33:55.478378 140295626643200 trainer.py:508] step:  5954, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:49.219002 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.279243 log_pplx:4.1160073 loss:103.92918 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.75104\n",
      "I0710 09:34:02.060991 140295626643200 summary_utils.py:349] Steps/second: 0.177008, Examples/second: 25.090761\n",
      "I0710 09:34:02.061838 140295626643200 trainer.py:508] step:  5955, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:120.22245 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2778223 log_pplx:3.966697 loss:164.61792 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.75714\n",
      "I0710 09:34:08.748637 140295626643200 summary_utils.py:349] Steps/second: 0.176986, Examples/second: 25.079810\n",
      "I0710 09:34:08.749428 140295626643200 trainer.py:508] step:  5956, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:99.335838 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2775171 log_pplx:3.8638389 loss:152.13867 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.76361\n",
      "I0710 09:34:12.662587 140295626643200 summary_utils.py:349] Steps/second: 0.177024, Examples/second: 25.087520\n",
      "I0710 09:34:12.663383 140295626643200 trainer.py:508] step:  5957, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:73.110291 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.279309 log_pplx:4.1684656 loss:105.41006 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.76996\n",
      "I0710 09:34:22.886359 140295626643200 summary_utils.py:349] Steps/second: 0.176923, Examples/second: 25.060565\n",
      "I0710 09:34:22.887134 140295626643200 trainer.py:508] step:  5958, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:315.6293 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2767105 log_pplx:3.2109306 loss:234.07683 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.7757\n",
      "I0710 09:34:25.906034 140295626643200 summary_utils.py:349] Steps/second: 0.176981, Examples/second: 25.083009\n",
      "I0710 09:34:25.906843 140295626643200 trainer.py:508] step:  5959, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:57.267361 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2791998 log_pplx:4.1976252 loss:66.32576 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.78174\n",
      "I0710 09:34:32.262119 140295626643200 summary_utils.py:349] Steps/second: 0.176965, Examples/second: 25.073131\n",
      "I0710 09:34:32.262939 140295626643200 trainer.py:508] step:  5960, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:331.82724 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2775823 log_pplx:3.916471 loss:158.76395 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.78723\n",
      "I0710 09:34:36.113790 140295626643200 summary_utils.py:349] Steps/second: 0.177005, Examples/second: 25.081015\n",
      "I0710 09:34:36.114595 140295626643200 trainer.py:508] step:  5961, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:60.788776 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2794057 log_pplx:4.1521077 loss:105.43758 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.79211\n",
      "I0710 09:34:42.546608 140295626643200 summary_utils.py:349] Steps/second: 0.176988, Examples/second: 25.070912\n",
      "I0710 09:34:42.547396 140295626643200 trainer.py:508] step:  5962, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:133.44923 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2780808 log_pplx:3.8486588 loss:157.45825 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.79669\n",
      "I0710 09:34:48.825852 140295626643200 summary_utils.py:349] Steps/second: 0.176974, Examples/second: 25.061303\n",
      "I0710 09:34:48.826629 140295626643200 trainer.py:508] step:  5963, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:94.414909 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2779685 log_pplx:3.8760195 loss:160.66101 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.80121\n",
      "I0710 09:34:51.833267 140295626643200 summary_utils.py:349] Steps/second: 0.177032, Examples/second: 25.083712\n",
      "I0710 09:34:51.834088 140295626643200 trainer.py:508] step:  5964, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:39.71151 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2791828 log_pplx:4.106627 loss:64.11792 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.80591\n",
      "I0710 09:34:58.339021 140295626643200 summary_utils.py:349] Steps/second: 0.177013, Examples/second: 25.073400\n",
      "I0710 09:34:58.339766 140295626643200 trainer.py:508] step:  5965, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:116.35877 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2780365 log_pplx:3.792254 loss:158.4214 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.81067\n",
      "I0710 09:35:02.188561 140295626643200 summary_utils.py:349] Steps/second: 0.177053, Examples/second: 25.081265\n",
      "I0710 09:35:02.189371 140295626643200 trainer.py:508] step:  5966, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:117.28595 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2797559 log_pplx:4.3464327 loss:110.15491 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.81616\n",
      "I0710 09:35:12.213824 140295626643200 summary_utils.py:349] Steps/second: 0.176957, Examples/second: 25.055099\n",
      "I0710 09:35:12.214854 140295626643200 trainer.py:508] step:  5967, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:259.31665 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2768977 log_pplx:3.2871673 loss:232.97798 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.8222\n",
      "I0710 09:35:18.622103 140295626643200 summary_utils.py:349] Steps/second: 0.176940, Examples/second: 25.045138\n",
      "I0710 09:35:18.622859 140295626643200 trainer.py:508] step:  5968, steps/sec: 0.18, examples/sec: 25.05 grad_norm/all/loss:86.81324 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2778958 log_pplx:3.8731294 loss:155.11885 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.82916\n",
      "I0710 09:35:20.821523 140295626643200 summary_utils.py:349] Steps/second: 0.177016, Examples/second: 25.101619\n",
      "I0710 09:35:20.822368 140295626643200 trainer.py:508] step:  5969, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:26.338703 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.277668 log_pplx:4.0063391 loss:29.124208 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:522.83624\n",
      "I0710 09:35:27.144019 140295626643200 summary_utils.py:349] Steps/second: 0.177001, Examples/second: 25.091898\n",
      "I0710 09:35:27.144826 140295626643200 trainer.py:508] step:  5970, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:106.10064 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2780467 log_pplx:3.8986325 loss:155.99405 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.84296\n",
      "I0710 09:35:30.154227 140295626643200 summary_utils.py:349] Steps/second: 0.177059, Examples/second: 25.114180\n",
      "I0710 09:35:30.155009 140295626643200 trainer.py:508] step:  5971, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:28.753687 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2793257 log_pplx:4.0333881 loss:61.761257 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.84937\n",
      "I0710 09:35:34.008827 140295626643200 summary_utils.py:349] Steps/second: 0.177098, Examples/second: 25.121980\n",
      "I0710 09:35:34.009728 140295626643200 trainer.py:508] step:  5972, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:51.636379 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.279489 log_pplx:4.0540757 loss:100.74379 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.85529\n",
      "I0710 09:35:40.484672 140295626643200 summary_utils.py:349] Steps/second: 0.177080, Examples/second: 25.111783\n",
      "I0710 09:35:40.485495 140295626643200 trainer.py:508] step:  5973, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:140.7122 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.277952 log_pplx:3.8555746 loss:155.76521 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.86078\n",
      "I0710 09:35:47.026757 140295626643200 summary_utils.py:349] Steps/second: 0.177060, Examples/second: 25.101398\n",
      "I0710 09:35:47.027598 140295626643200 trainer.py:508] step:  5974, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:87.286461 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2783509 log_pplx:3.8219571 loss:153.49936 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.86578\n",
      "I0710 09:35:50.875973 140295626643200 summary_utils.py:349] Steps/second: 0.177099, Examples/second: 25.109204\n",
      "I0710 09:35:50.876784 140295626643200 trainer.py:508] step:  5975, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:38.999763 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.279848 log_pplx:4.082798 loss:103.39687 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.87067\n",
      "I0710 09:35:53.943616 140295626643200 summary_utils.py:349] Steps/second: 0.177156, Examples/second: 25.131237\n",
      "I0710 09:35:53.944386 140295626643200 trainer.py:508] step:  5976, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:23.422777 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2793044 log_pplx:4.0161424 loss:61.716816 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.87549\n",
      "I0710 09:36:00.034389 140295626643200 summary_utils.py:349] Steps/second: 0.177146, Examples/second: 25.122248\n",
      "I0710 09:36:00.035165 140295626643200 trainer.py:508] step:  5977, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:78.162285 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2781199 log_pplx:3.8200357 loss:145.68661 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.88\n",
      "I0710 09:36:03.821543 140295626643200 summary_utils.py:349] Steps/second: 0.177186, Examples/second: 25.130223\n",
      "I0710 09:36:03.822383 140295626643200 trainer.py:508] step:  5978, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:41.660343 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2800828 log_pplx:4.0908999 loss:103.26965 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.88489\n",
      "I0710 09:36:10.106699 140295626643200 summary_utils.py:349] Steps/second: 0.177172, Examples/second: 25.120646\n",
      "I0710 09:36:10.107439 140295626643200 trainer.py:508] step:  5979, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:93.019333 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2785867 log_pplx:3.862968 loss:160.36148 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.88983\n",
      "I0710 09:36:19.274967 140295626643200 summary_utils.py:349] Steps/second: 0.177096, Examples/second: 25.097290\n",
      "I0710 09:36:19.275766 140295626643200 trainer.py:508] step:  5980, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:182.46925 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2775022 log_pplx:3.0744581 loss:217.90222 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.89478\n",
      "I0710 09:36:25.810430 140295626643200 summary_utils.py:349] Steps/second: 0.177077, Examples/second: 25.086986\n",
      "I0710 09:36:25.811264 140295626643200 trainer.py:508] step:  5981, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:84.285179 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2782212 log_pplx:3.86093 loss:154.34068 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.89966\n",
      "I0710 09:36:28.895775 140295626643200 summary_utils.py:349] Steps/second: 0.177132, Examples/second: 25.108879\n",
      "I0710 09:36:28.896559 140295626643200 trainer.py:508] step:  5982, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:27.502838 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2795451 log_pplx:4.0718069 loss:62.842613 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.9046\n",
      "I0710 09:36:32.710846 140295626643200 summary_utils.py:349] Steps/second: 0.177172, Examples/second: 25.116746\n",
      "I0710 09:36:32.711643 140295626643200 trainer.py:508] step:  5983, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:48.284096 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2800926 log_pplx:4.1374302 loss:104.34082 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.9093\n",
      "I0710 09:36:39.143571 140295626643200 summary_utils.py:349] Steps/second: 0.177155, Examples/second: 25.106760\n",
      "I0710 09:36:39.144464 140295626643200 trainer.py:508] step:  5984, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:79.000336 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.278874 log_pplx:3.8127332 loss:161.13562 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.91345\n",
      "I0710 09:36:45.585330 140295626643200 summary_utils.py:349] Steps/second: 0.177137, Examples/second: 25.096761\n",
      "I0710 09:36:45.586178 140295626643200 trainer.py:508] step:  5985, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:84.438934 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2784433 log_pplx:3.7737885 loss:156.47072 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.91766\n",
      "I0710 09:36:51.985208 140295626643200 summary_utils.py:349] Steps/second: 0.177121, Examples/second: 25.086907\n",
      "I0710 09:36:51.986223 140295626643200 trainer.py:508] step:  5986, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:79.868233 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2786279 log_pplx:3.8131049 loss:156.95694 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.92181\n",
      "I0710 09:36:55.886369 140295626643200 summary_utils.py:349] Steps/second: 0.177159, Examples/second: 25.094498\n",
      "I0710 09:36:55.887177 140295626643200 trainer.py:508] step:  5987, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:40.832394 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2804152 log_pplx:4.1666965 loss:106.43307 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.92651\n",
      "I0710 09:36:58.962978 140295626643200 summary_utils.py:349] Steps/second: 0.177214, Examples/second: 25.116335\n",
      "I0710 09:36:58.963778 140295626643200 trainer.py:508] step:  5988, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:23.812685 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.280086 log_pplx:4.0785518 loss:64.125664 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.93091\n",
      "I0710 09:37:05.308281 140295626643200 summary_utils.py:349] Steps/second: 0.177199, Examples/second: 25.106649\n",
      "I0710 09:37:05.309079 140295626643200 trainer.py:508] step:  5989, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:78.338501 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2789108 log_pplx:3.8287892 loss:155.35312 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.93506\n",
      "I0710 09:37:16.190126 140295626643200 summary_utils.py:349] Steps/second: 0.177086, Examples/second: 25.078227\n",
      "I0710 09:37:16.190970 140295626643200 trainer.py:508] step:  5990, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:205.97916 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2777027 log_pplx:3.095988 loss:228.40652 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.93951\n",
      "I0710 09:37:22.398913 140295626643200 summary_utils.py:349] Steps/second: 0.177074, Examples/second: 25.069007\n",
      "I0710 09:37:22.399889 140295626643200 trainer.py:508] step:  5991, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:75.847984 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2786342 log_pplx:3.7154067 loss:151.77437 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.94476\n",
      "I0710 09:37:26.260217 140295626643200 summary_utils.py:349] Steps/second: 0.177113, Examples/second: 25.076700\n",
      "I0710 09:37:26.260922 140295626643200 trainer.py:508] step:  5992, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:43.041317 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2802531 log_pplx:4.0288639 loss:102.15688 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.95026\n",
      "I0710 09:37:32.588056 140295626643200 summary_utils.py:349] Steps/second: 0.177098, Examples/second: 25.067130\n",
      "I0710 09:37:32.588840 140295626643200 trainer.py:508] step:  5993, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:83.583191 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2791049 log_pplx:3.7735271 loss:160.94095 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.95544\n",
      "I0710 09:37:34.763215 140295626643200 summary_utils.py:349] Steps/second: 0.177173, Examples/second: 25.122759\n",
      "I0710 09:37:34.763955 140295626643200 trainer.py:508] step:  5994, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:16.492317 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2783377 log_pplx:3.9193032 loss:28.238886 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:522.96075\n",
      "I0710 09:37:37.811750 140295626643200 summary_utils.py:349] Steps/second: 0.177229, Examples/second: 25.144568\n",
      "I0710 09:37:37.812543 140295626643200 trainer.py:508] step:  5995, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:21.710499 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2796295 log_pplx:3.942167 loss:60.9342 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.96588\n",
      "I0710 09:37:44.327389 140295626643200 summary_utils.py:349] Steps/second: 0.177210, Examples/second: 25.134385\n",
      "I0710 09:37:44.328258 140295626643200 trainer.py:508] step:  5996, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:78.960953 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2784616 log_pplx:3.7386706 loss:149.35989 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.9707\n",
      "I0710 09:37:48.200588 140295626643200 summary_utils.py:349] Steps/second: 0.177248, Examples/second: 25.141991\n",
      "I0710 09:37:48.201395 140295626643200 trainer.py:508] step:  5997, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:47.933453 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2807291 log_pplx:4.1090393 loss:105.03732 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.97546\n",
      "I0710 09:37:54.546259 140295626643200 summary_utils.py:349] Steps/second: 0.177233, Examples/second: 25.132342\n",
      "I0710 09:37:54.547033 140295626643200 trainer.py:508] step:  5998, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:81.690468 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2786881 log_pplx:3.7823343 loss:152.23895 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.97992\n",
      "I0710 09:38:03.533986 140295626643200 summary_utils.py:349] Steps/second: 0.177161, Examples/second: 25.109817\n",
      "I0710 09:38:03.534780 140295626643200 trainer.py:508] step:  5999, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:287.31128 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2778915 log_pplx:3.1095302 loss:223.65292 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:522.98419\n",
      "I0710 09:38:09.749724 140295626643200 summary_utils.py:349] Steps/second: 0.177149, Examples/second: 25.100606\n",
      "I0710 09:38:09.750547 140295626643200 trainer.py:508] step:  6000, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:90.15477 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2791488 log_pplx:3.7778575 loss:159.37837 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:522.98865\n",
      "I0710 09:38:12.815413 140295626643200 summary_utils.py:349] Steps/second: 0.177204, Examples/second: 25.122278\n",
      "I0710 09:38:12.816170 140295626643200 trainer.py:508] step:  6001, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:24.964273 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2800257 log_pplx:3.9787891 loss:61.904366 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:522.99359\n",
      "I0710 09:38:16.715450 140295626643200 summary_utils.py:349] Steps/second: 0.177241, Examples/second: 25.129781\n",
      "I0710 09:38:16.716486 140295626643200 trainer.py:508] step:  6002, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:57.282825 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2806339 log_pplx:4.0455713 loss:103.66776 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:522.99854\n",
      "I0710 09:38:23.162140 140295626643200 summary_utils.py:349] Steps/second: 0.177224, Examples/second: 25.119868\n",
      "I0710 09:38:23.162950 140295626643200 trainer.py:508] step:  6003, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:106.94897 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2788906 log_pplx:3.8451729 loss:154.57596 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.00372\n",
      "I0710 09:38:29.545414 140295626643200 summary_utils.py:349] Steps/second: 0.177208, Examples/second: 25.110162\n",
      "I0710 09:38:29.546279 140295626643200 trainer.py:508] step:  6004, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:79.762154 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2788979 log_pplx:3.6759284 loss:145.15323 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.00928\n",
      "I0710 09:38:33.392013 140295626643200 summary_utils.py:349] Steps/second: 0.177247, Examples/second: 25.117819\n",
      "I0710 09:38:33.392787 140295626643200 trainer.py:508] step:  6005, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:38.845993 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2809176 log_pplx:4.0685654 loss:102.60415 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.01495\n",
      "I0710 09:38:39.626923 140295626643200 summary_utils.py:349] Steps/second: 0.177234, Examples/second: 25.108576\n",
      "I0710 09:38:39.627771 140295626643200 trainer.py:508] step:  6006, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:76.360809 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2788318 log_pplx:3.7143774 loss:150.57158 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.02039\n",
      "I0710 09:38:42.600970 140295626643200 summary_utils.py:349] Steps/second: 0.177291, Examples/second: 25.130445\n",
      "I0710 09:38:42.601816 140295626643200 trainer.py:508] step:  6007, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:46.294041 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2802225 log_pplx:4.0254078 loss:62.708305 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.02594\n",
      "I0710 09:38:49.106914 140295626643200 summary_utils.py:349] Steps/second: 0.177272, Examples/second: 25.120383\n",
      "I0710 09:38:49.107793 140295626643200 trainer.py:508] step:  6008, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:71.929985 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2788434 log_pplx:3.781805 loss:150.37401 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.03107\n",
      "I0710 09:38:53.012382 140295626643200 summary_utils.py:349] Steps/second: 0.177309, Examples/second: 25.127838\n",
      "I0710 09:38:53.013264 140295626643200 trainer.py:508] step:  6009, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:54.510326 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2805641 log_pplx:4.0586376 loss:100.34982 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.03601\n",
      "I0710 09:39:03.097654 140295626643200 summary_utils.py:349] Steps/second: 0.177215, Examples/second: 25.102162\n",
      "I0710 09:39:03.098475 140295626643200 trainer.py:508] step:  6010, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:339.31726 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2780932 log_pplx:3.2047625 loss:226.25626 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:523.04071\n",
      "I0710 09:39:09.563453 140295626643200 summary_utils.py:349] Steps/second: 0.177197, Examples/second: 25.092267\n",
      "I0710 09:39:09.564329 140295626643200 trainer.py:508] step:  6011, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:87.337494 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.278735 log_pplx:3.6746876 loss:143.03722 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.0473\n",
      "I0710 09:39:16.394973 140295626643200 summary_utils.py:349] Steps/second: 0.177172, Examples/second: 25.081286\n",
      "I0710 09:39:16.395852 140295626643200 trainer.py:508] step:  6012, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:94.8423 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2788109 log_pplx:3.739429 loss:145.41702 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.05371\n",
      "I0710 09:39:19.550361 140295626643200 summary_utils.py:349] Steps/second: 0.177225, Examples/second: 25.102522\n",
      "I0710 09:39:19.551169 140295626643200 trainer.py:508] step:  6013, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:23.534599 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2805448 log_pplx:4.0064678 loss:63.774826 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.06012\n",
      "I0710 09:39:23.364286 140295626643200 summary_utils.py:349] Steps/second: 0.177264, Examples/second: 25.110234\n",
      "I0710 09:39:23.365051 140295626643200 trainer.py:508] step:  6014, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:38.718307 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.280854 log_pplx:4.0135493 loss:100.61467 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.06628\n",
      "I0710 09:39:29.715899 140295626643200 summary_utils.py:349] Steps/second: 0.177249, Examples/second: 25.100701\n",
      "I0710 09:39:29.716609 140295626643200 trainer.py:508] step:  6015, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:90.089302 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2791915 log_pplx:3.8209465 loss:151.83487 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.0722\n",
      "I0710 09:39:31.885422 140295626643200 summary_utils.py:349] Steps/second: 0.177322, Examples/second: 25.155558\n",
      "I0710 09:39:31.886218 140295626643200 trainer.py:508] step:  6016, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:19.102928 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2791249 log_pplx:4.0231018 loss:29.332497 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:523.07776\n",
      "I0710 09:39:38.267670 140295626643200 summary_utils.py:349] Steps/second: 0.177307, Examples/second: 25.145908\n",
      "I0710 09:39:38.268412 140295626643200 trainer.py:508] step:  6017, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:173.34325 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2793626 log_pplx:3.8208451 loss:160.14116 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.08289\n",
      "I0710 09:39:42.039814 140295626643200 summary_utils.py:349] Steps/second: 0.177346, Examples/second: 25.153709\n",
      "I0710 09:39:42.040681 140295626643200 trainer.py:508] step:  6018, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:65.774948 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2809513 log_pplx:4.0698438 loss:101.8224 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.08765\n",
      "I0710 09:39:48.469006 140295626643200 summary_utils.py:349] Steps/second: 0.177329, Examples/second: 25.143931\n",
      "I0710 09:39:48.469728 140295626643200 trainer.py:508] step:  6019, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:84.118179 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2791712 log_pplx:3.8064234 loss:149.54485 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.09265\n",
      "I0710 09:39:51.505801 140295626643200 summary_utils.py:349] Steps/second: 0.177385, Examples/second: 25.165419\n",
      "I0710 09:39:51.506599 140295626643200 trainer.py:508] step:  6020, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:36.695789 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2808821 log_pplx:4.0711932 loss:65.139084 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.0976\n",
      "I0710 09:39:57.954346 140295626643200 summary_utils.py:349] Steps/second: 0.177367, Examples/second: 25.155584\n",
      "I0710 09:39:57.955137 140295626643200 trainer.py:508] step:  6021, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:86.686005 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2796677 log_pplx:3.7884197 loss:156.13025 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.10242\n",
      "I0710 09:40:07.946833 140295626643200 summary_utils.py:349] Steps/second: 0.177275, Examples/second: 25.130352\n",
      "I0710 09:40:07.947807 140295626643200 trainer.py:508] step:  6022, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:400.2153 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2783551 log_pplx:3.2402229 loss:233.45805 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:523.10742\n",
      "I0710 09:40:14.375939 140295626643200 summary_utils.py:349] Steps/second: 0.177259, Examples/second: 25.120622\n",
      "I0710 09:40:14.376716 140295626643200 trainer.py:508] step:  6023, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:90.034325 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2799901 log_pplx:3.7851751 loss:159.4032 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.11115\n",
      "I0710 09:40:18.178403 140295626643200 summary_utils.py:349] Steps/second: 0.177297, Examples/second: 25.128310\n",
      "I0710 09:40:18.179168 140295626643200 trainer.py:508] step:  6024, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:43.19194 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2812316 log_pplx:4.0364499 loss:101.18876 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.11554\n",
      "I0710 09:40:24.722592 140295626643200 summary_utils.py:349] Steps/second: 0.177278, Examples/second: 25.118250\n",
      "I0710 09:40:24.723385 140295626643200 trainer.py:508] step:  6025, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:111.40554 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2792463 log_pplx:3.7951417 loss:150.66713 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.11987\n",
      "I0710 09:40:27.757752 140295626643200 summary_utils.py:349] Steps/second: 0.177333, Examples/second: 25.139659\n",
      "I0710 09:40:27.758609 140295626643200 trainer.py:508] step:  6026, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:28.988747 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2806066 log_pplx:3.9878764 loss:62.310566 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.12506\n",
      "I0710 09:40:31.567070 140295626643200 summary_utils.py:349] Steps/second: 0.177372, Examples/second: 25.147305\n",
      "I0710 09:40:31.567856 140295626643200 trainer.py:508] step:  6027, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:54.604076 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2808939 log_pplx:4.0860796 loss:102.94366 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.13\n",
      "I0710 09:40:37.933976 140295626643200 summary_utils.py:349] Steps/second: 0.177357, Examples/second: 25.137776\n",
      "I0710 09:40:37.934720 140295626643200 trainer.py:508] step:  6028, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:89.202644 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2793674 log_pplx:3.8339491 loss:148.66138 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.13458\n",
      "I0710 09:40:44.027078 140295626643200 summary_utils.py:349] Steps/second: 0.177347, Examples/second: 25.129079\n",
      "I0710 09:40:44.027849 140295626643200 trainer.py:508] step:  6029, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:142.75871 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2791233 log_pplx:3.7206354 loss:143.384 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.13977\n",
      "I0710 09:40:47.772365 140295626643200 summary_utils.py:349] Steps/second: 0.177387, Examples/second: 25.136907\n",
      "I0710 09:40:47.773098 140295626643200 trainer.py:508] step:  6030, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:58.460949 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2816437 log_pplx:4.097753 loss:105.69641 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.14545\n",
      "I0710 09:40:54.150525 140295626643200 summary_utils.py:349] Steps/second: 0.177371, Examples/second: 25.127370\n",
      "I0710 09:40:54.151433 140295626643200 trainer.py:508] step:  6031, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:83.236641 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2798123 log_pplx:3.7837305 loss:155.88969 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.15063\n",
      "I0710 09:40:57.150468 140295626643200 summary_utils.py:349] Steps/second: 0.177427, Examples/second: 25.148806\n",
      "I0710 09:40:57.151294 140295626643200 trainer.py:508] step:  6032, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:26.464132 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.281068 log_pplx:4.0622187 loss:63.488033 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.1557\n",
      "I0710 09:41:08.272602 140295626643200 summary_utils.py:349] Steps/second: 0.177312, Examples/second: 25.120395\n",
      "I0710 09:41:08.273370 140295626643200 trainer.py:508] step:  6033, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:308.72244 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2788451 log_pplx:3.1265018 loss:225.49898 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:523.16052\n",
      "I0710 09:41:14.438894 140295626643200 summary_utils.py:349] Steps/second: 0.177300, Examples/second: 25.111524\n",
      "I0710 09:41:14.439639 140295626643200 trainer.py:508] step:  6034, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:117.36808 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2797104 log_pplx:3.7548995 loss:153.57538 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.16638\n",
      "I0710 09:41:18.323269 140295626643200 summary_utils.py:349] Steps/second: 0.177337, Examples/second: 25.118918\n",
      "I0710 09:41:18.324143 140295626643200 trainer.py:508] step:  6035, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:51.258938 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2815607 log_pplx:4.0860829 loss:102.5096 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.17206\n",
      "I0710 09:41:24.542561 140295626643200 summary_utils.py:349] Steps/second: 0.177325, Examples/second: 25.109901\n",
      "I0710 09:41:24.543312 140295626643200 trainer.py:508] step:  6036, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:128.55649 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2797784 log_pplx:3.7338858 loss:151.31572 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.17737\n",
      "I0710 09:41:26.738760 140295626643200 summary_utils.py:349] Steps/second: 0.177397, Examples/second: 25.163931\n",
      "I0710 09:41:26.739555 140295626643200 base_runner.py:111] step:  6037, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:11.951951 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2796314 log_pplx:3.9140005 loss:28.881039 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:523.18237\n",
      "I0710 09:41:33.092441 140295626643200 summary_utils.py:349] Steps/second: 0.177382, Examples/second: 25.154491\n",
      "I0710 09:41:33.093223 140295626643200 trainer.py:508] step:  6038, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:101.24007 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2797192 log_pplx:3.8593454 loss:151.67229 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.18713\n",
      "I0710 09:41:36.128796 140295626643200 summary_utils.py:349] Steps/second: 0.177437, Examples/second: 25.175710\n",
      "I0710 09:41:36.129604 140295626643200 trainer.py:508] step:  6039, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:43.527962 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2812423 log_pplx:4.0442162 loss:62.52737 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.19208\n",
      "I0710 09:41:42.676260 140295626643200 summary_utils.py:349] Steps/second: 0.177418, Examples/second: 25.165696\n",
      "I0710 09:41:42.677028 140295626643200 trainer.py:508] step:  6040, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:104.77008 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2799511 log_pplx:3.8206697 loss:158.03244 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.19708\n",
      "I0710 09:41:46.417717 140295626643200 summary_utils.py:349] Steps/second: 0.177457, Examples/second: 25.173465\n",
      "I0710 09:41:46.418520 140295626643200 trainer.py:508] step:  6041, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:109.99507 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2811979 log_pplx:4.1391115 loss:105.08169 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.20258\n",
      "I0710 09:41:48.540489 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 09:41:52.595129 140295626643200 summary_utils.py:349] Steps/second: 0.177446, Examples/second: 25.164562\n",
      "I0710 09:41:52.595965 140295626643200 trainer.py:508] step:  6042, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:78.10936 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2797391 log_pplx:3.7740116 loss:148.55453 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.20856\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 09:41:53.552387 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 09:41:54.055740 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00006041\n",
      "I0710 09:41:58.624759 140295626643200 summary_utils.py:349] Steps/second: 0.177438, Examples/second: 25.156111\n",
      "I0710 09:41:58.625631 140295626643200 trainer.py:508] step:  6043, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:109.49801 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2800864 log_pplx:3.8725352 loss:154.27213 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.21466\n",
      "I0710 09:42:02.441756 140295626643200 summary_utils.py:349] Steps/second: 0.177476, Examples/second: 25.163645\n",
      "I0710 09:42:02.442718 140295626643200 trainer.py:508] step:  6044, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:55.830017 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2817327 log_pplx:4.1624756 loss:104.21798 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.22015\n",
      "I0710 09:42:05.485040 140295626643200 summary_utils.py:349] Steps/second: 0.177530, Examples/second: 25.184768\n",
      "I0710 09:42:05.485815 140295626643200 trainer.py:508] step:  6045, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:49.934948 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.281056 log_pplx:4.0336847 loss:62.663918 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.2251\n",
      "I0710 09:42:16.228956 140295626643200 summary_utils.py:349] Steps/second: 0.177423, Examples/second: 25.157656\n",
      "I0710 09:42:16.229729 140295626643200 trainer.py:508] step:  6046, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:277.41183 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2791663 log_pplx:3.2282255 loss:230.41463 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:523.22949\n",
      "I0710 09:42:18.555258 140295635035904 trainer.py:345] Write summary @6046\n",
      "I0710 09:42:27.493029 140295626643200 summary_utils.py:349] Steps/second: 0.177306, Examples/second: 25.133776\n",
      "I0710 09:42:27.494362 140295626643200 trainer.py:508] step:  6047, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:77.208862 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2801205 log_pplx:3.7929976 loss:154.70688 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.23438\n",
      "I0710 09:42:38.472090 140295626643200 summary_utils.py:349] Steps/second: 0.177195, Examples/second: 25.110797\n",
      "I0710 09:42:38.473107 140295626643200 trainer.py:508] step:  6048, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:100.43837 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2798731 log_pplx:3.6899679 loss:152.62631 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.2395\n",
      "I0710 09:42:44.089519 140295626643200 summary_utils.py:349] Steps/second: 0.177196, Examples/second: 25.113017\n",
      "I0710 09:42:44.090984 140295626643200 trainer.py:508] step:  6049, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:54.108383 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.281654 log_pplx:4.0964255 loss:103.38354 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.24487\n",
      "I0710 09:42:53.802361 140295626643200 summary_utils.py:349] Steps/second: 0.177111, Examples/second: 25.093829\n",
      "I0710 09:42:53.803541 140295626643200 trainer.py:508] step:  6050, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:112.52422 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2803465 log_pplx:3.7405784 loss:155.04697 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.25\n",
      "I0710 09:42:58.231566 140295626643200 summary_utils.py:349] Steps/second: 0.177136, Examples/second: 25.110777\n",
      "I0710 09:42:58.233183 140295626643200 trainer.py:508] step:  6051, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:28.356075 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2814559 log_pplx:3.9977272 loss:62.77681 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.25586\n",
      "I0710 09:43:04.240790 140295635035904 trainer.py:354] Write summary done: step 6046\n",
      "I0710 09:43:06.931732 140295626643200 summary_utils.py:349] Steps/second: 0.177073, Examples/second: 25.094592\n",
      "I0710 09:43:06.932789 140295626643200 trainer.py:508] step:  6052, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:74.379623 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2797107 log_pplx:3.765764 loss:145.12315 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.26129\n",
      "I0710 09:43:10.730395 140295626643200 summary_utils.py:349] Steps/second: 0.177111, Examples/second: 25.102148\n",
      "I0710 09:43:10.731174 140295626643200 trainer.py:508] step:  6053, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:49.236858 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2821085 log_pplx:4.0387635 loss:102.45837 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.26685\n",
      "I0710 09:43:17.263492 140295626643200 summary_utils.py:349] Steps/second: 0.177093, Examples/second: 25.092343\n",
      "I0710 09:43:17.264335 140295626643200 trainer.py:508] step:  6054, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:92.359337 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2806232 log_pplx:3.7798944 loss:156.01515 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.27185\n",
      "I0710 09:43:28.229865 140295626643200 summary_utils.py:349] Steps/second: 0.176983, Examples/second: 25.064924\n",
      "I0710 09:43:28.230713 140295626643200 trainer.py:508] step:  6055, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:279.25082 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2791092 log_pplx:3.1912189 loss:225.06071 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:523.27655\n",
      "I0710 09:43:34.495615 140295626643200 summary_utils.py:349] Steps/second: 0.176971, Examples/second: 25.055947\n",
      "I0710 09:43:34.496377 140295626643200 trainer.py:508] step:  6056, steps/sec: 0.18, examples/sec: 25.06 grad_norm/all/loss:80.812866 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2802138 log_pplx:3.6872354 loss:147.99641 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.28302\n",
      "I0710 09:43:37.545752 140295626643200 summary_utils.py:349] Steps/second: 0.177024, Examples/second: 25.076861\n",
      "I0710 09:43:37.546856 140295626643200 trainer.py:508] step:  6057, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:29.041811 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2817384 log_pplx:4.0278859 loss:63.155994 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.28918\n",
      "I0710 09:43:43.961272 140295626643200 summary_utils.py:349] Steps/second: 0.177008, Examples/second: 25.067448\n",
      "I0710 09:43:43.962312 140295626643200 trainer.py:508] step:  6058, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:131.07381 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2799413 log_pplx:3.778167 loss:152.49625 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.2951\n",
      "I0710 09:43:47.877658 140295626643200 summary_utils.py:349] Steps/second: 0.177044, Examples/second: 25.074640\n",
      "I0710 09:43:47.878499 140295626643200 trainer.py:508] step:  6059, steps/sec: 0.18, examples/sec: 25.07 grad_norm/all/loss:51.550358 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2821292 log_pplx:4.0716672 loss:102.9114 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.30042\n",
      "I0710 09:43:50.074194 140295626643200 summary_utils.py:349] Steps/second: 0.177115, Examples/second: 25.127777\n",
      "I0710 09:43:50.074947 140295626643200 trainer.py:508] step:  6060, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:14.225658 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2802893 log_pplx:3.9631848 loss:29.460709 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:523.30548\n",
      "I0710 09:43:56.176066 140295626643200 summary_utils.py:349] Steps/second: 0.177106, Examples/second: 25.119256\n",
      "I0710 09:43:56.177064 140295626643200 trainer.py:508] step:  6061, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:94.780418 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2805918 log_pplx:3.7357271 loss:153.77187 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.31018\n",
      "I0710 09:44:02.175929 140295626643200 summary_utils.py:349] Steps/second: 0.177098, Examples/second: 25.111044\n",
      "I0710 09:44:02.176751 140295626643200 trainer.py:508] step:  6062, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:85.053986 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2800821 log_pplx:3.7034802 loss:148.55585 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.31506\n",
      "I0710 09:44:05.285118 140295626643200 summary_utils.py:349] Steps/second: 0.177151, Examples/second: 25.131699\n",
      "I0710 09:44:05.285929 140295626643200 trainer.py:508] step:  6063, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:47.059708 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2821496 log_pplx:4.0981517 loss:64.337776 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.32013\n",
      "I0710 09:44:09.217530 140295626643200 summary_utils.py:349] Steps/second: 0.177186, Examples/second: 25.138797\n",
      "I0710 09:44:09.218325 140295626643200 trainer.py:508] step:  6064, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:70.576645 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.282045 log_pplx:4.0519791 loss:101.93259 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.3252\n",
      "I0710 09:44:15.656671 140295626643200 summary_utils.py:349] Steps/second: 0.177169, Examples/second: 25.129304\n",
      "I0710 09:44:15.657586 140295626643200 trainer.py:508] step:  6065, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:89.796738 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2808563 log_pplx:3.823534 loss:157.48181 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.33044\n",
      "I0710 09:44:26.164549 140295626643200 summary_utils.py:349] Steps/second: 0.177070, Examples/second: 25.103357\n",
      "I0710 09:44:26.165401 140295626643200 trainer.py:508] step:  6066, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:250.76186 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.279586 log_pplx:3.2269125 loss:233.62846 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:523.33563\n",
      "I0710 09:44:32.720683 140295626643200 summary_utils.py:349] Steps/second: 0.177051, Examples/second: 25.093569\n",
      "I0710 09:44:32.721414 140295626643200 trainer.py:508] step:  6067, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:95.768341 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2807674 log_pplx:3.8554552 loss:155.32664 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.34027\n",
      "I0710 09:44:36.555090 140295626643200 summary_utils.py:349] Steps/second: 0.177088, Examples/second: 25.100947\n",
      "I0710 09:44:36.555883 140295626643200 trainer.py:508] step:  6068, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:42.137714 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2820803 log_pplx:4.0314221 loss:102.02018 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.34521\n",
      "I0710 09:44:42.881671 140295626643200 summary_utils.py:349] Steps/second: 0.177074, Examples/second: 25.091838\n",
      "I0710 09:44:42.882636 140295626643200 trainer.py:508] step:  6069, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:95.366295 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2805744 log_pplx:3.7245264 loss:150.28464 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.34998\n",
      "I0710 09:44:45.983225 140295626643200 summary_utils.py:349] Steps/second: 0.177126, Examples/second: 25.112425\n",
      "I0710 09:44:45.984218 140295626643200 trainer.py:508] step:  6070, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:25.642899 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2819126 log_pplx:3.9903574 loss:62.59874 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.35504\n",
      "I0710 09:44:52.305703 140295626643200 summary_utils.py:349] Steps/second: 0.177113, Examples/second: 25.103330\n",
      "I0710 09:44:52.306502 140295626643200 trainer.py:508] step:  6071, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:94.770424 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2805781 log_pplx:3.7047045 loss:146.79892 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.3598\n",
      "I0710 09:44:56.216590 140295626643200 summary_utils.py:349] Steps/second: 0.177148, Examples/second: 25.110465\n",
      "I0710 09:44:56.217487 140295626643200 trainer.py:508] step:  6072, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:48.7869 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2820528 log_pplx:4.0387402 loss:100.13552 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.36499\n",
      "I0710 09:45:02.631871 140295626643200 summary_utils.py:349] Steps/second: 0.177132, Examples/second: 25.101114\n",
      "I0710 09:45:02.632674 140295626643200 trainer.py:508] step:  6073, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:76.319168 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2806622 log_pplx:3.7212262 loss:151.08179 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.36993\n",
      "I0710 09:45:09.083505 140295626643200 summary_utils.py:349] Steps/second: 0.177116, Examples/second: 25.091671\n",
      "I0710 09:45:09.084382 140295626643200 trainer.py:508] step:  6074, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:81.621025 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2805575 log_pplx:3.7056296 loss:149.10527 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.37518\n",
      "I0710 09:45:12.152220 140295626643200 summary_utils.py:349] Steps/second: 0.177168, Examples/second: 25.112291\n",
      "I0710 09:45:12.153027 140295626643200 trainer.py:508] step:  6075, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:37.5793 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2817785 log_pplx:3.9571118 loss:60.361412 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.38043\n",
      "I0710 09:45:15.942138 140295626643200 summary_utils.py:349] Steps/second: 0.177206, Examples/second: 25.119755\n",
      "I0710 09:45:15.943063 140295626643200 trainer.py:508] step:  6076, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:64.231407 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2826606 log_pplx:4.1008868 loss:104.05998 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.38519\n",
      "I0710 09:45:26.676901 140295626643200 summary_utils.py:349] Steps/second: 0.177102, Examples/second: 25.093345\n",
      "I0710 09:45:26.677657 140295626643200 trainer.py:508] step:  6077, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:264.83554 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2798673 log_pplx:3.2088952 loss:231.9229 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:523.38959\n",
      "I0710 09:45:33.057783 140295626643200 summary_utils.py:349] Steps/second: 0.177087, Examples/second: 25.084138\n",
      "I0710 09:45:33.058822 140295626643200 trainer.py:508] step:  6078, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:79.502281 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2809415 log_pplx:3.6992173 loss:148.61604 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.39508\n",
      "I0710 09:45:39.233979 140295626643200 summary_utils.py:349] Steps/second: 0.177077, Examples/second: 25.075534\n",
      "I0710 09:45:39.234752 140295626643200 trainer.py:508] step:  6079, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:100.65392 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2808944 log_pplx:3.7847903 loss:153.23672 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.40063\n",
      "I0710 09:45:43.116125 140295626643200 summary_utils.py:349] Steps/second: 0.177113, Examples/second: 25.082726\n",
      "I0710 09:45:43.116892 140295626643200 trainer.py:508] step:  6080, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:38.926983 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2825233 log_pplx:4.0146623 loss:100.216 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.40607\n",
      "I0710 09:45:46.188119 140295626643200 summary_utils.py:349] Steps/second: 0.177165, Examples/second: 25.103258\n",
      "I0710 09:45:46.189243 140295626643200 trainer.py:508] step:  6081, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:29.179661 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2822045 log_pplx:4.0127611 loss:63.436108 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.41138\n",
      "I0710 09:45:52.502669 140295626643200 summary_utils.py:349] Steps/second: 0.177151, Examples/second: 25.094256\n",
      "I0710 09:45:52.503444 140295626643200 trainer.py:508] step:  6082, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:74.159058 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2808418 log_pplx:3.7112782 loss:149.19339 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.41675\n",
      "I0710 09:45:54.698669 140295626643200 summary_utils.py:349] Steps/second: 0.177221, Examples/second: 25.146630\n",
      "I0710 09:45:54.699422 140295626643200 trainer.py:508] step:  6083, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:10.114007 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2807177 log_pplx:3.9013078 loss:28.414017 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:523.42188\n",
      "I0710 09:46:01.059403 140295626643200 summary_utils.py:349] Steps/second: 0.177207, Examples/second: 25.137472\n",
      "I0710 09:46:01.060248 140295626643200 trainer.py:508] step:  6084, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:78.752174 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2808858 log_pplx:3.6794405 loss:149.43127 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.42676\n",
      "I0710 09:46:04.814363 140295626643200 summary_utils.py:349] Steps/second: 0.177245, Examples/second: 25.144984\n",
      "I0710 09:46:04.815152 140295626643200 trainer.py:508] step:  6085, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:59.640499 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2827804 log_pplx:4.1144066 loss:103.83733 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.43207\n",
      "I0710 09:46:11.430088 140295626643200 summary_utils.py:349] Steps/second: 0.177225, Examples/second: 25.135105\n",
      "I0710 09:46:11.431067 140295626643200 trainer.py:508] step:  6086, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:77.542206 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2810351 log_pplx:3.6707914 loss:147.38229 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.43768\n",
      "I0710 09:46:14.499427 140295626643200 summary_utils.py:349] Steps/second: 0.177277, Examples/second: 25.155560\n",
      "I0710 09:46:14.500466 140295626643200 trainer.py:508] step:  6087, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:24.557102 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2821909 log_pplx:3.9832101 loss:62.066513 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.44336\n",
      "I0710 09:46:24.348815 140295626643200 summary_utils.py:349] Steps/second: 0.177192, Examples/second: 25.131824\n",
      "I0710 09:46:24.349586 140295626643200 trainer.py:508] step:  6088, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:270.74472 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2801492 log_pplx:3.1616046 loss:227.87265 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:523.44867\n",
      "I0710 09:46:30.648111 140295626643200 summary_utils.py:349] Steps/second: 0.177179, Examples/second: 25.122884\n",
      "I0710 09:46:30.648905 140295626643200 trainer.py:508] step:  6089, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:89.61586 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2812567 log_pplx:3.7575676 loss:152.4633 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.45374\n",
      "I0710 09:46:34.440994 140295626643200 summary_utils.py:349] Steps/second: 0.177216, Examples/second: 25.130268\n",
      "I0710 09:46:34.442026 140295626643200 trainer.py:508] step:  6090, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:56.201744 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2827128 log_pplx:4.0037355 loss:101.44466 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.45874\n",
      "I0710 09:46:40.850081 140295626643200 summary_utils.py:349] Steps/second: 0.177201, Examples/second: 25.121025\n",
      "I0710 09:46:40.850876 140295626643200 trainer.py:508] step:  6091, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:84.860367 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2810414 log_pplx:3.7633958 loss:157.78036 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.46332\n",
      "I0710 09:46:43.893380 140295626643200 summary_utils.py:349] Steps/second: 0.177253, Examples/second: 25.141492\n",
      "I0710 09:46:43.894265 140295626643200 trainer.py:508] step:  6092, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:21.407875 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2821475 log_pplx:3.9207921 loss:61.323639 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.46802\n",
      "I0710 09:46:50.208609 140295626643200 summary_utils.py:349] Steps/second: 0.177240, Examples/second: 25.132519\n",
      "I0710 09:46:50.209547 140295626643200 trainer.py:508] step:  6093, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:117.75208 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2813774 log_pplx:3.7569628 loss:157.04105 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.4726\n",
      "I0710 09:46:54.101630 140295626643200 summary_utils.py:349] Steps/second: 0.177275, Examples/second: 25.139596\n",
      "I0710 09:46:54.102460 140295626643200 trainer.py:508] step:  6094, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:47.023815 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2828459 log_pplx:4.0519271 loss:102.18455 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.47784\n",
      "I0710 09:47:00.122536 140295626643200 summary_utils.py:349] Steps/second: 0.177267, Examples/second: 25.131476\n",
      "I0710 09:47:00.123275 140295626643200 trainer.py:508] step:  6095, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:102.14122 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2816427 log_pplx:3.8788278 loss:159.22588 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.48297\n",
      "I0710 09:47:10.697007 140295626643200 summary_utils.py:349] Steps/second: 0.177168, Examples/second: 25.105821\n",
      "I0710 09:47:10.697887 140295626643200 trainer.py:508] step:  6096, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:265.64001 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2804416 log_pplx:3.106359 loss:225.98759 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:523.48877\n",
      "I0710 09:47:17.125242 140295626643200 summary_utils.py:349] Steps/second: 0.177152, Examples/second: 25.096579\n",
      "I0710 09:47:17.126148 140295626643200 trainer.py:508] step:  6097, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:110.38052 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2816799 log_pplx:3.8449123 loss:161.43823 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.49554\n",
      "I0710 09:47:20.955350 140295626643200 summary_utils.py:349] Steps/second: 0.177189, Examples/second: 25.103830\n",
      "I0710 09:47:20.956255 140295626643200 trainer.py:508] step:  6098, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:72.875648 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2830647 log_pplx:4.0613551 loss:102.95535 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.50195\n",
      "I0710 09:47:24.048662 140295626643200 summary_utils.py:349] Steps/second: 0.177240, Examples/second: 25.124067\n",
      "I0710 09:47:24.049678 140295626643200 trainer.py:508] step:  6099, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:33.62899 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2827638 log_pplx:4.0406237 loss:62.456047 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.50775\n",
      "I0710 09:47:30.233951 140295626643200 summary_utils.py:349] Steps/second: 0.177229, Examples/second: 25.115517\n",
      "I0710 09:47:30.234743 140295626643200 trainer.py:508] step:  6100, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:95.285362 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.281081 log_pplx:3.7964551 loss:149.20068 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.51312\n",
      "I0710 09:47:36.682292 140295626643200 summary_utils.py:349] Steps/second: 0.177213, Examples/second: 25.106231\n",
      "I0710 09:47:36.683092 140295626643200 trainer.py:508] step:  6101, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:91.457428 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2815362 log_pplx:3.7897027 loss:155.14096 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.51819\n",
      "I0710 09:47:40.387815 140295626643200 summary_utils.py:349] Steps/second: 0.177252, Examples/second: 25.113816\n",
      "I0710 09:47:40.388631 140295626643200 trainer.py:508] step:  6102, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:53.442467 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2832791 log_pplx:4.0401287 loss:101.786 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.52319\n",
      "I0710 09:47:46.820918 140295626643200 summary_utils.py:349] Steps/second: 0.177236, Examples/second: 25.104585\n",
      "I0710 09:47:46.821676 140295626643200 trainer.py:508] step:  6103, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:86.38121 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2815914 log_pplx:3.8831491 loss:156.97632 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.52844\n",
      "I0710 09:47:49.907376 140295626643200 summary_utils.py:349] Steps/second: 0.177287, Examples/second: 25.124782\n",
      "I0710 09:47:49.908263 140295626643200 trainer.py:508] step:  6104, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:35.714684 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2829736 log_pplx:4.0384688 loss:62.596272 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.53394\n",
      "I0710 09:47:52.080720 140295626643200 summary_utils.py:349] Steps/second: 0.177357, Examples/second: 25.176518\n",
      "I0710 09:47:52.081502 140295626643200 trainer.py:508] step:  6105, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:11.122602 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2816124 log_pplx:4.0110598 loss:29.330877 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:523.53943\n",
      "I0710 09:47:58.450985 140295626643200 summary_utils.py:349] Steps/second: 0.177342, Examples/second: 25.167433\n",
      "I0710 09:47:58.451801 140295626643200 trainer.py:508] step:  6106, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:102.5465 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2814358 log_pplx:3.7727666 loss:152.27829 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.54456\n",
      "I0710 09:48:02.365009 140295626643200 summary_utils.py:349] Steps/second: 0.177377, Examples/second: 25.174380\n",
      "I0710 09:48:02.365842 140295626643200 trainer.py:508] step:  6107, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:46.660343 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2830812 log_pplx:3.9489801 loss:98.625778 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.54999\n",
      "I0710 09:48:08.620169 140295626643200 summary_utils.py:349] Steps/second: 0.177364, Examples/second: 25.165634\n",
      "I0710 09:48:08.620941 140295626643200 trainer.py:508] step:  6108, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:74.72419 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2809347 log_pplx:3.6944299 loss:143.9904 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.55536\n",
      "I0710 09:48:15.019278 140295626643200 summary_utils.py:349] Steps/second: 0.177349, Examples/second: 25.156492\n",
      "I0710 09:48:15.020166 140295626643200 trainer.py:508] step:  6109, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:81.271225 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2816393 log_pplx:3.7649696 loss:149.37517 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.56061\n",
      "I0710 09:48:25.065531 140295626643200 summary_utils.py:349] Steps/second: 0.177261, Examples/second: 25.132519\n",
      "I0710 09:48:25.066430 140295626643200 trainer.py:508] step:  6110, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:604.97565 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.280895 log_pplx:3.2964642 loss:247.23482 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:523.56543\n",
      "I0710 09:48:28.061970 140295626643200 summary_utils.py:349] Steps/second: 0.177314, Examples/second: 25.152875\n",
      "I0710 09:48:28.062777 140295626643200 trainer.py:508] step:  6111, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:35.787224 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2830184 log_pplx:3.9787056 loss:62.244987 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.57001\n",
      "I0710 09:48:31.900885 140295626643200 summary_utils.py:349] Steps/second: 0.177350, Examples/second: 25.160018\n",
      "I0710 09:48:31.901650 140295626643200 trainer.py:508] step:  6112, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:43.232399 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2836468 log_pplx:4.0815439 loss:102.2937 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.5744\n",
      "I0710 09:48:38.236258 140295626643200 summary_utils.py:349] Steps/second: 0.177336, Examples/second: 25.151084\n",
      "I0710 09:48:38.237027 140295626643200 trainer.py:508] step:  6113, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:146.13545 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2818907 log_pplx:3.7818468 loss:155.103 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.57892\n",
      "I0710 09:48:44.699187 140295626643200 summary_utils.py:349] Steps/second: 0.177319, Examples/second: 25.141802\n",
      "I0710 09:48:44.699959 140295626643200 trainer.py:508] step:  6114, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:234.22607 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.282099 log_pplx:3.8918047 loss:161.21802 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.58447\n",
      "I0710 09:48:48.553001 140295626643200 summary_utils.py:349] Steps/second: 0.177355, Examples/second: 25.148895\n",
      "I0710 09:48:48.553764 140295626643200 trainer.py:508] step:  6115, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:43.566612 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2834145 log_pplx:3.9583766 loss:99.256302 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.59076\n",
      "I0710 09:48:54.591835 140295626643200 summary_utils.py:349] Steps/second: 0.177347, Examples/second: 25.140823\n",
      "I0710 09:48:54.592629 140295626643200 trainer.py:508] step:  6116, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:82.208763 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2816381 log_pplx:3.7587376 loss:147.57742 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.59662\n",
      "I0710 09:48:57.631303 140295626643200 summary_utils.py:349] Steps/second: 0.177399, Examples/second: 25.160986\n",
      "I0710 09:48:57.632138 140295626643200 trainer.py:508] step:  6117, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:32.749138 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2831908 log_pplx:3.975657 loss:61.622684 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.60248\n",
      "I0710 09:49:03.948882 140295626643200 summary_utils.py:349] Steps/second: 0.177385, Examples/second: 25.152127\n",
      "I0710 09:49:03.949789 140295626643200 trainer.py:508] step:  6118, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:124.72925 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2820278 log_pplx:3.8030908 loss:156.21196 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.60785\n",
      "I0710 09:49:13.579809 140295626643200 summary_utils.py:349] Steps/second: 0.177306, Examples/second: 25.129461\n",
      "I0710 09:49:13.580626 140295626643200 trainer.py:508] step:  6119, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:508.0567 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2809407 log_pplx:3.3369551 loss:238.42545 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:523.61285\n",
      "I0710 09:49:17.436154 140295626643200 summary_utils.py:349] Steps/second: 0.177341, Examples/second: 25.136529\n",
      "I0710 09:49:17.436963 140295626643200 trainer.py:508] step:  6120, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:101.94135 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2839259 log_pplx:4.2530265 loss:107.17627 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.62006\n",
      "I0710 09:49:23.797519 140295626643200 summary_utils.py:349] Steps/second: 0.177327, Examples/second: 25.127584\n",
      "I0710 09:49:23.798354 140295626643200 trainer.py:508] step:  6121, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:233.95729 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2818989 log_pplx:3.8911462 loss:156.18088 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.62646\n",
      "I0710 09:49:26.881876 140295626643200 summary_utils.py:349] Steps/second: 0.177378, Examples/second: 25.147560\n",
      "I0710 09:49:26.882730 140295626643200 trainer.py:508] step:  6122, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:37.932148 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2830623 log_pplx:4.0709114 loss:62.876492 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.63226\n",
      "I0710 09:49:33.595403 140295626643200 summary_utils.py:349] Steps/second: 0.177356, Examples/second: 25.137627\n",
      "I0710 09:49:33.596187 140295626643200 trainer.py:508] step:  6123, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:157.11632 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2819883 log_pplx:4.0137601 loss:160.3999 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.63788\n",
      "I0710 09:49:37.731889 140295626643200 summary_utils.py:349] Steps/second: 0.177386, Examples/second: 25.143888\n",
      "I0710 09:49:37.732758 140295626643200 trainer.py:508] step:  6124, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:85.420021 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2837315 log_pplx:4.1368093 loss:102.85142 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.64337\n",
      "I0710 09:49:43.908562 140295626643200 summary_utils.py:349] Steps/second: 0.177375, Examples/second: 25.135477\n",
      "I0710 09:49:43.909351 140295626643200 trainer.py:508] step:  6125, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:99.918198 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2822721 log_pplx:3.8027735 loss:155.15317 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.64905\n",
      "I0710 09:49:50.070487 140295626643200 summary_utils.py:349] Steps/second: 0.177365, Examples/second: 25.127118\n",
      "I0710 09:49:50.071309 140295626643200 trainer.py:508] step:  6126, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:93.347572 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2819915 log_pplx:3.8104496 loss:150.70328 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.65472\n",
      "I0710 09:49:52.232963 140295626643200 summary_utils.py:349] Steps/second: 0.177434, Examples/second: 25.178190\n",
      "I0710 09:49:52.233745 140295626643200 trainer.py:508] step:  6127, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:13.047037 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2820389 log_pplx:3.9681134 loss:29.140831 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:523.6604\n",
      "I0710 09:50:01.211824 140295626643200 summary_utils.py:349] Steps/second: 0.177368, Examples/second: 25.157447\n",
      "I0710 09:50:01.212591 140295626643200 trainer.py:508] step:  6128, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:326.01062 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2810776 log_pplx:3.2708514 loss:228.71428 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:523.66577\n",
      "I0710 09:50:04.272916 140295626643200 summary_utils.py:349] Steps/second: 0.177419, Examples/second: 25.177394\n",
      "I0710 09:50:04.273994 140295626643200 trainer.py:508] step:  6129, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:30.504917 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2834359 log_pplx:4.0603952 loss:63.285065 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.67126\n",
      "I0710 09:50:10.488039 140295626643200 summary_utils.py:349] Steps/second: 0.177407, Examples/second: 25.168877\n",
      "I0710 09:50:10.488915 140295626643200 trainer.py:508] step:  6130, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:91.94088 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2824401 log_pplx:3.7738721 loss:153.92682 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.67657\n",
      "I0710 09:50:14.384771 140295626643200 summary_utils.py:349] Steps/second: 0.177442, Examples/second: 25.175770\n",
      "I0710 09:50:14.385522 140295626643200 trainer.py:508] step:  6131, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:50.638317 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2837081 log_pplx:4.0667644 loss:101.61828 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.68213\n",
      "I0710 09:50:20.596290 140295626643200 summary_utils.py:349] Steps/second: 0.177430, Examples/second: 25.167273\n",
      "I0710 09:50:20.597084 140295626643200 trainer.py:508] step:  6132, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:90.355568 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2821441 log_pplx:3.7214098 loss:150.43797 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.68756\n",
      "I0710 09:50:27.085210 140295626643200 summary_utils.py:349] Steps/second: 0.177413, Examples/second: 25.158013\n",
      "I0710 09:50:27.086251 140295626643200 trainer.py:508] step:  6133, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:100.26624 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2826445 log_pplx:3.9014177 loss:163.07925 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.69293\n",
      "I0710 09:50:30.960908 140295626643200 summary_utils.py:349] Steps/second: 0.177448, Examples/second: 25.164957\n",
      "I0710 09:50:30.961731 140295626643200 trainer.py:508] step:  6134, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:46.537342 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2838995 log_pplx:4.0829887 loss:101.94713 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.6983\n",
      "I0710 09:50:34.063431 140295626643200 summary_utils.py:349] Steps/second: 0.177498, Examples/second: 25.184720\n",
      "I0710 09:50:34.064227 140295626643200 trainer.py:508] step:  6135, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:41.121025 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2837467 log_pplx:4.0614734 loss:63.333599 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.70331\n",
      "I0710 09:50:40.182849 140295626643200 summary_utils.py:349] Steps/second: 0.177488, Examples/second: 25.176493\n",
      "I0710 09:50:40.183898 140295626643200 trainer.py:508] step:  6136, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:75.926651 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2821378 log_pplx:3.8253136 loss:153.87325 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.70795\n",
      "I0710 09:50:46.389472 140295626643200 summary_utils.py:349] Steps/second: 0.177477, Examples/second: 25.168034\n",
      "I0710 09:50:46.390282 140295626643200 base_runner.py:111] step:  6137, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:75.175583 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2823296 log_pplx:3.804245 loss:149.64948 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.71265\n",
      "I0710 09:50:50.241456 140295626643200 summary_utils.py:349] Steps/second: 0.177512, Examples/second: 25.175025\n",
      "I0710 09:50:50.242362 140295626643200 trainer.py:508] step:  6138, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:39.759224 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2841938 log_pplx:4.0276127 loss:102.57827 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.71747\n",
      "I0710 09:50:56.824554 140295626643200 summary_utils.py:349] Steps/second: 0.177494, Examples/second: 25.165527\n",
      "I0710 09:50:56.825347 140295626643200 trainer.py:508] step:  6139, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:80.583862 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.282395 log_pplx:3.8475037 loss:155.34296 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.72211\n",
      "I0710 09:50:59.850861 140295626643200 summary_utils.py:349] Steps/second: 0.177545, Examples/second: 25.185445\n",
      "I0710 09:50:59.851649 140295626643200 trainer.py:508] step:  6140, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:24.49044 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2836051 log_pplx:3.9602549 loss:61.631462 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.72711\n",
      "I0710 09:51:05.584355 140295626643200 summary_utils.py:349] Steps/second: 0.177543, Examples/second: 25.178317\n",
      "I0710 09:51:05.585177 140295626643200 trainer.py:508] step:  6141, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:72.075722 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.282308 log_pplx:3.8280582 loss:148.33725 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.73193\n",
      "I0710 09:51:15.404919 140295626643200 summary_utils.py:349] Steps/second: 0.177461, Examples/second: 25.155400\n",
      "I0710 09:51:15.405678 140295626643200 trainer.py:508] step:  6142, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:339.61359 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2815899 log_pplx:3.2932856 loss:241.39783 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:523.73688\n",
      "I0710 09:51:19.252495 140295626643200 summary_utils.py:349] Steps/second: 0.177496, Examples/second: 25.162387\n",
      "I0710 09:51:19.253347 140295626643200 trainer.py:508] step:  6143, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:52.186249 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2842697 log_pplx:4.010932 loss:102.1785 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.74286\n",
      "I0710 09:51:25.472882 140295626643200 summary_utils.py:349] Steps/second: 0.177484, Examples/second: 25.153935\n",
      "I0710 09:51:25.473659 140295626643200 trainer.py:508] step:  6144, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:95.384285 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2824695 log_pplx:3.8159556 loss:153.783 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.74847\n",
      "I0710 09:51:31.848529 140295626643200 summary_utils.py:349] Steps/second: 0.177470, Examples/second: 25.145065\n",
      "I0710 09:51:31.849310 140295626643200 trainer.py:508] step:  6145, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:95.99897 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2826427 log_pplx:3.8413053 loss:153.84427 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.75378\n",
      "I0710 09:51:34.912650 140295626643200 summary_utils.py:349] Steps/second: 0.177520, Examples/second: 25.164809\n",
      "I0710 09:51:34.913426 140295626643200 trainer.py:508] step:  6146, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:21.602726 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2837082 log_pplx:4.0086989 loss:62.541969 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.75897\n",
      "I0710 09:51:38.757327 140295626643200 summary_utils.py:349] Steps/second: 0.177555, Examples/second: 25.171784\n",
      "I0710 09:51:38.758172 140295626643200 trainer.py:508] step:  6147, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:36.904888 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2841793 log_pplx:4.0052872 loss:100.2323 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.76398\n",
      "I0710 09:51:44.697105 140295635035904 trainer.py:345] Write summary @6147\n",
      "I0710 09:51:47.985622 140295626643200 summary_utils.py:349] Steps/second: 0.177485, Examples/second: 25.155006\n",
      "I0710 09:51:47.987221 140295626643200 trainer.py:508] step:  6148, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:71.485611 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2828058 log_pplx:3.710125 loss:150.67746 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.7688\n",
      "I0710 09:51:57.846507 140295626643200 summary_utils.py:349] Steps/second: 0.177402, Examples/second: 25.136512\n",
      "I0710 09:51:57.847669 140295626643200 trainer.py:508] step:  6149, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:83.88089 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2828242 log_pplx:3.8060663 loss:154.19327 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.7738\n",
      "I0710 09:52:03.608936 140295626643200 summary_utils.py:349] Steps/second: 0.177400, Examples/second: 25.138178\n",
      "I0710 09:52:03.610149 140295626643200 trainer.py:508] step:  6150, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:43.489372 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2842506 log_pplx:4.0625324 loss:102.80746 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.77899\n",
      "I0710 09:52:17.446566 140295626643200 summary_utils.py:349] Steps/second: 0.177240, Examples/second: 25.104372\n",
      "I0710 09:52:17.447845 140295626643200 trainer.py:508] step:  6151, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:450.51407 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2814412 log_pplx:3.2274101 loss:223.49814 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:523.78406\n",
      "I0710 09:52:20.373036 140295626643200 summary_utils.py:349] Steps/second: 0.177293, Examples/second: 25.152513\n",
      "I0710 09:52:20.374939 140295626643200 trainer.py:508] step:  6152, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:14.149656 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2826856 log_pplx:3.9042428 loss:29.045429 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:523.78857\n",
      "I0710 09:52:29.782778 140295635035904 trainer.py:354] Write summary done: step 6147\n",
      "I0710 09:52:29.788830 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 09:52:31.026273 140295626643200 summary_utils.py:349] Steps/second: 0.177195, Examples/second: 25.131904\n",
      "I0710 09:52:31.027032 140295626643200 trainer.py:508] step:  6153, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:108.63907 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.282671 log_pplx:3.7092381 loss:146.19034 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.79303\n",
      "I0710 09:52:34.291601 140295626643200 summary_utils.py:349] Steps/second: 0.177241, Examples/second: 25.150969\n",
      "I0710 09:52:34.312717 140295626643200 trainer.py:508] step:  6154, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:29.274572 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2839235 log_pplx:4.0178752 loss:62.355541 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.79797\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 09:52:34.917968 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 09:52:35.420126 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00006152\n",
      "I0710 09:52:40.810747 140295626643200 summary_utils.py:349] Steps/second: 0.177224, Examples/second: 25.141773\n",
      "I0710 09:52:40.811490 140295626643200 trainer.py:508] step:  6155, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:115.95518 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2826233 log_pplx:3.8368084 loss:151.64987 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.80273\n",
      "I0710 09:52:47.395195 140295626643200 summary_utils.py:349] Steps/second: 0.177206, Examples/second: 25.132409\n",
      "I0710 09:52:47.396249 140295626643200 trainer.py:508] step:  6156, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:118.36181 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2827715 log_pplx:3.7775147 loss:150.72285 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.80798\n",
      "I0710 09:52:51.178362 140295626643200 summary_utils.py:349] Steps/second: 0.177242, Examples/second: 25.139512\n",
      "I0710 09:52:51.179171 140295626643200 trainer.py:508] step:  6157, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:46.544044 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2849714 log_pplx:4.0962305 loss:102.66177 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.8136\n",
      "I0710 09:52:57.736864 140295626643200 summary_utils.py:349] Steps/second: 0.177224, Examples/second: 25.130232\n",
      "I0710 09:52:57.737601 140295626643200 trainer.py:508] step:  6158, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:79.808228 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2830328 log_pplx:3.7216992 loss:155.70657 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.81885\n",
      "I0710 09:53:00.800818 140295626643200 summary_utils.py:349] Steps/second: 0.177274, Examples/second: 25.149796\n",
      "I0710 09:53:00.801773 140295626643200 trainer.py:508] step:  6159, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:45.838978 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2842867 log_pplx:3.963223 loss:62.157585 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.82397\n",
      "I0710 09:53:07.075443 140295626643200 summary_utils.py:349] Steps/second: 0.177262, Examples/second: 25.141298\n",
      "I0710 09:53:07.076246 140295626643200 trainer.py:508] step:  6160, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:81.122215 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2825881 log_pplx:3.7073605 loss:144.91145 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.82855\n",
      "I0710 09:53:10.944857 140295626643200 summary_utils.py:349] Steps/second: 0.177296, Examples/second: 25.148145\n",
      "I0710 09:53:10.945648 140295626643200 trainer.py:508] step:  6161, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:58.994808 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2846133 log_pplx:4.0336943 loss:101.64908 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.83301\n",
      "I0710 09:53:21.452446 140295626643200 summary_utils.py:349] Steps/second: 0.177202, Examples/second: 25.123692\n",
      "I0710 09:53:21.453325 140295626643200 trainer.py:508] step:  6162, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:327.26926 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2820096 log_pplx:3.3227553 loss:245.55161 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:523.83698\n",
      "I0710 09:53:27.404331 140295626643200 summary_utils.py:349] Steps/second: 0.177196, Examples/second: 25.116114\n",
      "I0710 09:53:27.405149 140295626643200 trainer.py:508] step:  6163, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:78.204704 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2831079 log_pplx:3.7923369 loss:156.14949 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.84296\n",
      "I0710 09:53:31.299146 140295626643200 summary_utils.py:349] Steps/second: 0.177230, Examples/second: 25.122887\n",
      "I0710 09:53:31.299978 140295626643200 trainer.py:508] step:  6164, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:43.448143 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.284799 log_pplx:4.0743651 loss:102.2411 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.849\n",
      "I0710 09:53:34.371944 140295626643200 summary_utils.py:349] Steps/second: 0.177280, Examples/second: 25.142358\n",
      "I0710 09:53:34.372696 140295626643200 trainer.py:508] step:  6165, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:24.421125 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2841029 log_pplx:3.9006844 loss:60.353951 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.85486\n",
      "I0710 09:53:40.763566 140295626643200 summary_utils.py:349] Steps/second: 0.177265, Examples/second: 25.133576\n",
      "I0710 09:53:40.764422 140295626643200 trainer.py:508] step:  6166, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:95.747177 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2829746 log_pplx:3.8360276 loss:153.82471 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.86053\n",
      "I0710 09:53:47.307956 140295626643200 summary_utils.py:349] Steps/second: 0.177248, Examples/second: 25.124389\n",
      "I0710 09:53:47.308942 140295626643200 trainer.py:508] step:  6167, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:115.26293 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2831061 log_pplx:3.7882309 loss:149.91924 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.86597\n",
      "I0710 09:53:51.091722 140295626643200 summary_utils.py:349] Steps/second: 0.177284, Examples/second: 25.131448\n",
      "I0710 09:53:51.092499 140295626643200 trainer.py:508] step:  6168, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:50.856003 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2848253 log_pplx:4.0742116 loss:102.79746 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.87134\n",
      "I0710 09:53:57.186727 140295626643200 summary_utils.py:349] Steps/second: 0.177275, Examples/second: 25.123499\n",
      "I0710 09:53:57.187616 140295626643200 trainer.py:508] step:  6169, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:76.04911 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2831678 log_pplx:3.8211565 loss:152.32085 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.87671\n",
      "I0710 09:54:00.250920 140295626643200 summary_utils.py:349] Steps/second: 0.177324, Examples/second: 25.142938\n",
      "I0710 09:54:00.251890 140295626643200 trainer.py:508] step:  6170, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:27.115517 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2846245 log_pplx:3.9640801 loss:62.031658 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.88184\n",
      "I0710 09:54:06.742455 140295626643200 summary_utils.py:349] Steps/second: 0.177308, Examples/second: 25.133908\n",
      "I0710 09:54:06.743300 140295626643200 trainer.py:508] step:  6171, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:79.674377 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.283583 log_pplx:3.7558339 loss:155.63235 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.88684\n",
      "I0710 09:54:17.281645 140295626643200 summary_utils.py:349] Steps/second: 0.177214, Examples/second: 25.109533\n",
      "I0710 09:54:17.282465 140295626643200 trainer.py:508] step:  6172, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:211.4525 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2822723 log_pplx:3.2435455 loss:237.10318 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:523.89154\n",
      "I0710 09:54:21.037541 140295626643200 summary_utils.py:349] Steps/second: 0.177250, Examples/second: 25.116650\n",
      "I0710 09:54:21.038358 140295626643200 trainer.py:508] step:  6173, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:59.682919 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2849728 log_pplx:4.02598 loss:100.90112 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.89539\n",
      "I0710 09:54:27.421509 140295626643200 summary_utils.py:349] Steps/second: 0.177236, Examples/second: 25.107951\n",
      "I0710 09:54:27.422370 140295626643200 trainer.py:508] step:  6174, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:93.480247 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2837899 log_pplx:3.7465041 loss:157.25951 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.89954\n",
      "I0710 09:54:33.689180 140295626643200 summary_utils.py:349] Steps/second: 0.177224, Examples/second: 25.099580\n",
      "I0710 09:54:33.689922 140295626643200 trainer.py:508] step:  6175, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:88.663284 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2834132 log_pplx:3.7548254 loss:153.80704 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.90436\n",
      "I0710 09:54:36.769149 140295626643200 summary_utils.py:349] Steps/second: 0.177273, Examples/second: 25.118908\n",
      "I0710 09:54:36.770299 140295626643200 trainer.py:508] step:  6176, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:31.245523 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2847904 log_pplx:3.9717734 loss:63.408741 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.90955\n",
      "I0710 09:54:38.968739 140295626643200 summary_utils.py:349] Steps/second: 0.177339, Examples/second: 25.168297\n",
      "I0710 09:54:38.969552 140295626643200 trainer.py:508] step:  6177, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:26.165207 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2833774 log_pplx:3.9699345 loss:28.758766 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:523.91437\n",
      "I0710 09:54:43.001156 140295626643200 summary_utils.py:349] Steps/second: 0.177370, Examples/second: 25.174622\n",
      "I0710 09:54:43.002026 140295626643200 trainer.py:508] step:  6178, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:59.986557 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2845987 log_pplx:4.0227652 loss:100.19199 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.91888\n",
      "I0710 09:54:49.800883 140295626643200 summary_utils.py:349] Steps/second: 0.177348, Examples/second: 25.164774\n",
      "I0710 09:54:49.801728 140295626643200 trainer.py:508] step:  6179, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:82.458618 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.283829 log_pplx:3.8004122 loss:157.85962 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.9231\n",
      "I0710 09:54:56.166197 140295626643200 summary_utils.py:349] Steps/second: 0.177334, Examples/second: 25.156119\n",
      "I0710 09:54:56.167028 140295626643200 trainer.py:508] step:  6180, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:83.400452 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2834581 log_pplx:3.7222025 loss:146.70131 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.92737\n",
      "2020-07-10 09:54:59.376300: I lingvo/core/ops/record_yielder.cc:532] Epoch 6: total records 46838\n",
      "2020-07-10 09:54:59.376374: I lingvo/core/ops/record_yielder.cc:485] Epoch 6 /tmp/punctuator_data/train.txt\n",
      "I0710 09:54:59.971497 140295626643200 summary_utils.py:349] Steps/second: 0.177369, Examples/second: 25.163054\n",
      "I0710 09:54:59.972260 140295626643200 trainer.py:508] step:  6181, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:44.56057 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2852466 log_pplx:4.0940795 loss:103.17079 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.93164\n",
      "I0710 09:55:06.121753 140295626643200 summary_utils.py:349] Steps/second: 0.177359, Examples/second: 25.154993\n",
      "I0710 09:55:06.122645 140295626643200 trainer.py:508] step:  6182, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:69.030815 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2837135 log_pplx:3.7212689 loss:148.52515 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.93573\n",
      "I0710 09:55:09.134007 140295626643200 summary_utils.py:349] Steps/second: 0.177409, Examples/second: 25.174419\n",
      "I0710 09:55:09.134782 140295626643200 trainer.py:508] step:  6183, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:36.326088 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2847937 log_pplx:4.0172071 loss:62.37656 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.94006\n",
      "I0710 09:55:19.611169 140295626643200 summary_utils.py:349] Steps/second: 0.177317, Examples/second: 25.150329\n",
      "I0710 09:55:19.611969 140295626643200 trainer.py:508] step:  6184, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:231.32069 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2826025 log_pplx:3.2825739 loss:241.59744 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:523.94446\n",
      "I0710 09:55:25.666788 140295626643200 summary_utils.py:349] Steps/second: 0.177309, Examples/second: 25.142550\n",
      "I0710 09:55:25.667700 140295626643200 trainer.py:508] step:  6185, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:88.995323 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2837324 log_pplx:3.6879854 loss:151.80669 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.95032\n",
      "I0710 09:55:29.465156 140295626643200 summary_utils.py:349] Steps/second: 0.177344, Examples/second: 25.149487\n",
      "I0710 09:55:29.465964 140295626643200 trainer.py:508] step:  6186, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:49.530041 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2856103 log_pplx:3.9977224 loss:101.76701 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.9566\n",
      "I0710 09:55:35.608265 140295626643200 summary_utils.py:349] Steps/second: 0.177334, Examples/second: 25.141480\n",
      "I0710 09:55:35.609038 140295626643200 trainer.py:508] step:  6187, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:82.195633 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.283493 log_pplx:3.5857701 loss:142.8033 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.96271\n",
      "I0710 09:55:38.656139 140295626643200 summary_utils.py:349] Steps/second: 0.177384, Examples/second: 25.160752\n",
      "I0710 09:55:38.656880 140295626643200 trainer.py:508] step:  6188, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:24.11694 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2851317 log_pplx:3.9024804 loss:61.174423 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.96857\n",
      "I0710 09:55:44.823209 140295626643200 summary_utils.py:349] Steps/second: 0.177374, Examples/second: 25.152681\n",
      "I0710 09:55:44.824240 140295626643200 trainer.py:508] step:  6189, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:83.077629 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2836564 log_pplx:3.5830519 loss:146.68118 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.97406\n",
      "I0710 09:55:48.604519 140295626643200 summary_utils.py:349] Steps/second: 0.177409, Examples/second: 25.159646\n",
      "I0710 09:55:48.605448 140295626643200 trainer.py:508] step:  6190, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:38.176449 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.285495 log_pplx:3.9349246 loss:99.184685 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:523.97925\n",
      "I0710 09:55:55.095581 140295626643200 summary_utils.py:349] Steps/second: 0.177393, Examples/second: 25.150710\n",
      "I0710 09:55:55.096329 140295626643200 trainer.py:508] step:  6191, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:83.375191 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.284048 log_pplx:3.5705595 loss:148.89232 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.98419\n",
      "I0710 09:56:01.465294 140295626643200 summary_utils.py:349] Steps/second: 0.177379, Examples/second: 25.142114\n",
      "I0710 09:56:01.466113 140295626643200 trainer.py:508] step:  6192, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:75.829453 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2840062 log_pplx:3.6328535 loss:146.08612 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:523.98889\n",
      "I0710 09:56:04.489341 140295626643200 summary_utils.py:349] Steps/second: 0.177428, Examples/second: 25.161397\n",
      "I0710 09:56:04.490342 140295626643200 trainer.py:508] step:  6193, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:25.457819 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2849116 log_pplx:3.8596313 loss:59.809208 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:523.99359\n",
      "I0710 09:56:13.474990 140295626643200 summary_utils.py:349] Steps/second: 0.177365, Examples/second: 25.141477\n",
      "I0710 09:56:13.475767 140295626643200 trainer.py:508] step:  6194, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:265.13577 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2826807 log_pplx:2.9599659 loss:206.60562 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:523.99823\n",
      "I0710 09:56:19.611839 140295626643200 summary_utils.py:349] Steps/second: 0.177355, Examples/second: 25.133531\n",
      "I0710 09:56:19.612837 140295626643200 trainer.py:508] step:  6195, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:75.696396 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.283667 log_pplx:3.5022063 loss:139.91315 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.00366\n",
      "I0710 09:56:23.500688 140295626643200 summary_utils.py:349] Steps/second: 0.177388, Examples/second: 25.140188\n",
      "I0710 09:56:23.501455 140295626643200 trainer.py:508] step:  6196, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:55.469296 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2851937 log_pplx:3.8853312 loss:96.817604 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.00952\n",
      "I0710 09:56:29.730901 140295626643200 summary_utils.py:349] Steps/second: 0.177377, Examples/second: 25.132000\n",
      "I0710 09:56:29.731669 140295626643200 trainer.py:508] step:  6197, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:108.34815 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2836955 log_pplx:3.5984485 loss:140.60939 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.01526\n",
      "I0710 09:56:31.871982 140295626643200 summary_utils.py:349] Steps/second: 0.177443, Examples/second: 25.180948\n",
      "I0710 09:56:31.872903 140295626643200 trainer.py:508] step:  6198, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:11.586488 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2836255 log_pplx:3.8683598 loss:27.698061 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:524.02106\n",
      "I0710 09:56:38.117367 140295626643200 summary_utils.py:349] Steps/second: 0.177432, Examples/second: 25.172699\n",
      "I0710 09:56:38.118136 140295626643200 trainer.py:508] step:  6199, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:84.917923 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2838348 log_pplx:3.5411954 loss:143.81677 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.02649\n",
      "I0710 09:56:41.941001 140295626643200 summary_utils.py:349] Steps/second: 0.177466, Examples/second: 25.179502\n",
      "I0710 09:56:41.941795 140295626643200 trainer.py:508] step:  6200, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:43.985676 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2858284 log_pplx:3.8653276 loss:97.140518 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.03192\n",
      "I0710 09:56:44.953739 140295626643200 summary_utils.py:349] Steps/second: 0.177516, Examples/second: 25.198720\n",
      "I0710 09:56:44.954519 140295626643200 trainer.py:508] step:  6201, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:23.827822 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2852794 log_pplx:3.8928335 loss:61.388157 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.03729\n",
      "I0710 09:56:51.137295 140295626643200 summary_utils.py:349] Steps/second: 0.177505, Examples/second: 25.190637\n",
      "I0710 09:56:51.138106 140295626643200 trainer.py:508] step:  6202, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:70.087181 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2836921 log_pplx:3.5703259 loss:137.99309 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.04224\n",
      "I0710 09:56:57.171635 140295626643200 summary_utils.py:349] Steps/second: 0.177498, Examples/second: 25.182965\n",
      "I0710 09:56:57.172433 140295626643200 trainer.py:508] step:  6203, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:75.138412 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2838849 log_pplx:3.4858084 loss:141.48026 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.04736\n",
      "I0710 09:57:00.994748 140295626643200 summary_utils.py:349] Steps/second: 0.177532, Examples/second: 25.189751\n",
      "I0710 09:57:00.995539 140295626643200 trainer.py:508] step:  6204, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:45.770985 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2854878 log_pplx:3.8624146 loss:97.888069 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.05243\n",
      "I0710 09:57:07.068671 140295626643200 summary_utils.py:349] Steps/second: 0.177524, Examples/second: 25.181982\n",
      "I0710 09:57:07.069589 140295626643200 trainer.py:508] step:  6205, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:70.792763 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2841743 log_pplx:3.552675 loss:141.97377 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.05737\n",
      "I0710 09:57:13.496894 140295626643200 summary_utils.py:349] Steps/second: 0.177509, Examples/second: 25.173274\n",
      "I0710 09:57:13.497667 140295626643200 trainer.py:508] step:  6206, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:73.244102 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2839556 log_pplx:3.535959 loss:143.86934 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.06232\n",
      "I0710 09:57:16.532667 140295626643200 summary_utils.py:349] Steps/second: 0.177558, Examples/second: 25.192367\n",
      "I0710 09:57:16.533650 140295626643200 trainer.py:508] step:  6207, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:25.774784 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2853547 log_pplx:3.8476639 loss:60.270046 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.06702\n",
      "I0710 09:57:25.222675 140295626643200 summary_utils.py:349] Steps/second: 0.177500, Examples/second: 25.173361\n",
      "I0710 09:57:25.223513 140295626643200 trainer.py:508] step:  6208, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:258.42468 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2829152 log_pplx:2.8105531 loss:196.0361 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:524.07135\n",
      "I0710 09:57:29.003096 140295626643200 summary_utils.py:349] Steps/second: 0.177535, Examples/second: 25.180246\n",
      "I0710 09:57:29.004157 140295626643200 trainer.py:508] step:  6209, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:41.916565 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2858058 log_pplx:3.8455701 loss:96.475731 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.07568\n",
      "I0710 09:57:34.944065 140295626643200 summary_utils.py:349] Steps/second: 0.177529, Examples/second: 25.172860\n",
      "I0710 09:57:34.944813 140295626643200 trainer.py:508] step:  6210, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:93.717377 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2841965 log_pplx:3.5398962 loss:144.60477 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.08008\n",
      "I0710 09:57:41.347027 140295626643200 summary_utils.py:349] Steps/second: 0.177515, Examples/second: 25.164252\n",
      "I0710 09:57:41.347771 140295626643200 trainer.py:508] step:  6211, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:136.84094 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2841145 log_pplx:3.5794921 loss:142.73225 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.08514\n",
      "I0710 09:57:45.083846 140295626643200 summary_utils.py:349] Steps/second: 0.177550, Examples/second: 25.171244\n",
      "I0710 09:57:45.084668 140295626643200 trainer.py:508] step:  6212, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:36.577946 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2857881 log_pplx:3.8334115 loss:96.482178 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.09113\n",
      "I0710 09:57:51.235891 140295626643200 summary_utils.py:349] Steps/second: 0.177541, Examples/second: 25.163315\n",
      "I0710 09:57:51.236636 140295626643200 trainer.py:508] step:  6213, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:78.789886 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2842069 log_pplx:3.4966459 loss:142.18236 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.09692\n",
      "I0710 09:57:54.252066 140295626643200 summary_utils.py:349] Steps/second: 0.177590, Examples/second: 25.182387\n",
      "I0710 09:57:54.252825 140295626643200 trainer.py:508] step:  6214, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:40.920036 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2853971 log_pplx:3.8742204 loss:60.262287 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.10315\n",
      "I0710 09:58:00.490844 140295626643200 summary_utils.py:349] Steps/second: 0.177578, Examples/second: 25.174227\n",
      "I0710 09:58:00.491575 140295626643200 trainer.py:508] step:  6215, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:92.216209 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.28441 log_pplx:3.5845485 loss:148.62431 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.1087\n",
      "I0710 09:58:04.204990 140295626643200 summary_utils.py:349] Steps/second: 0.177614, Examples/second: 25.181262\n",
      "I0710 09:58:04.205781 140295626643200 trainer.py:508] step:  6216, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:74.065018 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2858901 log_pplx:3.9031394 loss:99.066551 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.11377\n",
      "I0710 09:58:12.927175 140295626643200 summary_utils.py:349] Steps/second: 0.177556, Examples/second: 25.162277\n",
      "I0710 09:58:12.927967 140295626643200 trainer.py:508] step:  6217, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:343.16965 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2835965 log_pplx:2.9991786 loss:219.23996 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:524.11816\n",
      "I0710 09:58:19.206017 140295626643200 summary_utils.py:349] Steps/second: 0.177544, Examples/second: 25.154039\n",
      "I0710 09:58:19.206842 140295626643200 trainer.py:508] step:  6218, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:102.59566 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2846166 log_pplx:3.6087391 loss:146.60503 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.1239\n",
      "I0710 09:58:25.495249 140295626643200 summary_utils.py:349] Steps/second: 0.177532, Examples/second: 25.145785\n",
      "I0710 09:58:25.496014 140295626643200 trainer.py:508] step:  6219, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:86.083618 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2843509 log_pplx:3.5113547 loss:141.90262 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.12921\n",
      "I0710 09:58:28.512896 140295626643200 summary_utils.py:349] Steps/second: 0.177581, Examples/second: 25.164791\n",
      "I0710 09:58:28.513682 140295626643200 trainer.py:508] step:  6220, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:50.858402 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2857901 log_pplx:3.9799418 loss:62.171043 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.13446\n",
      "I0710 09:58:30.665219 140295626643200 summary_utils.py:349] Steps/second: 0.177646, Examples/second: 25.213088\n",
      "I0710 09:58:30.665995 140295626643200 trainer.py:508] step:  6221, steps/sec: 0.18, examples/sec: 25.21 grad_norm/all/loss:18.106638 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2842864 log_pplx:3.8454256 loss:28.915798 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:524.13989\n",
      "I0710 09:58:34.445672 140295626643200 summary_utils.py:349] Steps/second: 0.177681, Examples/second: 25.219909\n",
      "I0710 09:58:34.446455 140295626643200 trainer.py:508] step:  6222, steps/sec: 0.18, examples/sec: 25.22 grad_norm/all/loss:65.023949 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2864718 log_pplx:3.9301198 loss:100.38999 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.1452\n",
      "I0710 09:58:40.586116 140295626643200 summary_utils.py:349] Steps/second: 0.177671, Examples/second: 25.212020\n",
      "I0710 09:58:40.586901 140295626643200 trainer.py:508] step:  6223, steps/sec: 0.18, examples/sec: 25.21 grad_norm/all/loss:115.87965 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2843817 log_pplx:3.6076596 loss:146.87686 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.15082\n",
      "I0710 09:58:46.834117 140295626643200 summary_utils.py:349] Steps/second: 0.177660, Examples/second: 25.203856\n",
      "I0710 09:58:46.834931 140295626643200 trainer.py:508] step:  6224, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:88.263695 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2848139 log_pplx:3.6485574 loss:146.30716 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.15613\n",
      "I0710 09:58:50.794905 140295626643200 summary_utils.py:349] Steps/second: 0.177691, Examples/second: 25.210190\n",
      "I0710 09:58:50.795660 140295626643200 trainer.py:508] step:  6225, steps/sec: 0.18, examples/sec: 25.21 grad_norm/all/loss:47.926743 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2863767 log_pplx:3.9576437 loss:98.347458 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.16095\n",
      "I0710 09:58:53.864576 140295626643200 summary_utils.py:349] Steps/second: 0.177739, Examples/second: 25.228986\n",
      "I0710 09:58:53.865334 140295626643200 trainer.py:508] step:  6226, steps/sec: 0.18, examples/sec: 25.23 grad_norm/all/loss:27.284346 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2856212 log_pplx:3.8230231 loss:59.182194 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.16595\n",
      "I0710 09:59:04.559877 140295626643200 summary_utils.py:349] Steps/second: 0.177644, Examples/second: 25.204827\n",
      "I0710 09:59:04.560685 140295626643200 trainer.py:508] step:  6227, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:183.71898 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2837393 log_pplx:2.9092507 loss:210.26611 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:524.17053\n",
      "I0710 09:59:10.859928 140295626643200 summary_utils.py:349] Steps/second: 0.177631, Examples/second: 25.196551\n",
      "I0710 09:59:10.860717 140295626643200 trainer.py:508] step:  6228, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:79.634644 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2844182 log_pplx:3.5570586 loss:142.37128 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.17554\n",
      "I0710 09:59:17.112719 140295626643200 summary_utils.py:349] Steps/second: 0.177620, Examples/second: 25.188410\n",
      "I0710 09:59:17.113656 140295626643200 trainer.py:508] step:  6229, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:86.008217 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2847972 log_pplx:3.5609248 loss:146.44302 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.18091\n",
      "I0710 09:59:20.907969 140295626643200 summary_utils.py:349] Steps/second: 0.177654, Examples/second: 25.195168\n",
      "I0710 09:59:20.908710 140295626643200 trainer.py:508] step:  6230, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:42.946625 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2863367 log_pplx:3.8864565 loss:96.772766 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.1864\n",
      "I0710 09:59:27.232821 140295626643200 summary_utils.py:349] Steps/second: 0.177641, Examples/second: 25.186846\n",
      "I0710 09:59:27.233654 140295626643200 trainer.py:508] step:  6231, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:73.9841 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2849004 log_pplx:3.5729358 loss:145.01651 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.19159\n",
      "I0710 09:59:30.325579 140295626643200 summary_utils.py:349] Steps/second: 0.177688, Examples/second: 25.205517\n",
      "I0710 09:59:30.326429 140295626643200 trainer.py:508] step:  6232, steps/sec: 0.18, examples/sec: 25.21 grad_norm/all/loss:65.159492 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2860733 log_pplx:3.9831634 loss:62.29916 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.1969\n",
      "I0710 09:59:36.778270 140295626643200 summary_utils.py:349] Steps/second: 0.177673, Examples/second: 25.196859\n",
      "I0710 09:59:36.779055 140295626643200 trainer.py:508] step:  6233, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:72.651222 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2847039 log_pplx:3.4804173 loss:140.95688 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.20142\n",
      "I0710 09:59:43.237946 140295626643200 summary_utils.py:349] Steps/second: 0.177657, Examples/second: 25.188195\n",
      "I0710 09:59:43.238726 140295626643200 trainer.py:508] step:  6234, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:76.8638 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2846774 log_pplx:3.5461783 loss:144.50676 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.20654\n",
      "I0710 09:59:47.145121 140295626643200 summary_utils.py:349] Steps/second: 0.177689, Examples/second: 25.194640\n",
      "I0710 09:59:47.145947 140295626643200 trainer.py:508] step:  6235, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:39.754833 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2867267 log_pplx:3.9841239 loss:101.44576 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.21167\n",
      "I0710 09:59:58.508425 140295626643200 summary_utils.py:349] Steps/second: 0.177583, Examples/second: 25.168896\n",
      "I0710 09:59:58.509231 140295626643200 trainer.py:508] step:  6236, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:192.71878 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2839556 log_pplx:2.8760026 loss:203.47716 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:524.21667\n",
      "I0710 10:00:05.064210 140295626643200 summary_utils.py:349] Steps/second: 0.177566, Examples/second: 25.160018\n",
      "I0710 10:00:05.065032 140295626643200 base_runner.py:111] step:  6237, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:77.364662 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2851847 log_pplx:3.5497115 loss:144.96135 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.22174\n",
      "I0710 10:00:08.112388 140295626643200 summary_utils.py:349] Steps/second: 0.177614, Examples/second: 25.178741\n",
      "I0710 10:00:08.113204 140295626643200 trainer.py:508] step:  6238, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:30.664417 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2862042 log_pplx:3.8823791 loss:59.433769 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.22736\n",
      "I0710 10:00:14.447361 140295626643200 summary_utils.py:349] Steps/second: 0.177601, Examples/second: 25.170444\n",
      "I0710 10:00:14.448155 140295626643200 trainer.py:508] step:  6239, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:99.951385 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2850351 log_pplx:3.7561922 loss:149.35559 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.23291\n",
      "I0710 10:00:18.299256 140295626643200 summary_utils.py:349] Steps/second: 0.177634, Examples/second: 25.177020\n",
      "I0710 10:00:18.300037 140295626643200 trainer.py:508] step:  6240, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:38.409351 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2864044 log_pplx:3.8827269 loss:97.092445 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.23871\n",
      "I0710 10:00:24.539168 140295626643200 summary_utils.py:349] Steps/second: 0.177622, Examples/second: 25.168983\n",
      "I0710 10:00:24.539962 140295626643200 trainer.py:508] step:  6241, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:75.791481 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.285073 log_pplx:3.5208926 loss:139.73543 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.24426\n",
      "I0710 10:00:27.578435 140295626643200 summary_utils.py:349] Steps/second: 0.177670, Examples/second: 25.187689\n",
      "I0710 10:00:27.579130 140295626643200 trainer.py:508] step:  6242, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:23.000542 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2859479 log_pplx:3.800277 loss:58.651932 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.24982\n",
      "I0710 10:00:33.828696 140295626643200 summary_utils.py:349] Steps/second: 0.177659, Examples/second: 25.179625\n",
      "I0710 10:00:33.829476 140295626643200 trainer.py:508] step:  6243, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:83.143593 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.284984 log_pplx:3.5838549 loss:140.62151 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.25507\n",
      "I0710 10:00:37.655147 140295626643200 summary_utils.py:349] Steps/second: 0.177692, Examples/second: 25.186251\n",
      "I0710 10:00:37.655968 140295626643200 trainer.py:508] step:  6244, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:63.315536 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2870957 log_pplx:3.9669096 loss:99.594223 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.26001\n",
      "I0710 10:00:39.831798 140295626643200 summary_utils.py:349] Steps/second: 0.177756, Examples/second: 25.233831\n",
      "I0710 10:00:39.832565 140295626643200 trainer.py:508] step:  6245, steps/sec: 0.18, examples/sec: 25.23 grad_norm/all/loss:23.423912 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2850039 log_pplx:3.8522692 loss:28.628681 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:524.26434\n",
      "I0710 10:00:50.421662 140295626643200 summary_utils.py:349] Steps/second: 0.177664, Examples/second: 25.210209\n",
      "I0710 10:00:50.422653 140295626643200 trainer.py:508] step:  6246, steps/sec: 0.18, examples/sec: 25.21 grad_norm/all/loss:217.06108 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2845217 log_pplx:2.9350965 loss:217.71078 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:524.26831\n",
      "I0710 10:00:56.660750 140295626643200 summary_utils.py:349] Steps/second: 0.177653, Examples/second: 25.202180\n",
      "I0710 10:00:56.661504 140295626643200 trainer.py:508] step:  6247, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:76.7202 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2854614 log_pplx:3.5305667 loss:146.38612 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.27344\n",
      "I0710 10:01:00.251511 140295635035904 trainer.py:345] Write summary @6247\n",
      "2020-07-10 10:01:02.512852: I lingvo/core/ops/record_batcher.cc:394] 9606 total seconds passed. Total records yielded: 3205. Total records skipped: 0\n",
      "I0710 10:01:06.733073 140295626643200 summary_utils.py:349] Steps/second: 0.177571, Examples/second: 25.184138\n",
      "I0710 10:01:06.734310 140295626643200 trainer.py:508] step:  6248, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:73.301132 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2852829 log_pplx:3.4659898 loss:139.02951 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.27863\n",
      "I0710 10:01:11.827307 140295626643200 summary_utils.py:349] Steps/second: 0.177581, Examples/second: 25.187427\n",
      "I0710 10:01:11.828321 140295626643200 trainer.py:508] step:  6249, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:44.066429 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2870263 log_pplx:3.9250927 loss:100.55598 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.28394\n",
      "I0710 10:01:15.951275 140295626643200 summary_utils.py:349] Steps/second: 0.177609, Examples/second: 25.203200\n",
      "I0710 10:01:15.952248 140295626643200 trainer.py:508] step:  6250, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:35.61414 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2863853 log_pplx:3.8919568 loss:60.751015 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.28967\n",
      "I0710 10:01:25.235988 140295626643200 summary_utils.py:349] Steps/second: 0.177541, Examples/second: 25.187248\n",
      "I0710 10:01:25.237214 140295626643200 trainer.py:508] step:  6251, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:128.35127 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2854198 log_pplx:3.5582778 loss:146.42313 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.29547\n",
      "I0710 10:01:34.494400 140295626643200 summary_utils.py:349] Steps/second: 0.177475, Examples/second: 25.171395\n",
      "I0710 10:01:34.495929 140295626643200 trainer.py:508] step:  6252, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:77.997078 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2852762 log_pplx:3.5476875 loss:143.37094 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.30078\n",
      "I0710 10:01:40.314486 140295626643200 summary_utils.py:349] Steps/second: 0.177471, Examples/second: 25.172791\n",
      "I0710 10:01:40.315809 140295626643200 trainer.py:508] step:  6253, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:37.908482 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.286976 log_pplx:3.8761895 loss:97.146996 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.30627\n",
      "I0710 10:01:50.500005 140295626643200 summary_utils.py:349] Steps/second: 0.177388, Examples/second: 25.154567\n",
      "I0710 10:01:50.501471 140295626643200 trainer.py:508] step:  6254, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:69.516647 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2853715 log_pplx:3.4461029 loss:140.041 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.31165\n",
      "I0710 10:01:53.094895 140295635035904 trainer.py:354] Write summary done: step 6247\n",
      "I0710 10:01:54.642083 140295626643200 summary_utils.py:349] Steps/second: 0.177415, Examples/second: 25.170244\n",
      "I0710 10:01:54.642799 140295626643200 trainer.py:508] step:  6255, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:24.155638 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2865509 log_pplx:3.8095102 loss:59.50872 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.3172\n",
      "I0710 10:02:00.846313 140295626643200 summary_utils.py:349] Steps/second: 0.177405, Examples/second: 25.162383\n",
      "I0710 10:02:00.847182 140295626643200 trainer.py:508] step:  6256, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:82.131966 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2854902 log_pplx:3.5125704 loss:142.03957 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.32251\n",
      "I0710 10:02:04.695682 140295626643200 summary_utils.py:349] Steps/second: 0.177438, Examples/second: 25.168897\n",
      "I0710 10:02:04.696571 140295626643200 trainer.py:508] step:  6257, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:37.687145 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2870437 log_pplx:3.9143126 loss:96.536736 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.32794\n",
      "I0710 10:02:13.924338 140295626643200 summary_utils.py:349] Steps/second: 0.177372, Examples/second: 25.149081\n",
      "I0710 10:02:13.925098 140295626643200 trainer.py:508] step:  6258, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:194.45068 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2845467 log_pplx:2.8424611 loss:207.28647 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:524.33319\n",
      "I0710 10:02:20.148371 140295626643200 summary_utils.py:349] Steps/second: 0.177361, Examples/second: 25.141198\n",
      "I0710 10:02:20.149199 140295626643200 trainer.py:508] step:  6259, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:75.348587 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2853104 log_pplx:3.5291898 loss:139.57945 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.33746\n",
      "I0710 10:02:26.543898 140295626643200 summary_utils.py:349] Steps/second: 0.177347, Examples/second: 25.132883\n",
      "I0710 10:02:26.544759 140295626643200 trainer.py:508] step:  6260, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:77.533951 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2859247 log_pplx:3.6240885 loss:149.40305 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.34222\n",
      "I0710 10:02:29.558783 140295626643200 summary_utils.py:349] Steps/second: 0.177395, Examples/second: 25.151427\n",
      "I0710 10:02:29.559559 140295626643200 trainer.py:508] step:  6261, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:51.617393 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2866299 log_pplx:3.8876307 loss:60.04567 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.34735\n",
      "I0710 10:02:33.141059 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 10:02:33.608249 140295626643200 summary_utils.py:349] Steps/second: 0.177424, Examples/second: 25.157408\n",
      "I0710 10:02:33.609064 140295626643200 trainer.py:508] step:  6262, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:45.237774 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2872277 log_pplx:3.9301956 loss:98.254883 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.35199\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 10:02:38.412351 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 10:02:38.926529 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00006261\n",
      "I0710 10:02:39.916883 140295626643200 summary_utils.py:349] Steps/second: 0.177412, Examples/second: 25.149318\n",
      "I0710 10:02:39.917687 140295626643200 trainer.py:508] step:  6263, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:120.19116 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2855296 log_pplx:3.5237594 loss:144.69437 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.35626\n",
      "I0710 10:02:46.253952 140295626643200 summary_utils.py:349] Steps/second: 0.177399, Examples/second: 25.141165\n",
      "I0710 10:02:46.254743 140295626643200 trainer.py:508] step:  6264, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:96.143456 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2854162 log_pplx:3.6088047 loss:144.98373 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.36139\n",
      "I0710 10:02:50.101302 140295626643200 summary_utils.py:349] Steps/second: 0.177432, Examples/second: 25.147662\n",
      "I0710 10:02:50.102153 140295626643200 trainer.py:508] step:  6265, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:42.796993 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2869945 log_pplx:3.8666472 loss:96.013687 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.36713\n",
      "I0710 10:02:55.940174 140295626643200 summary_utils.py:349] Steps/second: 0.177428, Examples/second: 25.140803\n",
      "I0710 10:02:55.941084 140295626643200 trainer.py:508] step:  6266, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:81.902473 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2853087 log_pplx:3.4498756 loss:135.49385 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.3725\n",
      "I0710 10:02:59.016489 140295626643200 summary_utils.py:349] Steps/second: 0.177475, Examples/second: 25.159131\n",
      "I0710 10:02:59.017589 140295626643200 trainer.py:508] step:  6267, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:24.769032 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2867573 log_pplx:3.851068 loss:59.571205 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.37762\n",
      "I0710 10:03:05.256498 140295626643200 summary_utils.py:349] Steps/second: 0.177464, Examples/second: 25.151238\n",
      "I0710 10:03:05.257277 140295626643200 trainer.py:508] step:  6268, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:86.029198 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2858889 log_pplx:3.5125899 loss:143.48929 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.38263\n",
      "I0710 10:03:16.234969 140295626643200 summary_utils.py:349] Steps/second: 0.177367, Examples/second: 25.127058\n",
      "I0710 10:03:16.235787 140295626643200 trainer.py:508] step:  6269, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:279.45837 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2851731 log_pplx:3.0330102 loss:230.05383 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:524.38763\n",
      "I0710 10:03:20.065575 140295626643200 summary_utils.py:349] Steps/second: 0.177399, Examples/second: 25.133583\n",
      "I0710 10:03:20.066371 140295626643200 trainer.py:508] step:  6270, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:41.371277 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2870334 log_pplx:3.8004489 loss:95.177505 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.3938\n",
      "I0710 10:03:26.432536 140295626643200 summary_utils.py:349] Steps/second: 0.177386, Examples/second: 25.125397\n",
      "I0710 10:03:26.433429 140295626643200 trainer.py:508] step:  6271, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:130.49472 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2856477 log_pplx:3.5916145 loss:144.65225 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.3999\n",
      "I0710 10:03:28.641559 140295626643200 summary_utils.py:349] Steps/second: 0.177448, Examples/second: 25.172086\n",
      "I0710 10:03:28.642377 140295626643200 trainer.py:508] step:  6272, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:14.290131 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2856467 log_pplx:3.8666878 loss:28.169426 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:524.40552\n",
      "I0710 10:03:31.644855 140295626643200 summary_utils.py:349] Steps/second: 0.177496, Examples/second: 25.190530\n",
      "I0710 10:03:33.095072 140295626643200 trainer.py:508] step:  6273, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:32.855873 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2870688 log_pplx:3.8634703 loss:59.521587 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.41089\n",
      "I0710 10:03:39.608825 140295626643200 summary_utils.py:349] Steps/second: 0.177454, Examples/second: 25.178208\n",
      "I0710 10:03:39.609616 140295626643200 trainer.py:508] step:  6274, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:132.70282 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.285774 log_pplx:3.5864992 loss:145.20836 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.41614\n",
      "I0710 10:03:43.464248 140295626643200 summary_utils.py:349] Steps/second: 0.177486, Examples/second: 25.184634\n",
      "I0710 10:03:43.465047 140295626643200 trainer.py:508] step:  6275, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:52.879604 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2874755 log_pplx:3.9286017 loss:97.895844 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.42114\n",
      "I0710 10:03:49.917521 140295626643200 summary_utils.py:349] Steps/second: 0.177471, Examples/second: 25.176213\n",
      "I0710 10:03:49.918348 140295626643200 trainer.py:508] step:  6276, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:78.009644 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2859677 log_pplx:3.4953353 loss:143.74565 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.42633\n",
      "I0710 10:03:56.125357 140295626643200 summary_utils.py:349] Steps/second: 0.177461, Examples/second: 25.168432\n",
      "I0710 10:03:56.126259 140295626643200 trainer.py:508] step:  6277, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:89.006294 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2859962 log_pplx:3.5433724 loss:145.54402 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.43207\n",
      "I0710 10:04:07.567886 140295626643200 summary_utils.py:349] Steps/second: 0.177356, Examples/second: 25.143169\n",
      "I0710 10:04:07.568906 140295626643200 trainer.py:508] step:  6278, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:246.18643 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2849575 log_pplx:2.8978436 loss:216.54134 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:524.43842\n",
      "I0710 10:04:10.688422 140295626643200 summary_utils.py:349] Steps/second: 0.177401, Examples/second: 25.161249\n",
      "I0710 10:04:10.689222 140295626643200 trainer.py:508] step:  6279, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:46.459103 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.28697 log_pplx:3.9246004 loss:60.248749 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.44489\n",
      "I0710 10:04:14.566489 140295626643200 summary_utils.py:349] Steps/second: 0.177433, Examples/second: 25.167604\n",
      "I0710 10:04:14.567265 140295626643200 trainer.py:508] step:  6280, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:49.163006 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2878211 log_pplx:3.9867702 loss:100.56627 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.45087\n",
      "I0710 10:04:20.737857 140295626643200 summary_utils.py:349] Steps/second: 0.177423, Examples/second: 25.159941\n",
      "I0710 10:04:20.738666 140295626643200 trainer.py:508] step:  6281, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:119.21591 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2860167 log_pplx:3.6638098 loss:145.86542 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.4566\n",
      "I0710 10:04:27.398047 140295626643200 summary_utils.py:349] Steps/second: 0.177405, Examples/second: 25.151039\n",
      "I0710 10:04:27.398907 140295626643200 trainer.py:508] step:  6282, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:124.91025 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2862711 log_pplx:3.5403125 loss:148.33908 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.46228\n",
      "I0710 10:04:33.790356 140295626643200 summary_utils.py:349] Steps/second: 0.177391, Examples/second: 25.142832\n",
      "I0710 10:04:33.791209 140295626643200 trainer.py:508] step:  6283, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:81.168808 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2858975 log_pplx:3.4505491 loss:136.1673 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.46802\n",
      "I0710 10:04:37.774008 140295626643200 summary_utils.py:349] Steps/second: 0.177421, Examples/second: 25.148909\n",
      "I0710 10:04:37.774798 140295626643200 trainer.py:508] step:  6284, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:64.342102 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2878178 log_pplx:3.9716878 loss:99.019135 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.47382\n",
      "I0710 10:04:40.887567 140295626643200 summary_utils.py:349] Steps/second: 0.177467, Examples/second: 25.166950\n",
      "I0710 10:04:40.888394 140295626643200 trainer.py:508] step:  6285, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:41.494862 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2874889 log_pplx:3.8864431 loss:61.985729 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.47906\n",
      "I0710 10:04:47.358200 140295626643200 summary_utils.py:349] Steps/second: 0.177452, Examples/second: 25.158545\n",
      "I0710 10:04:47.359015 140295626643200 trainer.py:508] step:  6286, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:93.076309 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2863493 log_pplx:3.5445673 loss:144.88419 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.4837\n",
      "I0710 10:04:53.589864 140295626643200 summary_utils.py:349] Steps/second: 0.177441, Examples/second: 25.150760\n",
      "I0710 10:04:53.590637 140295626643200 trainer.py:508] step:  6287, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:95.212494 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2863051 log_pplx:3.540451 loss:144.89294 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.48889\n",
      "I0710 10:05:05.082681 140295626643200 summary_utils.py:349] Steps/second: 0.177336, Examples/second: 25.125536\n",
      "I0710 10:05:05.083527 140295626643200 trainer.py:508] step:  6288, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:220.27681 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2855276 log_pplx:2.9248848 loss:221.19441 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:524.49445\n",
      "I0710 10:05:09.169738 140295626643200 summary_utils.py:349] Steps/second: 0.177363, Examples/second: 25.131338\n",
      "I0710 10:05:09.170628 140295626643200 trainer.py:508] step:  6289, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:45.722065 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2876718 log_pplx:3.9109592 loss:97.505112 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.50067\n",
      "I0710 10:05:15.749505 140295626643200 summary_utils.py:349] Steps/second: 0.177346, Examples/second: 25.122703\n",
      "I0710 10:05:15.750318 140295626643200 trainer.py:508] step:  6290, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:80.056938 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2862366 log_pplx:3.5282109 loss:143.59818 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.50702\n",
      "I0710 10:05:18.886652 140295626643200 summary_utils.py:349] Steps/second: 0.177391, Examples/second: 25.140622\n",
      "I0710 10:05:18.887460 140295626643200 trainer.py:508] step:  6291, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:58.514835 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.288046 log_pplx:4.0457511 loss:63.515133 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.51294\n",
      "I0710 10:05:25.391015 140295626643200 summary_utils.py:349] Steps/second: 0.177376, Examples/second: 25.132181\n",
      "I0710 10:05:25.391841 140295626643200 trainer.py:508] step:  6292, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:105.44025 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2866066 log_pplx:3.6269479 loss:149.06755 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.51898\n",
      "I0710 10:05:29.250230 140295626643200 summary_utils.py:349] Steps/second: 0.177408, Examples/second: 25.138547\n",
      "I0710 10:05:29.251006 140295626643200 trainer.py:508] step:  6293, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:37.929543 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2879685 log_pplx:3.8436635 loss:97.965378 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.52478\n",
      "I0710 10:05:35.590523 140295626643200 summary_utils.py:349] Steps/second: 0.177395, Examples/second: 25.130531\n",
      "I0710 10:05:35.591252 140295626643200 trainer.py:508] step:  6294, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:76.864281 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.285998 log_pplx:3.511332 loss:138.56596 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.53046\n",
      "I0710 10:05:38.691884 140295626643200 summary_utils.py:349] Steps/second: 0.177440, Examples/second: 25.148504\n",
      "I0710 10:05:38.692718 140295626643200 trainer.py:508] step:  6295, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:22.741833 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2881671 log_pplx:3.8867261 loss:61.094482 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.53583\n",
      "I0710 10:05:45.056899 140295626643200 summary_utils.py:349] Steps/second: 0.177427, Examples/second: 25.140427\n",
      "I0710 10:05:45.057862 140295626643200 trainer.py:508] step:  6296, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:74.245209 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2866322 log_pplx:3.5982506 loss:143.21037 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.54095\n",
      "I0710 10:05:47.257281 140295626643200 summary_utils.py:349] Steps/second: 0.177489, Examples/second: 25.186482\n",
      "I0710 10:05:47.258368 140295626643200 trainer.py:508] step:  6297, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:24.76951 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2865211 log_pplx:3.9416168 loss:28.707598 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:524.54645\n",
      "I0710 10:05:51.110284 140295626643200 summary_utils.py:349] Steps/second: 0.177521, Examples/second: 25.192828\n",
      "I0710 10:05:51.111082 140295626643200 trainer.py:508] step:  6298, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:43.47953 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2884591 log_pplx:3.9589241 loss:100.21027 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.55133\n",
      "I0710 10:05:57.608514 140295626643200 summary_utils.py:349] Steps/second: 0.177505, Examples/second: 25.184395\n",
      "I0710 10:05:57.609358 140295626643200 trainer.py:508] step:  6299, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:85.856323 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2863544 log_pplx:3.5274849 loss:136.77823 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.55627\n",
      "I0710 10:06:03.924352 140295626643200 summary_utils.py:349] Steps/second: 0.177493, Examples/second: 25.176436\n",
      "I0710 10:06:03.925161 140295626643200 trainer.py:508] step:  6300, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:75.374649 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2866898 log_pplx:3.601239 loss:143.5994 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.56177\n",
      "I0710 10:06:07.791793 140295626643200 summary_utils.py:349] Steps/second: 0.177525, Examples/second: 25.182739\n",
      "I0710 10:06:07.792532 140295626643200 trainer.py:508] step:  6301, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:48.987835 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.288031 log_pplx:3.8453152 loss:96.733711 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.56738\n",
      "I0710 10:06:18.097172 140295626643200 summary_utils.py:349] Steps/second: 0.177441, Examples/second: 25.160670\n",
      "I0710 10:06:18.097939 140295626643200 trainer.py:508] step:  6302, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:195.60065 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2858185 log_pplx:2.961082 loss:216.60315 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:524.57239\n",
      "I0710 10:06:21.187597 140295626643200 summary_utils.py:349] Steps/second: 0.177487, Examples/second: 25.178584\n",
      "I0710 10:06:21.188380 140295626643200 trainer.py:508] step:  6303, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:40.877007 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2878667 log_pplx:3.8548908 loss:61.467438 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.57727\n",
      "I0710 10:06:27.664624 140295626643200 summary_utils.py:349] Steps/second: 0.177472, Examples/second: 25.170239\n",
      "I0710 10:06:27.665972 140295626643200 trainer.py:508] step:  6304, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:87.778099 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2866989 log_pplx:3.5381474 loss:142.45465 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.58185\n",
      "I0710 10:06:33.940471 140295626643200 summary_utils.py:349] Steps/second: 0.177460, Examples/second: 25.162414\n",
      "I0710 10:06:33.941298 140295626643200 trainer.py:508] step:  6305, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:104.80062 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2872169 log_pplx:3.6316369 loss:150.16818 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.58655\n",
      "I0710 10:06:37.783194 140295626643200 summary_utils.py:349] Steps/second: 0.177492, Examples/second: 25.168765\n",
      "I0710 10:06:37.784174 140295626643200 trainer.py:508] step:  6306, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:45.843105 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2885941 log_pplx:3.9468303 loss:99.780807 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.59137\n",
      "I0710 10:06:43.799111 140295626643200 summary_utils.py:349] Steps/second: 0.177485, Examples/second: 25.161605\n",
      "I0710 10:06:43.799904 140295626643200 trainer.py:508] step:  6307, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:77.504501 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2869786 log_pplx:3.5758989 loss:144.37692 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.59619\n",
      "I0710 10:06:46.884086 140295626643200 summary_utils.py:349] Steps/second: 0.177531, Examples/second: 25.179486\n",
      "I0710 10:06:46.884850 140295626643200 trainer.py:508] step:  6308, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:24.936771 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2878889 log_pplx:3.8097024 loss:59.109917 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.60132\n",
      "I0710 10:06:53.334852 140295626643200 summary_utils.py:349] Steps/second: 0.177516, Examples/second: 25.171228\n",
      "I0710 10:06:53.335618 140295626643200 trainer.py:508] step:  6309, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:80.669243 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2869358 log_pplx:3.5299253 loss:145.47705 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.60626\n",
      "I0710 10:07:02.434973 140295626643200 summary_utils.py:349] Steps/second: 0.177455, Examples/second: 25.152306\n",
      "I0710 10:07:02.435946 140295626643200 trainer.py:508] step:  6310, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:172.60255 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2854564 log_pplx:2.8042192 loss:190.61679 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:524.61108\n",
      "I0710 10:07:06.303558 140295626643200 summary_utils.py:349] Steps/second: 0.177486, Examples/second: 25.158578\n",
      "I0710 10:07:06.304401 140295626643200 trainer.py:508] step:  6311, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:47.624546 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2886339 log_pplx:3.8899736 loss:97.054832 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.6167\n",
      "I0710 10:07:12.745594 140295626643200 summary_utils.py:349] Steps/second: 0.177472, Examples/second: 25.150372\n",
      "I0710 10:07:12.746366 140295626643200 trainer.py:508] step:  6312, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:82.287109 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2867846 log_pplx:3.5048952 loss:142.60542 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.62195\n",
      "I0710 10:07:19.116038 140295626643200 summary_utils.py:349] Steps/second: 0.177459, Examples/second: 25.142357\n",
      "I0710 10:07:19.116784 140295626643200 trainer.py:508] step:  6313, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:80.911697 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2866766 log_pplx:3.483916 loss:139.83569 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.62708\n",
      "I0710 10:07:25.359811 140295626643200 summary_utils.py:349] Steps/second: 0.177448, Examples/second: 25.134669\n",
      "I0710 10:07:25.360578 140295626643200 trainer.py:508] step:  6314, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:74.877487 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2871035 log_pplx:3.5339634 loss:140.43088 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.6322\n",
      "I0710 10:07:28.410132 140295626643200 summary_utils.py:349] Steps/second: 0.177494, Examples/second: 25.152572\n",
      "I0710 10:07:28.410915 140295626643200 trainer.py:508] step:  6315, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:28.525192 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.288269 log_pplx:3.8507767 loss:59.942749 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.63782\n",
      "I0710 10:07:32.208661 140295626643200 summary_utils.py:349] Steps/second: 0.177526, Examples/second: 25.159004\n",
      "I0710 10:07:32.209458 140295626643200 trainer.py:508] step:  6316, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:50.477032 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2886281 log_pplx:3.8854856 loss:98.472778 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.64337\n",
      "I0710 10:07:38.613283 140295626643200 summary_utils.py:349] Steps/second: 0.177512, Examples/second: 25.150913\n",
      "I0710 10:07:38.614095 140295626643200 trainer.py:508] step:  6317, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:78.563255 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2870729 log_pplx:3.5900118 loss:147.729 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.64911\n",
      "I0710 10:07:44.821602 140295626643200 summary_utils.py:349] Steps/second: 0.177502, Examples/second: 25.143324\n",
      "I0710 10:07:44.822476 140295626643200 trainer.py:508] step:  6318, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:75.508141 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2872373 log_pplx:3.4252434 loss:140.94878 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.65503\n",
      "I0710 10:07:47.025511 140295626643200 summary_utils.py:349] Steps/second: 0.177563, Examples/second: 25.188819\n",
      "I0710 10:07:47.026358 140295626643200 trainer.py:508] step:  6319, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:11.576428 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2870831 log_pplx:3.8169663 loss:28.642157 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:524.66113\n",
      "I0710 10:07:50.896130 140295626643200 summary_utils.py:349] Steps/second: 0.177594, Examples/second: 25.195045\n",
      "I0710 10:07:50.896943 140295626643200 trainer.py:508] step:  6320, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:70.335983 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2889377 log_pplx:3.9235985 loss:99.904617 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.66675\n",
      "I0710 10:08:02.223239 140295626643200 summary_utils.py:349] Steps/second: 0.177493, Examples/second: 25.170631\n",
      "I0710 10:08:02.224123 140295626643200 trainer.py:508] step:  6321, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:222.36469 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.286328 log_pplx:2.8185062 loss:201.80505 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:524.67157\n",
      "I0710 10:08:05.255576 140295626643200 summary_utils.py:349] Steps/second: 0.177539, Examples/second: 25.188501\n",
      "I0710 10:08:05.256374 140295626643200 trainer.py:508] step:  6322, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:41.702179 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2883811 log_pplx:3.8304999 loss:59.776749 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.67706\n",
      "I0710 10:08:11.746695 140295626643200 summary_utils.py:349] Steps/second: 0.177524, Examples/second: 25.180202\n",
      "I0710 10:08:11.747705 140295626643200 trainer.py:508] step:  6323, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:82.544304 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2871624 log_pplx:3.5804336 loss:145.09708 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.68201\n",
      "I0710 10:08:18.235563 140295626643200 summary_utils.py:349] Steps/second: 0.177509, Examples/second: 25.171918\n",
      "I0710 10:08:18.236347 140295626643200 trainer.py:508] step:  6324, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:78.864952 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2870206 log_pplx:3.5331903 loss:142.91753 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.68713\n",
      "I0710 10:08:22.129117 140295626643200 summary_utils.py:349] Steps/second: 0.177540, Examples/second: 25.178074\n",
      "I0710 10:08:22.130064 140295626643200 trainer.py:508] step:  6325, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:42.582138 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2888712 log_pplx:3.8427644 loss:97.101852 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.69226\n",
      "I0710 10:08:28.586050 140295626643200 summary_utils.py:349] Steps/second: 0.177525, Examples/second: 25.169880\n",
      "I0710 10:08:28.586851 140295626643200 trainer.py:508] step:  6326, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:79.432243 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2875048 log_pplx:3.577347 loss:148.59407 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.69739\n",
      "I0710 10:08:31.641383 140295626643200 summary_utils.py:349] Steps/second: 0.177571, Examples/second: 25.187646\n",
      "I0710 10:08:31.642424 140295626643200 trainer.py:508] step:  6327, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:23.577404 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2888355 log_pplx:3.9118032 loss:61.717865 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.70282\n",
      "I0710 10:08:38.107348 140295626643200 summary_utils.py:349] Steps/second: 0.177556, Examples/second: 25.179432\n",
      "I0710 10:08:38.108106 140295626643200 trainer.py:508] step:  6328, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:81.449532 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2874268 log_pplx:3.5170507 loss:140.02257 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.70801\n",
      "I0710 10:08:46.966329 140295626643200 summary_utils.py:349] Steps/second: 0.177499, Examples/second: 25.161299\n",
      "I0710 10:08:46.967130 140295626643200 trainer.py:508] step:  6329, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:188.83498 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2860184 log_pplx:2.8142769 loss:193.0594 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:524.7132\n",
      "I0710 10:08:50.773753 140295626643200 summary_utils.py:349] Steps/second: 0.177531, Examples/second: 25.167656\n",
      "I0710 10:08:50.774771 140295626643200 trainer.py:508] step:  6330, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:42.870819 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.289036 log_pplx:3.9124055 loss:98.788239 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.71924\n",
      "I0710 10:08:57.169038 140295626643200 summary_utils.py:349] Steps/second: 0.177518, Examples/second: 25.159645\n",
      "I0710 10:08:57.169850 140295626643200 trainer.py:508] step:  6331, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:74.439072 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2873325 log_pplx:3.5198784 loss:143.43503 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.72516\n",
      "I0710 10:09:03.192360 140295626643200 summary_utils.py:349] Steps/second: 0.177511, Examples/second: 25.152570\n",
      "I0710 10:09:03.193154 140295626643200 trainer.py:508] step:  6332, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:82.130539 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2874422 log_pplx:3.4343476 loss:139.26279 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.73096\n",
      "I0710 10:09:09.607153 140295626643200 summary_utils.py:349] Steps/second: 0.177497, Examples/second: 25.144530\n",
      "I0710 10:09:09.607959 140295626643200 trainer.py:508] step:  6333, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:72.325134 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2872373 log_pplx:3.4855647 loss:139.1176 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.73688\n",
      "I0710 10:09:13.468273 140295626643200 summary_utils.py:349] Steps/second: 0.177528, Examples/second: 25.150746\n",
      "I0710 10:09:13.469106 140295626643200 trainer.py:508] step:  6334, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:40.669037 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2890519 log_pplx:3.8412611 loss:96.919823 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.74286\n",
      "I0710 10:09:16.555693 140295626643200 summary_utils.py:349] Steps/second: 0.177573, Examples/second: 25.168360\n",
      "I0710 10:09:16.556610 140295626643200 trainer.py:508] step:  6335, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:23.080177 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2886565 log_pplx:3.799386 loss:59.55835 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.7486\n",
      "I0710 10:09:22.952670 140295626643200 summary_utils.py:349] Steps/second: 0.177560, Examples/second: 25.160365\n",
      "I0710 10:09:22.953518 140295626643200 trainer.py:508] step:  6336, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:72.505882 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2872784 log_pplx:3.4631212 loss:141.12219 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.75409\n",
      "I0710 10:09:29.474318 140295626643200 summary_utils.py:349] Steps/second: 0.177544, Examples/second: 25.152071\n",
      "I0710 10:09:29.475102 140295626643200 base_runner.py:111] step:  6337, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:73.894432 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2876016 log_pplx:3.4602909 loss:142.95328 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.75995\n",
      "I0710 10:09:39.351620 140295626643200 summary_utils.py:349] Steps/second: 0.177470, Examples/second: 25.131532\n",
      "I0710 10:09:39.352426 140295626643200 trainer.py:508] step:  6338, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:211.3374 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2865987 log_pplx:2.9580657 loss:211.50168 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:524.76605\n",
      "I0710 10:09:43.258202 140295626643200 summary_utils.py:349] Steps/second: 0.177500, Examples/second: 25.137621\n",
      "I0710 10:09:43.258984 140295626643200 trainer.py:508] step:  6339, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:56.711048 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2889977 log_pplx:3.9287179 loss:98.954575 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.77307\n",
      "I0710 10:09:46.306851 140295626643200 summary_utils.py:349] Steps/second: 0.177545, Examples/second: 25.155284\n",
      "I0710 10:09:46.307751 140295626643200 trainer.py:508] step:  6340, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:24.840267 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2888923 log_pplx:3.8576601 loss:59.477283 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.77966\n",
      "I0710 10:09:52.875458 140295626643200 summary_utils.py:349] Steps/second: 0.177529, Examples/second: 25.146896\n",
      "I0710 10:09:52.876269 140295626643200 trainer.py:508] step:  6341, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:137.03633 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2878026 log_pplx:3.5383976 loss:149.89536 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.78595\n",
      "I0710 10:09:55.103029 140295626643200 summary_utils.py:349] Steps/second: 0.177588, Examples/second: 25.191759\n",
      "I0710 10:09:55.103946 140295626643200 trainer.py:508] step:  6342, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:10.421144 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2873087 log_pplx:3.7719057 loss:27.294746 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:524.79144\n",
      "I0710 10:10:01.575151 140295626643200 summary_utils.py:349] Steps/second: 0.177574, Examples/second: 25.183594\n",
      "I0710 10:10:01.575911 140295626643200 trainer.py:508] step:  6343, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:90.019875 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2877604 log_pplx:3.5522933 loss:144.93356 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.79663\n",
      "I0710 10:10:05.427568 140295626643200 summary_utils.py:349] Steps/second: 0.177605, Examples/second: 25.189784\n",
      "I0710 10:10:05.428386 140295626643200 trainer.py:508] step:  6344, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:88.620392 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2896158 log_pplx:3.9367754 loss:99.625023 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.80164\n",
      "I0710 10:10:11.861152 140295626643200 summary_utils.py:349] Steps/second: 0.177591, Examples/second: 25.181724\n",
      "I0710 10:10:11.862091 140295626643200 trainer.py:508] step:  6345, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:74.909225 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2878506 log_pplx:3.472846 loss:143.90604 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.80737\n",
      "I0710 10:10:15.097227 140295626643200 summary_utils.py:349] Steps/second: 0.177632, Examples/second: 25.198859\n",
      "I0710 10:10:15.098074 140295626643200 trainer.py:508] step:  6346, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:24.47106 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2890546 log_pplx:3.8346674 loss:59.497265 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.81311\n",
      "I0710 10:10:21.963688 140295626643200 summary_utils.py:349] Steps/second: 0.177611, Examples/second: 25.189731\n",
      "I0710 10:10:21.964502 140295626643200 trainer.py:508] step:  6347, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:89.571503 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2876655 log_pplx:3.5325682 loss:139.75723 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.81842\n",
      "I0710 10:10:23.548827 140295635035904 trainer.py:345] Write summary @6347\n",
      "I0710 10:10:29.796244 140295626643200 summary_utils.py:349] Steps/second: 0.177572, Examples/second: 25.186073\n",
      "I0710 10:10:29.797266 140295626643200 trainer.py:508] step:  6348, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:49.327259 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2897273 log_pplx:3.9246292 loss:99.047836 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.82343\n",
      "I0710 10:10:44.678550 140295626643200 summary_utils.py:349] Steps/second: 0.177412, Examples/second: 25.153291\n",
      "I0710 10:10:44.679689 140295626643200 trainer.py:508] step:  6349, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:215.00453 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2869102 log_pplx:2.8423433 loss:209.62285 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:524.82794\n",
      "I0710 10:10:53.755336 140295626643200 summary_utils.py:349] Steps/second: 0.177352, Examples/second: 25.138783\n",
      "I0710 10:10:53.756263 140295626643200 trainer.py:508] step:  6350, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:100.01157 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.288022 log_pplx:3.5431349 loss:146.11003 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.83258\n",
      "I0710 10:11:03.127191 140295626643200 summary_utils.py:349] Steps/second: 0.177287, Examples/second: 25.123576\n",
      "I0710 10:11:03.128912 140295626643200 trainer.py:508] step:  6351, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:98.436562 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2881683 log_pplx:3.4341781 loss:141.01593 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.83771\n",
      "I0710 10:11:07.612992 140295626643200 summary_utils.py:349] Steps/second: 0.177307, Examples/second: 25.137575\n",
      "I0710 10:11:07.614156 140295626643200 trainer.py:508] step:  6352, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:29.218019 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2896324 log_pplx:3.8419163 loss:60.495167 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.84344\n",
      "I0710 10:11:13.470406 140295626643200 summary_utils.py:349] Steps/second: 0.177303, Examples/second: 25.138821\n",
      "I0710 10:11:13.472268 140295626643200 trainer.py:508] step:  6353, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:44.452888 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2899823 log_pplx:3.8524394 loss:97.972366 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.84882\n",
      "I0710 10:11:23.916643 140295626643200 summary_utils.py:349] Steps/second: 0.177220, Examples/second: 25.121010\n",
      "I0710 10:11:23.917868 140295626643200 trainer.py:508] step:  6354, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:91.757797 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2879761 log_pplx:3.5389802 loss:139.3031 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.85376\n",
      "I0710 10:11:28.366403 140295635035904 trainer.py:354] Write summary done: step 6347\n",
      "I0710 10:11:32.236926 140295626643200 summary_utils.py:349] Steps/second: 0.177174, Examples/second: 25.108437\n",
      "I0710 10:11:32.237719 140295626643200 trainer.py:508] step:  6355, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:82.576584 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2883004 log_pplx:3.4848084 loss:144.27107 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.85913\n",
      "I0710 10:11:36.142225 140295626643200 summary_utils.py:349] Steps/second: 0.177204, Examples/second: 25.114471\n",
      "I0710 10:11:36.143038 140295626643200 trainer.py:508] step:  6356, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:49.80809 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.289865 log_pplx:3.8429151 loss:94.415611 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.86481\n",
      "I0710 10:11:42.383487 140295626643200 summary_utils.py:349] Steps/second: 0.177194, Examples/second: 25.106999\n",
      "I0710 10:11:42.384551 140295626643200 trainer.py:508] step:  6357, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:98.493484 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2880414 log_pplx:3.6026926 loss:143.52226 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.87012\n",
      "I0710 10:11:45.440548 140295626643200 summary_utils.py:349] Steps/second: 0.177238, Examples/second: 25.124446\n",
      "I0710 10:11:45.441389 140295626643200 trainer.py:508] step:  6358, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:26.798553 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2891933 log_pplx:3.7892005 loss:58.214554 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.87494\n",
      "I0710 10:11:52.079766 140295626643200 summary_utils.py:349] Steps/second: 0.177221, Examples/second: 25.116002\n",
      "I0710 10:11:52.080824 140295626643200 trainer.py:508] step:  6359, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:72.51635 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2884043 log_pplx:3.5197017 loss:145.1877 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.8797\n",
      "I0710 10:12:02.193945 140295626643200 summary_utils.py:349] Steps/second: 0.177144, Examples/second: 25.095205\n",
      "I0710 10:12:02.194890 140295626643200 trainer.py:508] step:  6360, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:321.21469 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2874831 log_pplx:2.9878061 loss:218.55801 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:524.88483\n",
      "I0710 10:12:06.045074 140295626643200 summary_utils.py:349] Steps/second: 0.177175, Examples/second: 25.101359\n",
      "I0710 10:12:06.045982 140295626643200 trainer.py:508] step:  6361, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:38.420521 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2902144 log_pplx:3.8843689 loss:98.711525 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.89136\n",
      "I0710 10:12:12.451929 140295626643200 summary_utils.py:349] Steps/second: 0.177162, Examples/second: 25.093513\n",
      "I0710 10:12:12.452723 140295626643200 trainer.py:508] step:  6362, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:151.15741 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2883526 log_pplx:3.5209966 loss:144.97704 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.89777\n",
      "I0710 10:12:18.811504 140295626643200 summary_utils.py:349] Steps/second: 0.177150, Examples/second: 25.085792\n",
      "I0710 10:12:18.812304 140295626643200 trainer.py:508] step:  6363, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:125.56911 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2882144 log_pplx:3.5696146 loss:145.86337 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.90405\n",
      "I0710 10:12:21.837580 140295626643200 summary_utils.py:349] Steps/second: 0.177195, Examples/second: 25.103258\n",
      "I0710 10:12:21.838414 140295626643200 trainer.py:508] step:  6364, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:28.766932 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.289721 log_pplx:3.8092043 loss:59.518822 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.90997\n",
      "I0710 10:12:24.088192 140295626643200 summary_utils.py:349] Steps/second: 0.177253, Examples/second: 25.147426\n",
      "I0710 10:12:24.089125 140295626643200 trainer.py:508] step:  6365, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:11.639979 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2878543 log_pplx:3.8789852 loss:27.65292 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:524.91577\n",
      "I0710 10:12:30.643136 140295626643200 summary_utils.py:349] Steps/second: 0.177237, Examples/second: 25.139205\n",
      "I0710 10:12:30.643947 140295626643200 trainer.py:508] step:  6366, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:102.7493 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2882718 log_pplx:3.4792175 loss:141.77811 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.92126\n",
      "I0710 10:12:34.630980 140295626643200 summary_utils.py:349] Steps/second: 0.177266, Examples/second: 25.144993\n",
      "I0710 10:12:34.631874 140295626643200 trainer.py:508] step:  6367, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:71.771423 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2898793 log_pplx:3.875196 loss:97.848701 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.92725\n",
      "I0710 10:12:38.437083 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 10:12:41.321650 140295626643200 summary_utils.py:349] Steps/second: 0.177248, Examples/second: 25.136451\n",
      "I0710 10:12:41.322491 140295626643200 trainer.py:508] step:  6368, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:75.0979 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.288466 log_pplx:3.5391045 loss:143.201 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.93353\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 10:12:43.735093 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 10:12:44.227866 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00006367\n",
      "I0710 10:12:47.825862 140295626643200 summary_utils.py:349] Steps/second: 0.177233, Examples/second: 25.128374\n",
      "I0710 10:12:47.826668 140295626643200 trainer.py:508] step:  6369, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:72.178955 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2883337 log_pplx:3.4988785 loss:139.0367 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.93951\n",
      "I0710 10:12:51.685892 140295626643200 summary_utils.py:349] Steps/second: 0.177263, Examples/second: 25.134467\n",
      "I0710 10:12:51.686750 140295626643200 trainer.py:508] step:  6370, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:48.099617 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2899965 log_pplx:3.8849657 loss:97.876862 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.94525\n",
      "I0710 10:12:54.771025 140295626643200 summary_utils.py:349] Steps/second: 0.177307, Examples/second: 25.151719\n",
      "I0710 10:12:54.771863 140295626643200 trainer.py:508] step:  6371, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:48.676601 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2892927 log_pplx:3.872076 loss:59.896175 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.95038\n",
      "I0710 10:13:04.134247 140295626643200 summary_utils.py:349] Steps/second: 0.177243, Examples/second: 25.132836\n",
      "I0710 10:13:04.135024 140295626643200 trainer.py:508] step:  6372, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:504.24774 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2877295 log_pplx:2.9540148 loss:210.99051 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:524.95496\n",
      "I0710 10:13:10.514562 140295626643200 summary_utils.py:349] Steps/second: 0.177231, Examples/second: 25.125079\n",
      "I0710 10:13:10.515369 140295626643200 trainer.py:508] step:  6373, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:116.99211 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2887127 log_pplx:3.627553 loss:147.86813 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.95923\n",
      "I0710 10:13:14.435068 140295626643200 summary_utils.py:349] Steps/second: 0.177260, Examples/second: 25.131013\n",
      "I0710 10:13:14.435868 140295626643200 trainer.py:508] step:  6374, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:63.322891 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2900898 log_pplx:3.9025767 loss:97.808327 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.96454\n",
      "I0710 10:13:20.871770 140295626643200 summary_utils.py:349] Steps/second: 0.177247, Examples/second: 25.123129\n",
      "I0710 10:13:20.872578 140295626643200 trainer.py:508] step:  6375, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:202.82178 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2889162 log_pplx:3.6090126 loss:149.41312 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.97009\n",
      "I0710 10:13:23.987211 140295626643200 summary_utils.py:349] Steps/second: 0.177290, Examples/second: 25.140263\n",
      "I0710 10:13:23.988277 140295626643200 trainer.py:508] step:  6376, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:22.979155 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2897725 log_pplx:3.7540767 loss:58.760094 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:524.97675\n",
      "I0710 10:13:30.508538 140295626643200 summary_utils.py:349] Steps/second: 0.177275, Examples/second: 25.132175\n",
      "I0710 10:13:30.509333 140295626643200 trainer.py:508] step:  6377, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:100.96564 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.288775 log_pplx:3.5868919 loss:140.92001 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.98309\n",
      "I0710 10:13:34.462560 140295626643200 summary_utils.py:349] Steps/second: 0.177304, Examples/second: 25.138014\n",
      "I0710 10:13:34.463368 140295626643200 trainer.py:508] step:  6378, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:94.541649 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2900965 log_pplx:3.915668 loss:98.821671 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:524.98944\n",
      "I0710 10:13:41.105563 140295626643200 summary_utils.py:349] Steps/second: 0.177287, Examples/second: 25.129641\n",
      "I0710 10:13:41.106616 140295626643200 trainer.py:508] step:  6379, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:83.096031 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2886838 log_pplx:3.5873413 loss:143.40398 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:524.99487\n",
      "I0710 10:13:47.323402 140295626643200 summary_utils.py:349] Steps/second: 0.177277, Examples/second: 25.122306\n",
      "I0710 10:13:47.324208 140295626643200 trainer.py:508] step:  6380, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:85.776848 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2886508 log_pplx:3.5272388 loss:138.66457 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.00018\n",
      "I0710 10:13:50.416151 140295626643200 summary_utils.py:349] Steps/second: 0.177320, Examples/second: 25.139452\n",
      "I0710 10:13:50.416943 140295626643200 trainer.py:508] step:  6381, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:23.586449 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.290078 log_pplx:3.8469195 loss:60.303478 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.00537\n",
      "I0710 10:13:56.921661 140295626643200 summary_utils.py:349] Steps/second: 0.177305, Examples/second: 25.131423\n",
      "I0710 10:13:56.922527 140295626643200 trainer.py:508] step:  6382, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:83.885765 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2890505 log_pplx:3.5285144 loss:145.94817 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.01025\n",
      "I0710 10:14:07.502551 140295626643200 summary_utils.py:349] Steps/second: 0.177221, Examples/second: 25.109735\n",
      "I0710 10:14:07.503365 140295626643200 trainer.py:508] step:  6383, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:741.61517 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2877643 log_pplx:3.3922675 loss:246.95708 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:525.01562\n",
      "I0710 10:14:11.324269 140295626643200 summary_utils.py:349] Steps/second: 0.177252, Examples/second: 25.115881\n",
      "I0710 10:14:11.325095 140295626643200 trainer.py:508] step:  6384, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:47.190159 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2903913 log_pplx:3.8249106 loss:95.694481 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.02301\n",
      "I0710 10:14:17.631947 140295626643200 summary_utils.py:349] Steps/second: 0.177241, Examples/second: 25.108359\n",
      "I0710 10:14:17.632736 140295626643200 trainer.py:508] step:  6385, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:167.23364 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2890614 log_pplx:3.7067518 loss:153.8302 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.03027\n",
      "I0710 10:14:23.696001 140295626643200 summary_utils.py:349] Steps/second: 0.177234, Examples/second: 25.101433\n",
      "I0710 10:14:23.696786 140295626643200 trainer.py:508] step:  6386, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:194.86382 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2887706 log_pplx:3.716759 loss:147.2301 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.03723\n",
      "I0710 10:14:27.619074 140295626643200 summary_utils.py:349] Steps/second: 0.177263, Examples/second: 25.107328\n",
      "I0710 10:14:27.619857 140295626643200 trainer.py:508] step:  6387, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:44.537098 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2906935 log_pplx:3.8964505 loss:99.067261 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.0437\n",
      "I0710 10:14:29.839673 140295626643200 summary_utils.py:349] Steps/second: 0.177321, Examples/second: 25.151035\n",
      "I0710 10:14:29.840627 140295626643200 trainer.py:508] step:  6388, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:17.118423 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2891324 log_pplx:3.8811445 loss:28.684082 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:525.05011\n",
      "I0710 10:14:32.959751 140295626643200 summary_utils.py:349] Steps/second: 0.177364, Examples/second: 25.168037\n",
      "I0710 10:14:32.960537 140295626643200 trainer.py:508] step:  6389, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:44.133602 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.290689 log_pplx:3.9240942 loss:62.386967 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.05627\n",
      "I0710 10:14:39.472170 140295626643200 summary_utils.py:349] Steps/second: 0.177349, Examples/second: 25.160006\n",
      "I0710 10:14:39.472924 140295626643200 trainer.py:508] step:  6390, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:122.80476 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2893299 log_pplx:3.6904035 loss:150.56845 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.06238\n",
      "I0710 10:14:45.923129 140295626643200 summary_utils.py:349] Steps/second: 0.177335, Examples/second: 25.152134\n",
      "I0710 10:14:45.923931 140295626643200 trainer.py:508] step:  6391, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:127.85275 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.289503 log_pplx:3.7360737 loss:158.45625 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.06824\n",
      "I0710 10:14:49.922926 140295626643200 summary_utils.py:349] Steps/second: 0.177363, Examples/second: 25.157813\n",
      "I0710 10:14:49.923721 140295626643200 trainer.py:508] step:  6392, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:74.376297 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2909921 log_pplx:4.0187335 loss:101.27208 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.07373\n",
      "I0710 10:14:56.184365 140295626643200 summary_utils.py:349] Steps/second: 0.177352, Examples/second: 25.150405\n",
      "I0710 10:14:56.185153 140295626643200 trainer.py:508] step:  6393, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:104.11861 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2890172 log_pplx:3.6035733 loss:146.44022 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.07953\n",
      "I0710 10:15:07.253108 140295626643200 summary_utils.py:349] Steps/second: 0.177260, Examples/second: 25.127650\n",
      "I0710 10:15:07.253878 140295626643200 trainer.py:508] step:  6394, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:317.0441 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2881837 log_pplx:3.184581 loss:226.74216 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:525.08521\n",
      "I0710 10:15:10.350386 140295626643200 summary_utils.py:349] Steps/second: 0.177304, Examples/second: 25.144653\n",
      "I0710 10:15:10.351184 140295626643200 trainer.py:508] step:  6395, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:26.246435 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2907579 log_pplx:3.8635035 loss:60.065407 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.09088\n",
      "I0710 10:15:16.817318 140295626643200 summary_utils.py:349] Steps/second: 0.177290, Examples/second: 25.136774\n",
      "I0710 10:15:16.818117 140295626643200 trainer.py:508] step:  6396, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:100.96373 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2887989 log_pplx:3.5556753 loss:139.51582 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.09631\n",
      "I0710 10:15:20.678643 140295626643200 summary_utils.py:349] Steps/second: 0.177320, Examples/second: 25.142775\n",
      "I0710 10:15:20.679486 140295626643200 trainer.py:508] step:  6397, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:52.098717 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2909678 log_pplx:3.9063845 loss:99.344254 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.10193\n",
      "I0710 10:15:27.549343 140295626643200 summary_utils.py:349] Steps/second: 0.177299, Examples/second: 25.133938\n",
      "I0710 10:15:27.550227 140295626643200 trainer.py:508] step:  6398, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:85.641113 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2892889 log_pplx:3.6465061 loss:148.23047 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.1076\n",
      "I0710 10:15:34.425501 140295626643200 summary_utils.py:349] Steps/second: 0.177278, Examples/second: 25.125101\n",
      "I0710 10:15:34.426332 140295626643200 trainer.py:508] step:  6399, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:92.847061 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2894447 log_pplx:3.5884511 loss:146.54338 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.11304\n",
      "I0710 10:15:37.431106 140295626643200 summary_utils.py:349] Steps/second: 0.177322, Examples/second: 25.142279\n",
      "I0710 10:15:37.432056 140295626643200 trainer.py:508] step:  6400, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:58.153271 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2907209 log_pplx:3.930665 loss:60.833187 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.11871\n",
      "I0710 10:15:41.371615 140295626643200 summary_utils.py:349] Steps/second: 0.177351, Examples/second: 25.148076\n",
      "I0710 10:15:41.372373 140295626643200 trainer.py:508] step:  6401, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:73.905472 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2911385 log_pplx:4.0035524 loss:100.23895 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.1239\n",
      "I0710 10:15:47.784952 140295626643200 summary_utils.py:349] Steps/second: 0.177338, Examples/second: 25.140347\n",
      "I0710 10:15:47.785808 140295626643200 trainer.py:508] step:  6402, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:109.84653 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2895979 log_pplx:3.7056684 loss:157.49089 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.12848\n",
      "I0710 10:15:54.217500 140295626643200 summary_utils.py:349] Steps/second: 0.177325, Examples/second: 25.132582\n",
      "I0710 10:15:54.218268 140295626643200 trainer.py:508] step:  6403, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:89.564293 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2892592 log_pplx:3.4908662 loss:142.38371 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.13391\n",
      "I0710 10:16:05.049930 140295626643200 summary_utils.py:349] Steps/second: 0.177237, Examples/second: 25.110534\n",
      "I0710 10:16:05.050976 140295626643200 trainer.py:508] step:  6404, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:278.12674 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2884637 log_pplx:3.0558982 loss:222.3166 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:525.13989\n",
      "I0710 10:16:08.990105 140295626643200 summary_utils.py:349] Steps/second: 0.177266, Examples/second: 25.116328\n",
      "I0710 10:16:08.991115 140295626643200 trainer.py:508] step:  6405, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:53.350533 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2911863 log_pplx:3.9294448 loss:99.120255 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.14777\n",
      "I0710 10:16:15.529956 140295626643200 summary_utils.py:349] Steps/second: 0.177251, Examples/second: 25.108338\n",
      "I0710 10:16:15.530744 140295626643200 trainer.py:508] step:  6406, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:90.05719 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2894202 log_pplx:3.5772049 loss:142.32803 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.15527\n",
      "I0710 10:16:18.542890 140295626643200 summary_utils.py:349] Steps/second: 0.177295, Examples/second: 25.125437\n",
      "I0710 10:16:18.543665 140295626643200 trainer.py:508] step:  6407, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:29.625097 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2906435 log_pplx:3.8349781 loss:59.367264 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.16248\n",
      "I0710 10:16:25.068563 140295626643200 summary_utils.py:349] Steps/second: 0.177280, Examples/second: 25.117481\n",
      "I0710 10:16:25.069542 140295626643200 trainer.py:508] step:  6408, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:103.97683 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2896565 log_pplx:3.6313591 loss:147.79633 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.16925\n",
      "I0710 10:16:28.888733 140295626643200 summary_utils.py:349] Steps/second: 0.177311, Examples/second: 25.123548\n",
      "I0710 10:16:28.889589 140295626643200 trainer.py:508] step:  6409, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:40.536335 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2911514 log_pplx:3.8490603 loss:95.793495 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.17548\n",
      "I0710 10:16:35.049037 140295626643200 summary_utils.py:349] Steps/second: 0.177302, Examples/second: 25.116470\n",
      "I0710 10:16:35.049917 140295626643200 trainer.py:508] step:  6410, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:86.774544 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2897115 log_pplx:3.597424 loss:145.11111 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.18146\n",
      "I0710 10:16:37.246587 140295626643200 summary_utils.py:349] Steps/second: 0.177360, Examples/second: 25.159703\n",
      "I0710 10:16:37.247504 140295626643200 trainer.py:508] step:  6411, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:13.84108 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2892679 log_pplx:3.7449627 loss:27.421612 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:525.18713\n",
      "I0710 10:16:40.264780 140295626643200 summary_utils.py:349] Steps/second: 0.177404, Examples/second: 25.176740\n",
      "I0710 10:16:40.265604 140295626643200 trainer.py:508] step:  6412, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:45.214432 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2906882 log_pplx:3.953021 loss:61.487999 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.19232\n",
      "I0710 10:16:46.572001 140295626643200 summary_utils.py:349] Steps/second: 0.177392, Examples/second: 25.169289\n",
      "I0710 10:16:46.572889 140295626643200 trainer.py:508] step:  6413, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:94.320885 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2893248 log_pplx:3.5742004 loss:140.60011 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.19763\n",
      "I0710 10:16:57.617441 140295626643200 summary_utils.py:349] Steps/second: 0.177302, Examples/second: 25.146807\n",
      "I0710 10:16:57.618270 140295626643200 trainer.py:508] step:  6414, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:215.78418 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2890031 log_pplx:2.9320493 loss:218.21777 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:525.20294\n",
      "I0710 10:17:01.544983 140295626643200 summary_utils.py:349] Steps/second: 0.177330, Examples/second: 25.152589\n",
      "I0710 10:17:01.545790 140295626643200 trainer.py:508] step:  6415, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:59.995872 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.291594 log_pplx:3.914005 loss:99.513573 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.20752\n",
      "I0710 10:17:07.910416 140295626643200 summary_utils.py:349] Steps/second: 0.177318, Examples/second: 25.145030\n",
      "I0710 10:17:07.911208 140295626643200 trainer.py:508] step:  6416, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:77.888702 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2898608 log_pplx:3.5048735 loss:143.74362 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.21259\n",
      "I0710 10:17:14.169564 140295626643200 summary_utils.py:349] Steps/second: 0.177308, Examples/second: 25.137731\n",
      "I0710 10:17:14.170383 140295626643200 trainer.py:508] step:  6417, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:75.735718 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2899768 log_pplx:3.5682933 loss:143.57919 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.2179\n",
      "I0710 10:17:18.035504 140295626643200 summary_utils.py:349] Steps/second: 0.177338, Examples/second: 25.143653\n",
      "I0710 10:17:18.036286 140295626643200 trainer.py:508] step:  6418, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:39.589149 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2916701 log_pplx:3.8855824 loss:98.135239 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.22327\n",
      "I0710 10:17:24.256481 140295626643200 summary_utils.py:349] Steps/second: 0.177328, Examples/second: 25.136453\n",
      "I0710 10:17:24.257260 140295626643200 trainer.py:508] step:  6419, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:91.910965 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2899926 log_pplx:3.5551045 loss:143.71512 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.22858\n",
      "I0710 10:17:27.347248 140295626643200 summary_utils.py:349] Steps/second: 0.177370, Examples/second: 25.153249\n",
      "I0710 10:17:27.348022 140295626643200 trainer.py:508] step:  6420, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:27.41106 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2910281 log_pplx:3.863179 loss:60.331985 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.23346\n",
      "I0710 10:17:34.020213 140295626643200 summary_utils.py:349] Steps/second: 0.177353, Examples/second: 25.144979\n",
      "I0710 10:17:34.020998 140295626643200 trainer.py:508] step:  6421, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:79.448318 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2900044 log_pplx:3.6350212 loss:144.6284 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.2381\n",
      "I0710 10:17:37.849317 140295626643200 summary_utils.py:349] Steps/second: 0.177383, Examples/second: 25.150975\n",
      "I0710 10:17:37.850119 140295626643200 trainer.py:508] step:  6422, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:48.853336 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2917862 log_pplx:3.8676777 loss:98.311523 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.24255\n",
      "I0710 10:17:44.056751 140295626643200 summary_utils.py:349] Steps/second: 0.177374, Examples/second: 25.143816\n",
      "I0710 10:17:44.057519 140295626643200 trainer.py:508] step:  6423, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:70.834465 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2900964 log_pplx:3.5273991 loss:142.94785 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.24658\n",
      "I0710 10:17:47.151772 140295626643200 summary_utils.py:349] Steps/second: 0.177416, Examples/second: 25.160569\n",
      "I0710 10:17:47.152774 140295626643200 trainer.py:508] step:  6424, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:22.271999 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2914293 log_pplx:3.8134007 loss:59.852512 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.25092\n",
      "I0710 10:17:53.688543 140295626643200 summary_utils.py:349] Steps/second: 0.177401, Examples/second: 25.152632\n",
      "I0710 10:17:53.689515 140295626643200 trainer.py:508] step:  6425, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:89.997734 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2900145 log_pplx:3.4681239 loss:143.53699 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.255\n",
      "I0710 10:17:57.564390 140295626643200 summary_utils.py:349] Steps/second: 0.177431, Examples/second: 25.158503\n",
      "I0710 10:17:57.565219 140295626643200 trainer.py:508] step:  6426, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:39.015415 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2915814 log_pplx:3.8120885 loss:94.706573 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.25964\n",
      "I0710 10:18:07.324211 140295626643200 summary_utils.py:349] Steps/second: 0.177362, Examples/second: 25.139215\n",
      "I0710 10:18:07.325004 140295626643200 trainer.py:508] step:  6427, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:266.34564 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2888952 log_pplx:2.850085 loss:194.37582 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:525.26434\n",
      "I0710 10:18:13.791369 140295626643200 summary_utils.py:349] Steps/second: 0.177348, Examples/second: 25.131470\n",
      "I0710 10:18:13.792181 140295626643200 trainer.py:508] step:  6428, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:71.44664 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2899456 log_pplx:3.4699497 loss:141.92094 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.27032\n",
      "I0710 10:18:20.071352 140295626643200 summary_utils.py:349] Steps/second: 0.177337, Examples/second: 25.124176\n",
      "I0710 10:18:20.072129 140295626643200 trainer.py:508] step:  6429, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:75.89045 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2900827 log_pplx:3.4600036 loss:139.52464 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.27643\n",
      "I0710 10:18:23.943483 140295626643200 summary_utils.py:349] Steps/second: 0.177367, Examples/second: 25.130052\n",
      "I0710 10:18:23.944451 140295626643200 trainer.py:508] step:  6430, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:50.198162 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2918667 log_pplx:3.8028858 loss:95.048378 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.28229\n",
      "I0710 10:18:27.032959 140295626643200 summary_utils.py:349] Steps/second: 0.177409, Examples/second: 25.146759\n",
      "I0710 10:18:27.033767 140295626643200 trainer.py:508] step:  6431, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:27.254267 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2915828 log_pplx:3.8125374 loss:60.1964 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.28766\n",
      "I0710 10:18:33.258175 140295626643200 summary_utils.py:349] Steps/second: 0.177399, Examples/second: 25.139593\n",
      "I0710 10:18:33.258965 140295626643200 trainer.py:508] step:  6432, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:91.83609 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2898115 log_pplx:3.5962884 loss:139.94058 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.2926\n",
      "I0710 10:18:39.704380 140295626643200 summary_utils.py:349] Steps/second: 0.177386, Examples/second: 25.131916\n",
      "I0710 10:18:39.705110 140295626643200 trainer.py:508] step:  6433, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:88.194283 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.290279 log_pplx:3.4684584 loss:138.39151 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.29736\n",
      "I0710 10:18:43.528011 140295626643200 summary_utils.py:349] Steps/second: 0.177416, Examples/second: 25.137893\n",
      "I0710 10:18:43.528875 140295626643200 trainer.py:508] step:  6434, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:52.600063 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2919004 log_pplx:3.8587692 loss:96.830994 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.302\n",
      "I0710 10:18:45.728677 140295626643200 summary_utils.py:349] Steps/second: 0.177473, Examples/second: 25.180595\n",
      "I0710 10:18:45.729672 140295626643200 trainer.py:508] step:  6435, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:11.240555 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2900069 log_pplx:3.749759 loss:27.830242 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:525.30682\n",
      "I0710 10:18:52.275451 140295626643200 summary_utils.py:349] Steps/second: 0.177458, Examples/second: 25.172666\n",
      "I0710 10:18:52.276264 140295626643200 trainer.py:508] step:  6436, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:74.836533 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2901689 log_pplx:3.5092003 loss:139.97324 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.31128\n",
      "I0710 10:18:55.375493 140295626643200 summary_utils.py:349] Steps/second: 0.177500, Examples/second: 25.189291\n",
      "I0710 10:18:55.376303 140295626643200 base_runner.py:111] step:  6437, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:30.957582 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2919083 log_pplx:3.8608372 loss:60.612133 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.31586\n",
      "I0710 10:19:05.734918 140295626643200 summary_utils.py:349] Steps/second: 0.177422, Examples/second: 25.168670\n",
      "I0710 10:19:05.735761 140295626643200 trainer.py:508] step:  6438, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:412.09885 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2893028 log_pplx:2.9613829 loss:215.36661 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:525.32019\n",
      "I0710 10:19:11.993003 140295626643200 summary_utils.py:349] Steps/second: 0.177411, Examples/second: 25.161440\n",
      "I0710 10:19:11.993837 140295626643200 trainer.py:508] step:  6439, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:92.47905 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2901248 log_pplx:3.4997916 loss:140.60413 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.32434\n",
      "I0710 10:19:15.856357 140295626643200 summary_utils.py:349] Steps/second: 0.177441, Examples/second: 25.167295\n",
      "I0710 10:19:15.857144 140295626643200 trainer.py:508] step:  6440, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:54.016647 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2922298 log_pplx:3.8845932 loss:98.328766 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.32922\n",
      "I0710 10:19:22.228635 140295626643200 summary_utils.py:349] Steps/second: 0.177428, Examples/second: 25.159806\n",
      "I0710 10:19:22.229379 140295626643200 trainer.py:508] step:  6441, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:91.643082 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2898502 log_pplx:3.5723116 loss:139.09689 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.33435\n",
      "I0710 10:19:28.485191 140295626643200 summary_utils.py:349] Steps/second: 0.177418, Examples/second: 25.152596\n",
      "I0710 10:19:28.486136 140295626643200 trainer.py:508] step:  6442, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:81.801285 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2905406 log_pplx:3.5726817 loss:145.05087 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.33966\n",
      "I0710 10:19:31.619127 140295626643200 summary_utils.py:349] Steps/second: 0.177460, Examples/second: 25.169092\n",
      "I0710 10:19:31.620079 140295626643200 trainer.py:508] step:  6443, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:39.573807 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2916108 log_pplx:3.8549771 loss:59.812378 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.34515\n",
      "I0710 10:19:35.459190 140295626643200 summary_utils.py:349] Steps/second: 0.177489, Examples/second: 25.174988\n",
      "I0710 10:19:35.460118 140295626643200 trainer.py:508] step:  6444, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:65.747307 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2923549 log_pplx:3.8546526 loss:98.100906 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.35016\n",
      "I0710 10:19:41.650550 140295626643200 summary_utils.py:349] Steps/second: 0.177480, Examples/second: 25.167931\n",
      "I0710 10:19:41.651334 140295626643200 trainer.py:508] step:  6445, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:75.781662 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2903243 log_pplx:3.5466428 loss:141.46671 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.35455\n",
      "I0710 10:19:48.042786 140295626643200 summary_utils.py:349] Steps/second: 0.177468, Examples/second: 25.160412\n",
      "I0710 10:19:48.043587 140295626643200 trainer.py:508] step:  6446, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:83.389114 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2904545 log_pplx:3.5746617 loss:145.13127 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.35907\n",
      "I0710 10:19:52.038139 140295626643200 summary_utils.py:349] Steps/second: 0.177495, Examples/second: 25.165939\n",
      "I0710 10:19:52.038992 140295626643200 trainer.py:508] step:  6447, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:50.832493 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2918338 log_pplx:3.7971017 loss:94.785149 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.36383\n",
      "I0710 10:19:58.588517 140295626643200 summary_utils.py:349] Steps/second: 0.177479, Examples/second: 25.158059\n",
      "I0710 10:19:58.589518 140295626643200 trainer.py:508] step:  6448, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:72.566765 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2901984 log_pplx:3.4979413 loss:138.51848 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.36816\n",
      "I0710 10:19:58.839680 140295635035904 trainer.py:345] Write summary @6448\n",
      "I0710 10:20:14.858461 140295626643200 summary_utils.py:349] Steps/second: 0.177304, Examples/second: 25.123811\n",
      "I0710 10:20:14.859851 140295626643200 trainer.py:508] step:  6449, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:449.6441 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2896974 log_pplx:2.9930956 loss:214.67978 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:525.37286\n",
      "I0710 10:20:18.932401 140295626643200 summary_utils.py:349] Steps/second: 0.177330, Examples/second: 25.138054\n",
      "I0710 10:20:18.934231 140295626643200 trainer.py:508] step:  6450, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:23.529112 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2915467 log_pplx:3.7525196 loss:57.343185 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.37903\n",
      "I0710 10:20:28.529742 140295626643200 summary_utils.py:349] Steps/second: 0.177265, Examples/second: 25.123119\n",
      "I0710 10:20:28.531052 140295626643200 trainer.py:508] step:  6451, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:109.8511 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2906648 log_pplx:3.5769265 loss:144.01602 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.38507\n",
      "I0710 10:20:33.744737 140295626643200 summary_utils.py:349] Steps/second: 0.177272, Examples/second: 25.125802\n",
      "I0710 10:20:33.745860 140295626643200 trainer.py:508] step:  6452, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:68.932755 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2926135 log_pplx:4.0192676 loss:99.42662 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.39093\n",
      "I0710 10:20:44.455771 140295626643200 summary_utils.py:349] Steps/second: 0.177189, Examples/second: 25.108312\n",
      "I0710 10:20:44.456757 140295626643200 trainer.py:508] step:  6453, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:209.68481 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2909042 log_pplx:3.6329458 loss:147.77007 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.39642\n",
      "I0710 10:20:48.938349 140295626643200 summary_utils.py:349] Steps/second: 0.177208, Examples/second: 25.121573\n",
      "I0710 10:20:48.939982 140295626643200 trainer.py:508] step:  6454, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:38.333698 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2918222 log_pplx:3.843806 loss:59.323734 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.40088\n",
      "I0710 10:20:57.254179 140295635035904 trainer.py:354] Write summary done: step 6448\n",
      "I0710 10:20:59.150586 140295626643200 summary_utils.py:349] Steps/second: 0.177133, Examples/second: 25.105268\n",
      "I0710 10:20:59.151430 140295626643200 trainer.py:508] step:  6455, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:80.569817 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2909074 log_pplx:3.5958796 loss:145.00385 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.40558\n",
      "I0710 10:21:03.007112 140295626643200 summary_utils.py:349] Steps/second: 0.177163, Examples/second: 25.111101\n",
      "I0710 10:21:03.007863 140295626643200 trainer.py:508] step:  6456, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:89.848465 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2925524 log_pplx:3.9688709 loss:99.569046 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.4104\n",
      "I0710 10:21:14.118777 140295626643200 summary_utils.py:349] Steps/second: 0.177073, Examples/second: 25.089056\n",
      "I0710 10:21:14.119545 140295626643200 trainer.py:508] step:  6457, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:293.02872 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2898245 log_pplx:2.9932954 loss:212.5988 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:525.41571\n",
      "I0710 10:21:20.341551 140295626643200 summary_utils.py:349] Steps/second: 0.177064, Examples/second: 25.082040\n",
      "I0710 10:21:20.342373 140295626643200 trainer.py:508] step:  6458, steps/sec: 0.18, examples/sec: 25.08 grad_norm/all/loss:104.22428 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2908953 log_pplx:3.6712871 loss:150.10976 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.42139\n",
      "I0710 10:21:22.541791 140295626643200 summary_utils.py:349] Steps/second: 0.177120, Examples/second: 25.124137\n",
      "I0710 10:21:22.542989 140295626643200 trainer.py:508] step:  6459, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:10.861312 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.290745 log_pplx:3.8843768 loss:28.412092 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:525.42731\n",
      "I0710 10:21:29.013084 140295626643200 summary_utils.py:349] Steps/second: 0.177107, Examples/second: 25.116531\n",
      "I0710 10:21:29.013848 140295626643200 trainer.py:508] step:  6460, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:96.454346 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2908574 log_pplx:3.4588473 loss:136.23535 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.43286\n",
      "I0710 10:21:32.885967 140295626643200 summary_utils.py:349] Steps/second: 0.177135, Examples/second: 25.122306\n",
      "I0710 10:21:32.886934 140295626643200 trainer.py:508] step:  6461, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:55.800262 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.292653 log_pplx:3.9207165 loss:97.968895 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.43878\n",
      "I0710 10:21:35.917780 140295626643200 summary_utils.py:349] Steps/second: 0.177178, Examples/second: 25.138858\n",
      "I0710 10:21:35.918565 140295626643200 trainer.py:508] step:  6462, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:29.92495 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.292285 log_pplx:3.8966084 loss:61.402023 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.44458\n",
      "I0710 10:21:42.435435 140295626643200 summary_utils.py:349] Steps/second: 0.177164, Examples/second: 25.131145\n",
      "I0710 10:21:42.436210 140295626643200 trainer.py:508] step:  6463, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:83.585999 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.291155 log_pplx:3.593154 loss:144.44479 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.45007\n",
      "I0710 10:21:48.768069 140295626643200 summary_utils.py:349] Steps/second: 0.177153, Examples/second: 25.123869\n",
      "I0710 10:21:48.768838 140295626643200 trainer.py:508] step:  6464, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:79.38945 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2911847 log_pplx:3.4878137 loss:141.38724 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.45502\n",
      "I0710 10:21:52.634254 140295626643200 summary_utils.py:349] Steps/second: 0.177182, Examples/second: 25.129647\n",
      "I0710 10:21:52.635277 140295626643200 trainer.py:508] step:  6465, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:53.216354 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2927179 log_pplx:3.8283136 loss:95.92318 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.46002\n",
      "I0710 10:21:59.015619 140295626643200 summary_utils.py:349] Steps/second: 0.177170, Examples/second: 25.122266\n",
      "I0710 10:21:59.016453 140295626643200 trainer.py:508] step:  6466, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:73.257805 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2910312 log_pplx:3.5947733 loss:147.25092 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.4646\n",
      "I0710 10:22:02.084407 140295626643200 summary_utils.py:349] Steps/second: 0.177212, Examples/second: 25.138693\n",
      "I0710 10:22:02.085197 140295626643200 trainer.py:508] step:  6467, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:41.459053 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2920593 log_pplx:3.8226893 loss:59.326347 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.46918\n",
      "I0710 10:22:12.508358 140295626643200 summary_utils.py:349] Steps/second: 0.177134, Examples/second: 25.118324\n",
      "I0710 10:22:12.509183 140295626643200 trainer.py:508] step:  6468, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:174.01942 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2904451 log_pplx:2.9766707 loss:219.30623 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:525.47333\n",
      "I0710 10:22:18.876793 140295626643200 summary_utils.py:349] Steps/second: 0.177122, Examples/second: 25.110993\n",
      "I0710 10:22:18.877633 140295626643200 trainer.py:508] step:  6469, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:79.212486 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2908087 log_pplx:3.6309557 loss:143.69507 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.47784\n",
      "I0710 10:22:22.767111 140295626643200 summary_utils.py:349] Steps/second: 0.177151, Examples/second: 25.116704\n",
      "I0710 10:22:22.767910 140295626643200 trainer.py:508] step:  6470, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:49.282101 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2930418 log_pplx:3.9316077 loss:100.03484 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.48297\n",
      "I0710 10:22:29.378760 140295626643200 summary_utils.py:349] Steps/second: 0.177135, Examples/second: 25.108821\n",
      "I0710 10:22:29.379566 140295626643200 trainer.py:508] step:  6471, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:76.997284 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2910247 log_pplx:3.6038063 loss:145.32347 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.48798\n",
      "I0710 10:22:35.834415 140295626643200 summary_utils.py:349] Steps/second: 0.177122, Examples/second: 25.101307\n",
      "I0710 10:22:35.835227 140295626643200 trainer.py:508] step:  6472, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:71.637451 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2907366 log_pplx:3.4897275 loss:131.5191 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.4928\n",
      "I0710 10:22:38.927929 140295626643200 summary_utils.py:349] Steps/second: 0.177163, Examples/second: 25.117627\n",
      "I0710 10:22:38.928744 140295626643200 trainer.py:508] step:  6473, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:21.398033 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2923988 log_pplx:3.8184915 loss:58.918133 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.49762\n",
      "I0710 10:22:42.788715 140295626643200 summary_utils.py:349] Steps/second: 0.177192, Examples/second: 25.123393\n",
      "I0710 10:22:42.789519 140295626643200 trainer.py:508] step:  6474, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:40.394318 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2928957 log_pplx:3.8834751 loss:98.179115 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.50226\n",
      "I0710 10:22:47.361126 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 10:22:49.114043 140295626643200 summary_utils.py:349] Steps/second: 0.177181, Examples/second: 25.116178\n",
      "I0710 10:22:49.114870 140295626643200 trainer.py:508] step:  6475, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:78.275658 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2911099 log_pplx:3.4912806 loss:137.6001 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.50653\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 10:22:52.818183 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 10:22:53.324433 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00006474\n",
      "I0710 10:22:55.327178 140295626643200 summary_utils.py:349] Steps/second: 0.177172, Examples/second: 25.109229\n",
      "I0710 10:22:55.328177 140295626643200 trainer.py:508] step:  6476, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:72.884827 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2911042 log_pplx:3.4680004 loss:135.81558 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.51093\n",
      "I0710 10:23:04.685798 140295626643200 summary_utils.py:349] Steps/second: 0.177112, Examples/second: 25.091428\n",
      "I0710 10:23:04.686592 140295626643200 trainer.py:508] step:  6477, steps/sec: 0.18, examples/sec: 25.09 grad_norm/all/loss:212.19118 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2902615 log_pplx:2.91241 loss:209.83914 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:525.51562\n",
      "I0710 10:23:07.783571 140295626643200 summary_utils.py:349] Steps/second: 0.177153, Examples/second: 25.107699\n",
      "I0710 10:23:07.784470 140295626643200 trainer.py:508] step:  6478, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:36.882965 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2929307 log_pplx:3.9133208 loss:61.390217 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.52094\n",
      "I0710 10:23:11.728845 140295626643200 summary_utils.py:349] Steps/second: 0.177180, Examples/second: 25.113259\n",
      "I0710 10:23:11.729609 140295626643200 trainer.py:508] step:  6479, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:47.308239 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2931129 log_pplx:3.9056394 loss:99.959953 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.52637\n",
      "I0710 10:23:18.280876 140295626643200 summary_utils.py:349] Steps/second: 0.177166, Examples/second: 25.105550\n",
      "I0710 10:23:18.281639 140295626643200 trainer.py:508] step:  6480, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:75.893463 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2912029 log_pplx:3.5701344 loss:143.11775 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.53192\n",
      "I0710 10:23:24.220310 140295626643200 summary_utils.py:349] Steps/second: 0.177161, Examples/second: 25.099252\n",
      "I0710 10:23:24.221161 140295626643200 trainer.py:508] step:  6481, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:73.915291 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2914264 log_pplx:3.5897849 loss:139.91187 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.53754\n",
      "I0710 10:23:27.477084 140295626643200 summary_utils.py:349] Steps/second: 0.177199, Examples/second: 25.138450\n",
      "I0710 10:23:27.477837 140295626643200 trainer.py:508] step:  6482, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:14.412556 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2912133 log_pplx:3.7955389 loss:27.762289 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:525.54321\n",
      "I0710 10:23:31.389643 140295626643200 summary_utils.py:349] Steps/second: 0.177227, Examples/second: 25.144064\n",
      "I0710 10:23:31.390435 140295626643200 trainer.py:508] step:  6483, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:44.20248 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2932198 log_pplx:3.8678598 loss:97.30085 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.54852\n",
      "I0710 10:23:37.867379 140295626643200 summary_utils.py:349] Steps/second: 0.177214, Examples/second: 25.136521\n",
      "I0710 10:23:37.868142 140295626643200 trainer.py:508] step:  6484, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:106.59305 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2917084 log_pplx:3.60431 loss:149.12833 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.55353\n",
      "I0710 10:23:40.920664 140295626643200 summary_utils.py:349] Steps/second: 0.177256, Examples/second: 25.152832\n",
      "I0710 10:23:40.921473 140295626643200 trainer.py:508] step:  6485, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:26.762758 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.292751 log_pplx:3.7758727 loss:58.791519 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.55835\n",
      "I0710 10:23:47.555138 140295626643200 summary_utils.py:349] Steps/second: 0.177240, Examples/second: 25.144931\n",
      "I0710 10:23:47.555879 140295626643200 trainer.py:508] step:  6486, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:76.178101 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2918584 log_pplx:3.5403068 loss:148.02908 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.56287\n",
      "I0710 10:23:54.034393 140295626643200 summary_utils.py:349] Steps/second: 0.177226, Examples/second: 25.137395\n",
      "I0710 10:23:54.035197 140295626643200 trainer.py:508] step:  6487, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:77.858459 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2918594 log_pplx:3.55059 loss:148.10399 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.5672\n",
      "I0710 10:23:57.854755 140295626643200 summary_utils.py:349] Steps/second: 0.177256, Examples/second: 25.143207\n",
      "I0710 10:23:57.855575 140295626643200 trainer.py:508] step:  6488, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:45.93605 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2934535 log_pplx:3.8869379 loss:97.294914 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.57178\n",
      "I0710 10:24:03.966284 140295626643200 summary_utils.py:349] Steps/second: 0.177248, Examples/second: 25.136518\n",
      "I0710 10:24:03.967089 140295626643200 trainer.py:508] step:  6489, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:77.641975 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2916456 log_pplx:3.511518 loss:140.54851 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.57611\n",
      "I0710 10:24:07.076229 140295626643200 summary_utils.py:349] Steps/second: 0.177289, Examples/second: 25.152661\n",
      "I0710 10:24:07.077008 140295626643200 trainer.py:508] step:  6490, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:23.521196 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2928867 log_pplx:3.7597079 loss:58.392963 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.58075\n",
      "I0710 10:24:13.382519 140295626643200 summary_utils.py:349] Steps/second: 0.177278, Examples/second: 25.145528\n",
      "I0710 10:24:13.383393 140295626643200 trainer.py:508] step:  6491, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:76.027847 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.291737 log_pplx:3.584708 loss:145.35989 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.58514\n",
      "I0710 10:24:17.307453 140295626643200 summary_utils.py:349] Steps/second: 0.177306, Examples/second: 25.151088\n",
      "I0710 10:24:17.308241 140295626643200 trainer.py:508] step:  6492, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:41.972954 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2933826 log_pplx:3.8744056 loss:96.206337 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.58948\n",
      "I0710 10:24:27.908529 140295626643200 summary_utils.py:349] Steps/second: 0.177226, Examples/second: 25.130554\n",
      "I0710 10:24:27.909507 140295626643200 trainer.py:508] step:  6493, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:252.10645 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2909771 log_pplx:3.0084577 loss:233.08028 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:525.59351\n",
      "I0710 10:24:34.268293 140295626643200 summary_utils.py:349] Steps/second: 0.177214, Examples/second: 25.123325\n",
      "I0710 10:24:34.269069 140295626643200 trainer.py:508] step:  6494, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:90.138351 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2919351 log_pplx:3.5843902 loss:149.73788 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.59827\n",
      "I0710 10:24:40.474647 140295626643200 summary_utils.py:349] Steps/second: 0.177205, Examples/second: 25.116454\n",
      "I0710 10:24:40.475438 140295626643200 trainer.py:508] step:  6495, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:83.029419 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2915719 log_pplx:3.4942009 loss:139.37492 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.60291\n",
      "I0710 10:24:44.311333 140295626643200 summary_utils.py:349] Steps/second: 0.177234, Examples/second: 25.122211\n",
      "I0710 10:24:44.312108 140295626643200 trainer.py:508] step:  6496, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:39.063046 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2935795 log_pplx:3.8575838 loss:98.850586 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.60785\n",
      "I0710 10:24:47.397199 140295626643200 summary_utils.py:349] Steps/second: 0.177275, Examples/second: 25.138354\n",
      "I0710 10:24:47.398127 140295626643200 trainer.py:508] step:  6497, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:24.802164 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2929486 log_pplx:3.773838 loss:59.246304 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.61261\n",
      "I0710 10:24:53.734218 140295626643200 summary_utils.py:349] Steps/second: 0.177264, Examples/second: 25.131185\n",
      "I0710 10:24:53.735154 140295626643200 trainer.py:508] step:  6498, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:85.606163 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2920099 log_pplx:3.5347888 loss:145.54495 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.61725\n",
      "I0710 10:25:00.282248 140295626643200 summary_utils.py:349] Steps/second: 0.177250, Examples/second: 25.123546\n",
      "I0710 10:25:00.283128 140295626643200 trainer.py:508] step:  6499, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:81.249039 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2917013 log_pplx:3.5065622 loss:141.57745 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.62183\n",
      "I0710 10:25:04.215829 140295626643200 summary_utils.py:349] Steps/second: 0.177277, Examples/second: 25.129071\n",
      "I0710 10:25:04.216636 140295626643200 trainer.py:508] step:  6500, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:43.451221 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2935512 log_pplx:3.9062335 loss:98.192947 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.62653\n",
      "I0710 10:25:10.445172 140295626643200 summary_utils.py:349] Steps/second: 0.177268, Examples/second: 25.122162\n",
      "I0710 10:25:10.445948 140295626643200 trainer.py:508] step:  6501, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:76.685463 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2921382 log_pplx:3.5872984 loss:147.30344 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.63129\n",
      "I0710 10:25:13.468319 140295626643200 summary_utils.py:349] Steps/second: 0.177309, Examples/second: 25.138409\n",
      "I0710 10:25:13.469356 140295626643200 trainer.py:508] step:  6502, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:31.588251 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.293108 log_pplx:3.8051374 loss:59.797138 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.6366\n",
      "I0710 10:25:23.356885 140295626643200 summary_utils.py:349] Steps/second: 0.177241, Examples/second: 25.119605\n",
      "I0710 10:25:23.357736 140295626643200 trainer.py:508] step:  6503, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:497.91858 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2906356 log_pplx:2.9564848 loss:205.77133 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:525.64136\n",
      "I0710 10:25:29.872517 140295626643200 summary_utils.py:349] Steps/second: 0.177228, Examples/second: 25.112066\n",
      "I0710 10:25:29.873570 140295626643200 trainer.py:508] step:  6504, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:95.341064 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2918235 log_pplx:3.566256 loss:142.24905 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.6452\n",
      "I0710 10:25:33.759700 140295626643200 summary_utils.py:349] Steps/second: 0.177256, Examples/second: 25.117685\n",
      "I0710 10:25:33.760487 140295626643200 trainer.py:508] step:  6505, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:45.204735 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2936795 log_pplx:3.8981493 loss:97.502457 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.64972\n",
      "I0710 10:25:35.965176 140295626643200 summary_utils.py:349] Steps/second: 0.177310, Examples/second: 25.158803\n",
      "I0710 10:25:35.966095 140295626643200 trainer.py:508] step:  6506, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:12.430334 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2920086 log_pplx:3.8024433 loss:28.399498 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:525.65442\n",
      "I0710 10:25:42.437288 140295626643200 summary_utils.py:349] Steps/second: 0.177297, Examples/second: 25.151348\n",
      "I0710 10:25:42.438110 140295626643200 trainer.py:508] step:  6507, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:164.25578 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2917199 log_pplx:3.5529609 loss:140.96375 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.659\n",
      "I0710 10:25:45.668805 140295626643200 summary_utils.py:349] Steps/second: 0.177336, Examples/second: 25.167068\n",
      "I0710 10:25:45.669651 140295626643200 trainer.py:508] step:  6508, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:24.567488 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2931241 log_pplx:3.7470753 loss:58.870064 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.66479\n",
      "I0710 10:25:52.523250 140295626643200 summary_utils.py:349] Steps/second: 0.177316, Examples/second: 25.158750\n",
      "I0710 10:25:52.524056 140295626643200 trainer.py:508] step:  6509, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:173.27171 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2922422 log_pplx:3.678174 loss:151.54077 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.67041\n",
      "I0710 10:25:56.419817 140295626643200 summary_utils.py:349] Steps/second: 0.177344, Examples/second: 25.164320\n",
      "I0710 10:25:56.420620 140295626643200 trainer.py:508] step:  6510, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:92.00634 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2938538 log_pplx:3.9231315 loss:100.18697 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.677\n",
      "2020-07-10 10:25:57.044058: I lingvo/core/ops/record_yielder.cc:532] Epoch 7: total records 46838\n",
      "2020-07-10 10:25:57.044120: I lingvo/core/ops/record_yielder.cc:485] Epoch 7 /tmp/punctuator_data/train.txt\n",
      "I0710 10:26:02.787052 140295626643200 summary_utils.py:349] Steps/second: 0.177332, Examples/second: 25.157113\n",
      "I0710 10:26:02.787835 140295626643200 trainer.py:508] step:  6511, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:87.843826 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2923537 log_pplx:3.6346321 loss:147.15715 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.68274\n",
      "I0710 10:26:09.125840 140295626643200 summary_utils.py:349] Steps/second: 0.177321, Examples/second: 25.149979\n",
      "I0710 10:26:09.126713 140295626643200 trainer.py:508] step:  6512, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:94.28447 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2922828 log_pplx:3.6303315 loss:146.84691 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.68842\n",
      "I0710 10:26:12.969126 140295626643200 summary_utils.py:349] Steps/second: 0.177350, Examples/second: 25.155664\n",
      "I0710 10:26:12.969941 140295626643200 trainer.py:508] step:  6513, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:52.119175 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2937201 log_pplx:3.842315 loss:95.625618 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.69403\n",
      "I0710 10:26:23.739839 140295626643200 summary_utils.py:349] Steps/second: 0.177268, Examples/second: 25.134956\n",
      "I0710 10:26:23.740719 140295626643200 trainer.py:508] step:  6514, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:398.68805 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2917044 log_pplx:3.2729359 loss:240.88809 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:525.6991\n",
      "I0710 10:26:26.835546 140295626643200 summary_utils.py:349] Steps/second: 0.177309, Examples/second: 25.150929\n",
      "I0710 10:26:26.836327 140295626643200 trainer.py:508] step:  6515, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:25.215212 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2933446 log_pplx:3.7738023 loss:58.154888 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.70703\n",
      "I0710 10:26:33.168947 140295626643200 summary_utils.py:349] Steps/second: 0.177298, Examples/second: 25.143825\n",
      "I0710 10:26:33.169818 140295626643200 trainer.py:508] step:  6516, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:148.65768 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2920367 log_pplx:3.6272016 loss:144.77068 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.7146\n",
      "I0710 10:26:39.633263 140295626643200 summary_utils.py:349] Steps/second: 0.177284, Examples/second: 25.136435\n",
      "I0710 10:26:39.634129 140295626643200 trainer.py:508] step:  6517, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:134.84123 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2924367 log_pplx:3.4225776 loss:135.748 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.72119\n",
      "I0710 10:26:43.535887 140295626643200 summary_utils.py:349] Steps/second: 0.177312, Examples/second: 25.141976\n",
      "I0710 10:26:43.536705 140295626643200 trainer.py:508] step:  6518, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:62.258999 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2940301 log_pplx:3.8240969 loss:97.371071 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.72711\n",
      "I0710 10:26:50.105552 140295626643200 summary_utils.py:349] Steps/second: 0.177297, Examples/second: 25.134356\n",
      "I0710 10:26:50.106359 140295626643200 trainer.py:508] step:  6519, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:77.169815 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2923696 log_pplx:3.4117804 loss:136.68445 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.73303\n",
      "I0710 10:26:53.200045 140295626643200 summary_utils.py:349] Steps/second: 0.177338, Examples/second: 25.150295\n",
      "I0710 10:26:53.201200 140295626643200 trainer.py:508] step:  6520, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:45.316784 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2937261 log_pplx:3.8321869 loss:60.192276 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.73871\n",
      "I0710 10:26:59.267813 140295626643200 summary_utils.py:349] Steps/second: 0.177331, Examples/second: 25.143805\n",
      "I0710 10:26:59.268623 140295626643200 trainer.py:508] step:  6521, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:74.849846 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2925023 log_pplx:3.4134243 loss:134.65958 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.74457\n",
      "I0710 10:27:03.204231 140295626643200 summary_utils.py:349] Steps/second: 0.177358, Examples/second: 25.149257\n",
      "I0710 10:27:03.205047 140295626643200 trainer.py:508] step:  6522, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:66.763664 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2943845 log_pplx:3.8765471 loss:98.003952 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.75006\n",
      "I0710 10:27:09.742504 140295626643200 summary_utils.py:349] Steps/second: 0.177344, Examples/second: 25.141718\n",
      "I0710 10:27:09.743371 140295626643200 trainer.py:508] step:  6523, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:78.737213 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2926133 log_pplx:3.334064 loss:137.48848 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.75586\n",
      "I0710 10:27:15.900793 140295626643200 summary_utils.py:349] Steps/second: 0.177335, Examples/second: 25.135039\n",
      "I0710 10:27:15.901787 140295626643200 trainer.py:508] step:  6524, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:70.661209 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.292315 log_pplx:3.3452232 loss:133.51624 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.76154\n",
      "I0710 10:27:26.401416 140295626643200 summary_utils.py:349] Steps/second: 0.177259, Examples/second: 25.115073\n",
      "I0710 10:27:26.402231 140295626643200 trainer.py:508] step:  6525, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:276.56168 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2914431 log_pplx:2.806885 loss:201.95535 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:525.76709\n",
      "I0710 10:27:30.263710 140295626643200 summary_utils.py:349] Steps/second: 0.177287, Examples/second: 25.120688\n",
      "I0710 10:27:30.264498 140295626643200 trainer.py:508] step:  6526, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:47.744274 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.294085 log_pplx:3.7349329 loss:92.136131 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.77179\n",
      "I0710 10:27:33.368247 140295626643200 summary_utils.py:349] Steps/second: 0.177327, Examples/second: 25.136550\n",
      "I0710 10:27:33.369047 140295626643200 trainer.py:508] step:  6527, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:56.854832 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2938724 log_pplx:3.7721245 loss:58.585812 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.77637\n",
      "I0710 10:27:39.737388 140295626643200 summary_utils.py:349] Steps/second: 0.177315, Examples/second: 25.129417\n",
      "I0710 10:27:39.738220 140295626643200 trainer.py:508] step:  6528, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:95.798874 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2922461 log_pplx:3.4118738 loss:134.04399 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.7807\n",
      "I0710 10:27:41.950534 140295626643200 summary_utils.py:349] Steps/second: 0.177369, Examples/second: 25.170054\n",
      "I0710 10:27:41.951428 140295626643200 trainer.py:508] step:  6529, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:22.167368 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2927537 log_pplx:3.9355264 loss:28.90152 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:525.78564\n",
      "I0710 10:27:48.351069 140295626643200 summary_utils.py:349] Steps/second: 0.177357, Examples/second: 25.162837\n",
      "I0710 10:27:48.351864 140295626643200 trainer.py:508] step:  6530, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:132.01907 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2926915 log_pplx:3.4818347 loss:139.27339 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.79028\n",
      "I0710 10:27:52.207912 140295626643200 summary_utils.py:349] Steps/second: 0.177385, Examples/second: 25.168437\n",
      "I0710 10:27:52.208757 140295626643200 trainer.py:508] step:  6531, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:39.586727 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2945256 log_pplx:3.7828767 loss:95.304855 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.79596\n",
      "I0710 10:27:58.090583 140295626643200 summary_utils.py:349] Steps/second: 0.177381, Examples/second: 25.162385\n",
      "I0710 10:27:58.091354 140295626643200 trainer.py:508] step:  6532, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:82.675598 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2926115 log_pplx:3.3662641 loss:133.68277 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.80145\n",
      "I0710 10:28:01.140383 140295626643200 summary_utils.py:349] Steps/second: 0.177422, Examples/second: 25.178320\n",
      "I0710 10:28:01.141159 140295626643200 trainer.py:508] step:  6533, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:25.189638 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2936397 log_pplx:3.7113929 loss:56.743721 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.80725\n",
      "I0710 10:28:07.385674 140295626643200 summary_utils.py:349] Steps/second: 0.177413, Examples/second: 25.171457\n",
      "I0710 10:28:07.386476 140295626643200 trainer.py:508] step:  6534, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:76.87339 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2926531 log_pplx:3.3710144 loss:134.71416 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.81244\n",
      "I0710 10:28:11.231737 140295626643200 summary_utils.py:349] Steps/second: 0.177441, Examples/second: 25.177069\n",
      "I0710 10:28:11.232590 140295626643200 trainer.py:508] step:  6535, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:58.615917 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.294687 log_pplx:3.8136859 loss:96.867615 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.81757\n",
      "I0710 10:28:17.700121 140295626643200 summary_utils.py:349] Steps/second: 0.177428, Examples/second: 25.169715\n",
      "I0710 10:28:17.701095 140295626643200 trainer.py:508] step:  6536, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:75.003914 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2930433 log_pplx:3.2976024 loss:134.25363 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.82196\n",
      "I0710 10:28:26.880477 140295626643200 summary_utils.py:349] Steps/second: 0.177372, Examples/second: 25.152770\n",
      "I0710 10:28:26.881288 140295626643200 base_runner.py:111] step:  6537, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:182.85974 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2920588 log_pplx:2.8527715 loss:198.69554 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:525.82642\n",
      "I0710 10:28:33.457677 140295626643200 summary_utils.py:349] Steps/second: 0.177357, Examples/second: 25.145197\n",
      "I0710 10:28:33.458567 140295626643200 trainer.py:508] step:  6538, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:72.961594 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.292474 log_pplx:3.3760059 loss:132.04404 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.83209\n",
      "I0710 10:28:37.310506 140295626643200 summary_utils.py:349] Steps/second: 0.177385, Examples/second: 25.150790\n",
      "I0710 10:28:37.311327 140295626643200 trainer.py:508] step:  6539, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:40.21196 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2947954 log_pplx:3.7442861 loss:93.513542 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.83765\n",
      "I0710 10:28:40.399078 140295626643200 summary_utils.py:349] Steps/second: 0.177425, Examples/second: 25.166586\n",
      "I0710 10:28:40.399869 140295626643200 trainer.py:508] step:  6540, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:51.064262 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2940146 log_pplx:3.7527349 loss:57.537048 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.84308\n",
      "I0710 10:28:46.919615 140295626643200 summary_utils.py:349] Steps/second: 0.177411, Examples/second: 25.159140\n",
      "I0710 10:28:46.920696 140295626643200 trainer.py:508] step:  6541, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:89.818298 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.29329 log_pplx:3.3323681 loss:137.9184 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.84857\n",
      "I0710 10:28:53.380820 140295626643200 summary_utils.py:349] Steps/second: 0.177399, Examples/second: 25.151836\n",
      "I0710 10:28:53.381611 140295626643200 trainer.py:508] step:  6542, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:73.646179 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2933178 log_pplx:3.4022055 loss:137.57669 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.85388\n",
      "I0710 10:28:57.231391 140295626643200 summary_utils.py:349] Steps/second: 0.177427, Examples/second: 25.157421\n",
      "I0710 10:28:57.232312 140295626643200 trainer.py:508] step:  6543, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:37.39735 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2947544 log_pplx:3.7797937 loss:95.368919 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.85919\n",
      "I0710 10:29:03.393968 140295626643200 summary_utils.py:349] Steps/second: 0.177418, Examples/second: 25.150788\n",
      "I0710 10:29:03.394807 140295626643200 trainer.py:508] step:  6544, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:75.633469 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2929547 log_pplx:3.3612466 loss:137.81111 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.86426\n",
      "I0710 10:29:06.433541 140295626643200 summary_utils.py:349] Steps/second: 0.177459, Examples/second: 25.166656\n",
      "I0710 10:29:06.434361 140295626643200 trainer.py:508] step:  6545, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:24.552652 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.294161 log_pplx:3.6357296 loss:56.794071 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.86938\n",
      "I0710 10:29:12.587052 140295626643200 summary_utils.py:349] Steps/second: 0.177451, Examples/second: 25.160044\n",
      "I0710 10:29:12.587859 140295626643200 trainer.py:508] step:  6546, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:72.439552 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2928176 log_pplx:3.3433332 loss:131.93628 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.87439\n",
      "I0710 10:29:21.902536 140295626643200 summary_utils.py:349] Steps/second: 0.177393, Examples/second: 25.142889\n",
      "I0710 10:29:21.903319 140295626643200 trainer.py:508] step:  6547, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:165.96184 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.292141 log_pplx:2.7125435 loss:196.72723 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:525.87939\n",
      "I0710 10:29:25.742385 140295626643200 summary_utils.py:349] Steps/second: 0.177421, Examples/second: 25.148487\n",
      "I0710 10:29:25.743217 140295626643200 trainer.py:508] step:  6548, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:39.58902 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2947519 log_pplx:3.6669071 loss:91.512253 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.88489\n",
      "I0710 10:29:27.719546 140295635035904 trainer.py:345] Write summary @6548\n",
      "I0710 10:29:37.680099 140295626643200 summary_utils.py:349] Steps/second: 0.177323, Examples/second: 25.129079\n",
      "I0710 10:29:37.681957 140295626643200 trainer.py:508] step:  6549, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:67.955383 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.293141 log_pplx:3.23698 loss:135.66992 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.89026\n",
      "I0710 10:29:47.486471 140295626643200 summary_utils.py:349] Steps/second: 0.177258, Examples/second: 25.114424\n",
      "I0710 10:29:47.487560 140295626643200 trainer.py:508] step:  6550, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:76.397766 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2932878 log_pplx:3.2908812 loss:134.14455 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.89581\n",
      "I0710 10:29:50.283253 140295626643200 summary_utils.py:349] Steps/second: 0.177302, Examples/second: 25.153315\n",
      "I0710 10:29:50.285399 140295626643200 trainer.py:508] step:  6551, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:11.860515 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2929738 log_pplx:3.7165964 loss:27.482489 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:525.90106\n",
      "I0710 10:29:54.622584 140295626643200 summary_utils.py:349] Steps/second: 0.177322, Examples/second: 25.166238\n",
      "I0710 10:29:54.623704 140295626643200 trainer.py:508] step:  6552, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:25.987623 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2943401 log_pplx:3.7033963 loss:57.981304 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.90607\n",
      "I0710 10:30:05.078487 140295626643200 summary_utils.py:349] Steps/second: 0.177247, Examples/second: 25.150137\n",
      "I0710 10:30:05.080680 140295626643200 trainer.py:508] step:  6553, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:74.788719 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2933934 log_pplx:3.3662336 loss:134.31271 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.91083\n",
      "I0710 10:30:08.637316 140295635035904 trainer.py:354] Write summary done: step 6548\n",
      "I0710 10:30:10.689098 140295626643200 summary_utils.py:349] Steps/second: 0.177248, Examples/second: 25.151797\n",
      "I0710 10:30:10.689852 140295626643200 trainer.py:508] step:  6554, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:38.967438 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2950503 log_pplx:3.6769807 loss:94.222641 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.91559\n",
      "I0710 10:30:17.072740 140295626643200 summary_utils.py:349] Steps/second: 0.177236, Examples/second: 25.144721\n",
      "I0710 10:30:17.073544 140295626643200 trainer.py:508] step:  6555, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:69.31916 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2933342 log_pplx:3.2069035 loss:131.12228 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.92035\n",
      "I0710 10:30:27.279946 140295626643200 summary_utils.py:349] Steps/second: 0.177165, Examples/second: 25.125710\n",
      "I0710 10:30:27.280742 140295626643200 trainer.py:508] step:  6556, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:209.3593 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2922108 log_pplx:2.7310836 loss:199.71048 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:525.92505\n",
      "I0710 10:30:33.767049 140295626643200 summary_utils.py:349] Steps/second: 0.177152, Examples/second: 25.118431\n",
      "I0710 10:30:33.767963 140295626643200 trainer.py:508] step:  6557, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:73.470512 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2929316 log_pplx:3.3289187 loss:129.03719 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.92987\n",
      "I0710 10:30:37.601345 140295626643200 summary_utils.py:349] Steps/second: 0.177180, Examples/second: 25.124015\n",
      "I0710 10:30:37.602169 140295626643200 trainer.py:508] step:  6558, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:47.294109 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2950366 log_pplx:3.7480788 loss:93.819099 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.93512\n",
      "I0710 10:30:40.702081 140295626643200 summary_utils.py:349] Steps/second: 0.177220, Examples/second: 25.139624\n",
      "I0710 10:30:40.702875 140295626643200 trainer.py:508] step:  6559, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:21.703094 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2948842 log_pplx:3.7479751 loss:58.869568 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.94055\n",
      "I0710 10:30:47.008665 140295626643200 summary_utils.py:349] Steps/second: 0.177210, Examples/second: 25.132744\n",
      "I0710 10:30:47.009441 140295626643200 trainer.py:508] step:  6560, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:79.630943 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2936057 log_pplx:3.439307 loss:137.87323 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.9458\n",
      "I0710 10:30:53.390162 140295626643200 summary_utils.py:349] Steps/second: 0.177198, Examples/second: 25.125706\n",
      "I0710 10:30:53.391139 140295626643200 trainer.py:508] step:  6561, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:78.160675 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2935441 log_pplx:3.3878129 loss:137.12173 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.95129\n",
      "I0710 10:30:57.415702 140295626643200 summary_utils.py:349] Steps/second: 0.177223, Examples/second: 25.130857\n",
      "I0710 10:30:57.416527 140295626643200 trainer.py:508] step:  6562, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:42.0821 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2952211 log_pplx:3.7865207 loss:95.041672 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.95715\n",
      "I0710 10:31:04.371122 140295626643200 summary_utils.py:349] Steps/second: 0.177203, Examples/second: 25.122566\n",
      "I0710 10:31:04.372044 140295626643200 trainer.py:508] step:  6563, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:71.7425 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.293964 log_pplx:3.3243997 loss:138.62747 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.96265\n",
      "I0710 10:31:07.499211 140295626643200 summary_utils.py:349] Steps/second: 0.177242, Examples/second: 25.138079\n",
      "I0710 10:31:07.500017 140295626643200 trainer.py:508] step:  6564, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:24.015198 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2945709 log_pplx:3.6636901 loss:56.701324 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.9682\n",
      "I0710 10:31:13.999411 140295626643200 summary_utils.py:349] Steps/second: 0.177228, Examples/second: 25.130791\n",
      "I0710 10:31:14.000456 140295626643200 trainer.py:508] step:  6565, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:76.370674 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2937657 log_pplx:3.3592997 loss:140.33475 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.97339\n",
      "I0710 10:31:17.903376 140295626643200 summary_utils.py:349] Steps/second: 0.177255, Examples/second: 25.136197\n",
      "I0710 10:31:17.904177 140295626643200 trainer.py:508] step:  6566, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:48.137527 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2950858 log_pplx:3.7037697 loss:92.339607 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:525.97876\n",
      "I0710 10:31:26.903064 140295626643200 summary_utils.py:349] Steps/second: 0.177203, Examples/second: 25.119942\n",
      "I0710 10:31:26.903877 140295626643200 trainer.py:508] step:  6567, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:192.44386 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2924207 log_pplx:2.6964419 loss:191.58218 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:525.98358\n",
      "I0710 10:31:33.294539 140295626643200 summary_utils.py:349] Steps/second: 0.177192, Examples/second: 25.112914\n",
      "I0710 10:31:33.295315 140295626643200 trainer.py:508] step:  6568, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:72.029282 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2937298 log_pplx:3.3032875 loss:135.51736 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.98895\n",
      "I0710 10:31:36.416284 140295626643200 summary_utils.py:349] Steps/second: 0.177231, Examples/second: 25.128405\n",
      "I0710 10:31:36.417043 140295626643200 trainer.py:508] step:  6569, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:22.889351 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2948354 log_pplx:3.6523936 loss:56.398098 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:525.99426\n",
      "I0710 10:31:43.018027 140295626643200 summary_utils.py:349] Steps/second: 0.177216, Examples/second: 25.120918\n",
      "I0710 10:31:43.018800 140295626643200 trainer.py:508] step:  6570, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:93.500839 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2938097 log_pplx:3.3813052 loss:137.78818 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:525.99927\n",
      "I0710 10:31:46.816313 140295626643200 summary_utils.py:349] Steps/second: 0.177245, Examples/second: 25.126546\n",
      "I0710 10:31:46.817096 140295626643200 trainer.py:508] step:  6571, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:39.077042 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2955438 log_pplx:3.7576735 loss:93.988808 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.00409\n",
      "I0710 10:31:53.017721 140295626643200 summary_utils.py:349] Steps/second: 0.177236, Examples/second: 25.119942\n",
      "I0710 10:31:53.018512 140295626643200 trainer.py:508] step:  6572, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:73.896194 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2936182 log_pplx:3.2557261 loss:129.08952 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.00867\n",
      "I0710 10:31:55.205317 140295626643200 summary_utils.py:349] Steps/second: 0.177289, Examples/second: 25.159741\n",
      "I0710 10:31:55.206147 140295626643200 trainer.py:508] step:  6573, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:11.783264 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2934852 log_pplx:3.6940005 loss:27.257683 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:526.01331\n",
      "I0710 10:32:01.444945 140295626643200 summary_utils.py:349] Steps/second: 0.177280, Examples/second: 25.153040\n",
      "I0710 10:32:01.445704 140295626643200 trainer.py:508] step:  6574, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:73.302765 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2938182 log_pplx:3.3350191 loss:132.06676 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.01776\n",
      "I0710 10:32:05.388803 140295626643200 summary_utils.py:349] Steps/second: 0.177306, Examples/second: 25.158330\n",
      "I0710 10:32:05.389747 140295626643200 trainer.py:508] step:  6575, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:54.420902 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2955018 log_pplx:3.7196734 loss:92.898842 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.02252\n",
      "I0710 10:32:08.516264 140295626643200 summary_utils.py:349] Steps/second: 0.177345, Examples/second: 25.173753\n",
      "I0710 10:32:08.517284 140295626643200 trainer.py:508] step:  6576, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:23.719212 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2950708 log_pplx:3.6509619 loss:56.860874 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.02771\n",
      "I0710 10:32:14.756323 140295626643200 summary_utils.py:349] Steps/second: 0.177336, Examples/second: 25.167052\n",
      "I0710 10:32:14.757137 140295626643200 trainer.py:508] step:  6577, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:76.640312 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2935429 log_pplx:3.2515981 loss:129.45425 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.03278\n",
      "I0710 10:32:25.417268 140295626643200 summary_utils.py:349] Steps/second: 0.177258, Examples/second: 25.147224\n",
      "I0710 10:32:25.418107 140295626643200 trainer.py:508] step:  6578, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:217.07545 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2933708 log_pplx:2.8418436 loss:218.39571 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:526.0379\n",
      "I0710 10:32:31.796444 140295626643200 summary_utils.py:349] Steps/second: 0.177247, Examples/second: 25.140243\n",
      "I0710 10:32:31.797222 140295626643200 trainer.py:508] step:  6579, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:69.85289 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2939503 log_pplx:3.2701623 loss:134.64894 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.0437\n",
      "I0710 10:32:38.137782 140295626643200 summary_utils.py:349] Steps/second: 0.177236, Examples/second: 25.133353\n",
      "I0710 10:32:38.138775 140295626643200 trainer.py:508] step:  6580, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:71.888016 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2937889 log_pplx:3.3154719 loss:133.90364 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.0495\n",
      "I0710 10:32:42.073253 140295626643200 summary_utils.py:349] Steps/second: 0.177262, Examples/second: 25.138651\n",
      "I0710 10:32:42.074083 140295626643200 trainer.py:508] step:  6581, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:46.951519 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2955772 log_pplx:3.7303827 loss:93.212936 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.05542\n",
      "I0710 10:32:45.118119 140295626643200 summary_utils.py:349] Steps/second: 0.177302, Examples/second: 25.154210\n",
      "I0710 10:32:45.118890 140295626643200 trainer.py:508] step:  6582, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:46.969646 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2954599 log_pplx:3.7397747 loss:59.091362 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.06085\n",
      "I0710 10:32:48.785060 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 10:32:51.977813 140295626643200 summary_utils.py:349] Steps/second: 0.177284, Examples/second: 25.146190\n",
      "I0710 10:32:51.978655 140295626643200 trainer.py:508] step:  6583, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:88.346733 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2943919 log_pplx:3.366282 loss:141.76254 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.06567\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 10:32:54.187615 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 10:32:54.660511 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00006582\n",
      "I0710 10:32:58.106404 140295626643200 summary_utils.py:349] Steps/second: 0.177276, Examples/second: 25.139772\n",
      "I0710 10:32:58.107310 140295626643200 trainer.py:508] step:  6584, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:93.228157 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2938805 log_pplx:3.3813856 loss:134.45236 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.07104\n",
      "I0710 10:33:02.043718 140295626643200 summary_utils.py:349] Steps/second: 0.177302, Examples/second: 25.145055\n",
      "I0710 10:33:02.044467 140295626643200 trainer.py:508] step:  6585, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:37.963123 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2956935 log_pplx:3.7474957 loss:95.022438 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.0766\n",
      "I0710 10:33:12.710620 140295626643200 summary_utils.py:349] Steps/second: 0.177225, Examples/second: 25.125315\n",
      "I0710 10:33:12.711494 140295626643200 trainer.py:508] step:  6586, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:324.78464 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2931728 log_pplx:2.7050843 loss:199.70285 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:526.08197\n",
      "I0710 10:33:19.211995 140295626643200 summary_utils.py:349] Steps/second: 0.177212, Examples/second: 25.118110\n",
      "I0710 10:33:19.212800 140295626643200 trainer.py:508] step:  6587, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:88.474213 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2943503 log_pplx:3.3259573 loss:135.61591 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.08868\n",
      "I0710 10:33:22.318384 140295626643200 summary_utils.py:349] Steps/second: 0.177251, Examples/second: 25.133491\n",
      "I0710 10:33:22.319168 140295626643200 trainer.py:508] step:  6588, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:27.715176 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2957153 log_pplx:3.7644777 loss:60.025768 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.09515\n",
      "I0710 10:33:28.653930 140295626643200 summary_utils.py:349] Steps/second: 0.177240, Examples/second: 25.126647\n",
      "I0710 10:33:28.654777 140295626643200 trainer.py:508] step:  6589, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:204.2695 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2942692 log_pplx:3.445889 loss:139.17085 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.10126\n",
      "I0710 10:33:32.529965 140295626643200 summary_utils.py:349] Steps/second: 0.177267, Examples/second: 25.132053\n",
      "I0710 10:33:32.530786 140295626643200 trainer.py:508] step:  6590, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:41.83696 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2957187 log_pplx:3.6861188 loss:91.093201 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.10651\n",
      "I0710 10:33:39.020324 140295626643200 summary_utils.py:349] Steps/second: 0.177254, Examples/second: 25.124880\n",
      "I0710 10:33:39.021157 140295626643200 trainer.py:508] step:  6591, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:74.139076 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2940804 log_pplx:3.2392132 loss:127.98943 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.11169\n",
      "I0710 10:33:45.245715 140295626643200 summary_utils.py:349] Steps/second: 0.177245, Examples/second: 25.118290\n",
      "I0710 10:33:45.246542 140295626643200 trainer.py:508] step:  6592, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:96.826447 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2940086 log_pplx:3.3086805 loss:127.96323 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.117\n",
      "I0710 10:33:48.361222 140295626643200 summary_utils.py:349] Steps/second: 0.177284, Examples/second: 25.133616\n",
      "I0710 10:33:48.362097 140295626643200 trainer.py:508] step:  6593, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:75.495979 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2953973 log_pplx:3.8303268 loss:59.220436 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.12305\n",
      "I0710 10:33:52.235338 140295626643200 summary_utils.py:349] Steps/second: 0.177311, Examples/second: 25.139015\n",
      "I0710 10:33:52.236128 140295626643200 trainer.py:508] step:  6594, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:67.642746 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2960248 log_pplx:3.7692494 loss:95.503357 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.12946\n",
      "I0710 10:34:03.663340 140295626643200 summary_utils.py:349] Steps/second: 0.177223, Examples/second: 25.117720\n",
      "I0710 10:34:03.664108 140295626643200 trainer.py:508] step:  6595, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:470.4043 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2934567 log_pplx:2.9872816 loss:226.13721 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:526.13586\n",
      "I0710 10:34:10.064363 140295626643200 summary_utils.py:349] Steps/second: 0.177211, Examples/second: 25.110767\n",
      "I0710 10:34:10.065121 140295626643200 trainer.py:508] step:  6596, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:88.417664 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2945896 log_pplx:3.4533021 loss:141.15372 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.14197\n",
      "I0710 10:34:16.250528 140295626643200 summary_utils.py:349] Steps/second: 0.177203, Examples/second: 25.104286\n",
      "I0710 10:34:16.251300 140295626643200 trainer.py:508] step:  6597, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:97.955055 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.294457 log_pplx:3.392916 loss:136.48004 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.14825\n",
      "I0710 10:34:20.106113 140295626643200 summary_utils.py:349] Steps/second: 0.177230, Examples/second: 25.109722\n",
      "I0710 10:34:20.107197 140295626643200 trainer.py:508] step:  6598, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:50.36158 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2961718 log_pplx:3.7752044 loss:95.276718 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.15491\n",
      "I0710 10:34:22.294502 140295626643200 summary_utils.py:349] Steps/second: 0.177283, Examples/second: 25.149017\n",
      "I0710 10:34:22.295307 140295626643200 trainer.py:508] step:  6599, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:35.581501 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2939532 log_pplx:3.8293674 loss:27.508621 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:526.16107\n",
      "I0710 10:34:25.378471 140295626643200 summary_utils.py:349] Steps/second: 0.177322, Examples/second: 25.164355\n",
      "I0710 10:34:25.379238 140295626643200 trainer.py:508] step:  6600, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:54.643646 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2959883 log_pplx:3.8069842 loss:60.123581 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.16675\n",
      "I0710 10:34:31.986339 140295626643200 summary_utils.py:349] Steps/second: 0.177307, Examples/second: 25.156943\n",
      "I0710 10:34:31.987161 140295626643200 trainer.py:508] step:  6601, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:128.74243 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2947326 log_pplx:3.3900206 loss:137.67722 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.17188\n",
      "I0710 10:34:38.559145 140295626643200 summary_utils.py:349] Steps/second: 0.177293, Examples/second: 25.149615\n",
      "I0710 10:34:38.559993 140295626643200 trainer.py:508] step:  6602, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:196.68497 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2948426 log_pplx:3.4259429 loss:142.99031 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.17786\n",
      "I0710 10:34:42.384070 140295626643200 summary_utils.py:349] Steps/second: 0.177320, Examples/second: 25.155091\n",
      "I0710 10:34:42.384888 140295626643200 trainer.py:508] step:  6603, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:61.514427 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2962904 log_pplx:3.794209 loss:94.523232 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.18427\n",
      "I0710 10:34:48.639611 140295626643200 summary_utils.py:349] Steps/second: 0.177311, Examples/second: 25.148455\n",
      "I0710 10:34:48.640373 140295626643200 trainer.py:508] step:  6604, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:92.771225 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2949769 log_pplx:3.4192848 loss:143.73819 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.18994\n",
      "I0710 10:34:51.706469 140295626643200 summary_utils.py:349] Steps/second: 0.177350, Examples/second: 25.163795\n",
      "I0710 10:34:51.707242 140295626643200 trainer.py:508] step:  6605, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:29.748312 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2959923 log_pplx:3.7543681 loss:58.969978 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.19574\n",
      "I0710 10:34:58.065460 140295626643200 summary_utils.py:349] Steps/second: 0.177339, Examples/second: 25.156937\n",
      "I0710 10:34:58.066280 140295626643200 trainer.py:508] step:  6606, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:85.041733 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2948878 log_pplx:3.3381555 loss:140.24426 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.20117\n",
      "I0710 10:35:08.230654 140295626643200 summary_utils.py:349] Steps/second: 0.177270, Examples/second: 25.138462\n",
      "I0710 10:35:08.231621 140295626643200 trainer.py:508] step:  6607, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:245.96143 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.293674 log_pplx:2.8164356 loss:197.36172 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:526.20648\n",
      "I0710 10:35:14.867997 140295626643200 summary_utils.py:349] Steps/second: 0.177255, Examples/second: 25.131029\n",
      "I0710 10:35:14.868820 140295626643200 trainer.py:508] step:  6608, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:127.56692 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.294883 log_pplx:3.3963072 loss:142.77226 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.21277\n",
      "I0710 10:35:18.690627 140295626643200 summary_utils.py:349] Steps/second: 0.177283, Examples/second: 25.136499\n",
      "I0710 10:35:18.691478 140295626643200 trainer.py:508] step:  6609, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:64.513634 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.296293 log_pplx:3.7360992 loss:93.916206 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.21863\n",
      "I0710 10:35:25.032069 140295626643200 summary_utils.py:349] Steps/second: 0.177272, Examples/second: 25.129709\n",
      "I0710 10:35:25.032883 140295626643200 trainer.py:508] step:  6610, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:109.33803 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2948469 log_pplx:3.397902 loss:137.02039 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.22412\n",
      "I0710 10:35:28.106544 140295626643200 summary_utils.py:349] Steps/second: 0.177311, Examples/second: 25.144990\n",
      "I0710 10:35:28.107351 140295626643200 trainer.py:508] step:  6611, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:58.752701 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2961596 log_pplx:3.8586063 loss:59.672749 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.22925\n",
      "I0710 10:35:34.603328 140295626643200 summary_utils.py:349] Steps/second: 0.177298, Examples/second: 25.137867\n",
      "I0710 10:35:34.604133 140295626643200 trainer.py:508] step:  6612, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:92.777931 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2949698 log_pplx:3.3584859 loss:136.48047 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.23462\n",
      "I0710 10:35:38.443170 140295626643200 summary_utils.py:349] Steps/second: 0.177325, Examples/second: 25.143289\n",
      "I0710 10:35:38.443960 140295626643200 trainer.py:508] step:  6613, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:83.930298 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2962022 log_pplx:3.8178198 loss:95.278458 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.24048\n",
      "I0710 10:35:44.825459 140295626643200 summary_utils.py:349] Steps/second: 0.177314, Examples/second: 25.136419\n",
      "I0710 10:35:44.826289 140295626643200 trainer.py:508] step:  6614, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:607.27148 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2950475 log_pplx:3.3372586 loss:136.82761 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.2467\n",
      "I0710 10:35:51.311850 140295626643200 summary_utils.py:349] Steps/second: 0.177301, Examples/second: 25.129333\n",
      "I0710 10:35:51.312624 140295626643200 trainer.py:508] step:  6615, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:90.75531 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.295035 log_pplx:3.4421897 loss:139.23657 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.25195\n",
      "I0710 10:35:55.194365 140295626643200 summary_utils.py:349] Steps/second: 0.177328, Examples/second: 25.134659\n",
      "I0710 10:35:55.195192 140295626643200 trainer.py:508] step:  6616, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:55.669193 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2965277 log_pplx:3.7918556 loss:95.270378 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.25763\n",
      "I0710 10:36:04.441689 140295626643200 summary_utils.py:349] Steps/second: 0.177273, Examples/second: 25.118258\n",
      "I0710 10:36:04.442522 140295626643200 trainer.py:508] step:  6617, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:336.89697 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2942275 log_pplx:2.7776499 loss:204.01839 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:526.26361\n",
      "I0710 10:36:07.652288 140295626643200 summary_utils.py:349] Steps/second: 0.177310, Examples/second: 25.133200\n",
      "I0710 10:36:07.653108 140295626643200 trainer.py:508] step:  6618, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:33.445145 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2962338 log_pplx:3.7251689 loss:57.114407 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.27032\n",
      "I0710 10:36:14.465997 140295626643200 summary_utils.py:349] Steps/second: 0.177292, Examples/second: 25.125429\n",
      "I0710 10:36:14.466840 140295626643200 trainer.py:508] step:  6619, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:85.791977 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2949215 log_pplx:3.3424394 loss:137.45784 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.27673\n",
      "I0710 10:36:20.851303 140295626643200 summary_utils.py:349] Steps/second: 0.177281, Examples/second: 25.118584\n",
      "I0710 10:36:20.852125 140295626643200 trainer.py:508] step:  6620, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:98.545586 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2953895 log_pplx:3.4224975 loss:141.47751 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.2829\n",
      "I0710 10:36:24.728817 140295626643200 summary_utils.py:349] Steps/second: 0.177308, Examples/second: 25.123910\n",
      "I0710 10:36:24.729812 140295626643200 trainer.py:508] step:  6621, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:76.279915 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2969279 log_pplx:3.8578043 loss:97.626556 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.28918\n",
      "I0710 10:36:26.946540 140295626643200 summary_utils.py:349] Steps/second: 0.177359, Examples/second: 25.162723\n",
      "I0710 10:36:26.947323 140295626643200 trainer.py:508] step:  6622, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:15.422029 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2949817 log_pplx:3.7287152 loss:27.58667 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:526.29486\n",
      "I0710 10:36:33.319362 140295626643200 summary_utils.py:349] Steps/second: 0.177348, Examples/second: 25.155891\n",
      "I0710 10:36:33.320119 140295626643200 trainer.py:508] step:  6623, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:83.324364 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2949589 log_pplx:3.3612008 loss:132.38928 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.30042\n",
      "I0710 10:36:36.382978 140295626643200 summary_utils.py:349] Steps/second: 0.177387, Examples/second: 25.171100\n",
      "I0710 10:36:36.383743 140295626643200 trainer.py:508] step:  6624, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:36.850014 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2964872 log_pplx:3.7941968 loss:60.484833 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.30591\n",
      "I0710 10:36:42.512267 140295626643200 summary_utils.py:349] Steps/second: 0.177380, Examples/second: 25.164789\n",
      "I0710 10:36:42.513089 140295626643200 trainer.py:508] step:  6625, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:91.650505 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.295079 log_pplx:3.4014683 loss:136.39888 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.31097\n",
      "I0710 10:36:46.326213 140295626643200 summary_utils.py:349] Steps/second: 0.177407, Examples/second: 25.170227\n",
      "I0710 10:36:46.326998 140295626643200 trainer.py:508] step:  6626, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:46.022491 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2971281 log_pplx:3.7429981 loss:93.7855 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.31628\n",
      "I0710 10:36:52.454775 140295626643200 summary_utils.py:349] Steps/second: 0.177400, Examples/second: 25.163924\n",
      "I0710 10:36:52.455521 140295626643200 trainer.py:508] step:  6627, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:111.92107 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2952299 log_pplx:3.4098358 loss:138.69508 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.32153\n",
      "I0710 10:36:58.812096 140295626643200 summary_utils.py:349] Steps/second: 0.177389, Examples/second: 25.157139\n",
      "I0710 10:36:58.812877 140295626643200 trainer.py:508] step:  6628, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:131.33148 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2956297 log_pplx:3.4403987 loss:142.3035 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.32758\n",
      "I0710 10:37:10.143302 140295626643200 summary_utils.py:349] Steps/second: 0.177303, Examples/second: 25.136369\n",
      "I0710 10:37:10.144136 140295626643200 trainer.py:508] step:  6629, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:263.12592 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.294389 log_pplx:2.7942617 loss:208.03276 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:526.3335\n",
      "I0710 10:37:13.185558 140295626643200 summary_utils.py:349] Steps/second: 0.177342, Examples/second: 25.151581\n",
      "I0710 10:37:13.186341 140295626643200 trainer.py:508] step:  6630, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:26.270079 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.296676 log_pplx:3.7886844 loss:59.227795 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.33868\n",
      "I0710 10:37:17.032057 140295626643200 summary_utils.py:349] Steps/second: 0.177369, Examples/second: 25.156939\n",
      "I0710 10:37:17.032902 140295626643200 trainer.py:508] step:  6631, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:53.351566 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.297129 log_pplx:3.7507439 loss:95.479874 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.34363\n",
      "I0710 10:37:23.348891 140295626643200 summary_utils.py:349] Steps/second: 0.177359, Examples/second: 25.150258\n",
      "I0710 10:37:23.349678 140295626643200 trainer.py:508] step:  6632, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:125.89537 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2955034 log_pplx:3.5041428 loss:141.65497 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.34851\n",
      "I0710 10:37:29.684846 140295626643200 summary_utils.py:349] Steps/second: 0.177348, Examples/second: 25.143544\n",
      "I0710 10:37:29.685618 140295626643200 trainer.py:508] step:  6633, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:100.73341 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2956573 log_pplx:3.3544486 loss:138.11942 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.35437\n",
      "I0710 10:37:33.646042 140295626643200 summary_utils.py:349] Steps/second: 0.177374, Examples/second: 25.148653\n",
      "I0710 10:37:33.646864 140295626643200 trainer.py:508] step:  6634, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:44.651489 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2971157 log_pplx:3.6607559 loss:92.548485 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.36072\n",
      "I0710 10:37:40.171143 140295626643200 summary_utils.py:349] Steps/second: 0.177360, Examples/second: 25.141544\n",
      "I0710 10:37:40.171944 140295626643200 trainer.py:508] step:  6635, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:94.4198 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2958866 log_pplx:3.433723 loss:144.21635 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.36658\n",
      "I0710 10:37:43.284713 140295626643200 summary_utils.py:349] Steps/second: 0.177398, Examples/second: 25.156564\n",
      "I0710 10:37:43.285528 140295626643200 trainer.py:508] step:  6636, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:22.554399 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2967551 log_pplx:3.7313643 loss:57.806995 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.37244\n",
      "I0710 10:37:49.565994 140295626643200 summary_utils.py:349] Steps/second: 0.177389, Examples/second: 25.149974\n",
      "I0710 10:37:49.566768 140295626643200 base_runner.py:111] step:  6637, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:86.175194 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2952678 log_pplx:3.3609366 loss:132.37889 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.37799\n",
      "I0710 10:37:55.779503 140295626643200 summary_utils.py:349] Steps/second: 0.177380, Examples/second: 25.143535\n",
      "I0710 10:37:55.780247 140295626643200 trainer.py:508] step:  6638, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:87.800224 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.295314 log_pplx:3.3389559 loss:132.43134 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.38367\n",
      "I0710 10:37:59.623408 140295626643200 summary_utils.py:349] Steps/second: 0.177407, Examples/second: 25.148881\n",
      "I0710 10:37:59.624197 140295626643200 trainer.py:508] step:  6639, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:43.46629 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2970326 log_pplx:3.7493923 loss:93.687943 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.38922\n",
      "I0710 10:38:05.800105 140295626643200 summary_utils.py:349] Steps/second: 0.177399, Examples/second: 25.142526\n",
      "I0710 10:38:05.801159 140295626643200 trainer.py:508] step:  6640, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:72.438606 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2955515 log_pplx:3.3478696 loss:134.20773 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.39453\n",
      "I0710 10:38:16.523305 140295626643200 summary_utils.py:349] Steps/second: 0.177323, Examples/second: 25.123175\n",
      "I0710 10:38:16.524076 140295626643200 trainer.py:508] step:  6641, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:207.72324 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2949812 log_pplx:2.7913809 loss:201.18877 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:526.3999\n",
      "I0710 10:38:19.566033 140295626643200 summary_utils.py:349] Steps/second: 0.177361, Examples/second: 25.138304\n",
      "I0710 10:38:19.566823 140295626643200 trainer.py:508] step:  6642, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:23.884912 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2970446 log_pplx:3.7551897 loss:58.102764 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.40674\n",
      "I0710 10:38:25.870316 140295626643200 summary_utils.py:349] Steps/second: 0.177351, Examples/second: 25.131695\n",
      "I0710 10:38:25.871114 140295626643200 trainer.py:508] step:  6643, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:94.616051 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2955519 log_pplx:3.3808792 loss:136.92561 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.41315\n",
      "I0710 10:38:29.742580 140295626643200 summary_utils.py:349] Steps/second: 0.177378, Examples/second: 25.136972\n",
      "I0710 10:38:29.743668 140295626643200 trainer.py:508] step:  6644, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:46.375793 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2974584 log_pplx:3.6975603 loss:93.247856 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.41913\n",
      "I0710 10:38:35.896614 140295626643200 summary_utils.py:349] Steps/second: 0.177370, Examples/second: 25.130688\n",
      "I0710 10:38:35.897391 140295626643200 trainer.py:508] step:  6645, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:113.42904 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2958245 log_pplx:3.4368112 loss:135.92587 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.42456\n",
      "I0710 10:38:41.802994 140295626643200 summary_utils.py:349] Steps/second: 0.177366, Examples/second: 25.124932\n",
      "I0710 10:38:41.803912 140295626643200 trainer.py:508] step:  6646, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:72.68428 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2955856 log_pplx:3.2719085 loss:128.50423 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.42957\n",
      "I0710 10:38:44.013155 140295626643200 summary_utils.py:349] Steps/second: 0.177417, Examples/second: 25.163314\n",
      "I0710 10:38:44.013966 140295626643200 trainer.py:508] step:  6647, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:10.640682 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.295946 log_pplx:3.6991484 loss:27.425718 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:526.43439\n",
      "I0710 10:38:47.088938 140295626643200 summary_utils.py:349] Steps/second: 0.177455, Examples/second: 25.178328\n",
      "I0710 10:38:47.090021 140295626643200 trainer.py:508] step:  6648, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:38.641182 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2972034 log_pplx:3.7454901 loss:58.713478 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.43896\n",
      "I0710 10:38:49.093361 140295635035904 trainer.py:345] Write summary @6648\n",
      "I0710 10:38:54.804949 140295626643200 summary_utils.py:349] Steps/second: 0.177424, Examples/second: 25.175449\n",
      "I0710 10:38:54.806239 140295626643200 trainer.py:508] step:  6649, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:73.930984 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2980829 log_pplx:3.8039379 loss:97.333267 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.4436\n",
      "I0710 10:39:03.943010 140295626643200 summary_utils.py:349] Steps/second: 0.177372, Examples/second: 25.162853\n",
      "I0710 10:39:03.944319 140295626643200 trainer.py:508] step:  6650, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:82.210762 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2960832 log_pplx:3.3352938 loss:136.41353 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.44861\n",
      "I0710 10:39:13.633348 140295626643200 summary_utils.py:349] Steps/second: 0.177312, Examples/second: 25.149111\n",
      "I0710 10:39:13.634596 140295626643200 trainer.py:508] step:  6651, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:89.079338 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2960359 log_pplx:3.3751864 loss:134.75432 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.45374\n",
      "I0710 10:39:31.599140 140295626643200 summary_utils.py:349] Steps/second: 0.177129, Examples/second: 25.114621\n",
      "I0710 10:39:31.600337 140295626643200 trainer.py:508] step:  6652, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:176.34465 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2950897 log_pplx:2.779335 loss:199.69524 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:526.45892\n",
      "I0710 10:39:34.002738 140295635035904 trainer.py:354] Write summary done: step 6648\n",
      "I0710 10:39:38.781991 140295626643200 summary_utils.py:349] Steps/second: 0.177106, Examples/second: 25.106218\n",
      "I0710 10:39:38.782883 140295626643200 trainer.py:508] step:  6653, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:73.675674 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2958741 log_pplx:3.3414321 loss:133.86613 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.46393\n",
      "I0710 10:39:42.569083 140295626643200 summary_utils.py:349] Steps/second: 0.177134, Examples/second: 25.111650\n",
      "I0710 10:39:42.569838 140295626643200 trainer.py:508] step:  6654, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:67.463051 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2976663 log_pplx:3.7406895 loss:94.312134 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.46881\n",
      "I0710 10:39:45.612424 140295626643200 summary_utils.py:349] Steps/second: 0.177172, Examples/second: 25.126672\n",
      "I0710 10:39:45.613202 140295626643200 trainer.py:508] step:  6655, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:68.4748 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.297207 log_pplx:3.7355707 loss:58.032673 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.47327\n",
      "I0710 10:39:51.934824 140295626643200 summary_utils.py:349] Steps/second: 0.177162, Examples/second: 25.120078\n",
      "I0710 10:39:51.935600 140295626643200 trainer.py:508] step:  6656, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:85.757385 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2956241 log_pplx:3.397877 loss:132.72958 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.47729\n",
      "I0710 10:39:55.743795 140295626643200 summary_utils.py:349] Steps/second: 0.177189, Examples/second: 25.125454\n",
      "I0710 10:39:55.744800 140295626643200 trainer.py:508] step:  6657, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:39.614292 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2977988 log_pplx:3.6953871 loss:91.945847 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.48169\n",
      "I0710 10:40:02.287663 140295626643200 summary_utils.py:349] Steps/second: 0.177176, Examples/second: 25.118402\n",
      "I0710 10:40:02.288492 140295626643200 trainer.py:508] step:  6658, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:105.90464 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2960516 log_pplx:3.4121754 loss:140.19775 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.48608\n",
      "I0710 10:40:05.340059 140295626643200 summary_utils.py:349] Steps/second: 0.177214, Examples/second: 25.133379\n",
      "I0710 10:40:05.340802 140295626643200 trainer.py:508] step:  6659, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:22.940405 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2976698 log_pplx:3.7304156 loss:58.244026 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.49127\n",
      "I0710 10:40:11.989131 140295626643200 summary_utils.py:349] Steps/second: 0.177200, Examples/second: 25.126108\n",
      "I0710 10:40:11.990060 140295626643200 trainer.py:508] step:  6660, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:80.234291 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2959889 log_pplx:3.3225312 loss:139.29712 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.49622\n",
      "I0710 10:40:18.525004 140295626643200 summary_utils.py:349] Steps/second: 0.177186, Examples/second: 25.119082\n",
      "I0710 10:40:18.525765 140295626643200 trainer.py:508] step:  6661, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:76.362457 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2960891 log_pplx:3.3465497 loss:135.40976 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.50153\n",
      "I0710 10:40:22.335926 140295626643200 summary_utils.py:349] Steps/second: 0.177213, Examples/second: 25.124442\n",
      "I0710 10:40:22.336689 140295626643200 trainer.py:508] step:  6662, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:50.846565 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2978274 log_pplx:3.7167468 loss:94.544754 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.50684\n",
      "I0710 10:40:31.675922 140295626643200 summary_utils.py:349] Steps/second: 0.177159, Examples/second: 25.108221\n",
      "I0710 10:40:31.676704 140295626643200 trainer.py:508] step:  6663, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:165.38484 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.295013 log_pplx:2.6690049 loss:185.56258 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:526.51172\n",
      "I0710 10:40:38.125138 140295626643200 summary_utils.py:349] Steps/second: 0.177147, Examples/second: 25.101397\n",
      "I0710 10:40:38.125953 140295626643200 trainer.py:508] step:  6664, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:103.96523 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2963078 log_pplx:3.4264789 loss:138.85806 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.51752\n",
      "I0710 10:40:41.973824 140295626643200 summary_utils.py:349] Steps/second: 0.177173, Examples/second: 25.106675\n",
      "I0710 10:40:41.974818 140295626643200 trainer.py:508] step:  6665, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:41.490685 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2982287 log_pplx:3.713985 loss:94.33522 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.52295\n",
      "I0710 10:40:48.405826 140295626643200 summary_utils.py:349] Steps/second: 0.177162, Examples/second: 25.099895\n",
      "I0710 10:40:48.406699 140295626643200 trainer.py:508] step:  6666, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:81.407959 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2961026 log_pplx:3.2382267 loss:127.62661 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.52808\n",
      "I0710 10:40:51.390986 140295626643200 summary_utils.py:349] Steps/second: 0.177201, Examples/second: 25.114959\n",
      "I0710 10:40:51.391777 140295626643200 trainer.py:508] step:  6667, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:41.4949 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.297084 log_pplx:3.6970334 loss:56.683037 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.53284\n",
      "I0710 10:40:57.788119 140295626643200 summary_utils.py:349] Steps/second: 0.177190, Examples/second: 25.108252\n",
      "I0710 10:40:57.788937 140295626643200 trainer.py:508] step:  6668, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:81.559486 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2964828 log_pplx:3.3704329 loss:137.00809 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.53766\n",
      "I0710 10:40:59.989755 140295626643200 summary_utils.py:349] Steps/second: 0.177241, Examples/second: 25.146220\n",
      "I0710 10:40:59.990664 140295626643200 trainer.py:508] step:  6669, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:13.224456 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2960424 log_pplx:3.7565963 loss:27.220648 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:526.54279\n",
      "I0710 10:41:03.895511 140295626643200 summary_utils.py:349] Steps/second: 0.177266, Examples/second: 25.151355\n",
      "I0710 10:41:03.896514 140295626643200 trainer.py:508] step:  6670, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:63.271595 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2979356 log_pplx:3.7527003 loss:94.872955 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.54767\n",
      "I0710 10:41:13.485751 140295626643200 summary_utils.py:349] Steps/second: 0.177208, Examples/second: 25.134647\n",
      "I0710 10:41:13.486611 140295626643200 trainer.py:508] step:  6671, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:175.51068 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2954913 log_pplx:2.7002642 loss:190.30112 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:526.5528\n",
      "I0710 10:41:19.986725 140295626643200 summary_utils.py:349] Steps/second: 0.177195, Examples/second: 25.127725\n",
      "I0710 10:41:19.987626 140295626643200 trainer.py:508] step:  6672, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:75.936783 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2964357 log_pplx:3.3198783 loss:132.75363 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.55853\n",
      "I0710 10:41:23.217049 140295626643200 summary_utils.py:349] Steps/second: 0.177231, Examples/second: 25.142231\n",
      "I0710 10:41:23.217869 140295626643200 trainer.py:508] step:  6673, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:30.069632 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2978138 log_pplx:3.7050619 loss:58.065266 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.56409\n",
      "I0710 10:41:30.010950 140295626643200 summary_utils.py:349] Steps/second: 0.177214, Examples/second: 25.134700\n",
      "I0710 10:41:30.011751 140295626643200 trainer.py:508] step:  6674, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:83.770226 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2967278 log_pplx:3.3259916 loss:137.19717 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.56915\n",
      "I0710 10:41:33.856842 140295626643200 summary_utils.py:349] Steps/second: 0.177240, Examples/second: 25.139951\n",
      "I0710 10:41:33.857682 140295626643200 trainer.py:508] step:  6675, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:46.68837 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.298291 log_pplx:3.7599847 loss:95.174606 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.57367\n",
      "I0710 10:41:39.994776 140295626643200 summary_utils.py:349] Steps/second: 0.177233, Examples/second: 25.133794\n",
      "I0710 10:41:39.995602 140295626643200 trainer.py:508] step:  6676, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:81.573685 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2963545 log_pplx:3.3184123 loss:132.23874 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.57794\n",
      "I0710 10:41:46.389934 140295626643200 summary_utils.py:349] Steps/second: 0.177222, Examples/second: 25.127108\n",
      "I0710 10:41:46.390708 140295626643200 trainer.py:508] step:  6677, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:74.78302 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2967689 log_pplx:3.3299515 loss:134.23868 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.58215\n",
      "I0710 10:41:50.222039 140295626643200 summary_utils.py:349] Steps/second: 0.177249, Examples/second: 25.132382\n",
      "I0710 10:41:50.222817 140295626643200 trainer.py:508] step:  6678, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:37.334572 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2984245 log_pplx:3.6884255 loss:94.170113 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.58667\n",
      "I0710 10:41:53.290005 140295626643200 summary_utils.py:349] Steps/second: 0.177286, Examples/second: 25.147189\n",
      "I0710 10:41:53.290761 140295626643200 trainer.py:508] step:  6679, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:27.003811 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2978199 log_pplx:3.6699595 loss:56.583328 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.59094\n",
      "I0710 10:41:59.830149 140295626643200 summary_utils.py:349] Steps/second: 0.177273, Examples/second: 25.140202\n",
      "I0710 10:41:59.830918 140295626643200 trainer.py:508] step:  6680, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:95.640526 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2967846 log_pplx:3.3305008 loss:136.25912 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.59503\n",
      "I0710 10:42:09.042954 140295626643200 summary_utils.py:349] Steps/second: 0.177221, Examples/second: 25.124363\n",
      "I0710 10:42:09.043751 140295626643200 trainer.py:508] step:  6681, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:157.13492 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2954755 log_pplx:2.637656 loss:180.67941 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:526.59991\n",
      "I0710 10:42:15.604933 140295626643200 summary_utils.py:349] Steps/second: 0.177207, Examples/second: 25.117352\n",
      "I0710 10:42:15.605772 140295626643200 trainer.py:508] step:  6682, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:78.494537 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2970307 log_pplx:3.2395544 loss:132.25479 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.60461\n",
      "I0710 10:42:22.036121 140295626643200 summary_utils.py:349] Steps/second: 0.177196, Examples/second: 25.110620\n",
      "I0710 10:42:22.037024 140295626643200 trainer.py:508] step:  6683, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:89.579948 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2967252 log_pplx:3.2671103 loss:132.56299 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.60992\n",
      "I0710 10:42:25.877652 140295626643200 summary_utils.py:349] Steps/second: 0.177222, Examples/second: 25.115864\n",
      "I0710 10:42:25.878486 140295626643200 trainer.py:508] step:  6684, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:46.567429 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2982156 log_pplx:3.6940184 loss:93.181618 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.61566\n",
      "I0710 10:42:31.961178 140295626643200 summary_utils.py:349] Steps/second: 0.177216, Examples/second: 25.109859\n",
      "I0710 10:42:31.962119 140295626643200 trainer.py:508] step:  6685, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:84.297455 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2969464 log_pplx:3.2918193 loss:130.68523 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.62091\n",
      "I0710 10:42:35.019821 140295626643200 summary_utils.py:349] Steps/second: 0.177254, Examples/second: 25.124639\n",
      "I0710 10:42:35.020866 140295626643200 trainer.py:508] step:  6686, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:21.716154 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2977806 log_pplx:3.6236928 loss:56.181393 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.62585\n",
      "I0710 10:42:41.426326 140295626643200 summary_utils.py:349] Steps/second: 0.177242, Examples/second: 25.117965\n",
      "I0710 10:42:41.427097 140295626643200 trainer.py:508] step:  6687, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:78.216347 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2969861 log_pplx:3.3187084 loss:136.06703 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.63062\n",
      "I0710 10:42:45.253879 140295626643200 summary_utils.py:349] Steps/second: 0.177269, Examples/second: 25.123228\n",
      "I0710 10:42:45.254654 140295626643200 trainer.py:508] step:  6688, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:42.549587 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2983453 log_pplx:3.662776 loss:90.127174 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.63513\n",
      "I0710 10:42:54.196421 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 10:42:54.561331 140295626643200 summary_utils.py:349] Steps/second: 0.177215, Examples/second: 25.107266\n",
      "I0710 10:42:54.562228 140295626643200 trainer.py:508] step:  6689, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:209.7867 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2957106 log_pplx:2.6645837 loss:188.31944 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:526.63959\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 10:42:59.468274 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 10:42:59.988349 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00006688\n",
      "I0710 10:43:01.129739 140295626643200 summary_utils.py:349] Steps/second: 0.177202, Examples/second: 25.100277\n",
      "I0710 10:43:01.130544 140295626643200 trainer.py:508] step:  6690, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:68.16111 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2968948 log_pplx:3.2902257 loss:133.95332 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.64447\n",
      "I0710 10:43:04.234961 140295626643200 summary_utils.py:349] Steps/second: 0.177239, Examples/second: 25.114928\n",
      "I0710 10:43:04.235704 140295626643200 trainer.py:508] step:  6691, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:23.01273 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2982361 log_pplx:3.6101918 loss:56.705395 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.64954\n",
      "I0710 10:43:10.645738 140295626643200 summary_utils.py:349] Steps/second: 0.177228, Examples/second: 25.108266\n",
      "I0710 10:43:10.646515 140295626643200 trainer.py:508] step:  6692, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:97.812866 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2971768 log_pplx:3.3632247 loss:134.94939 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.65436\n",
      "I0710 10:43:14.483512 140295626643200 summary_utils.py:349] Steps/second: 0.177254, Examples/second: 25.113498\n",
      "I0710 10:43:14.484298 140295626643200 trainer.py:508] step:  6693, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:41.12405 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2983991 log_pplx:3.6558576 loss:92.196152 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.65924\n",
      "I0710 10:43:21.067028 140295626643200 summary_utils.py:349] Steps/second: 0.177240, Examples/second: 25.106486\n",
      "I0710 10:43:21.067767 140295626643200 trainer.py:508] step:  6694, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:91.708839 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2971543 log_pplx:3.3188465 loss:139.39157 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.66394\n",
      "I0710 10:43:23.264642 140295626643200 summary_utils.py:349] Steps/second: 0.177290, Examples/second: 25.144017\n",
      "I0710 10:43:23.265496 140295626643200 trainer.py:508] step:  6695, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:11.31996 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.29698 log_pplx:3.6910775 loss:27.185648 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:526.66907\n",
      "I0710 10:43:29.674233 140295626643200 summary_utils.py:349] Steps/second: 0.177279, Examples/second: 25.137352\n",
      "I0710 10:43:29.675013 140295626643200 trainer.py:508] step:  6696, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:90.874886 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2973869 log_pplx:3.2743955 loss:137.48367 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.67377\n",
      "I0710 10:43:33.610472 140295626643200 summary_utils.py:349] Steps/second: 0.177304, Examples/second: 25.142364\n",
      "I0710 10:43:33.611229 140295626643200 trainer.py:508] step:  6697, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:70.487053 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2985231 log_pplx:3.6616325 loss:92.570648 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.67896\n",
      "I0710 10:43:42.078909 140295626643200 summary_utils.py:349] Steps/second: 0.177263, Examples/second: 25.128180\n",
      "I0710 10:43:42.079951 140295626643200 trainer.py:508] step:  6698, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:143.22769 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2956685 log_pplx:2.645889 loss:179.32512 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:526.6842\n",
      "I0710 10:43:48.273468 140295626643200 summary_utils.py:349] Steps/second: 0.177255, Examples/second: 25.121977\n",
      "I0710 10:43:48.274331 140295626643200 trainer.py:508] step:  6699, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:74.778992 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.297245 log_pplx:3.3038263 loss:134.30054 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.68982\n",
      "I0710 10:43:51.377173 140295626643200 summary_utils.py:349] Steps/second: 0.177292, Examples/second: 25.136569\n",
      "I0710 10:43:51.378272 140295626643200 trainer.py:508] step:  6700, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:27.541792 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2986418 log_pplx:3.6536775 loss:57.702415 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.69537\n",
      "I0710 10:43:57.899190 140295626643200 summary_utils.py:349] Steps/second: 0.177279, Examples/second: 25.129692\n",
      "I0710 10:43:57.900067 140295626643200 trainer.py:508] step:  6701, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:106.49188 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2975508 log_pplx:3.3044572 loss:135.0284 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.70068\n",
      "I0710 10:44:01.772173 140295626643200 summary_utils.py:349] Steps/second: 0.177305, Examples/second: 25.134825\n",
      "I0710 10:44:01.772939 140295626643200 trainer.py:508] step:  6702, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:49.313038 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2988794 log_pplx:3.7007456 loss:94.091446 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.70526\n",
      "I0710 10:44:08.122793 140295626643200 summary_utils.py:349] Steps/second: 0.177294, Examples/second: 25.128308\n",
      "I0710 10:44:08.123628 140295626643200 trainer.py:508] step:  6703, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:68.718628 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2974583 log_pplx:3.3841116 loss:137.56413 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.70941\n",
      "I0710 10:44:14.639826 140295626643200 summary_utils.py:349] Steps/second: 0.177282, Examples/second: 25.121455\n",
      "I0710 10:44:14.640633 140295626643200 trainer.py:508] step:  6704, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:77.516335 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2973236 log_pplx:3.2767007 loss:133.11597 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.71375\n",
      "I0710 10:44:17.614806 140295626643200 summary_utils.py:349] Steps/second: 0.177320, Examples/second: 25.136281\n",
      "I0710 10:44:17.615564 140295626643200 trainer.py:508] step:  6705, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:21.620668 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2985702 log_pplx:3.7256126 loss:56.786488 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.71869\n",
      "I0710 10:44:21.476840 140295626643200 summary_utils.py:349] Steps/second: 0.177346, Examples/second: 25.141426\n",
      "I0710 10:44:21.477758 140295626643200 trainer.py:508] step:  6706, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:42.486343 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2992241 log_pplx:3.6609509 loss:91.889877 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.72345\n",
      "I0710 10:44:27.823862 140295626643200 summary_utils.py:349] Steps/second: 0.177336, Examples/second: 25.134923\n",
      "I0710 10:44:27.824650 140295626643200 trainer.py:508] step:  6707, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:73.452774 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2974761 log_pplx:3.2490635 loss:134.59244 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.72839\n",
      "I0710 10:44:38.722708 140295626643200 summary_utils.py:349] Steps/second: 0.177260, Examples/second: 25.115822\n",
      "I0710 10:44:38.723544 140295626643200 trainer.py:508] step:  6708, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:154.40482 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2968991 log_pplx:2.7404032 loss:210.94254 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:526.73352\n",
      "I0710 10:44:44.983780 140295626643200 summary_utils.py:349] Steps/second: 0.177251, Examples/second: 25.109518\n",
      "I0710 10:44:44.985015 140295626643200 trainer.py:508] step:  6709, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:71.384079 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2976955 log_pplx:3.3563907 loss:138.61893 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.73889\n",
      "I0710 10:44:48.819503 140295626643200 summary_utils.py:349] Steps/second: 0.177277, Examples/second: 25.114713\n",
      "I0710 10:44:48.820274 140295626643200 trainer.py:508] step:  6710, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:38.243622 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2991534 log_pplx:3.6601799 loss:91.801895 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.74426\n",
      "I0710 10:44:54.968674 140295626643200 summary_utils.py:349] Steps/second: 0.177269, Examples/second: 25.108644\n",
      "I0710 10:44:54.969480 140295626643200 trainer.py:508] step:  6711, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:71.354347 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2970096 log_pplx:3.2636397 loss:128.5466 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.74951\n",
      "I0710 10:44:58.057156 140295626643200 summary_utils.py:349] Steps/second: 0.177306, Examples/second: 25.123191\n",
      "I0710 10:44:58.057957 140295626643200 trainer.py:508] step:  6712, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:21.148434 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.29847 log_pplx:3.6651409 loss:57.482586 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.75482\n",
      "I0710 10:45:04.476161 140295626643200 summary_utils.py:349] Steps/second: 0.177295, Examples/second: 25.116570\n",
      "I0710 10:45:04.476892 140295626643200 trainer.py:508] step:  6713, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:70.646782 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.296993 log_pplx:3.244921 loss:127.03866 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.75983\n",
      "I0710 10:45:10.899740 140295626643200 summary_utils.py:349] Steps/second: 0.177284, Examples/second: 25.109946\n",
      "I0710 10:45:10.900517 140295626643200 trainer.py:508] step:  6714, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:72.126793 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2974485 log_pplx:3.2884674 loss:128.8257 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.76495\n",
      "I0710 10:45:14.706270 140295626643200 summary_utils.py:349] Steps/second: 0.177310, Examples/second: 25.115190\n",
      "I0710 10:45:14.707058 140295626643200 trainer.py:508] step:  6715, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:40.964527 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2990289 log_pplx:3.6960881 loss:91.524391 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.7702\n",
      "I0710 10:45:17.760801 140295626643200 summary_utils.py:349] Steps/second: 0.177347, Examples/second: 25.129782\n",
      "I0710 10:45:17.761635 140295626643200 trainer.py:508] step:  6716, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:21.852858 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2990775 log_pplx:3.7012312 loss:57.71608 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.77502\n",
      "I0710 10:45:24.303174 140295626643200 summary_utils.py:349] Steps/second: 0.177334, Examples/second: 25.122916\n",
      "I0710 10:45:24.304002 140295626643200 trainer.py:508] step:  6717, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:77.228004 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2976424 log_pplx:3.3781533 loss:137.91312 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.77948\n",
      "I0710 10:45:30.654229 140295626643200 summary_utils.py:349] Steps/second: 0.177324, Examples/second: 25.116448\n",
      "I0710 10:45:30.655026 140295626643200 trainer.py:508] step:  6718, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:73.165222 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2977797 log_pplx:3.3590653 loss:140.15701 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.78424\n",
      "I0710 10:45:34.449436 140295626643200 summary_utils.py:349] Steps/second: 0.177351, Examples/second: 25.121705\n",
      "I0710 10:45:34.450230 140295626643200 trainer.py:508] step:  6719, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:37.623512 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2994992 log_pplx:3.7013149 loss:94.059662 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.78925\n",
      "I0710 10:45:36.651994 140295626643200 summary_utils.py:349] Steps/second: 0.177400, Examples/second: 25.158815\n",
      "I0710 10:45:36.652749 140295626643200 trainer.py:508] step:  6720, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:11.861382 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2974777 log_pplx:3.688081 loss:27.055532 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:526.79401\n",
      "I0710 10:45:46.492126 140295626643200 summary_utils.py:349] Steps/second: 0.177340, Examples/second: 25.141961\n",
      "I0710 10:45:46.493125 140295626643200 trainer.py:508] step:  6721, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:191.37001 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2965373 log_pplx:2.6544852 loss:193.84378 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:526.79846\n",
      "I0710 10:45:52.749211 140295626643200 summary_utils.py:349] Steps/second: 0.177331, Examples/second: 25.135686\n",
      "I0710 10:45:52.750054 140295626643200 trainer.py:508] step:  6722, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:76.283897 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2975211 log_pplx:3.2567177 loss:129.45454 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.80334\n",
      "I0710 10:45:55.809764 140295626643200 summary_utils.py:349] Steps/second: 0.177368, Examples/second: 25.150216\n",
      "I0710 10:45:55.810513 140295626643200 trainer.py:508] step:  6723, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:22.200684 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2990675 log_pplx:3.665231 loss:57.655796 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.80835\n",
      "I0710 10:46:02.225506 140295626643200 summary_utils.py:349] Steps/second: 0.177357, Examples/second: 25.143618\n",
      "I0710 10:46:02.226275 140295626643200 trainer.py:508] step:  6724, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:83.007301 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2975187 log_pplx:3.2722933 loss:131.05536 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.81311\n",
      "I0710 10:46:06.076655 140295626643200 summary_utils.py:349] Steps/second: 0.177382, Examples/second: 25.148739\n",
      "I0710 10:46:06.077478 140295626643200 trainer.py:508] step:  6725, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:39.323757 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2993978 log_pplx:3.6986485 loss:92.997887 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.81757\n",
      "I0710 10:46:12.624339 140295626643200 summary_utils.py:349] Steps/second: 0.177369, Examples/second: 25.141879\n",
      "I0710 10:46:12.625192 140295626643200 trainer.py:508] step:  6726, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:76.310028 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2976601 log_pplx:3.3031142 loss:132.0007 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.82196\n",
      "I0710 10:46:19.024349 140295626643200 summary_utils.py:349] Steps/second: 0.177358, Examples/second: 25.135327\n",
      "I0710 10:46:19.025498 140295626643200 trainer.py:508] step:  6727, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:68.725395 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2975893 log_pplx:3.3200727 loss:130.22983 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.82672\n",
      "I0710 10:46:22.117504 140295626643200 summary_utils.py:349] Steps/second: 0.177395, Examples/second: 25.149759\n",
      "I0710 10:46:22.118491 140295626643200 trainer.py:508] step:  6728, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:21.970577 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2988875 log_pplx:3.6428432 loss:56.520988 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.83167\n",
      "I0710 10:46:26.007703 140295626643200 summary_utils.py:349] Steps/second: 0.177420, Examples/second: 25.154790\n",
      "I0710 10:46:26.008495 140295626643200 trainer.py:508] step:  6729, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:37.011002 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2994099 log_pplx:3.673939 loss:90.952957 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.83655\n",
      "I0710 10:46:32.706215 140295626643200 summary_utils.py:349] Steps/second: 0.177405, Examples/second: 25.147631\n",
      "I0710 10:46:32.707086 140295626643200 trainer.py:508] step:  6730, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:69.843544 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2978233 log_pplx:3.3452017 loss:135.81519 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.84125\n",
      "I0710 10:46:38.929450 140295626643200 summary_utils.py:349] Steps/second: 0.177396, Examples/second: 25.141446\n",
      "I0710 10:46:38.930238 140295626643200 trainer.py:508] step:  6731, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:74.298409 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2976315 log_pplx:3.3268709 loss:132.15994 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.84613\n",
      "I0710 10:46:42.726918 140295626643200 summary_utils.py:349] Steps/second: 0.177423, Examples/second: 25.146661\n",
      "I0710 10:46:42.727639 140295626643200 trainer.py:508] step:  6732, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:42.597279 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2994882 log_pplx:3.6791961 loss:91.634987 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.85126\n",
      "I0710 10:46:49.258693 140295626643200 summary_utils.py:349] Steps/second: 0.177410, Examples/second: 25.139855\n",
      "I0710 10:46:49.259465 140295626643200 trainer.py:508] step:  6733, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:71.873154 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2980992 log_pplx:3.3556683 loss:138.96661 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.85577\n",
      "I0710 10:46:58.752887 140295626643200 summary_utils.py:349] Steps/second: 0.177355, Examples/second: 25.123816\n",
      "I0710 10:46:58.753730 140295626643200 trainer.py:508] step:  6734, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:321.06686 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2967001 log_pplx:2.7270579 loss:194.64377 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:526.86053\n",
      "I0710 10:47:01.850520 140295626643200 summary_utils.py:349] Steps/second: 0.177391, Examples/second: 25.138196\n",
      "I0710 10:47:01.851294 140295626643200 trainer.py:508] step:  6735, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:34.092049 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2991351 log_pplx:3.6744847 loss:57.672188 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.86548\n",
      "I0710 10:47:08.059236 140295626643200 summary_utils.py:349] Steps/second: 0.177383, Examples/second: 25.132059\n",
      "I0710 10:47:08.060027 140295626643200 trainer.py:508] step:  6736, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:113.78216 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2981774 log_pplx:3.3672204 loss:135.74109 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.87018\n",
      "I0710 10:47:11.944180 140295626643200 summary_utils.py:349] Steps/second: 0.177408, Examples/second: 25.137088\n",
      "I0710 10:47:11.944998 140295626643200 base_runner.py:111] step:  6737, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:41.54538 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2997082 log_pplx:3.7170205 loss:93.854759 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.87476\n",
      "I0710 10:47:18.100262 140295626643200 summary_utils.py:349] Steps/second: 0.177400, Examples/second: 25.131063\n",
      "I0710 10:47:18.101096 140295626643200 trainer.py:508] step:  6738, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:121.26579 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2979439 log_pplx:3.3494244 loss:137.03334 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.87964\n",
      "I0710 10:47:24.081595 140295626643200 summary_utils.py:349] Steps/second: 0.177396, Examples/second: 25.125399\n",
      "I0710 10:47:24.082467 140295626643200 trainer.py:508] step:  6739, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:75.545937 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2977947 log_pplx:3.3534508 loss:133.3835 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.8844\n",
      "I0710 10:47:27.138809 140295626643200 summary_utils.py:349] Steps/second: 0.177432, Examples/second: 25.139830\n",
      "I0710 10:47:27.139678 140295626643200 trainer.py:508] step:  6740, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:50.133919 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2992768 log_pplx:3.7101343 loss:58.652004 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.88947\n",
      "I0710 10:47:30.988454 140295626643200 summary_utils.py:349] Steps/second: 0.177458, Examples/second: 25.144921\n",
      "I0710 10:47:30.989253 140295626643200 trainer.py:508] step:  6741, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:98.535988 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2996225 log_pplx:3.7705815 loss:95.277885 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.8949\n",
      "I0710 10:47:33.190763 140295626643200 summary_utils.py:349] Steps/second: 0.177507, Examples/second: 25.181680\n",
      "I0710 10:47:33.191552 140295626643200 trainer.py:508] step:  6742, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:10.216995 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2981677 log_pplx:3.7181323 loss:27.348608 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:526.90094\n",
      "I0710 10:47:39.341383 140295626643200 summary_utils.py:349] Steps/second: 0.177500, Examples/second: 25.175655\n",
      "I0710 10:47:39.342251 140295626643200 trainer.py:508] step:  6743, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:88.135391 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2981485 log_pplx:3.3017073 loss:134.58585 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.90656\n",
      "I0710 10:47:48.794990 140295626643200 summary_utils.py:349] Steps/second: 0.177445, Examples/second: 25.159735\n",
      "I0710 10:47:48.795848 140295626643200 trainer.py:508] step:  6744, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:558.56793 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2968388 log_pplx:3.0007164 loss:217.40193 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:526.91156\n",
      "I0710 10:47:55.060583 140295626643200 summary_utils.py:349] Steps/second: 0.177436, Examples/second: 25.153496\n",
      "I0710 10:47:55.061402 140295626643200 trainer.py:508] step:  6745, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:101.27515 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2986454 log_pplx:3.3196163 loss:138.88445 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.91669\n",
      "I0710 10:47:58.945000 140295626643200 summary_utils.py:349] Steps/second: 0.177461, Examples/second: 25.158500\n",
      "I0710 10:47:58.946064 140295626643200 trainer.py:508] step:  6746, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:49.74646 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2999781 log_pplx:3.7711692 loss:95.339867 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.9223\n",
      "I0710 10:48:05.035968 140295626643200 summary_utils.py:349] Steps/second: 0.177455, Examples/second: 25.152619\n",
      "I0710 10:48:05.036782 140295626643200 trainer.py:508] step:  6747, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:206.95378 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2982854 log_pplx:3.4303567 loss:137.85745 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.9278\n",
      "I0710 10:48:08.097929 140295626643200 summary_utils.py:349] Steps/second: 0.177491, Examples/second: 25.166988\n",
      "I0710 10:48:08.098721 140295626643200 trainer.py:508] step:  6748, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:59.960335 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2996571 log_pplx:3.8147781 loss:59.576103 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.93457\n",
      "I0710 10:48:14.463233 140295626643200 summary_utils.py:349] Steps/second: 0.177481, Examples/second: 25.160553\n",
      "I0710 10:48:14.464041 140295626643200 trainer.py:508] step:  6749, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:133.6169 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2984873 log_pplx:3.4222808 loss:144.16356 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.94055\n",
      "I0710 10:48:14.465249 140295635035904 trainer.py:345] Write summary @6749\n",
      "I0710 10:48:25.440377 140295626643200 summary_utils.py:349] Steps/second: 0.177405, Examples/second: 25.144825\n",
      "I0710 10:48:25.441510 140295626643200 trainer.py:508] step:  6750, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:83.167625 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.298275 log_pplx:3.3266757 loss:132.90071 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.94727\n",
      "I0710 10:48:30.924923 140295626643200 summary_utils.py:349] Steps/second: 0.177407, Examples/second: 25.146596\n",
      "I0710 10:48:30.926552 140295626643200 trainer.py:508] step:  6751, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:107.78964 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.300283 log_pplx:3.8722899 loss:97.920525 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.9541\n",
      "I0710 10:48:39.887926 140295626643200 summary_utils.py:349] Steps/second: 0.177360, Examples/second: 25.134954\n",
      "I0710 10:48:39.889366 140295626643200 trainer.py:508] step:  6752, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:84.009361 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2985401 log_pplx:3.4036956 loss:137.16891 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.95984\n",
      "I0710 10:48:43.944572 140295626643200 summary_utils.py:349] Steps/second: 0.177382, Examples/second: 25.147284\n",
      "I0710 10:48:43.945996 140295626643200 trainer.py:508] step:  6753, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:39.376118 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2994174 log_pplx:3.6986818 loss:57.661873 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.96558\n",
      "I0710 10:48:53.526538 140295626643200 summary_utils.py:349] Steps/second: 0.177326, Examples/second: 25.134410\n",
      "I0710 10:48:53.527661 140295626643200 trainer.py:508] step:  6754, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:80.773537 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2987678 log_pplx:3.402775 loss:140.87491 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.97083\n",
      "I0710 10:48:59.502800 140295626643200 summary_utils.py:349] Steps/second: 0.177321, Examples/second: 25.135192\n",
      "I0710 10:48:59.504905 140295626643200 trainer.py:508] step:  6755, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:43.136162 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3002741 log_pplx:3.7076828 loss:93.062843 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:526.97668\n",
      "I0710 10:49:09.759211 140295626643200 summary_utils.py:349] Steps/second: 0.177256, Examples/second: 25.120990\n",
      "I0710 10:49:09.760541 140295626643200 trainer.py:508] step:  6756, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:78.686127 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2988956 log_pplx:3.3588176 loss:143.04362 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.9826\n",
      "I0710 10:49:12.766107 140295635035904 trainer.py:354] Write summary done: step 6749\n",
      "I0710 10:49:14.179626 140295626643200 summary_utils.py:349] Steps/second: 0.177273, Examples/second: 25.132565\n",
      "I0710 10:49:14.180402 140295626643200 trainer.py:508] step:  6757, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:35.166588 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3000658 log_pplx:3.6633575 loss:56.452915 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:526.98859\n",
      "I0710 10:49:20.256120 140295626643200 summary_utils.py:349] Steps/second: 0.177267, Examples/second: 25.126761\n",
      "I0710 10:49:20.256968 140295626643200 trainer.py:508] step:  6758, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:77.689651 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.298315 log_pplx:3.2911115 loss:131.93245 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:526.99451\n",
      "I0710 10:49:24.114964 140295626643200 summary_utils.py:349] Steps/second: 0.177292, Examples/second: 25.131791\n",
      "I0710 10:49:24.115770 140295626643200 trainer.py:508] step:  6759, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:45.823898 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3006012 log_pplx:3.7250028 loss:94.917725 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.00006\n",
      "I0710 10:49:34.567013 140295626643200 summary_utils.py:349] Steps/second: 0.177224, Examples/second: 25.114038\n",
      "I0710 10:49:34.567857 140295626643200 trainer.py:508] step:  6760, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:543.6167 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2978415 log_pplx:2.8673306 loss:209.81693 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:527.00568\n",
      "I0710 10:49:40.531755 140295626643200 summary_utils.py:349] Steps/second: 0.177220, Examples/second: 25.108476\n",
      "I0710 10:49:40.532587 140295626643200 trainer.py:508] step:  6761, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:130.50954 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.298579 log_pplx:3.3520772 loss:133.9993 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.01312\n",
      "I0710 10:49:46.661583 140295626643200 summary_utils.py:349] Steps/second: 0.177213, Examples/second: 25.102590\n",
      "I0710 10:49:46.662395 140295626643200 trainer.py:508] step:  6762, steps/sec: 0.18, examples/sec: 25.10 grad_norm/all/loss:151.674 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2987738 log_pplx:3.3747172 loss:134.27156 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.01959\n",
      "I0710 10:49:49.711082 140295626643200 summary_utils.py:349] Steps/second: 0.177250, Examples/second: 25.116879\n",
      "I0710 10:49:49.711844 140295626643200 trainer.py:508] step:  6763, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:23.036505 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2998888 log_pplx:3.6155765 loss:55.645985 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.02545\n",
      "I0710 10:49:53.636631 140295626643200 summary_utils.py:349] Steps/second: 0.177274, Examples/second: 25.121766\n",
      "I0710 10:49:53.637350 140295626643200 trainer.py:508] step:  6764, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:42.346806 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3006128 log_pplx:3.804708 loss:96.829811 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.03119\n",
      "I0710 10:49:59.751353 140295626643200 summary_utils.py:349] Steps/second: 0.177267, Examples/second: 25.115909\n",
      "I0710 10:49:59.752130 140295626643200 trainer.py:508] step:  6765, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:102.89951 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2987592 log_pplx:3.4545517 loss:137.70706 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.0368\n",
      "I0710 10:50:01.947578 140295626643200 summary_utils.py:349] Steps/second: 0.177316, Examples/second: 25.152251\n",
      "I0710 10:50:01.948323 140295626643200 trainer.py:508] step:  6766, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:11.687856 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2983848 log_pplx:3.7056231 loss:26.619692 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:527.04266\n",
      "I0710 10:50:08.311173 140295626643200 summary_utils.py:349] Steps/second: 0.177305, Examples/second: 25.145885\n",
      "I0710 10:50:08.311995 140295626643200 trainer.py:508] step:  6767, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:92.505081 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2987034 log_pplx:3.4234486 loss:135.99649 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.0481\n",
      "I0710 10:50:14.600469 140295626643200 summary_utils.py:349] Steps/second: 0.177296, Examples/second: 25.139675\n",
      "I0710 10:50:14.601251 140295626643200 trainer.py:508] step:  6768, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:167.97643 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2988745 log_pplx:3.493639 loss:142.97717 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.05396\n",
      "I0710 10:50:18.470410 140295626643200 summary_utils.py:349] Steps/second: 0.177321, Examples/second: 25.144657\n",
      "I0710 10:50:18.471177 140295626643200 trainer.py:508] step:  6769, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:47.210232 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3008684 log_pplx:3.6588843 loss:92.226746 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.05969\n",
      "I0710 10:50:21.565005 140295626643200 summary_utils.py:349] Steps/second: 0.177357, Examples/second: 25.158810\n",
      "I0710 10:50:21.566008 140295626643200 trainer.py:508] step:  6770, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:28.792698 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3005617 log_pplx:3.766228 loss:59.538773 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.06531\n",
      "I0710 10:50:32.081126 140295626643200 summary_utils.py:349] Steps/second: 0.177288, Examples/second: 25.140988\n",
      "I0710 10:50:32.081918 140295626643200 trainer.py:508] step:  6771, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:286.30847 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.297897 log_pplx:2.8864202 loss:203.78125 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:527.07056\n",
      "I0710 10:50:38.372316 140295626643200 summary_utils.py:349] Steps/second: 0.177279, Examples/second: 25.134787\n",
      "I0710 10:50:38.373074 140295626643200 trainer.py:508] step:  6772, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:93.807663 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2992573 log_pplx:3.3217611 loss:137.89461 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.07495\n",
      "I0710 10:50:42.217286 140295626643200 summary_utils.py:349] Steps/second: 0.177304, Examples/second: 25.139811\n",
      "I0710 10:50:42.218016 140295626643200 trainer.py:508] step:  6773, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:43.439358 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3007715 log_pplx:3.7324862 loss:93.312164 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.0799\n",
      "I0710 10:50:48.382601 140295626643200 summary_utils.py:349] Steps/second: 0.177297, Examples/second: 25.133868\n",
      "I0710 10:50:48.383492 140295626643200 trainer.py:508] step:  6774, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:93.615761 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2990509 log_pplx:3.4147172 loss:134.02765 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.08466\n",
      "I0710 10:50:51.423650 140295626643200 summary_utils.py:349] Steps/second: 0.177334, Examples/second: 25.148096\n",
      "I0710 10:50:51.424442 140295626643200 trainer.py:508] step:  6775, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:25.82753 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3004692 log_pplx:3.7243474 loss:59.196758 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.0899\n",
      "I0710 10:50:57.898548 140295626643200 summary_utils.py:349] Steps/second: 0.177322, Examples/second: 25.141536\n",
      "I0710 10:50:57.899340 140295626643200 trainer.py:508] step:  6776, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:83.810234 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2991031 log_pplx:3.4079554 loss:135.80702 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.09491\n",
      "I0710 10:51:01.783822 140295626643200 summary_utils.py:349] Steps/second: 0.177347, Examples/second: 25.146470\n",
      "I0710 10:51:01.784610 140295626643200 trainer.py:508] step:  6777, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:52.23254 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3005859 log_pplx:3.7252107 loss:93.549355 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.10016\n",
      "I0710 10:51:08.298647 140295626643200 summary_utils.py:349] Steps/second: 0.177334, Examples/second: 25.139837\n",
      "I0710 10:51:08.299447 140295626643200 trainer.py:508] step:  6778, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:83.845093 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.299208 log_pplx:3.4071743 loss:137.52206 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.10504\n",
      "I0710 10:51:14.753335 140295626643200 summary_utils.py:349] Steps/second: 0.177323, Examples/second: 25.133330\n",
      "I0710 10:51:14.754160 140295626643200 trainer.py:508] step:  6779, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:75.961647 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.299505 log_pplx:3.3580077 loss:137.97214 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.11029\n",
      "I0710 10:51:18.729783 140295626643200 summary_utils.py:349] Steps/second: 0.177346, Examples/second: 25.138078\n",
      "I0710 10:51:18.730586 140295626643200 trainer.py:508] step:  6780, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:53.842903 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3007054 log_pplx:3.6943901 loss:92.7061 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.11578\n",
      "I0710 10:51:21.854422 140295626643200 summary_utils.py:349] Steps/second: 0.177381, Examples/second: 25.152106\n",
      "I0710 10:51:21.855376 140295626643200 trainer.py:508] step:  6781, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:28.062326 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3004832 log_pplx:3.7166796 loss:58.537701 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.12079\n",
      "I0710 10:51:31.667800 140295626643200 summary_utils.py:349] Steps/second: 0.177323, Examples/second: 25.135769\n",
      "I0710 10:51:31.668614 140295626643200 trainer.py:508] step:  6782, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:221.9657 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2986439 log_pplx:2.8210428 loss:206.99405 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:527.12543\n",
      "I0710 10:51:37.919535 140295626643200 summary_utils.py:349] Steps/second: 0.177314, Examples/second: 25.129679\n",
      "I0710 10:51:37.920418 140295626643200 trainer.py:508] step:  6783, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:75.579239 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2992706 log_pplx:3.3136842 loss:133.00301 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.13092\n",
      "I0710 10:51:44.639765 140295626643200 summary_utils.py:349] Steps/second: 0.177299, Examples/second: 25.122666\n",
      "I0710 10:51:44.640622 140295626643200 trainer.py:508] step:  6784, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:77.438889 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2990984 log_pplx:3.3022511 loss:131.67728 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.13654\n",
      "I0710 10:51:48.678371 140295626643200 summary_utils.py:349] Steps/second: 0.177322, Examples/second: 25.127284\n",
      "I0710 10:51:48.679176 140295626643200 trainer.py:508] step:  6785, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:42.454857 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3011729 log_pplx:3.7110806 loss:93.704788 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.14203\n",
      "I0710 10:51:51.801655 140295626643200 summary_utils.py:349] Steps/second: 0.177357, Examples/second: 25.141283\n",
      "I0710 10:51:51.802481 140295626643200 trainer.py:508] step:  6786, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:23.693098 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3004841 log_pplx:3.6236517 loss:55.996742 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.14746\n",
      "I0710 10:51:58.229121 140295626643200 summary_utils.py:349] Steps/second: 0.177346, Examples/second: 25.134852\n",
      "I0710 10:51:58.229968 140295626643200 trainer.py:508] step:  6787, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:130.9855 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.299546 log_pplx:3.3235219 loss:134.43648 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.15283\n",
      "I0710 10:52:04.799813 140295626643200 summary_utils.py:349] Steps/second: 0.177333, Examples/second: 25.128143\n",
      "I0710 10:52:04.800627 140295626643200 trainer.py:508] step:  6788, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:69.585381 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2991896 log_pplx:3.3011556 loss:130.27185 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.15759\n",
      "I0710 10:52:07.030531 140295626643200 summary_utils.py:349] Steps/second: 0.177380, Examples/second: 25.164056\n",
      "I0710 10:52:07.031312 140295626643200 trainer.py:508] step:  6789, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:12.151739 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.299167 log_pplx:3.6928346 loss:26.758627 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:527.16241\n",
      "I0710 10:52:10.865612 140295626643200 summary_utils.py:349] Steps/second: 0.177405, Examples/second: 25.169056\n",
      "I0710 10:52:10.866347 140295626643200 trainer.py:508] step:  6790, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:56.815559 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3009477 log_pplx:3.6467078 loss:91.874245 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.16699\n",
      "I0710 10:52:21.910752 140295626643200 summary_utils.py:349] Steps/second: 0.177330, Examples/second: 25.150332\n",
      "I0710 10:52:21.911520 140295626643200 trainer.py:508] step:  6791, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:225.17871 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2984573 log_pplx:2.7875829 loss:199.52126 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:527.17206\n",
      "I0710 10:52:28.126737 140295626643200 summary_utils.py:349] Steps/second: 0.177322, Examples/second: 25.144330\n",
      "I0710 10:52:28.127768 140295626643200 trainer.py:508] step:  6792, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:76.074272 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2997129 log_pplx:3.3808198 loss:136.92319 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.17804\n",
      "I0710 10:52:31.169196 140295626643200 summary_utils.py:349] Steps/second: 0.177358, Examples/second: 25.158441\n",
      "I0710 10:52:31.169981 140295626643200 trainer.py:508] step:  6793, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:24.665192 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.300858 log_pplx:3.6764815 loss:57.243961 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.18408\n",
      "I0710 10:52:37.659927 140295626643200 summary_utils.py:349] Steps/second: 0.177346, Examples/second: 25.151896\n",
      "I0710 10:52:37.660706 140295626643200 trainer.py:508] step:  6794, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:106.66031 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.299243 log_pplx:3.406126 loss:132.49832 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.18988\n",
      "I0710 10:52:41.555050 140295626643200 summary_utils.py:349] Steps/second: 0.177371, Examples/second: 25.156769\n",
      "I0710 10:52:41.555824 140295626643200 trainer.py:508] step:  6795, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:40.94183 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.301443 log_pplx:3.6312025 loss:91.27935 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.19537\n",
      "I0710 10:52:48.006604 140295626643200 summary_utils.py:349] Steps/second: 0.177359, Examples/second: 25.150307\n",
      "I0710 10:52:48.007413 140295626643200 trainer.py:508] step:  6796, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:76.770439 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2992605 log_pplx:3.3129385 loss:129.28743 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.20056\n",
      "I0710 10:52:54.261843 140295626643200 summary_utils.py:349] Steps/second: 0.177351, Examples/second: 25.144239\n",
      "I0710 10:52:54.262681 140295626643200 trainer.py:508] step:  6797, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:72.907143 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2998414 log_pplx:3.3202028 loss:134.88322 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.20538\n",
      "I0710 10:52:57.353375 140295626643200 summary_utils.py:349] Steps/second: 0.177386, Examples/second: 25.158225\n",
      "I0710 10:52:57.354310 140295626643200 trainer.py:508] step:  6798, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:23.118258 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3012251 log_pplx:3.633461 loss:57.127655 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.21008\n",
      "I0710 10:53:01.187146 140295626643200 summary_utils.py:349] Steps/second: 0.177411, Examples/second: 25.163209\n",
      "I0710 10:53:01.187930 140295626643200 trainer.py:508] step:  6799, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:49.487118 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3013628 log_pplx:3.6945581 loss:93.010513 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.2146\n",
      "I0710 10:53:02.965680 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 10:53:07.429554 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 10:53:08.041074 140295626643200 summary_utils.py:349] Steps/second: 0.177394, Examples/second: 25.155961\n",
      "I0710 10:53:08.041307 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00006799\n",
      "I0710 10:53:08.042317 140295626643200 trainer.py:508] step:  6800, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:80.640884 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2999108 log_pplx:3.3379364 loss:132.34917 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.21942\n",
      "I0710 10:53:18.634991 140295626643200 summary_utils.py:349] Steps/second: 0.177325, Examples/second: 25.138221\n",
      "I0710 10:53:18.635765 140295626643200 trainer.py:508] step:  6801, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:171.7901 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.298797 log_pplx:2.8528473 loss:211.96655 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:527.22449\n",
      "I0710 10:53:24.844745 140295626643200 summary_utils.py:349] Steps/second: 0.177317, Examples/second: 25.132263\n",
      "I0710 10:53:24.845666 140295626643200 trainer.py:508] step:  6802, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:68.206055 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2996168 log_pplx:3.2563035 loss:132.65366 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.23004\n",
      "I0710 10:53:30.918927 140295626643200 summary_utils.py:349] Steps/second: 0.177311, Examples/second: 25.126577\n",
      "I0710 10:53:30.919689 140295626643200 trainer.py:508] step:  6803, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:68.010941 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2997236 log_pplx:3.3062654 loss:130.96944 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.23578\n",
      "I0710 10:53:34.693192 140295626643200 summary_utils.py:349] Steps/second: 0.177337, Examples/second: 25.131675\n",
      "I0710 10:53:34.694107 140295626643200 trainer.py:508] step:  6804, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:47.856831 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3014719 log_pplx:3.6900258 loss:92.550461 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.24139\n",
      "I0710 10:53:37.711658 140295626643200 summary_utils.py:349] Steps/second: 0.177374, Examples/second: 25.145763\n",
      "I0710 10:53:37.712645 140295626643200 trainer.py:508] step:  6805, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:30.555189 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.301347 log_pplx:3.7198987 loss:57.832802 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.24658\n",
      "I0710 10:53:44.151774 140295626643200 summary_utils.py:349] Steps/second: 0.177363, Examples/second: 25.139358\n",
      "I0710 10:53:44.152618 140295626643200 trainer.py:508] step:  6806, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:89.408615 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2997755 log_pplx:3.33217 loss:134.86958 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.25134\n",
      "I0710 10:53:48.070132 140295626643200 summary_utils.py:349] Steps/second: 0.177386, Examples/second: 25.144164\n",
      "I0710 10:53:48.070940 140295626643200 trainer.py:508] step:  6807, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:39.57309 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3013904 log_pplx:3.7243981 loss:93.59877 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.25574\n",
      "I0710 10:53:54.461282 140295626643200 summary_utils.py:349] Steps/second: 0.177376, Examples/second: 25.137861\n",
      "I0710 10:53:54.462182 140295626643200 trainer.py:508] step:  6808, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:80.744621 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.300012 log_pplx:3.4142542 loss:138.66141 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.25995\n",
      "I0710 10:53:57.524034 140295626643200 summary_utils.py:349] Steps/second: 0.177412, Examples/second: 25.151839\n",
      "I0710 10:53:57.524855 140295626643200 trainer.py:508] step:  6809, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:30.732658 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3014977 log_pplx:3.711 loss:58.085846 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.26471\n",
      "I0710 10:54:03.761252 140295626643200 summary_utils.py:349] Steps/second: 0.177403, Examples/second: 25.145839\n",
      "I0710 10:54:03.762098 140295626643200 trainer.py:508] step:  6810, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:75.286728 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2996534 log_pplx:3.274214 loss:128.9222 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.26941\n",
      "I0710 10:54:05.939661 140295626643200 summary_utils.py:349] Steps/second: 0.177451, Examples/second: 25.181518\n",
      "I0710 10:54:05.940581 140295626643200 trainer.py:508] step:  6811, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:12.1997 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2999598 log_pplx:3.7608519 loss:27.72159 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:527.27435\n",
      "I0710 10:54:15.609411 140295626643200 summary_utils.py:349] Steps/second: 0.177395, Examples/second: 25.165650\n",
      "I0710 10:54:15.610246 140295626643200 trainer.py:508] step:  6812, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:172.55872 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2991238 log_pplx:2.7594874 loss:196.75143 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:527.27899\n",
      "I0710 10:54:21.878633 140295626643200 summary_utils.py:349] Steps/second: 0.177387, Examples/second: 25.159589\n",
      "I0710 10:54:21.879398 140295626643200 trainer.py:508] step:  6813, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:78.579613 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2999312 log_pplx:3.3043149 loss:129.44652 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.28412\n",
      "I0710 10:54:25.647432 140295626643200 summary_utils.py:349] Steps/second: 0.177412, Examples/second: 25.164667\n",
      "I0710 10:54:25.648270 140295626643200 trainer.py:508] step:  6814, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:50.74453 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.301789 log_pplx:3.6687031 loss:91.557083 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.28931\n",
      "I0710 10:54:32.064603 140295626643200 summary_utils.py:349] Steps/second: 0.177402, Examples/second: 25.158321\n",
      "I0710 10:54:32.065525 140295626643200 trainer.py:508] step:  6815, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:72.036552 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2999183 log_pplx:3.3064651 loss:134.15982 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.29413\n",
      "I0710 10:54:35.158741 140295626643200 summary_utils.py:349] Steps/second: 0.177437, Examples/second: 25.172192\n",
      "I0710 10:54:35.159461 140295626643200 trainer.py:508] step:  6816, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:24.936817 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3012818 log_pplx:3.6610537 loss:57.361275 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.29901\n",
      "I0710 10:54:41.171330 140295626643200 summary_utils.py:349] Steps/second: 0.177432, Examples/second: 25.166639\n",
      "I0710 10:54:41.172251 140295626643200 trainer.py:508] step:  6817, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:78.030022 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3000312 log_pplx:3.3330963 loss:135.53203 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.30377\n",
      "I0710 10:54:45.017431 140295626643200 summary_utils.py:349] Steps/second: 0.177456, Examples/second: 25.171557\n",
      "I0710 10:54:45.018292 140295626643200 trainer.py:508] step:  6818, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:42.796268 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3018235 log_pplx:3.6475966 loss:91.919441 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.30908\n",
      "I0710 10:54:51.350754 140295626643200 summary_utils.py:349] Steps/second: 0.177447, Examples/second: 25.165381\n",
      "I0710 10:54:51.351581 140295626643200 trainer.py:508] step:  6819, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:76.815247 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3001384 log_pplx:3.3620129 loss:138.01062 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.31433\n",
      "I0710 10:54:57.452984 140295626643200 summary_utils.py:349] Steps/second: 0.177440, Examples/second: 25.159663\n",
      "I0710 10:54:57.453801 140295626643200 trainer.py:508] step:  6820, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:72.711754 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3000599 log_pplx:3.3381968 loss:136.86606 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.31946\n",
      "I0710 10:55:01.325612 140295626643200 summary_utils.py:349] Steps/second: 0.177464, Examples/second: 25.164525\n",
      "I0710 10:55:01.326380 140295626643200 trainer.py:508] step:  6821, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:37.393368 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3019445 log_pplx:3.6672328 loss:91.222397 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.32458\n",
      "I0710 10:55:04.328841 140295626643200 summary_utils.py:349] Steps/second: 0.177501, Examples/second: 25.178540\n",
      "I0710 10:55:04.329964 140295626643200 trainer.py:508] step:  6822, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:40.448032 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3014807 log_pplx:3.7319415 loss:57.772202 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.32947\n",
      "I0710 10:55:14.518351 140295626643200 summary_utils.py:349] Steps/second: 0.177438, Examples/second: 25.161731\n",
      "I0710 10:55:14.519197 140295626643200 trainer.py:508] step:  6823, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:150.22357 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2993304 log_pplx:2.7141635 loss:194.67339 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:527.3338\n",
      "I0710 10:55:20.855539 140295626643200 summary_utils.py:349] Steps/second: 0.177428, Examples/second: 25.155567\n",
      "I0710 10:55:20.856239 140295626643200 trainer.py:508] step:  6824, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:74.008583 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3005623 log_pplx:3.3454947 loss:138.96347 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.33832\n",
      "I0710 10:55:24.626090 140295626643200 summary_utils.py:349] Steps/second: 0.177454, Examples/second: 25.160620\n",
      "I0710 10:55:24.626855 140295626643200 trainer.py:508] step:  6825, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:37.616543 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3021891 log_pplx:3.6305499 loss:91.852913 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.34314\n",
      "I0710 10:55:30.828991 140295626643200 summary_utils.py:349] Steps/second: 0.177446, Examples/second: 25.154723\n",
      "I0710 10:55:30.829774 140295626643200 trainer.py:508] step:  6826, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:76.306351 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3004403 log_pplx:3.3038101 loss:135.53879 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.34784\n",
      "I0710 10:55:37.349857 140295626643200 summary_utils.py:349] Steps/second: 0.177434, Examples/second: 25.148212\n",
      "I0710 10:55:37.350681 140295626643200 trainer.py:508] step:  6827, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:84.911072 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3004977 log_pplx:3.3062563 loss:136.7137 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.35284\n",
      "I0710 10:55:41.189927 140295626643200 summary_utils.py:349] Steps/second: 0.177459, Examples/second: 25.153125\n",
      "I0710 10:55:41.190674 140295626643200 trainer.py:508] step:  6828, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:40.780548 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3021704 log_pplx:3.6931558 loss:92.328903 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.35834\n",
      "I0710 10:55:44.220799 140295626643200 summary_utils.py:349] Steps/second: 0.177494, Examples/second: 25.167047\n",
      "I0710 10:55:44.221645 140295626643200 trainer.py:508] step:  6829, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:23.9921 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3019253 log_pplx:3.6699829 loss:56.425987 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.36365\n",
      "I0710 10:55:50.070022 140295626643200 summary_utils.py:349] Steps/second: 0.177492, Examples/second: 25.161844\n",
      "I0710 10:55:50.070806 140295626643200 trainer.py:508] step:  6830, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:69.985382 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3002626 log_pplx:3.2974753 loss:131.52805 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.3689\n",
      "I0710 10:55:56.259877 140295626643200 summary_utils.py:349] Steps/second: 0.177484, Examples/second: 25.155984\n",
      "I0710 10:55:56.260632 140295626643200 trainer.py:508] step:  6831, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:72.960007 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3005013 log_pplx:3.2733788 loss:130.89423 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.37378\n",
      "I0710 10:56:00.077976 140295626643200 summary_utils.py:349] Steps/second: 0.177509, Examples/second: 25.160930\n",
      "I0710 10:56:00.078896 140295626643200 trainer.py:508] step:  6832, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:40.116882 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3020047 log_pplx:3.5570886 loss:89.527481 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.3786\n",
      "I0710 10:56:09.685372 140295626643200 summary_utils.py:349] Steps/second: 0.177454, Examples/second: 25.145339\n",
      "I0710 10:56:09.686152 140295626643200 trainer.py:508] step:  6833, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:171.64378 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2998011 log_pplx:2.706764 loss:196.71408 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:527.38306\n",
      "I0710 10:56:16.041948 140295626643200 summary_utils.py:349] Steps/second: 0.177444, Examples/second: 25.139171\n",
      "I0710 10:56:16.042685 140295626643200 trainer.py:508] step:  6834, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:71.121574 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3007224 log_pplx:3.3114684 loss:135.02512 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.38776\n",
      "I0710 10:56:19.103842 140295626643200 summary_utils.py:349] Steps/second: 0.177480, Examples/second: 25.152998\n",
      "I0710 10:56:19.104625 140295626643200 trainer.py:508] step:  6835, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:21.871332 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3016375 log_pplx:3.5595558 loss:55.506824 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.39252\n",
      "I0710 10:56:21.310623 140295626643200 summary_utils.py:349] Steps/second: 0.177527, Examples/second: 25.188248\n",
      "I0710 10:56:21.311357 140295626643200 trainer.py:508] step:  6836, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:10.951549 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3007289 log_pplx:3.6885827 loss:27.76523 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:527.39703\n",
      "I0710 10:56:27.667436 140295626643200 summary_utils.py:349] Steps/second: 0.177517, Examples/second: 25.182064\n",
      "I0710 10:56:27.668317 140295626643200 base_runner.py:111] step:  6837, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:80.016594 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3006206 log_pplx:3.3413446 loss:134.44736 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.40137\n",
      "I0710 10:56:31.532540 140295626643200 summary_utils.py:349] Steps/second: 0.177541, Examples/second: 25.186900\n",
      "I0710 10:56:31.533316 140295626643200 trainer.py:508] step:  6838, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:36.025055 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3027402 log_pplx:3.6536446 loss:92.962418 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.40564\n",
      "I0710 10:56:37.365836 140295626643200 summary_utils.py:349] Steps/second: 0.177538, Examples/second: 25.181738\n",
      "I0710 10:56:37.366678 140295626643200 trainer.py:508] step:  6839, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:71.227661 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.300257 log_pplx:3.3932128 loss:132.75948 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.40985\n",
      "I0710 10:56:40.419323 140295626643200 summary_utils.py:349] Steps/second: 0.177574, Examples/second: 25.195548\n",
      "I0710 10:56:40.420104 140295626643200 trainer.py:508] step:  6840, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:22.091158 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3018883 log_pplx:3.57689 loss:56.112457 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.41449\n",
      "2020-07-10 10:56:41.177360: I lingvo/core/ops/record_yielder.cc:532] Epoch 8: total records 46838\n",
      "2020-07-10 10:56:41.177414: I lingvo/core/ops/record_yielder.cc:485] Epoch 8 /tmp/punctuator_data/train.txt\n",
      "I0710 10:56:47.014236 140295626643200 summary_utils.py:349] Steps/second: 0.177560, Examples/second: 25.188908\n",
      "I0710 10:56:47.014994 140295626643200 trainer.py:508] step:  6841, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:68.57003 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3008206 log_pplx:3.3061812 loss:131.79263 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.41895\n",
      "I0710 10:56:50.977657 140295626643200 summary_utils.py:349] Steps/second: 0.177583, Examples/second: 25.193544\n",
      "I0710 10:56:50.978659 140295626643200 trainer.py:508] step:  6842, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:41.630844 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.302858 log_pplx:3.7235925 loss:94.974869 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.42371\n",
      "I0710 10:56:57.876127 140295626643200 summary_utils.py:349] Steps/second: 0.177566, Examples/second: 25.186321\n",
      "I0710 10:56:57.876938 140295626643200 trainer.py:508] step:  6843, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:76.18148 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3008958 log_pplx:3.3449721 loss:139.94527 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.42841\n",
      "I0710 10:57:08.768811 140295626643200 summary_utils.py:349] Steps/second: 0.177494, Examples/second: 25.168291\n",
      "I0710 10:57:08.769636 140295626643200 trainer.py:508] step:  6844, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:206.50171 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2999285 log_pplx:2.6864994 loss:202.2934 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:527.43335\n",
      "I0710 10:57:12.649845 140295626643200 summary_utils.py:349] Steps/second: 0.177518, Examples/second: 25.173084\n",
      "I0710 10:57:12.650685 140295626643200 trainer.py:508] step:  6845, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:37.004112 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.302514 log_pplx:3.6216049 loss:90.85701 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.43811\n",
      "I0710 10:57:18.508999 140295626643200 summary_utils.py:349] Steps/second: 0.177515, Examples/second: 25.167896\n",
      "I0710 10:57:18.509865 140295626643200 trainer.py:508] step:  6846, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:74.427048 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3006325 log_pplx:3.1804039 loss:125.82473 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.44281\n",
      "I0710 10:57:21.566412 140295626643200 summary_utils.py:349] Steps/second: 0.177550, Examples/second: 25.181658\n",
      "I0710 10:57:21.567143 140295626643200 trainer.py:508] step:  6847, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:23.36791 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3020102 log_pplx:3.5975177 loss:55.044827 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.44781\n",
      "I0710 10:57:28.010445 140295626643200 summary_utils.py:349] Steps/second: 0.177539, Examples/second: 25.175337\n",
      "I0710 10:57:28.011426 140295626643200 trainer.py:508] step:  6848, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:90.806496 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3007603 log_pplx:3.2162666 loss:129.85677 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.45258\n",
      "I0710 10:57:34.113551 140295626643200 summary_utils.py:349] Steps/second: 0.177532, Examples/second: 25.169682\n",
      "I0710 10:57:34.114388 140295626643200 trainer.py:508] step:  6849, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:69.183868 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3008994 log_pplx:3.1296656 loss:124.40422 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.45819\n",
      "I0710 10:57:37.967051 140295626643200 summary_utils.py:349] Steps/second: 0.177557, Examples/second: 25.174519\n",
      "I0710 10:57:37.967874 140295626643200 trainer.py:508] step:  6850, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:64.155083 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3029428 log_pplx:3.6141179 loss:91.527534 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.46368\n",
      "I0710 10:57:43.219454 140295635035904 trainer.py:345] Write summary @6850\n",
      "I0710 10:57:47.763587 140295626643200 summary_utils.py:349] Steps/second: 0.177500, Examples/second: 25.161738\n",
      "I0710 10:57:47.765133 140295626643200 trainer.py:508] step:  6851, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:72.145363 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3008353 log_pplx:3.0822058 loss:126.71719 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.46832\n",
      "I0710 10:57:52.122277 140295626643200 summary_utils.py:349] Steps/second: 0.177517, Examples/second: 25.172958\n",
      "I0710 10:57:52.123778 140295626643200 trainer.py:508] step:  6852, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:22.91954 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.301958 log_pplx:3.4604297 loss:53.879974 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.47308\n",
      "I0710 10:58:02.096334 140295626643200 summary_utils.py:349] Steps/second: 0.177458, Examples/second: 25.159850\n",
      "I0710 10:58:02.097345 140295626643200 trainer.py:508] step:  6853, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:68.314804 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3014412 log_pplx:3.093518 loss:125.0168 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.4776\n",
      "I0710 10:58:07.672950 140295626643200 summary_utils.py:349] Steps/second: 0.177459, Examples/second: 25.161359\n",
      "I0710 10:58:07.674308 140295626643200 trainer.py:508] step:  6854, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:49.997135 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3027108 log_pplx:3.6679265 loss:90.230995 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.48212\n",
      "I0710 10:58:25.089919 140295626643200 summary_utils.py:349] Steps/second: 0.177299, Examples/second: 25.130899\n",
      "I0710 10:58:25.091336 140295626643200 trainer.py:508] step:  6855, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:220.99426 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.2997433 log_pplx:2.5710037 loss:184.46953 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:527.48694\n",
      "I0710 10:58:28.786719 140295635035904 trainer.py:354] Write summary done: step 6850\n",
      "I0710 10:58:33.010370 140295626643200 summary_utils.py:349] Steps/second: 0.177268, Examples/second: 25.121798\n",
      "I0710 10:58:33.011163 140295626643200 trainer.py:508] step:  6856, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:83.067032 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3011991 log_pplx:3.1840546 loss:131.22285 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.49316\n",
      "I0710 10:58:35.212908 140295626643200 summary_utils.py:349] Steps/second: 0.177315, Examples/second: 25.156701\n",
      "I0710 10:58:35.213710 140295626643200 trainer.py:508] step:  6857, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:13.909893 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3010802 log_pplx:3.6402113 loss:26.910545 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:527.49902\n",
      "I0710 10:58:41.667863 140295626643200 summary_utils.py:349] Steps/second: 0.177304, Examples/second: 25.150407\n",
      "I0710 10:58:41.668714 140295626643200 trainer.py:508] step:  6858, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:138.68071 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3008244 log_pplx:3.1549954 loss:123.47862 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.50452\n",
      "I0710 10:58:44.763844 140295626643200 summary_utils.py:349] Steps/second: 0.177338, Examples/second: 25.164012\n",
      "I0710 10:58:44.764609 140295626643200 trainer.py:508] step:  6859, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:22.419443 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3019518 log_pplx:3.5203066 loss:54.275978 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.50903\n",
      "I0710 10:58:48.631286 140295626643200 summary_utils.py:349] Steps/second: 0.177362, Examples/second: 25.168797\n",
      "I0710 10:58:48.632024 140295626643200 trainer.py:508] step:  6860, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:40.140625 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3030162 log_pplx:3.55603 loss:89.700851 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.51337\n",
      "I0710 10:58:54.991311 140295626643200 summary_utils.py:349] Steps/second: 0.177352, Examples/second: 25.162687\n",
      "I0710 10:58:54.992100 140295626643200 trainer.py:508] step:  6861, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:87.221344 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3011857 log_pplx:3.1793165 loss:129.79559 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.51788\n",
      "I0710 10:59:01.287169 140295626643200 summary_utils.py:349] Steps/second: 0.177344, Examples/second: 25.156705\n",
      "I0710 10:59:01.288053 140295626643200 trainer.py:508] step:  6862, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:78.101875 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3011602 log_pplx:3.1011713 loss:127.88454 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.52313\n",
      "I0710 10:59:05.161531 140295626643200 summary_utils.py:349] Steps/second: 0.177367, Examples/second: 25.161474\n",
      "I0710 10:59:05.162428 140295626643200 trainer.py:508] step:  6863, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:57.383972 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3027786 log_pplx:3.5688839 loss:89.668198 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.52887\n",
      "I0710 10:59:11.384917 140295626643200 summary_utils.py:349] Steps/second: 0.177360, Examples/second: 25.155636\n",
      "I0710 10:59:11.385688 140295626643200 trainer.py:508] step:  6864, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:66.653618 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3010223 log_pplx:3.1422653 loss:122.27339 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.53503\n",
      "I0710 10:59:14.476938 140295626643200 summary_utils.py:349] Steps/second: 0.177394, Examples/second: 25.169216\n",
      "I0710 10:59:14.477756 140295626643200 trainer.py:508] step:  6865, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:25.290995 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3024955 log_pplx:3.6016073 loss:55.979679 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.54114\n",
      "I0710 10:59:23.429299 140295626643200 summary_utils.py:349] Steps/second: 0.177349, Examples/second: 25.155104\n",
      "I0710 10:59:23.430155 140295626643200 trainer.py:508] step:  6866, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:268.88837 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.300302 log_pplx:2.5553551 loss:183.09119 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:527.54694\n",
      "I0710 10:59:29.651724 140295626643200 summary_utils.py:349] Steps/second: 0.177341, Examples/second: 25.149280\n",
      "I0710 10:59:29.652491 140295626643200 trainer.py:508] step:  6867, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:81.315033 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3016642 log_pplx:3.1567743 loss:129.62505 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.55151\n",
      "I0710 10:59:33.487120 140295626643200 summary_utils.py:349] Steps/second: 0.177366, Examples/second: 25.154115\n",
      "I0710 10:59:33.487932 140295626643200 trainer.py:508] step:  6868, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:48.996704 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3028446 log_pplx:3.6083324 loss:89.125809 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.5564\n",
      "I0710 10:59:39.718476 140295626643200 summary_utils.py:349] Steps/second: 0.177358, Examples/second: 25.148278\n",
      "I0710 10:59:39.719269 140295626643200 trainer.py:508] step:  6869, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:112.56916 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3013996 log_pplx:3.1858029 loss:128.90555 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.56128\n",
      "I0710 10:59:42.791798 140295626643200 summary_utils.py:349] Steps/second: 0.177392, Examples/second: 25.161866\n",
      "I0710 10:59:42.792557 140295626643200 trainer.py:508] step:  6870, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:27.784544 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3027259 log_pplx:3.5735223 loss:55.850243 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.56689\n",
      "I0710 10:59:49.055292 140295626643200 summary_utils.py:349] Steps/second: 0.177384, Examples/second: 25.155969\n",
      "I0710 10:59:49.056229 140295626643200 trainer.py:508] step:  6871, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:120.31574 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3017209 log_pplx:3.2252927 loss:134.17216 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.5722\n",
      "I0710 10:59:52.888900 140295626643200 summary_utils.py:349] Steps/second: 0.177408, Examples/second: 25.160798\n",
      "I0710 10:59:52.889814 140295626643200 trainer.py:508] step:  6872, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:62.226036 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3033879 log_pplx:3.6163051 loss:91.063095 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.57867\n",
      "I0710 10:59:59.149718 140295626643200 summary_utils.py:349] Steps/second: 0.177400, Examples/second: 25.154910\n",
      "I0710 10:59:59.150583 140295626643200 trainer.py:508] step:  6873, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:87.076912 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3015606 log_pplx:3.1699023 loss:126.79608 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.58441\n",
      "I0710 11:00:05.073029 140295626643200 summary_utils.py:349] Steps/second: 0.177396, Examples/second: 25.149673\n",
      "I0710 11:00:05.073815 140295626643200 trainer.py:508] step:  6874, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:71.939362 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3013417 log_pplx:3.1615605 loss:125.71155 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.59009\n",
      "I0710 11:00:14.336646 140295626643200 summary_utils.py:349] Steps/second: 0.177347, Examples/second: 25.135035\n",
      "I0710 11:00:14.337728 140295626643200 trainer.py:508] step:  6875, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:264.95514 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3007084 log_pplx:2.5360544 loss:176.9532 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:527.59576\n",
      "I0710 11:00:18.182337 140295626643200 summary_utils.py:349] Steps/second: 0.177371, Examples/second: 25.139838\n",
      "I0710 11:00:18.183408 140295626643200 trainer.py:508] step:  6876, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:84.543678 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3034197 log_pplx:3.5976777 loss:90.391647 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.60333\n",
      "I0710 11:00:21.231115 140295626643200 summary_utils.py:349] Steps/second: 0.177406, Examples/second: 25.153435\n",
      "I0710 11:00:21.231996 140295626643200 trainer.py:508] step:  6877, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:31.287008 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3027126 log_pplx:3.5904815 loss:55.189629 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.60992\n",
      "I0710 11:00:27.530200 140295626643200 summary_utils.py:349] Steps/second: 0.177397, Examples/second: 25.147491\n",
      "I0710 11:00:27.530985 140295626643200 trainer.py:508] step:  6878, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:154.2397 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3020357 log_pplx:3.2583246 loss:134.28371 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.61609\n",
      "I0710 11:00:33.688672 140295626643200 summary_utils.py:349] Steps/second: 0.177390, Examples/second: 25.141820\n",
      "I0710 11:00:33.689468 140295626643200 trainer.py:508] step:  6879, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:140.45982 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3020562 log_pplx:3.1881001 loss:129.11806 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.62122\n",
      "I0710 11:00:35.900514 140295626643200 summary_utils.py:349] Steps/second: 0.177436, Examples/second: 25.176383\n",
      "I0710 11:00:35.901348 140295626643200 trainer.py:508] step:  6880, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:17.533125 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3016641 log_pplx:3.6896334 loss:27.254286 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:527.62573\n",
      "I0710 11:00:42.283692 140295626643200 summary_utils.py:349] Steps/second: 0.177426, Examples/second: 25.170274\n",
      "I0710 11:00:42.284525 140295626643200 trainer.py:508] step:  6881, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:143.06152 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3017527 log_pplx:3.2724533 loss:130.89813 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.63025\n",
      "I0710 11:00:46.074523 140295626643200 summary_utils.py:349] Steps/second: 0.177451, Examples/second: 25.175161\n",
      "I0710 11:00:46.075314 140295626643200 trainer.py:508] step:  6882, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:172.2675 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3037527 log_pplx:3.9238305 loss:99.24839 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.63568\n",
      "I0710 11:00:49.158909 140295626643200 summary_utils.py:349] Steps/second: 0.177485, Examples/second: 25.188654\n",
      "I0710 11:00:49.159677 140295626643200 trainer.py:508] step:  6883, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:47.285179 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3032553 log_pplx:3.6796579 loss:58.702038 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.64264\n",
      "I0710 11:00:55.420357 140295626643200 summary_utils.py:349] Steps/second: 0.177477, Examples/second: 25.182777\n",
      "I0710 11:00:55.421153 140295626643200 trainer.py:508] step:  6884, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:94.401299 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3015347 log_pplx:3.07142 loss:123.50948 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.64966\n",
      "I0710 11:01:01.727076 140295626643200 summary_utils.py:349] Steps/second: 0.177468, Examples/second: 25.176820\n",
      "I0710 11:01:01.727919 140295626643200 trainer.py:508] step:  6885, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:107.4681 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3017195 log_pplx:3.1528487 loss:126.74452 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.65649\n",
      "I0710 11:01:05.537261 140295626643200 summary_utils.py:349] Steps/second: 0.177492, Examples/second: 25.181661\n",
      "I0710 11:01:05.538101 140295626643200 trainer.py:508] step:  6886, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:71.263145 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3036599 log_pplx:3.5948398 loss:90.410233 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.66302\n",
      "I0710 11:01:11.770338 140295626643200 summary_utils.py:349] Steps/second: 0.177484, Examples/second: 25.175849\n",
      "I0710 11:01:11.771107 140295626643200 trainer.py:508] step:  6887, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:88.102013 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3019508 log_pplx:3.2160969 loss:126.47302 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.66858\n",
      "I0710 11:01:21.412717 140295626643200 summary_utils.py:349] Steps/second: 0.177430, Examples/second: 25.160547\n",
      "I0710 11:01:21.413453 140295626643200 trainer.py:508] step:  6888, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:174.7256 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3010385 log_pplx:2.5818267 loss:186.0206 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:527.67371\n",
      "I0710 11:01:27.943063 140295626643200 summary_utils.py:349] Steps/second: 0.177418, Examples/second: 25.154187\n",
      "I0710 11:01:27.943900 140295626643200 trainer.py:508] step:  6889, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:84.344749 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3018302 log_pplx:3.1863387 loss:128.44926 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.67871\n",
      "I0710 11:01:31.021298 140295626643200 summary_utils.py:349] Steps/second: 0.177453, Examples/second: 25.167654\n",
      "I0710 11:01:31.022109 140295626643200 trainer.py:508] step:  6890, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:78.918518 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3033006 log_pplx:3.7505124 loss:59.641937 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.68427\n",
      "I0710 11:01:34.957924 140295626643200 summary_utils.py:349] Steps/second: 0.177475, Examples/second: 25.172247\n",
      "I0710 11:01:34.958908 140295626643200 trainer.py:508] step:  6891, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:57.963062 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3039818 log_pplx:3.6904011 loss:93.989906 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.68896\n",
      "I0710 11:01:41.362479 140295626643200 summary_utils.py:349] Steps/second: 0.177465, Examples/second: 25.166127\n",
      "I0710 11:01:41.363261 140295626643200 trainer.py:508] step:  6892, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:85.540802 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3019924 log_pplx:3.123486 loss:129.8199 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.69324\n",
      "I0710 11:01:47.489852 140295626643200 summary_utils.py:349] Steps/second: 0.177458, Examples/second: 25.160538\n",
      "I0710 11:01:47.490678 140295626643200 trainer.py:508] step:  6893, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:94.454002 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3015813 log_pplx:3.1104517 loss:125.1179 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.698\n",
      "I0710 11:01:53.780061 140295626643200 summary_utils.py:349] Steps/second: 0.177450, Examples/second: 25.154646\n",
      "I0710 11:01:53.780861 140295626643200 trainer.py:508] step:  6894, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:77.185745 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3020135 log_pplx:3.0866399 loss:122.5396 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.70331\n",
      "I0710 11:01:57.598705 140295626643200 summary_utils.py:349] Steps/second: 0.177474, Examples/second: 25.159458\n",
      "I0710 11:01:57.599565 140295626643200 trainer.py:508] step:  6895, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:40.811611 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3036981 log_pplx:3.6018987 loss:90.407654 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.70886\n",
      "I0710 11:02:00.702150 140295626643200 summary_utils.py:349] Steps/second: 0.177508, Examples/second: 25.172846\n",
      "I0710 11:02:00.703045 140295626643200 trainer.py:508] step:  6896, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:27.66223 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3034424 log_pplx:3.6167853 loss:56.879597 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.71411\n",
      "I0710 11:02:07.296083 140295626643200 summary_utils.py:349] Steps/second: 0.177495, Examples/second: 25.166379\n",
      "I0710 11:02:07.296975 140295626643200 trainer.py:508] step:  6897, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:72.226883 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3021785 log_pplx:3.124716 loss:123.15288 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.71887\n",
      "I0710 11:02:11.333735 140295626643200 summary_utils.py:349] Steps/second: 0.177516, Examples/second: 25.170769\n",
      "I0710 11:02:11.334526 140295626643200 trainer.py:508] step:  6898, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:38.829777 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3038821 log_pplx:3.5038099 loss:88.296005 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.72375\n",
      "I0710 11:02:20.314598 140295626643200 summary_utils.py:349] Steps/second: 0.177472, Examples/second: 25.156790\n",
      "I0710 11:02:20.315390 140295626643200 trainer.py:508] step:  6899, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:150.37312 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3012829 log_pplx:2.5775018 loss:176.9455 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:527.72839\n",
      "I0710 11:02:26.634302 140295626643200 summary_utils.py:349] Steps/second: 0.177462, Examples/second: 25.150858\n",
      "I0710 11:02:26.635120 140295626643200 trainer.py:508] step:  6900, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:70.948936 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3020904 log_pplx:3.12745 loss:123.18243 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.73431\n",
      "I0710 11:02:33.103452 140295626643200 summary_utils.py:349] Steps/second: 0.177451, Examples/second: 25.144650\n",
      "I0710 11:02:33.104251 140295626643200 trainer.py:508] step:  6901, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:72.653961 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3024927 log_pplx:3.1329229 loss:125.7477 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.74005\n",
      "I0710 11:02:36.195772 140295626643200 summary_utils.py:349] Steps/second: 0.177485, Examples/second: 25.158026\n",
      "I0710 11:02:36.196614 140295626643200 trainer.py:508] step:  6902, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:38.153812 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3038366 log_pplx:3.6384223 loss:56.750858 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.74585\n",
      "I0710 11:02:42.492718 140295626643200 summary_utils.py:349] Steps/second: 0.177476, Examples/second: 25.152144\n",
      "I0710 11:02:42.493529 140295626643200 trainer.py:508] step:  6903, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:79.744545 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3027169 log_pplx:3.2091835 loss:131.2556 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.75171\n",
      "I0710 11:02:46.329923 140295626643200 summary_utils.py:349] Steps/second: 0.177500, Examples/second: 25.156905\n",
      "I0710 11:02:46.330716 140295626643200 trainer.py:508] step:  6904, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:43.449276 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3042073 log_pplx:3.566457 loss:89.651817 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.75745\n",
      "I0710 11:02:48.500764 140295626643200 summary_utils.py:349] Steps/second: 0.177546, Examples/second: 25.191199\n",
      "I0710 11:02:48.501544 140295626643200 trainer.py:508] step:  6905, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:13.269755 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3021096 log_pplx:3.5969284 loss:26.850506 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:527.76294\n",
      "I0710 11:02:55.145975 140295626643200 summary_utils.py:349] Steps/second: 0.177533, Examples/second: 25.184649\n",
      "I0710 11:02:55.146802 140295626643200 trainer.py:508] step:  6906, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:70.350746 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3026142 log_pplx:3.0710001 loss:124.83616 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.76813\n",
      "I0710 11:02:59.052234 140295626643200 summary_utils.py:349] Steps/second: 0.177556, Examples/second: 25.189265\n",
      "I0710 11:02:59.053001 140295626643200 trainer.py:508] step:  6907, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:47.292171 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3042177 log_pplx:3.5980949 loss:89.210266 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.77313\n",
      "I0710 11:03:05.414366 140295626643200 summary_utils.py:349] Steps/second: 0.177546, Examples/second: 25.183256\n",
      "I0710 11:03:05.415245 140295626643200 trainer.py:508] step:  6908, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:71.635872 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3024987 log_pplx:3.1054978 loss:124.72456 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.7782\n",
      "I0710 11:03:08.405105 140295626643200 summary_utils.py:349] Steps/second: 0.177581, Examples/second: 25.196783\n",
      "I0710 11:03:08.405959 140295626643200 trainer.py:508] step:  6909, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:26.96858 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3036031 log_pplx:3.5372722 loss:55.242245 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.78326\n",
      "I0710 11:03:09.039014 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 11:03:14.218060 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 11:03:14.841314 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00006909\n",
      "I0710 11:03:14.841586 140295626643200 summary_utils.py:349] Steps/second: 0.177571, Examples/second: 25.190633\n",
      "I0710 11:03:14.842467 140295626643200 trainer.py:508] step:  6910, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:70.518059 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3022788 log_pplx:3.0820227 loss:124.43667 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.7879\n",
      "I0710 11:03:23.999905 140295626643200 summary_utils.py:349] Steps/second: 0.177524, Examples/second: 25.176373\n",
      "I0710 11:03:24.000724 140295626643200 trainer.py:508] step:  6911, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:173.63278 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3018417 log_pplx:2.5604341 loss:191.77652 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:527.79291\n",
      "I0710 11:03:27.930830 140295626643200 summary_utils.py:349] Steps/second: 0.177546, Examples/second: 25.180935\n",
      "I0710 11:03:27.931851 140295626643200 trainer.py:508] step:  6912, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:42.597492 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.304318 log_pplx:3.5433667 loss:89.912926 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.79852\n",
      "I0710 11:03:34.356224 140295626643200 summary_utils.py:349] Steps/second: 0.177536, Examples/second: 25.174823\n",
      "I0710 11:03:34.357012 140295626643200 trainer.py:508] step:  6913, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:79.416992 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3026948 log_pplx:3.1815462 loss:128.25607 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.80402\n",
      "I0710 11:03:40.902974 140295626643200 summary_utils.py:349] Steps/second: 0.177524, Examples/second: 25.168489\n",
      "I0710 11:03:40.903747 140295626643200 trainer.py:508] step:  6914, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:81.235779 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3030421 log_pplx:3.1351862 loss:130.38455 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.80939\n",
      "I0710 11:03:44.013641 140295626643200 summary_utils.py:349] Steps/second: 0.177557, Examples/second: 25.181758\n",
      "I0710 11:03:44.014454 140295626643200 trainer.py:508] step:  6915, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:26.529564 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3037295 log_pplx:3.6232586 loss:54.745171 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.81482\n",
      "I0710 11:03:50.312282 140295626643200 summary_utils.py:349] Steps/second: 0.177548, Examples/second: 25.175891\n",
      "I0710 11:03:50.313062 140295626643200 trainer.py:508] step:  6916, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:71.159058 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3029126 log_pplx:3.142966 loss:129.37234 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.81982\n",
      "I0710 11:03:54.266891 140295626643200 summary_utils.py:349] Steps/second: 0.177571, Examples/second: 25.180401\n",
      "I0710 11:03:54.267647 140295626643200 trainer.py:508] step:  6917, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:39.067986 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3042088 log_pplx:3.5798807 loss:90.50386 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.82495\n",
      "I0710 11:04:00.393312 140295626643200 summary_utils.py:349] Steps/second: 0.177564, Examples/second: 25.174862\n",
      "I0710 11:04:00.394113 140295626643200 trainer.py:508] step:  6918, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:72.946083 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3026881 log_pplx:3.0815189 loss:125.18671 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.82971\n",
      "I0710 11:04:03.503945 140295626643200 summary_utils.py:349] Steps/second: 0.177597, Examples/second: 25.188110\n",
      "I0710 11:04:03.504813 140295626643200 trainer.py:508] step:  6919, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:22.226793 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3040807 log_pplx:3.5535548 loss:54.663662 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.83453\n",
      "I0710 11:04:10.095789 140295626643200 summary_utils.py:349] Steps/second: 0.177585, Examples/second: 25.181699\n",
      "I0710 11:04:10.096575 140295626643200 trainer.py:508] step:  6920, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:77.592766 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3033959 log_pplx:3.128808 loss:131.84015 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.83905\n",
      "I0710 11:04:13.946489 140295626643200 summary_utils.py:349] Steps/second: 0.177608, Examples/second: 25.186395\n",
      "I0710 11:04:13.947299 140295626643200 trainer.py:508] step:  6921, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:37.589775 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3047717 log_pplx:3.5797873 loss:90.165894 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.84369\n",
      "I0710 11:04:24.727747 140295626643200 summary_utils.py:349] Steps/second: 0.177540, Examples/second: 25.169159\n",
      "I0710 11:04:24.728518 140295626643200 trainer.py:508] step:  6922, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:193.75073 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.301712 log_pplx:2.5632868 loss:189.36282 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:527.84827\n",
      "I0710 11:04:31.199583 140295626643200 summary_utils.py:349] Steps/second: 0.177529, Examples/second: 25.162991\n",
      "I0710 11:04:31.200588 140295626643200 trainer.py:508] step:  6923, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:69.830872 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3032777 log_pplx:3.1243157 loss:128.60466 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.85242\n",
      "I0710 11:04:37.560130 140295626643200 summary_utils.py:349] Steps/second: 0.177519, Examples/second: 25.157038\n",
      "I0710 11:04:37.560969 140295626643200 trainer.py:508] step:  6924, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:92.543358 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3027442 log_pplx:3.1239874 loss:123.90515 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.85699\n",
      "I0710 11:04:40.660525 140295626643200 summary_utils.py:349] Steps/second: 0.177553, Examples/second: 25.170273\n",
      "I0710 11:04:40.661328 140295626643200 trainer.py:508] step:  6925, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:29.656443 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3042279 log_pplx:3.5845218 loss:55.448074 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.86243\n",
      "I0710 11:04:44.508679 140295626643200 summary_utils.py:349] Steps/second: 0.177576, Examples/second: 25.174966\n",
      "I0710 11:04:44.509483 140295626643200 trainer.py:508] step:  6926, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:37.556683 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3045822 log_pplx:3.5113752 loss:88.815849 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.8678\n",
      "I0710 11:04:50.596962 140295626643200 summary_utils.py:349] Steps/second: 0.177570, Examples/second: 25.169523\n",
      "I0710 11:04:50.597955 140295626643200 trainer.py:508] step:  6927, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:77.062271 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3029778 log_pplx:3.1123114 loss:127.33244 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.87317\n",
      "I0710 11:04:57.071330 140295626643200 summary_utils.py:349] Steps/second: 0.177559, Examples/second: 25.163362\n",
      "I0710 11:04:57.072113 140295626643200 trainer.py:508] step:  6928, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:77.627075 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3032596 log_pplx:3.1188581 loss:128.53595 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.87909\n",
      "I0710 11:05:07.808271 140295626643200 summary_utils.py:349] Steps/second: 0.177492, Examples/second: 25.146282\n",
      "I0710 11:05:07.809048 140295626643200 trainer.py:508] step:  6929, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:182.24011 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3026311 log_pplx:2.5967436 loss:194.62595 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:527.88507\n",
      "I0710 11:05:10.030085 140295626643200 summary_utils.py:349] Steps/second: 0.177537, Examples/second: 25.180123\n",
      "I0710 11:05:10.030855 140295626643200 trainer.py:508] step:  6930, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:18.385466 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3028935 log_pplx:3.6083353 loss:26.371859 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:527.89166\n",
      "I0710 11:05:13.916027 140295626643200 summary_utils.py:349] Steps/second: 0.177560, Examples/second: 25.184732\n",
      "I0710 11:05:13.916784 140295626643200 trainer.py:508] step:  6931, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:107.04823 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3047868 log_pplx:3.6781511 loss:92.689407 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.89789\n",
      "I0710 11:05:17.030593 140295626643200 summary_utils.py:349] Steps/second: 0.177593, Examples/second: 25.197899\n",
      "I0710 11:05:17.051867 140295626643200 trainer.py:508] step:  6932, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:35.256012 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3043232 log_pplx:3.5679212 loss:55.414276 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.90283\n",
      "I0710 11:05:23.669813 140295626643200 summary_utils.py:349] Steps/second: 0.177580, Examples/second: 25.191429\n",
      "I0710 11:05:23.670615 140295626643200 trainer.py:508] step:  6933, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:105.27979 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3035082 log_pplx:3.1394005 loss:132.40421 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.90723\n",
      "I0710 11:05:29.891192 140295626643200 summary_utils.py:349] Steps/second: 0.177572, Examples/second: 25.185745\n",
      "I0710 11:05:29.891964 140295626643200 trainer.py:508] step:  6934, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:83.751343 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3032153 log_pplx:3.1274781 loss:127.21017 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.91144\n",
      "I0710 11:05:36.434215 140295626643200 summary_utils.py:349] Steps/second: 0.177560, Examples/second: 25.179467\n",
      "I0710 11:05:36.435470 140295626643200 trainer.py:508] step:  6935, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:90.82991 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3031334 log_pplx:3.1492701 loss:126.28573 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.91571\n",
      "I0710 11:05:40.407642 140295626643200 summary_utils.py:349] Steps/second: 0.177582, Examples/second: 25.183904\n",
      "I0710 11:05:40.408582 140295626643200 trainer.py:508] step:  6936, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:103.45092 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.304708 log_pplx:3.6006377 loss:89.678391 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.92078\n",
      "I0710 11:05:43.517854 140295626643200 summary_utils.py:349] Steps/second: 0.177615, Examples/second: 25.197053\n",
      "I0710 11:05:43.518692 140295626643200 base_runner.py:111] step:  6937, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:48.545506 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3044513 log_pplx:3.5826232 loss:54.565029 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.92651\n",
      "I0710 11:05:49.736275 140295626643200 summary_utils.py:349] Steps/second: 0.177607, Examples/second: 25.191381\n",
      "I0710 11:05:49.737060 140295626643200 trainer.py:508] step:  6938, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:82.475449 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3035991 log_pplx:3.1820955 loss:131.34099 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.93237\n",
      "I0710 11:05:56.116144 140295626643200 summary_utils.py:349] Steps/second: 0.177597, Examples/second: 25.185413\n",
      "I0710 11:05:56.116914 140295626643200 trainer.py:508] step:  6939, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:99.496979 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3031032 log_pplx:3.1823301 loss:126.57717 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.9386\n",
      "I0710 11:06:06.825636 140295626643200 summary_utils.py:349] Steps/second: 0.177531, Examples/second: 25.168441\n",
      "I0710 11:06:06.826649 140295626643200 trainer.py:508] step:  6940, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:168.33696 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3025298 log_pplx:2.6298862 loss:200.85757 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:527.94476\n",
      "I0710 11:06:10.758613 140295626643200 summary_utils.py:349] Steps/second: 0.177553, Examples/second: 25.172947\n",
      "I0710 11:06:10.759509 140295626643200 trainer.py:508] step:  6941, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:65.337585 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3049133 log_pplx:3.5195928 loss:88.95771 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.95093\n",
      "I0710 11:06:17.031749 140295626643200 summary_utils.py:349] Steps/second: 0.177545, Examples/second: 25.167195\n",
      "I0710 11:06:17.032682 140295626643200 trainer.py:508] step:  6942, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:74.693474 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3034598 log_pplx:3.1720531 loss:128.70604 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.9566\n",
      "I0710 11:06:23.408346 140295626643200 summary_utils.py:349] Steps/second: 0.177535, Examples/second: 25.161257\n",
      "I0710 11:06:23.409138 140295626643200 trainer.py:508] step:  6943, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:77.804726 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3037941 log_pplx:3.1431541 loss:132.68039 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.96246\n",
      "I0710 11:06:26.511615 140295626643200 summary_utils.py:349] Steps/second: 0.177568, Examples/second: 25.174382\n",
      "I0710 11:06:26.512504 140295626643200 trainer.py:508] step:  6944, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:32.901768 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3047224 log_pplx:3.5398612 loss:55.503918 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:527.9682\n",
      "I0710 11:06:30.345460 140295626643200 summary_utils.py:349] Steps/second: 0.177592, Examples/second: 25.179064\n",
      "I0710 11:06:30.346271 140295626643200 trainer.py:508] step:  6945, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:54.393196 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3052731 log_pplx:3.6138353 loss:90.368469 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.97351\n",
      "I0710 11:06:36.842739 140295626643200 summary_utils.py:349] Steps/second: 0.177580, Examples/second: 25.172902\n",
      "I0710 11:06:36.843537 140295626643200 trainer.py:508] step:  6946, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:103.59903 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.303494 log_pplx:3.1127753 loss:127.15688 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.97906\n",
      "I0710 11:06:43.367739 140295626643200 summary_utils.py:349] Steps/second: 0.177569, Examples/second: 25.166694\n",
      "I0710 11:06:43.368539 140295626643200 trainer.py:508] step:  6947, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:85.061989 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3036863 log_pplx:3.1384251 loss:127.26314 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.98511\n",
      "I0710 11:06:47.184957 140295626643200 summary_utils.py:349] Steps/second: 0.177592, Examples/second: 25.171403\n",
      "I0710 11:06:47.185749 140295626643200 trainer.py:508] step:  6948, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:40.217533 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3055723 log_pplx:3.6207352 loss:90.925705 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:527.99133\n",
      "I0710 11:06:53.530838 140295626643200 summary_utils.py:349] Steps/second: 0.177583, Examples/second: 25.165533\n",
      "I0710 11:06:53.531600 140295626643200 trainer.py:508] step:  6949, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:75.134865 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3032562 log_pplx:3.0411947 loss:121.49573 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:527.99707\n",
      "I0710 11:06:56.557603 140295626643200 summary_utils.py:349] Steps/second: 0.177617, Examples/second: 25.178770\n",
      "I0710 11:06:56.558392 140295626643200 trainer.py:508] step:  6950, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:46.167091 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3046483 log_pplx:3.5463676 loss:55.037964 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.0022\n",
      "I0710 11:06:59.249548 140295635035904 trainer.py:345] Write summary @6950\n",
      "I0710 11:07:07.920348 140295626643200 summary_utils.py:349] Steps/second: 0.177542, Examples/second: 25.163612\n",
      "I0710 11:07:07.921810 140295626643200 trainer.py:508] step:  6951, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:91.292839 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3036457 log_pplx:3.1314058 loss:126.39137 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.00671\n",
      "I0710 11:07:25.317936 140295626643200 summary_utils.py:349] Steps/second: 0.177389, Examples/second: 25.134394\n",
      "I0710 11:07:25.319966 140295626643200 trainer.py:508] step:  6952, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:172.43932 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3028816 log_pplx:2.5687435 loss:192.3989 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:528.01074\n",
      "I0710 11:07:30.754103 140295626643200 summary_utils.py:349] Steps/second: 0.177391, Examples/second: 25.136109\n",
      "I0710 11:07:30.754981 140295626643200 trainer.py:508] step:  6953, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:41.87775 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3052651 log_pplx:3.5473003 loss:88.970726 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.01465\n",
      "I0710 11:07:40.689177 140295626643200 summary_utils.py:349] Steps/second: 0.177335, Examples/second: 25.123656\n",
      "I0710 11:07:40.690370 140295626643200 trainer.py:508] step:  6954, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:87.923889 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3038157 log_pplx:3.2310731 loss:131.1008 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.0188\n",
      "I0710 11:07:43.584344 140295626643200 summary_utils.py:349] Steps/second: 0.177371, Examples/second: 25.155878\n",
      "I0710 11:07:43.585814 140295626643200 trainer.py:508] step:  6955, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:22.416676 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3035377 log_pplx:3.6904242 loss:26.784405 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:528.02368\n",
      "I0710 11:07:47.965080 140295626643200 summary_utils.py:349] Steps/second: 0.177387, Examples/second: 25.166570\n",
      "I0710 11:07:47.966270 140295626643200 trainer.py:508] step:  6956, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:32.844807 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.30514 log_pplx:3.6208062 loss:57.197418 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.02856\n",
      "I0710 11:07:58.677612 140295635035904 trainer.py:354] Write summary done: step 6950\n",
      "I0710 11:07:59.055615 140295626643200 summary_utils.py:349] Steps/second: 0.177317, Examples/second: 25.151980\n",
      "I0710 11:07:59.056392 140295626643200 trainer.py:508] step:  6957, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:87.325546 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3042772 log_pplx:3.1500423 loss:130.17551 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.03357\n",
      "I0710 11:08:02.884522 140295626643200 summary_utils.py:349] Steps/second: 0.177340, Examples/second: 25.156646\n",
      "I0710 11:08:02.885385 140295626643200 trainer.py:508] step:  6958, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:38.585896 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.305908 log_pplx:3.5791955 loss:91.068153 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.03894\n",
      "I0710 11:08:09.342877 140295626643200 summary_utils.py:349] Steps/second: 0.177329, Examples/second: 25.150608\n",
      "I0710 11:08:09.343725 140295626643200 trainer.py:508] step:  6959, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:74.501007 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3036934 log_pplx:3.1267827 loss:124.95406 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.04425\n",
      "I0710 11:08:19.953192 140295626643200 summary_utils.py:349] Steps/second: 0.177265, Examples/second: 25.134011\n",
      "I0710 11:08:19.954186 140295626643200 trainer.py:508] step:  6960, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:167.69176 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3026224 log_pplx:2.5859711 loss:186.31921 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:528.0498\n",
      "I0710 11:08:26.260095 140295626643200 summary_utils.py:349] Steps/second: 0.177256, Examples/second: 25.128270\n",
      "I0710 11:08:26.260882 140295626643200 trainer.py:508] step:  6961, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:92.82444 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3039768 log_pplx:3.1590314 loss:127.11153 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.0556\n",
      "I0710 11:08:29.352824 140295626643200 summary_utils.py:349] Steps/second: 0.177289, Examples/second: 25.141304\n",
      "I0710 11:08:29.353628 140295626643200 trainer.py:508] step:  6962, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:41.749397 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3050705 log_pplx:3.6043751 loss:55.881886 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.06134\n",
      "I0710 11:08:33.209529 140295626643200 summary_utils.py:349] Steps/second: 0.177312, Examples/second: 25.145911\n",
      "I0710 11:08:33.210363 140295626643200 trainer.py:508] step:  6963, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:45.945244 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3055702 log_pplx:3.5787261 loss:91.861427 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.06641\n",
      "I0710 11:08:39.534721 140295626643200 summary_utils.py:349] Steps/second: 0.177304, Examples/second: 25.140136\n",
      "I0710 11:08:39.535547 140295626643200 trainer.py:508] step:  6964, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:70.731323 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3038123 log_pplx:3.1008101 loss:122.44322 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.07117\n",
      "I0710 11:08:45.876034 140295626643200 summary_utils.py:349] Steps/second: 0.177294, Examples/second: 25.134337\n",
      "I0710 11:08:45.876839 140295626643200 trainer.py:508] step:  6965, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:76.608826 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3039844 log_pplx:3.1500309 loss:127.8125 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.07654\n",
      "I0710 11:08:49.782649 140295626643200 summary_utils.py:349] Steps/second: 0.177317, Examples/second: 25.138849\n",
      "I0710 11:08:49.783438 140295626643200 trainer.py:508] step:  6966, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:49.520039 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.305725 log_pplx:3.5843759 loss:90.348679 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.08228\n",
      "I0710 11:08:56.236918 140295626643200 summary_utils.py:349] Steps/second: 0.177306, Examples/second: 25.132847\n",
      "I0710 11:08:56.237706 140295626643200 trainer.py:508] step:  6967, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:77.61779 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3042874 log_pplx:3.1679955 loss:128.66022 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.08813\n",
      "I0710 11:08:59.262014 140295626643200 summary_utils.py:349] Steps/second: 0.177340, Examples/second: 25.145976\n",
      "I0710 11:08:59.262756 140295626643200 trainer.py:508] step:  6968, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:26.134314 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3053478 log_pplx:3.628824 loss:56.771248 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.09412\n",
      "I0710 11:09:05.800821 140295626643200 summary_utils.py:349] Steps/second: 0.177329, Examples/second: 25.139820\n",
      "I0710 11:09:05.801599 140295626643200 trainer.py:508] step:  6969, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:69.390366 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3041605 log_pplx:3.1072066 loss:126.07491 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.09973\n",
      "I0710 11:09:09.691466 140295626643200 summary_utils.py:349] Steps/second: 0.177351, Examples/second: 25.144353\n",
      "I0710 11:09:09.692248 140295626643200 trainer.py:508] step:  6970, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:37.673306 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3057064 log_pplx:3.5620599 loss:89.652596 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.10547\n",
      "I0710 11:09:15.688976 140295626643200 summary_utils.py:349] Steps/second: 0.177347, Examples/second: 25.139194\n",
      "I0710 11:09:15.689731 140295626643200 trainer.py:508] step:  6971, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:87.64637 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3042675 log_pplx:3.1701436 loss:127.79642 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.11072\n",
      "I0710 11:09:25.096270 140295626643200 summary_utils.py:349] Steps/second: 0.177298, Examples/second: 25.124887\n",
      "I0710 11:09:25.097059 140295626643200 trainer.py:508] step:  6972, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:136.90617 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3030657 log_pplx:2.4917819 loss:174.05098 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:528.11591\n",
      "I0710 11:09:31.481360 140295626643200 summary_utils.py:349] Steps/second: 0.177288, Examples/second: 25.119034\n",
      "I0710 11:09:31.482328 140295626643200 trainer.py:508] step:  6973, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:76.641136 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3044975 log_pplx:3.1561744 loss:130.54727 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.12164\n",
      "I0710 11:09:34.485728 140295626643200 summary_utils.py:349] Steps/second: 0.177322, Examples/second: 25.132170\n",
      "I0710 11:09:34.486668 140295626643200 trainer.py:508] step:  6974, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:23.503044 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3055444 log_pplx:3.588527 loss:55.8885 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.12756\n",
      "I0710 11:09:38.372195 140295626643200 summary_utils.py:349] Steps/second: 0.177345, Examples/second: 25.136704\n",
      "I0710 11:09:38.373126 140295626643200 trainer.py:508] step:  6975, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:40.748234 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3060886 log_pplx:3.5473375 loss:89.304214 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.13306\n",
      "I0710 11:09:44.906152 140295626643200 summary_utils.py:349] Steps/second: 0.177333, Examples/second: 25.130579\n",
      "I0710 11:09:44.907218 140295626643200 trainer.py:508] step:  6976, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:88.881096 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3045273 log_pplx:3.150465 loss:130.35049 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.13843\n",
      "I0710 11:09:47.105275 140295626643200 summary_utils.py:349] Steps/second: 0.177378, Examples/second: 25.163782\n",
      "I0710 11:09:47.106077 140295626643200 trainer.py:508] step:  6977, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:28.768724 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3042874 log_pplx:3.6330659 loss:27.247995 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:528.14417\n",
      "I0710 11:09:53.697781 140295626643200 summary_utils.py:349] Steps/second: 0.177365, Examples/second: 25.157541\n",
      "I0710 11:09:53.698593 140295626643200 trainer.py:508] step:  6978, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:72.826088 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3041117 log_pplx:3.1802964 loss:127.13235 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.14935\n",
      "I0710 11:09:57.536512 140295626643200 summary_utils.py:349] Steps/second: 0.177389, Examples/second: 25.162148\n",
      "I0710 11:09:57.537274 140295626643200 trainer.py:508] step:  6979, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:42.099926 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3059744 log_pplx:3.5343387 loss:89.043251 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.15472\n",
      "I0710 11:10:00.591897 140295626643200 summary_utils.py:349] Steps/second: 0.177422, Examples/second: 25.175156\n",
      "I0710 11:10:00.592670 140295626643200 trainer.py:508] step:  6980, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:21.954435 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3058758 log_pplx:3.5545161 loss:55.969742 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.15955\n",
      "I0710 11:10:06.995687 140295626643200 summary_utils.py:349] Steps/second: 0.177412, Examples/second: 25.169261\n",
      "I0710 11:10:06.996500 140295626643200 trainer.py:508] step:  6981, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:66.799492 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3040208 log_pplx:3.0838609 loss:118.30461 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.16418\n",
      "I0710 11:10:15.723181 140295626643200 summary_utils.py:349] Steps/second: 0.177372, Examples/second: 25.156227\n",
      "I0710 11:10:15.724014 140295626643200 trainer.py:508] step:  6982, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:136.51205 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3032449 log_pplx:2.4579759 loss:174.08614 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:528.16876\n",
      "I0710 11:10:21.858526 140295626643200 summary_utils.py:349] Steps/second: 0.177366, Examples/second: 25.150836\n",
      "I0710 11:10:21.859287 140295626643200 trainer.py:508] step:  6983, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:72.288826 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3040534 log_pplx:3.1020665 loss:122.14388 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.17358\n",
      "I0710 11:10:25.694475 140295626643200 summary_utils.py:349] Steps/second: 0.177389, Examples/second: 25.155441\n",
      "I0710 11:10:25.695297 140295626643200 trainer.py:508] step:  6984, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:41.657036 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3062736 log_pplx:3.5426836 loss:88.522812 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.17859\n",
      "I0710 11:10:32.139733 140295626643200 summary_utils.py:349] Steps/second: 0.177379, Examples/second: 25.149490\n",
      "I0710 11:10:32.140758 140295626643200 trainer.py:508] step:  6985, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:72.809181 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3044788 log_pplx:3.1280529 loss:123.04978 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.18347\n",
      "I0710 11:10:35.230027 140295626643200 summary_utils.py:349] Steps/second: 0.177411, Examples/second: 25.162405\n",
      "I0710 11:10:35.230907 140295626643200 trainer.py:508] step:  6986, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:37.104641 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3058182 log_pplx:3.5952966 loss:56.611877 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.18848\n",
      "I0710 11:10:41.774945 140295626643200 summary_utils.py:349] Steps/second: 0.177400, Examples/second: 25.156273\n",
      "I0710 11:10:41.775722 140295626643200 trainer.py:508] step:  6987, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:72.115845 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3050164 log_pplx:3.1146798 loss:129.92108 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.19366\n",
      "I0710 11:10:45.680564 140295626643200 summary_utils.py:349] Steps/second: 0.177422, Examples/second: 25.160743\n",
      "I0710 11:10:45.681365 140295626643200 trainer.py:508] step:  6988, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:46.97237 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3061405 log_pplx:3.5362275 loss:88.273087 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.19885\n",
      "I0710 11:10:51.975561 140295626643200 summary_utils.py:349] Steps/second: 0.177413, Examples/second: 25.155071\n",
      "I0710 11:10:51.976324 140295626643200 trainer.py:508] step:  6989, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:69.846664 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3044775 log_pplx:3.1324635 loss:124.0847 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.20367\n",
      "I0710 11:11:01.268282 140295626643200 summary_utils.py:349] Steps/second: 0.177366, Examples/second: 25.141061\n",
      "I0710 11:11:01.269064 140295626643200 trainer.py:508] step:  6990, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:148.64351 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3037455 log_pplx:2.4632928 loss:173.53897 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:528.20856\n",
      "I0710 11:11:07.820454 140295626643200 summary_utils.py:349] Steps/second: 0.177355, Examples/second: 25.134938\n",
      "I0710 11:11:07.821219 140295626643200 trainer.py:508] step:  6991, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:69.228249 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3050395 log_pplx:3.0927188 loss:127.14941 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.21362\n",
      "I0710 11:11:11.721658 140295626643200 summary_utils.py:349] Steps/second: 0.177377, Examples/second: 25.139413\n",
      "I0710 11:11:11.722467 140295626643200 trainer.py:508] step:  6992, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:39.306435 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3065491 log_pplx:3.5293717 loss:89.712227 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.21881\n",
      "I0710 11:11:14.810385 140295626643200 summary_utils.py:349] Steps/second: 0.177410, Examples/second: 25.152297\n",
      "I0710 11:11:14.811165 140295626643200 trainer.py:508] step:  6993, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:25.597359 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3061035 log_pplx:3.563477 loss:56.403156 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.22388\n",
      "I0710 11:11:21.280772 140295626643200 summary_utils.py:349] Steps/second: 0.177399, Examples/second: 25.146322\n",
      "I0710 11:11:21.281767 140295626643200 trainer.py:508] step:  6994, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:75.474579 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3050518 log_pplx:3.0971291 loss:126.86615 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.22876\n",
      "I0710 11:11:27.705120 140295626643200 summary_utils.py:349] Steps/second: 0.177389, Examples/second: 25.140437\n",
      "I0710 11:11:27.706069 140295626643200 trainer.py:508] step:  6995, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:84.291428 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3050674 log_pplx:3.1434078 loss:130.88364 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.23401\n",
      "I0710 11:11:31.548040 140295626643200 summary_utils.py:349] Steps/second: 0.177412, Examples/second: 25.145010\n",
      "I0710 11:11:31.548974 140295626643200 trainer.py:508] step:  6996, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:38.282055 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3066627 log_pplx:3.5099745 loss:89.131416 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.23987\n",
      "I0710 11:11:34.584060 140295626643200 summary_utils.py:349] Steps/second: 0.177445, Examples/second: 25.157970\n",
      "I0710 11:11:34.585121 140295626643200 trainer.py:508] step:  6997, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:26.756666 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3062943 log_pplx:3.5840676 loss:55.497047 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.24573\n",
      "I0710 11:11:41.050543 140295626643200 summary_utils.py:349] Steps/second: 0.177435, Examples/second: 25.152008\n",
      "I0710 11:11:41.051337 140295626643200 trainer.py:508] step:  6998, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:70.287949 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3046927 log_pplx:3.0542169 loss:124.23027 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.2511\n",
      "I0710 11:11:47.442655 140295626643200 summary_utils.py:349] Steps/second: 0.177425, Examples/second: 25.146187\n",
      "I0710 11:11:47.443483 140295626643200 trainer.py:508] step:  6999, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:78.296661 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3047999 log_pplx:3.0947561 loss:124.25446 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.25623\n",
      "I0710 11:11:51.279476 140295626643200 summary_utils.py:349] Steps/second: 0.177448, Examples/second: 25.150763\n",
      "I0710 11:11:51.280238 140295626643200 trainer.py:508] step:  7000, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:39.041702 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3064195 log_pplx:3.5148501 loss:89.167358 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.26117\n",
      "I0710 11:11:53.484699 140295626643200 summary_utils.py:349] Steps/second: 0.177492, Examples/second: 25.183650\n",
      "I0710 11:11:53.485527 140295626643200 trainer.py:508] step:  7001, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:15.022914 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3047847 log_pplx:3.5830071 loss:26.417683 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:528.26617\n",
      "I0710 11:12:04.518657 140295626643200 summary_utils.py:349] Steps/second: 0.177423, Examples/second: 25.166528\n",
      "I0710 11:12:04.519457 140295626643200 trainer.py:508] step:  7002, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:222.6734 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3038676 log_pplx:2.4960794 loss:183.71146 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:528.27087\n",
      "I0710 11:12:11.077548 140295626643200 summary_utils.py:349] Steps/second: 0.177411, Examples/second: 25.160408\n",
      "I0710 11:12:11.078492 140295626643200 trainer.py:508] step:  7003, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:75.29142 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3052032 log_pplx:3.1543181 loss:132.4025 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.27625\n",
      "I0710 11:12:14.189713 140295626643200 summary_utils.py:349] Steps/second: 0.177443, Examples/second: 25.173189\n",
      "I0710 11:12:14.190605 140295626643200 trainer.py:508] step:  7004, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:21.700737 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3060935 log_pplx:3.5387459 loss:55.251434 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.28198\n",
      "I0710 11:12:20.664385 140295626643200 summary_utils.py:349] Steps/second: 0.177432, Examples/second: 25.167223\n",
      "I0710 11:12:20.665288 140295626643200 trainer.py:508] step:  7005, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:103.94441 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3048605 log_pplx:3.1366498 loss:129.34761 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.28748\n",
      "I0710 11:12:24.685787 140295626643200 summary_utils.py:349] Steps/second: 0.177453, Examples/second: 25.171449\n",
      "I0710 11:12:24.686855 140295626643200 trainer.py:508] step:  7006, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:36.667442 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3066643 log_pplx:3.5597801 loss:88.861015 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.29285\n",
      "I0710 11:12:31.534794 140295626643200 summary_utils.py:349] Steps/second: 0.177438, Examples/second: 25.164811\n",
      "I0710 11:12:31.535691 140295626643200 trainer.py:508] step:  7007, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:80.944794 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3051662 log_pplx:3.0850644 loss:126.68046 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.29803\n",
      "I0710 11:12:34.620702 140295626643200 summary_utils.py:349] Steps/second: 0.177470, Examples/second: 25.177620\n",
      "I0710 11:12:34.621467 140295626643200 trainer.py:508] step:  7008, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:22.331926 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3064497 log_pplx:3.5471501 loss:55.729046 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.30328\n",
      "I0710 11:12:41.100791 140295626643200 summary_utils.py:349] Steps/second: 0.177459, Examples/second: 25.171651\n",
      "I0710 11:12:41.101554 140295626643200 trainer.py:508] step:  7009, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:81.820175 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3050879 log_pplx:3.186024 loss:128.63573 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.30823\n",
      "I0710 11:12:44.982115 140295626643200 summary_utils.py:349] Steps/second: 0.177482, Examples/second: 25.176122\n",
      "I0710 11:12:44.982937 140295626643200 trainer.py:508] step:  7010, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:56.951138 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3068622 log_pplx:3.5608838 loss:88.977585 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.3136\n",
      "I0710 11:12:51.415658 140295626643200 summary_utils.py:349] Steps/second: 0.177471, Examples/second: 25.170242\n",
      "I0710 11:12:51.416590 140295626643200 trainer.py:508] step:  7011, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:75.285728 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3055515 log_pplx:3.1685438 loss:131.6926 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.31921\n",
      "I0710 11:13:02.406997 140295626643200 summary_utils.py:349] Steps/second: 0.177403, Examples/second: 25.153279\n",
      "I0710 11:13:02.407863 140295626643200 trainer.py:508] step:  7012, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:571.30438 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3041704 log_pplx:2.5950778 loss:182.10959 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:528.32507\n",
      "I0710 11:13:08.808434 140295626643200 summary_utils.py:349] Steps/second: 0.177394, Examples/second: 25.147474\n",
      "I0710 11:13:08.809261 140295626643200 trainer.py:508] step:  7013, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:93.437256 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3054587 log_pplx:3.1112506 loss:132.30594 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.32953\n",
      "I0710 11:13:12.698988 140295626643200 summary_utils.py:349] Steps/second: 0.177416, Examples/second: 25.151926\n",
      "I0710 11:13:12.699777 140295626643200 trainer.py:508] step:  7014, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:47.540443 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3069435 log_pplx:3.5239348 loss:87.767998 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.33453\n",
      "I0710 11:13:15.731763 140295626643200 summary_utils.py:349] Steps/second: 0.177449, Examples/second: 25.164796\n",
      "I0710 11:13:15.732657 140295626643200 trainer.py:508] step:  7015, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:33.249866 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.306537 log_pplx:3.5674226 loss:55.392593 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.33978\n",
      "I0710 11:13:18.983302 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 11:13:22.390650 140295626643200 summary_utils.py:349] Steps/second: 0.177436, Examples/second: 25.158528\n",
      "I0710 11:13:22.391604 140295626643200 trainer.py:508] step:  7016, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:214.02448 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3048511 log_pplx:3.3780861 loss:131.91426 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.34448\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 11:13:24.338042 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 11:13:24.807750 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00007015\n",
      "I0710 11:13:28.756243 140295626643200 summary_utils.py:349] Steps/second: 0.177427, Examples/second: 25.152794\n",
      "I0710 11:13:28.757077 140295626643200 trainer.py:508] step:  7017, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:310.63474 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3053706 log_pplx:3.2484963 loss:133.79744 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.35077\n",
      "I0710 11:13:32.663620 140295626643200 summary_utils.py:349] Steps/second: 0.177449, Examples/second: 25.157208\n",
      "I0710 11:13:32.664483 140295626643200 trainer.py:508] step:  7018, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:109.92532 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3067458 log_pplx:3.67241 loss:92.751305 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.35852\n",
      "I0710 11:13:39.169683 140295626643200 summary_utils.py:349] Steps/second: 0.177437, Examples/second: 25.151227\n",
      "I0710 11:13:39.170512 140295626643200 trainer.py:508] step:  7019, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:105.76314 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3059391 log_pplx:3.1712434 loss:132.43904 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.36517\n",
      "I0710 11:13:42.226046 140295626643200 summary_utils.py:349] Steps/second: 0.177470, Examples/second: 25.164030\n",
      "I0710 11:13:42.226833 140295626643200 trainer.py:508] step:  7020, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:45.981892 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3066472 log_pplx:3.6745346 loss:56.696915 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.37177\n",
      "I0710 11:13:48.761475 140295626643200 summary_utils.py:349] Steps/second: 0.177459, Examples/second: 25.157996\n",
      "I0710 11:13:48.762539 140295626643200 trainer.py:508] step:  7021, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:169.92442 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3053215 log_pplx:3.2093775 loss:129.77919 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.37799\n",
      "I0710 11:13:52.690445 140295626643200 summary_utils.py:349] Steps/second: 0.177480, Examples/second: 25.162364\n",
      "I0710 11:13:52.691381 140295626643200 trainer.py:508] step:  7022, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:72.932907 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3072997 log_pplx:3.5906951 loss:90.418198 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.38379\n",
      "I0710 11:14:03.650749 140295626643200 summary_utils.py:349] Steps/second: 0.177413, Examples/second: 25.145537\n",
      "I0710 11:14:03.651677 140295626643200 trainer.py:508] step:  7023, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:645.25519 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.304695 log_pplx:2.8701718 loss:209.23552 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:528.3894\n",
      "I0710 11:14:09.751240 140295626643200 summary_utils.py:349] Steps/second: 0.177407, Examples/second: 25.140301\n",
      "I0710 11:14:09.752178 140295626643200 trainer.py:508] step:  7024, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:124.58736 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3052542 log_pplx:3.2824798 loss:128.71423 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.39813\n",
      "I0710 11:14:11.980046 140295626643200 summary_utils.py:349] Steps/second: 0.177450, Examples/second: 25.172822\n",
      "I0710 11:14:11.980842 140295626643200 trainer.py:508] step:  7025, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:14.201964 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3052853 log_pplx:3.6393924 loss:26.115482 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:528.40686\n",
      "I0710 11:14:18.555656 140295626643200 summary_utils.py:349] Steps/second: 0.177438, Examples/second: 25.166724\n",
      "I0710 11:14:18.556425 140295626643200 trainer.py:508] step:  7026, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:327.4675 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3057266 log_pplx:3.3051262 loss:133.98154 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.41522\n",
      "I0710 11:14:22.431245 140295626643200 summary_utils.py:349] Steps/second: 0.177461, Examples/second: 25.171176\n",
      "I0710 11:14:22.432084 140295626643200 trainer.py:508] step:  7027, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:89.64109 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.307495 log_pplx:3.7133853 loss:93.066719 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.42218\n",
      "I0710 11:14:25.440683 140295626643200 summary_utils.py:349] Steps/second: 0.177494, Examples/second: 25.184020\n",
      "I0710 11:14:25.441468 140295626643200 trainer.py:508] step:  7028, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:63.295261 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3070475 log_pplx:3.7928748 loss:59.471096 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.4295\n",
      "I0710 11:14:31.985832 140295626643200 summary_utils.py:349] Steps/second: 0.177482, Examples/second: 25.177978\n",
      "I0710 11:14:31.986631 140295626643200 trainer.py:508] step:  7029, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:121.59864 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.305887 log_pplx:3.2860732 loss:135.30408 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.43689\n",
      "I0710 11:14:38.216855 140295626643200 summary_utils.py:349] Steps/second: 0.177475, Examples/second: 25.172505\n",
      "I0710 11:14:38.217640 140295626643200 trainer.py:508] step:  7030, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:221.6897 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3058742 log_pplx:3.3458827 loss:138.77049 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.44336\n",
      "I0710 11:14:42.085047 140295626643200 summary_utils.py:349] Steps/second: 0.177497, Examples/second: 25.176962\n",
      "I0710 11:14:42.085970 140295626643200 trainer.py:508] step:  7031, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:55.860416 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3074834 log_pplx:3.5816538 loss:90.235298 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.44916\n",
      "I0710 11:14:48.627734 140295626643200 summary_utils.py:349] Steps/second: 0.177486, Examples/second: 25.170935\n",
      "I0710 11:14:48.628530 140295626643200 trainer.py:508] step:  7032, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:100.28893 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.306003 log_pplx:3.2871044 loss:137.19553 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.45496\n",
      "I0710 11:14:58.271330 140295626643200 summary_utils.py:349] Steps/second: 0.177435, Examples/second: 25.156522\n",
      "I0710 11:14:58.272094 140295626643200 trainer.py:508] step:  7033, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:227.52319 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3047652 log_pplx:2.6684759 loss:183.52441 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:528.461\n",
      "I0710 11:15:01.376966 140295626643200 summary_utils.py:349] Steps/second: 0.177467, Examples/second: 25.169164\n",
      "I0710 11:15:01.377741 140295626643200 trainer.py:508] step:  7034, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:24.438377 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3068892 log_pplx:3.48945 loss:53.759335 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.46625\n",
      "I0710 11:15:07.641002 140295626643200 summary_utils.py:349] Steps/second: 0.177459, Examples/second: 25.163647\n",
      "I0710 11:15:07.642003 140295626643200 trainer.py:508] step:  7035, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:118.33926 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3059452 log_pplx:3.2502604 loss:130.86362 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.47144\n",
      "I0710 11:15:11.486361 140295626643200 summary_utils.py:349] Steps/second: 0.177481, Examples/second: 25.168138\n",
      "I0710 11:15:11.487180 140295626643200 trainer.py:508] step:  7036, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:48.439602 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3073843 log_pplx:3.5390723 loss:89.273087 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.47736\n",
      "I0710 11:15:17.673348 140295626643200 summary_utils.py:349] Steps/second: 0.177475, Examples/second: 25.162763\n",
      "I0710 11:15:17.674141 140295626643200 base_runner.py:111] step:  7037, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:145.70341 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3061062 log_pplx:3.2931266 loss:131.23108 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.4834\n",
      "I0710 11:15:20.688384 140295626643200 summary_utils.py:349] Steps/second: 0.177508, Examples/second: 25.175548\n",
      "I0710 11:15:20.689173 140295626643200 trainer.py:508] step:  7038, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:37.256981 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3076004 log_pplx:3.692179 loss:58.094128 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.49011\n",
      "I0710 11:15:27.230697 140295626643200 summary_utils.py:349] Steps/second: 0.177496, Examples/second: 25.169539\n",
      "I0710 11:15:27.231531 140295626643200 trainer.py:508] step:  7039, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:92.254265 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3058702 log_pplx:3.2728705 loss:130.01477 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.49628\n",
      "I0710 11:15:31.130001 140295626643200 summary_utils.py:349] Steps/second: 0.177518, Examples/second: 25.173926\n",
      "I0710 11:15:31.130846 140295626643200 trainer.py:508] step:  7040, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:60.680866 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3079201 log_pplx:3.6307988 loss:92.60807 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.50232\n",
      "I0710 11:15:37.447394 140295626643200 summary_utils.py:349] Steps/second: 0.177509, Examples/second: 25.168323\n",
      "I0710 11:15:37.448236 140295626643200 trainer.py:508] step:  7041, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:100.89225 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3060299 log_pplx:3.2226322 loss:130.39575 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.50763\n",
      "I0710 11:15:43.600090 140295626643200 summary_utils.py:349] Steps/second: 0.177503, Examples/second: 25.163019\n",
      "I0710 11:15:43.600896 140295626643200 trainer.py:508] step:  7042, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:81.617149 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3063319 log_pplx:3.2587187 loss:128.5972 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.51276\n",
      "I0710 11:15:46.694796 140295626643200 summary_utils.py:349] Steps/second: 0.177535, Examples/second: 25.175639\n",
      "I0710 11:15:46.695802 140295626643200 trainer.py:508] step:  7043, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:35.314499 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3073571 log_pplx:3.5926852 loss:56.247978 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.51782\n",
      "I0710 11:15:50.642938 140295626643200 summary_utils.py:349] Steps/second: 0.177556, Examples/second: 25.179931\n",
      "I0710 11:15:50.643813 140295626643200 trainer.py:508] step:  7044, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:39.215057 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3081285 log_pplx:3.5886407 loss:91.712196 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.52258\n",
      "I0710 11:16:00.819997 140295626643200 summary_utils.py:349] Steps/second: 0.177499, Examples/second: 25.164625\n",
      "I0710 11:16:00.820772 140295626643200 trainer.py:508] step:  7045, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:332.46518 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3050314 log_pplx:2.6992595 loss:197.65327 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:528.52698\n",
      "I0710 11:16:07.244561 140295626643200 summary_utils.py:349] Steps/second: 0.177489, Examples/second: 25.158848\n",
      "I0710 11:16:07.245409 140295626643200 trainer.py:508] step:  7046, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:91.911942 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3062826 log_pplx:3.2637587 loss:133.61014 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.53253\n",
      "I0710 11:16:09.428394 140295626643200 summary_utils.py:349] Steps/second: 0.177532, Examples/second: 25.191176\n",
      "I0710 11:16:09.429239 140295626643200 trainer.py:508] step:  7047, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:20.341677 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3059239 log_pplx:3.6155775 loss:26.523651 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:528.5387\n",
      "I0710 11:16:16.101355 140295626643200 summary_utils.py:349] Steps/second: 0.177519, Examples/second: 25.184947\n",
      "I0710 11:16:16.102296 140295626643200 trainer.py:508] step:  7048, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:93.423935 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.306556 log_pplx:3.1895719 loss:131.6097 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.54443\n",
      "I0710 11:16:22.579358 140295626643200 summary_utils.py:349] Steps/second: 0.177508, Examples/second: 25.179072\n",
      "I0710 11:16:22.580314 140295626643200 trainer.py:508] step:  7049, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:118.92677 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3065325 log_pplx:3.3176677 loss:135.60965 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.55029\n",
      "I0710 11:16:26.509406 140295626643200 summary_utils.py:349] Steps/second: 0.177530, Examples/second: 25.183384\n",
      "I0710 11:16:26.510277 140295626643200 trainer.py:508] step:  7050, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:52.011024 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3081603 log_pplx:3.5756156 loss:91.200546 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.55615\n",
      "I0710 11:16:29.131512 140295635035904 trainer.py:345] Write summary @7050\n",
      "I0710 11:16:32.370416 140295626643200 summary_utils.py:349] Steps/second: 0.177527, Examples/second: 25.191039\n",
      "I0710 11:16:32.371711 140295626643200 trainer.py:508] step:  7051, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:22.858904 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3077388 log_pplx:3.594028 loss:55.90398 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.56177\n",
      "I0710 11:16:41.984690 140295626643200 summary_utils.py:349] Steps/second: 0.177477, Examples/second: 25.179593\n",
      "I0710 11:16:41.985507 140295626643200 trainer.py:508] step:  7052, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:95.755997 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3063444 log_pplx:3.2353067 loss:130.70639 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.56708\n",
      "I0710 11:16:52.044707 140295626643200 summary_utils.py:349] Steps/second: 0.177422, Examples/second: 25.167372\n",
      "I0710 11:16:52.045883 140295626643200 trainer.py:508] step:  7053, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:78.810997 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3063319 log_pplx:3.1866019 loss:126.54793 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.57208\n",
      "I0710 11:16:57.459831 140295626643200 summary_utils.py:349] Steps/second: 0.177425, Examples/second: 25.169044\n",
      "I0710 11:16:57.461733 140295626643200 trainer.py:508] step:  7054, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:45.316566 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3081349 log_pplx:3.5291705 loss:88.273376 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.57697\n",
      "I0710 11:17:14.705253 140295635035904 trainer.py:354] Write summary done: step 7050\n",
      "I0710 11:17:15.962383 140295626643200 summary_utils.py:349] Steps/second: 0.177264, Examples/second: 25.139076\n",
      "I0710 11:17:15.963180 140295626643200 trainer.py:508] step:  7055, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:421.85641 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.305534 log_pplx:2.6951272 loss:191.89304 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:528.58197\n",
      "I0710 11:17:22.470465 140295626643200 summary_utils.py:349] Steps/second: 0.177253, Examples/second: 25.133193\n",
      "I0710 11:17:22.471303 140295626643200 trainer.py:508] step:  7056, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:91.314865 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3063338 log_pplx:3.076489 loss:123.79022 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.58636\n",
      "I0710 11:17:25.514805 140295626643200 summary_utils.py:349] Steps/second: 0.177285, Examples/second: 25.145820\n",
      "I0710 11:17:25.515769 140295626643200 trainer.py:508] step:  7057, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:55.132828 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3078523 log_pplx:3.6559007 loss:56.752148 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.59143\n",
      "I0710 11:17:32.008459 140295626643200 summary_utils.py:349] Steps/second: 0.177275, Examples/second: 25.139963\n",
      "I0710 11:17:32.009315 140295626643200 trainer.py:508] step:  7058, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:117.54729 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3064245 log_pplx:3.1806819 loss:126.6309 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.59662\n",
      "I0710 11:17:36.135983 140295626643200 summary_utils.py:349] Steps/second: 0.177294, Examples/second: 25.143916\n",
      "I0710 11:17:36.136791 140295626643200 trainer.py:508] step:  7059, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:50.447769 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3083349 log_pplx:3.5628824 loss:88.804855 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.60266\n",
      "I0710 11:17:42.802883 140295626643200 summary_utils.py:349] Steps/second: 0.177281, Examples/second: 25.137759\n",
      "I0710 11:17:42.803694 140295626643200 trainer.py:508] step:  7060, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:73.711426 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3062934 log_pplx:3.1450217 loss:124.30698 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.60858\n",
      "I0710 11:17:49.347522 140295626643200 summary_utils.py:349] Steps/second: 0.177270, Examples/second: 25.131823\n",
      "I0710 11:17:49.348284 140295626643200 trainer.py:508] step:  7061, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:77.344261 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3071982 log_pplx:3.1895204 loss:131.40825 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.61487\n",
      "I0710 11:17:53.191507 140295626643200 summary_utils.py:349] Steps/second: 0.177292, Examples/second: 25.136274\n",
      "I0710 11:17:53.192389 140295626643200 trainer.py:508] step:  7062, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:65.674103 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3084826 log_pplx:3.6031485 loss:88.930206 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.62134\n",
      "I0710 11:17:56.291052 140295626643200 summary_utils.py:349] Steps/second: 0.177324, Examples/second: 25.148775\n",
      "I0710 11:17:56.291833 140295626643200 trainer.py:508] step:  7063, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:42.9487 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.307862 log_pplx:3.5849123 loss:56.210304 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.62714\n",
      "I0710 11:18:02.709635 140295626643200 summary_utils.py:349] Steps/second: 0.177314, Examples/second: 25.143062\n",
      "I0710 11:18:02.710578 140295626643200 trainer.py:508] step:  7064, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:76.778709 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3067946 log_pplx:3.197351 loss:127.0947 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.63226\n",
      "I0710 11:18:08.912940 140295626643200 summary_utils.py:349] Steps/second: 0.177307, Examples/second: 25.137734\n",
      "I0710 11:18:08.913742 140295626643200 trainer.py:508] step:  7065, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:70.5662 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3063424 log_pplx:3.1416037 loss:123.70064 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.63739\n",
      "I0710 11:18:12.774161 140295626643200 summary_utils.py:349] Steps/second: 0.177329, Examples/second: 25.142147\n",
      "I0710 11:18:12.775047 140295626643200 trainer.py:508] step:  7066, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:42.076256 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3089088 log_pplx:3.6026406 loss:91.732239 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.64246\n",
      "I0710 11:18:19.239573 140295626643200 summary_utils.py:349] Steps/second: 0.177319, Examples/second: 25.136361\n",
      "I0710 11:18:19.240365 140295626643200 trainer.py:508] step:  7067, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:70.636528 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3065665 log_pplx:3.126292 loss:122.47249 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.64722\n",
      "I0710 11:18:29.542658 140295626643200 summary_utils.py:349] Steps/second: 0.177261, Examples/second: 25.121025\n",
      "I0710 11:18:29.543500 140295626643200 trainer.py:508] step:  7068, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:491.75391 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3058224 log_pplx:2.8298929 loss:201.13466 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:528.65192\n",
      "I0710 11:18:32.581107 140295626643200 summary_utils.py:349] Steps/second: 0.177293, Examples/second: 25.133605\n",
      "I0710 11:18:32.582085 140295626643200 trainer.py:508] step:  7069, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:24.738462 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3080198 log_pplx:3.4894526 loss:54.549961 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.65826\n",
      "I0710 11:18:39.032215 140295626643200 summary_utils.py:349] Steps/second: 0.177283, Examples/second: 25.127856\n",
      "I0710 11:18:39.033041 140295626643200 trainer.py:508] step:  7070, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:77.023346 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3066593 log_pplx:3.161689 loss:127.29752 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.66431\n",
      "I0710 11:18:41.247404 140295626643200 summary_utils.py:349] Steps/second: 0.177326, Examples/second: 25.159790\n",
      "I0710 11:18:41.248148 140295626643200 trainer.py:508] step:  7071, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:11.160898 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3065614 log_pplx:3.5853357 loss:25.87464 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:528.6701\n",
      "I0710 11:18:47.785929 140295626643200 summary_utils.py:349] Steps/second: 0.177314, Examples/second: 25.153879\n",
      "I0710 11:18:47.786748 140295626643200 trainer.py:508] step:  7072, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:105.7315 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3067422 log_pplx:3.163043 loss:125.33559 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.6756\n",
      "I0710 11:18:51.566527 140295626643200 summary_utils.py:349] Steps/second: 0.177337, Examples/second: 25.158418\n",
      "I0710 11:18:51.567316 140295626643200 trainer.py:508] step:  7073, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:42.027313 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3085649 log_pplx:3.5749369 loss:88.32328 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.68054\n",
      "I0710 11:18:57.899710 140295626643200 summary_utils.py:349] Steps/second: 0.177329, Examples/second: 25.152873\n",
      "I0710 11:18:57.900508 140295626643200 trainer.py:508] step:  7074, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:118.88915 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3067522 log_pplx:3.1542253 loss:126.91815 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.68512\n",
      "I0710 11:19:00.961762 140295626643200 summary_utils.py:349] Steps/second: 0.177361, Examples/second: 25.165380\n",
      "I0710 11:19:00.962618 140295626643200 trainer.py:508] step:  7075, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:25.979052 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3080782 log_pplx:3.5473943 loss:55.580463 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.68921\n",
      "I0710 11:19:04.837222 140295626643200 summary_utils.py:349] Steps/second: 0.177383, Examples/second: 25.169745\n",
      "I0710 11:19:04.838095 140295626643200 trainer.py:508] step:  7076, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:42.636459 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3085148 log_pplx:3.4351099 loss:86.113914 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.6933\n",
      "I0710 11:19:10.690619 140295626643200 summary_utils.py:349] Steps/second: 0.177380, Examples/second: 25.165043\n",
      "I0710 11:19:10.691518 140295626643200 trainer.py:508] step:  7077, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:85.930298 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3067755 log_pplx:3.2494156 loss:125.18374 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.69739\n",
      "I0710 11:19:21.742309 140295626643200 summary_utils.py:349] Steps/second: 0.177313, Examples/second: 25.148427\n",
      "I0710 11:19:21.743103 140295626643200 trainer.py:508] step:  7078, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:288.92465 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3064059 log_pplx:2.7892337 loss:212.05148 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:528.70154\n",
      "I0710 11:19:28.091647 140295626643200 summary_utils.py:349] Steps/second: 0.177304, Examples/second: 25.142870\n",
      "I0710 11:19:28.092414 140295626643200 trainer.py:508] step:  7079, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:83.851448 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3072301 log_pplx:3.1965394 loss:132.21686 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.70593\n",
      "I0710 11:19:34.421536 140295626643200 summary_utils.py:349] Steps/second: 0.177296, Examples/second: 25.137351\n",
      "I0710 11:19:34.422371 140295626643200 trainer.py:508] step:  7080, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:104.99387 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3070917 log_pplx:3.2310061 loss:129.60373 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.71088\n",
      "I0710 11:19:38.343139 140295626643200 summary_utils.py:349] Steps/second: 0.177317, Examples/second: 25.141632\n",
      "I0710 11:19:38.344263 140295626643200 trainer.py:508] step:  7081, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:56.584389 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3090118 log_pplx:3.5286844 loss:89.849121 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.71643\n",
      "I0710 11:19:41.437115 140295626643200 summary_utils.py:349] Steps/second: 0.177348, Examples/second: 25.154051\n",
      "I0710 11:19:41.438085 140295626643200 trainer.py:508] step:  7082, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:26.190756 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3084801 log_pplx:3.5440786 loss:55.860771 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.72229\n",
      "I0710 11:19:47.851469 140295626643200 summary_utils.py:349] Steps/second: 0.177339, Examples/second: 25.148385\n",
      "I0710 11:19:47.852250 140295626643200 trainer.py:508] step:  7083, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:74.826073 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3072008 log_pplx:3.1887083 loss:125.8344 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.72778\n",
      "I0710 11:19:53.779810 140295626643200 summary_utils.py:349] Steps/second: 0.177335, Examples/second: 25.143575\n",
      "I0710 11:19:53.780649 140295626643200 trainer.py:508] step:  7084, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:76.74189 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3071183 log_pplx:3.1697443 loss:125.71999 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.73334\n",
      "I0710 11:19:57.616214 140295626643200 summary_utils.py:349] Steps/second: 0.177357, Examples/second: 25.147997\n",
      "I0710 11:19:57.617016 140295626643200 trainer.py:508] step:  7085, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:56.999989 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3093915 log_pplx:3.5541523 loss:91.119583 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.73895\n",
      "I0710 11:20:03.898805 140295626643200 summary_utils.py:349] Steps/second: 0.177349, Examples/second: 25.142571\n",
      "I0710 11:20:03.899614 140295626643200 trainer.py:508] step:  7086, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:78.51767 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3071524 log_pplx:3.1172438 loss:123.20906 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.74396\n",
      "I0710 11:20:06.980234 140295626643200 summary_utils.py:349] Steps/second: 0.177381, Examples/second: 25.154989\n",
      "I0710 11:20:06.981223 140295626643200 trainer.py:508] step:  7087, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:34.425636 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3085634 log_pplx:3.6025276 loss:55.02298 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.74884\n",
      "I0710 11:20:13.182052 140295626643200 summary_utils.py:349] Steps/second: 0.177374, Examples/second: 25.149704\n",
      "I0710 11:20:13.182832 140295626643200 trainer.py:508] step:  7088, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:73.196136 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3075259 log_pplx:3.1269197 loss:126.52299 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.7533\n",
      "I0710 11:20:22.038766 140295626643200 summary_utils.py:349] Steps/second: 0.177334, Examples/second: 25.137006\n",
      "I0710 11:20:22.039551 140295626643200 trainer.py:508] step:  7089, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:219.08273 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3061457 log_pplx:2.5409622 loss:178.69318 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:528.75781\n",
      "I0710 11:20:25.882817 140295626643200 summary_utils.py:349] Steps/second: 0.177356, Examples/second: 25.141408\n",
      "I0710 11:20:25.883696 140295626643200 trainer.py:508] step:  7090, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:44.691254 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3087826 log_pplx:3.5270464 loss:88.815445 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.76245\n",
      "I0710 11:20:32.118100 140295626643200 summary_utils.py:349] Steps/second: 0.177349, Examples/second: 25.136078\n",
      "I0710 11:20:32.118913 140295626643200 trainer.py:508] step:  7091, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:80.690445 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3072038 log_pplx:3.061486 loss:126.4011 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.76691\n",
      "I0710 11:20:38.500593 140295626643200 summary_utils.py:349] Steps/second: 0.177340, Examples/second: 25.130495\n",
      "I0710 11:20:38.501370 140295626643200 trainer.py:508] step:  7092, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:83.726639 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3074329 log_pplx:3.1532722 loss:128.45642 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.77203\n",
      "I0710 11:20:42.287518 140295626643200 summary_utils.py:349] Steps/second: 0.177363, Examples/second: 25.134994\n",
      "I0710 11:20:42.288311 140295626643200 trainer.py:508] step:  7093, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:43.010548 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.309224 log_pplx:3.4991295 loss:87.259537 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.77783\n",
      "I0710 11:20:45.371949 140295626643200 summary_utils.py:349] Steps/second: 0.177394, Examples/second: 25.147376\n",
      "I0710 11:20:45.372729 140295626643200 trainer.py:508] step:  7094, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:21.368896 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3087127 log_pplx:3.4891181 loss:53.38623 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.78345\n",
      "I0710 11:20:51.311356 140295626643200 summary_utils.py:349] Steps/second: 0.177390, Examples/second: 25.142566\n",
      "I0710 11:20:51.312096 140295626643200 trainer.py:508] step:  7095, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:83.365532 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3075984 log_pplx:3.2210345 loss:129.12323 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.78876\n",
      "I0710 11:20:53.531683 140295626643200 summary_utils.py:349] Steps/second: 0.177432, Examples/second: 25.174195\n",
      "I0710 11:20:53.532482 140295626643200 trainer.py:508] step:  7096, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:10.969816 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3072351 log_pplx:3.586175 loss:25.80365 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:528.7937\n",
      "I0710 11:21:00.089565 140295626643200 summary_utils.py:349] Steps/second: 0.177421, Examples/second: 25.168298\n",
      "I0710 11:21:00.090525 140295626643200 trainer.py:508] step:  7097, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:81.155251 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3075489 log_pplx:3.158349 loss:130.00555 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.79828\n",
      "I0710 11:21:06.350294 140295626643200 summary_utils.py:349] Steps/second: 0.177413, Examples/second: 25.162924\n",
      "I0710 11:21:06.351090 140295626643200 trainer.py:508] step:  7098, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:72.241158 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3076537 log_pplx:3.1232665 loss:127.31216 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.80255\n",
      "I0710 11:21:10.272367 140295626643200 summary_utils.py:349] Steps/second: 0.177435, Examples/second: 25.167170\n",
      "I0710 11:21:10.273185 140295626643200 trainer.py:508] step:  7099, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:42.623695 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3093765 log_pplx:3.5174084 loss:88.155045 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.80682\n",
      "I0710 11:21:13.379628 140295626643200 summary_utils.py:349] Steps/second: 0.177466, Examples/second: 25.179481\n",
      "I0710 11:21:13.380407 140295626643200 trainer.py:508] step:  7100, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:26.599518 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.308782 log_pplx:3.534601 loss:55.103878 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.81085\n",
      "I0710 11:21:24.723232 140295626643200 summary_utils.py:349] Steps/second: 0.177396, Examples/second: 25.162487\n",
      "I0710 11:21:24.724093 140295626643200 trainer.py:508] step:  7101, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:146.22453 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3067194 log_pplx:2.537261 loss:186.36182 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:528.81482\n",
      "I0710 11:21:31.015894 140295626643200 summary_utils.py:349] Steps/second: 0.177387, Examples/second: 25.157070\n",
      "I0710 11:21:31.016689 140295626643200 trainer.py:508] step:  7102, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:75.959183 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3075879 log_pplx:3.1223814 loss:127.54928 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.81927\n",
      "I0710 11:21:37.622968 140295626643200 summary_utils.py:349] Steps/second: 0.177376, Examples/second: 25.151110\n",
      "I0710 11:21:37.623873 140295626643200 trainer.py:508] step:  7103, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:72.501541 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3077891 log_pplx:3.1432314 loss:130.79771 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.82379\n",
      "I0710 11:21:41.483912 140295626643200 summary_utils.py:349] Steps/second: 0.177397, Examples/second: 25.155456\n",
      "I0710 11:21:41.484918 140295626643200 trainer.py:508] step:  7104, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:36.356339 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3091522 log_pplx:3.4982529 loss:88.024796 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.82837\n",
      "I0710 11:21:47.809615 140295626643200 summary_utils.py:349] Steps/second: 0.177389, Examples/second: 25.149991\n",
      "I0710 11:21:47.810411 140295626643200 trainer.py:508] step:  7105, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:68.206146 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3073245 log_pplx:3.096802 loss:120.62042 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.83289\n",
      "I0710 11:21:50.889625 140295626643200 summary_utils.py:349] Steps/second: 0.177420, Examples/second: 25.162321\n",
      "I0710 11:21:50.890421 140295626643200 trainer.py:508] step:  7106, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:22.419508 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3088828 log_pplx:3.4886832 loss:54.006447 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.83752\n",
      "I0710 11:21:57.598863 140295626643200 summary_utils.py:349] Steps/second: 0.177407, Examples/second: 25.156190\n",
      "I0710 11:21:57.599743 140295626643200 trainer.py:508] step:  7107, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:70.662216 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3079441 log_pplx:3.0554767 loss:123.1357 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.84192\n",
      "I0710 11:22:01.449847 140295626643200 summary_utils.py:349] Steps/second: 0.177429, Examples/second: 25.160546\n",
      "I0710 11:22:01.450669 140295626643200 trainer.py:508] step:  7108, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:40.454945 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3093837 log_pplx:3.4506292 loss:86.179459 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.84656\n",
      "I0710 11:22:08.014169 140295626643200 summary_utils.py:349] Steps/second: 0.177418, Examples/second: 25.154671\n",
      "I0710 11:22:08.015165 140295626643200 trainer.py:508] step:  7109, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:72.582581 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3082347 log_pplx:3.1598094 loss:129.5127 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.85095\n",
      "I0710 11:22:14.100129 140295626643200 summary_utils.py:349] Steps/second: 0.177412, Examples/second: 25.149632\n",
      "I0710 11:22:14.100957 140295626643200 trainer.py:508] step:  7110, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:66.383553 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3075409 log_pplx:3.063144 loss:121.49194 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.85559\n",
      "I0710 11:22:24.199066 140295626643200 summary_utils.py:349] Steps/second: 0.177358, Examples/second: 25.134887\n",
      "I0710 11:22:24.199836 140295626643200 trainer.py:508] step:  7111, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:150.7886 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3069426 log_pplx:2.5278039 loss:189.39569 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:528.86047\n",
      "I0710 11:22:28.127602 140295626643200 summary_utils.py:349] Steps/second: 0.177378, Examples/second: 25.139107\n",
      "I0710 11:22:28.128396 140295626643200 trainer.py:508] step:  7112, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:40.21925 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3096737 log_pplx:3.5710738 loss:90.861511 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.86566\n",
      "I0710 11:22:31.209577 140295626643200 summary_utils.py:349] Steps/second: 0.177410, Examples/second: 25.151402\n",
      "I0710 11:22:31.210445 140295626643200 trainer.py:508] step:  7113, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:25.27124 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.309292 log_pplx:3.5831394 loss:56.728374 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.87091\n",
      "I0710 11:22:37.668757 140295626643200 summary_utils.py:349] Steps/second: 0.177400, Examples/second: 25.145726\n",
      "I0710 11:22:37.669627 140295626643200 trainer.py:508] step:  7114, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:80.450485 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3080871 log_pplx:3.1655271 loss:131.40897 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.87598\n",
      "I0710 11:22:43.912832 140295626643200 summary_utils.py:349] Steps/second: 0.177392, Examples/second: 25.140426\n",
      "I0710 11:22:43.913727 140295626643200 trainer.py:508] step:  7115, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:79.977112 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.307797 log_pplx:3.1007187 loss:126.58684 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.88141\n",
      "I0710 11:22:47.969856 140295626643200 summary_utils.py:349] Steps/second: 0.177411, Examples/second: 25.144417\n",
      "I0710 11:22:47.970770 140295626643200 trainer.py:508] step:  7116, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:38.980896 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3095962 log_pplx:3.4970081 loss:87.971603 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.88727\n",
      "I0710 11:22:54.829446 140295626643200 summary_utils.py:349] Steps/second: 0.177397, Examples/second: 25.138058\n",
      "I0710 11:22:54.830292 140295626643200 trainer.py:508] step:  7117, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:75.566795 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3080914 log_pplx:3.184386 loss:125.54442 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.8927\n",
      "I0710 11:22:57.935096 140295626643200 summary_utils.py:349] Steps/second: 0.177427, Examples/second: 25.150289\n",
      "I0710 11:22:57.935866 140295626643200 trainer.py:508] step:  7118, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:26.373301 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3094171 log_pplx:3.4008477 loss:53.882179 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.89777\n",
      "I0710 11:23:00.155846 140295626643200 summary_utils.py:349] Steps/second: 0.177469, Examples/second: 25.181642\n",
      "I0710 11:23:00.156686 140295626643200 trainer.py:508] step:  7119, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:11.333387 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.307947 log_pplx:3.5987659 loss:26.744734 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:528.9024\n",
      "I0710 11:23:06.430891 140295626643200 summary_utils.py:349] Steps/second: 0.177461, Examples/second: 25.176282\n",
      "I0710 11:23:06.431751 140295626643200 trainer.py:508] step:  7120, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:78.400314 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3082249 log_pplx:3.0870478 loss:121.78403 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.90674\n",
      "I0710 11:23:12.842884 140295626643200 summary_utils.py:349] Steps/second: 0.177452, Examples/second: 25.170690\n",
      "I0710 11:23:12.843903 140295626643200 trainer.py:508] step:  7121, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:72.770134 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3081023 log_pplx:3.1840351 loss:130.90364 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.91058\n",
      "I0710 11:23:16.784839 140295626643200 summary_utils.py:349] Steps/second: 0.177472, Examples/second: 25.174862\n",
      "I0710 11:23:16.785617 140295626643200 trainer.py:508] step:  7122, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:36.957493 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3096279 log_pplx:3.4626925 loss:86.372543 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.91437\n",
      "I0710 11:23:25.049555 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 11:23:27.693168 140295626643200 summary_utils.py:349] Steps/second: 0.177408, Examples/second: 25.158767\n",
      "I0710 11:23:27.694393 140295626643200 trainer.py:508] step:  7123, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:138.11093 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3072314 log_pplx:2.4638193 loss:178.87326 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:528.91809\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 11:23:30.447141 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 11:23:30.927531 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00007122\n",
      "I0710 11:23:33.859300 140295626643200 summary_utils.py:349] Steps/second: 0.177402, Examples/second: 25.153615\n",
      "I0710 11:23:33.860118 140295626643200 trainer.py:508] step:  7124, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:71.260933 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3081559 log_pplx:3.121182 loss:125.62759 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.92242\n",
      "I0710 11:23:36.978045 140295626643200 summary_utils.py:349] Steps/second: 0.177432, Examples/second: 25.165788\n",
      "I0710 11:23:36.978832 140295626643200 trainer.py:508] step:  7125, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:32.323631 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3093058 log_pplx:3.5182819 loss:54.409679 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.92725\n",
      "I0710 11:23:43.200881 140295626643200 summary_utils.py:349] Steps/second: 0.177425, Examples/second: 25.160538\n",
      "I0710 11:23:43.201775 140295626643200 trainer.py:508] step:  7126, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:68.537178 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3079876 log_pplx:3.1084933 loss:121.07582 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.93207\n",
      "I0710 11:23:47.082116 140295626643200 summary_utils.py:349] Steps/second: 0.177447, Examples/second: 25.164810\n",
      "I0710 11:23:47.082931 140295626643200 trainer.py:508] step:  7127, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:38.121609 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3101432 log_pplx:3.4819176 loss:87.744324 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.93713\n",
      "I0710 11:23:53.579956 140295626643200 summary_utils.py:349] Steps/second: 0.177436, Examples/second: 25.159090\n",
      "I0710 11:23:53.580718 140295626643200 trainer.py:508] step:  7128, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:78.410553 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3090215 log_pplx:3.1036506 loss:132.52588 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.94202\n",
      "I0710 11:23:57.470077 140295626643200 summary_utils.py:349] Steps/second: 0.177457, Examples/second: 25.163344\n",
      "I0710 11:23:57.471290 140295626643200 trainer.py:508] step:  7129, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:46.53764 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3102194 log_pplx:3.5148454 loss:88.310501 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.94714\n",
      "I0710 11:24:03.999693 140295626643200 summary_utils.py:349] Steps/second: 0.177446, Examples/second: 25.157574\n",
      "I0710 11:24:04.000554 140295626643200 trainer.py:508] step:  7130, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:78.858925 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3083442 log_pplx:3.0685866 loss:128.11348 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.95178\n",
      "I0710 11:24:07.124404 140295626643200 summary_utils.py:349] Steps/second: 0.177477, Examples/second: 25.169711\n",
      "I0710 11:24:07.125194 140295626643200 trainer.py:508] step:  7131, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:24.354542 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3101792 log_pplx:3.5018513 loss:55.920193 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.95648\n",
      "I0710 11:24:17.136004 140295626643200 summary_utils.py:349] Steps/second: 0.177424, Examples/second: 25.155217\n",
      "I0710 11:24:17.136774 140295626643200 trainer.py:508] step:  7132, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:141.51656 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3071952 log_pplx:2.4460943 loss:173.85616 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:528.96082\n",
      "I0710 11:24:23.560352 140295626643200 summary_utils.py:349] Steps/second: 0.177414, Examples/second: 25.149640\n",
      "I0710 11:24:23.561189 140295626643200 trainer.py:508] step:  7133, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:78.685158 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3078325 log_pplx:3.1143756 loss:123.21249 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.96558\n",
      "I0710 11:24:29.525230 140295626643200 summary_utils.py:349] Steps/second: 0.177410, Examples/second: 25.144857\n",
      "I0710 11:24:29.526226 140295626643200 trainer.py:508] step:  7134, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:74.95929 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3082831 log_pplx:3.1330297 loss:122.149 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.97089\n",
      "I0710 11:24:33.426116 140295626643200 summary_utils.py:349] Steps/second: 0.177431, Examples/second: 25.149085\n",
      "I0710 11:24:33.426856 140295626643200 trainer.py:508] step:  7135, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:39.711006 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3103626 log_pplx:3.5062604 loss:89.278152 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.9765\n",
      "I0710 11:24:39.800878 140295626643200 summary_utils.py:349] Steps/second: 0.177422, Examples/second: 25.143602\n",
      "I0710 11:24:39.801724 140295626643200 trainer.py:508] step:  7136, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:74.299355 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.308513 log_pplx:3.0921226 loss:124.26466 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.98187\n",
      "I0710 11:24:42.907693 140295626643200 summary_utils.py:349] Steps/second: 0.177453, Examples/second: 25.155743\n",
      "I0710 11:24:42.908538 140295626643200 base_runner.py:111] step:  7137, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:26.051207 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3099718 log_pplx:3.5451624 loss:55.62859 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:528.98712\n",
      "I0710 11:24:49.007915 140295626643200 summary_utils.py:349] Steps/second: 0.177447, Examples/second: 25.150732\n",
      "I0710 11:24:49.008741 140295626643200 trainer.py:508] step:  7138, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:68.904243 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3082364 log_pplx:3.1542661 loss:124.39638 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:528.99188\n",
      "I0710 11:24:52.914399 140295626643200 summary_utils.py:349] Steps/second: 0.177468, Examples/second: 25.154943\n",
      "I0710 11:24:52.915362 140295626643200 trainer.py:508] step:  7139, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:36.500118 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3102466 log_pplx:3.5237353 loss:89.37072 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:528.9967\n",
      "I0710 11:24:58.839298 140295626643200 summary_utils.py:349] Steps/second: 0.177465, Examples/second: 25.150236\n",
      "I0710 11:24:58.840124 140295626643200 trainer.py:508] step:  7140, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:69.018044 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3084493 log_pplx:3.114893 loss:122.06486 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.00116\n",
      "I0710 11:25:09.512979 140295626643200 summary_utils.py:349] Steps/second: 0.177404, Examples/second: 25.134673\n",
      "I0710 11:25:09.513782 140295626643200 trainer.py:508] step:  7141, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:170.6378 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3074754 log_pplx:2.5030763 loss:185.97859 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:529.00616\n",
      "I0710 11:25:11.713887 140295626643200 summary_utils.py:349] Steps/second: 0.177445, Examples/second: 25.165781\n",
      "I0710 11:25:11.714879 140295626643200 trainer.py:508] step:  7142, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:11.220583 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3084927 log_pplx:3.5528376 loss:25.785831 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:529.01141\n",
      "I0710 11:25:14.791805 140295626643200 summary_utils.py:349] Steps/second: 0.177476, Examples/second: 25.177940\n",
      "I0710 11:25:14.792825 140295626643200 trainer.py:508] step:  7143, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:24.39447 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3095746 log_pplx:3.4902458 loss:54.194252 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.01642\n",
      "I0710 11:25:20.856201 140295626643200 summary_utils.py:349] Steps/second: 0.177471, Examples/second: 25.172992\n",
      "I0710 11:25:20.856988 140295626643200 trainer.py:508] step:  7144, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:68.678772 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3080328 log_pplx:3.0641232 loss:119.11777 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.0213\n",
      "I0710 11:25:24.709621 140295626643200 summary_utils.py:349] Steps/second: 0.177493, Examples/second: 25.177279\n",
      "I0710 11:25:24.710430 140295626643200 trainer.py:508] step:  7145, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:45.543335 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3104179 log_pplx:3.5870299 loss:90.34832 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.02631\n",
      "I0710 11:25:30.973188 140295626643200 summary_utils.py:349] Steps/second: 0.177485, Examples/second: 25.171993\n",
      "I0710 11:25:30.973980 140295626643200 trainer.py:508] step:  7146, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:71.458733 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3084507 log_pplx:3.0336077 loss:118.95535 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.03143\n",
      "I0710 11:25:34.807489 140295626643200 summary_utils.py:349] Steps/second: 0.177507, Examples/second: 25.176311\n",
      "I0710 11:25:34.808443 140295626643200 trainer.py:508] step:  7147, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:35.208565 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3106918 log_pplx:3.533531 loss:87.764084 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.03644\n",
      "I0710 11:25:40.985581 140295626643200 summary_utils.py:349] Steps/second: 0.177500, Examples/second: 25.171176\n",
      "I0710 11:25:40.986365 140295626643200 trainer.py:508] step:  7148, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:79.709518 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3085238 log_pplx:3.147428 loss:126.29055 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.04132\n",
      "I0710 11:25:44.049406 140295626643200 summary_utils.py:349] Steps/second: 0.177531, Examples/second: 25.183334\n",
      "I0710 11:25:44.050273 140295626643200 trainer.py:508] step:  7149, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:22.361675 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3101654 log_pplx:3.5142496 loss:55.294521 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.04565\n",
      "I0710 11:25:50.461076 140295626643200 summary_utils.py:349] Steps/second: 0.177522, Examples/second: 25.177799\n",
      "I0710 11:25:50.461891 140295626643200 trainer.py:508] step:  7150, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:81.89325 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3091034 log_pplx:3.1059394 loss:127.53762 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.0498\n",
      "I0710 11:25:55.178051 140295635035904 trainer.py:345] Write summary @7150\n",
      "I0710 11:26:05.103624 140295626643200 summary_utils.py:349] Steps/second: 0.177413, Examples/second: 25.155492\n",
      "I0710 11:26:05.104845 140295626643200 trainer.py:508] step:  7151, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:154.54688 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.307845 log_pplx:2.5478048 loss:182.7413 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:529.0545\n",
      "I0710 11:26:10.460000 140295626643200 summary_utils.py:349] Steps/second: 0.177417, Examples/second: 25.157206\n",
      "I0710 11:26:10.461545 140295626643200 trainer.py:508] step:  7152, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:45.344166 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3104823 log_pplx:3.4923074 loss:86.456429 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.05945\n",
      "I0710 11:26:19.393689 140295626643200 summary_utils.py:349] Steps/second: 0.177377, Examples/second: 25.147391\n",
      "I0710 11:26:19.394494 140295626643200 trainer.py:508] step:  7153, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:70.577049 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3088189 log_pplx:3.0968502 loss:124.80306 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.06458\n",
      "I0710 11:26:28.692308 140295626643200 summary_utils.py:349] Steps/second: 0.177333, Examples/second: 25.136967\n",
      "I0710 11:26:28.693948 140295626643200 trainer.py:508] step:  7154, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:74.65451 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.308862 log_pplx:3.1680546 loss:127.87061 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.0697\n",
      "I0710 11:26:33.076479 140295626643200 summary_utils.py:349] Steps/second: 0.177348, Examples/second: 25.146843\n",
      "I0710 11:26:33.077570 140295626643200 trainer.py:508] step:  7155, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:34.529915 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3103722 log_pplx:3.477597 loss:53.970669 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.07495\n",
      "I0710 11:26:38.978989 140295626643200 summary_utils.py:349] Steps/second: 0.177345, Examples/second: 25.147627\n",
      "I0710 11:26:38.980442 140295626643200 trainer.py:508] step:  7156, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:55.018337 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3110751 log_pplx:3.547961 loss:90.783463 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.07977\n",
      "I0710 11:26:50.093261 140295635035904 trainer.py:354] Write summary done: step 7150\n",
      "I0710 11:26:50.113957 140295626643200 summary_utils.py:349] Steps/second: 0.177279, Examples/second: 25.134093\n",
      "I0710 11:26:50.114958 140295626643200 trainer.py:508] step:  7157, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:75.466087 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3089983 log_pplx:3.0712864 loss:124.92458 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.0838\n",
      "I0710 11:26:56.705813 140295626643200 summary_utils.py:349] Steps/second: 0.177268, Examples/second: 25.128298\n",
      "I0710 11:26:56.706642 140295626643200 trainer.py:508] step:  7158, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:68.960022 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3089852 log_pplx:3.1041594 loss:124.43799 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.08783\n",
      "I0710 11:27:00.507186 140295626643200 summary_utils.py:349] Steps/second: 0.177290, Examples/second: 25.132658\n",
      "I0710 11:27:00.507977 140295626643200 trainer.py:508] step:  7159, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:36.168373 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3107877 log_pplx:3.49424 loss:88.622665 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.09222\n",
      "I0710 11:27:06.773376 140295626643200 summary_utils.py:349] Steps/second: 0.177282, Examples/second: 25.127421\n",
      "I0710 11:27:06.774209 140295626643200 trainer.py:508] step:  7160, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:78.476334 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3090281 log_pplx:3.1089823 loss:124.6702 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.09668\n",
      "I0710 11:27:09.753858 140295626643200 summary_utils.py:349] Steps/second: 0.177314, Examples/second: 25.139659\n",
      "I0710 11:27:09.754878 140295626643200 trainer.py:508] step:  7161, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:22.955259 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3102804 log_pplx:3.5184901 loss:54.041813 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.10168\n",
      "I0710 11:27:19.340113 140295626643200 summary_utils.py:349] Steps/second: 0.177267, Examples/second: 25.126086\n",
      "I0710 11:27:19.341086 140295626643200 trainer.py:508] step:  7162, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:164.13503 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3080149 log_pplx:2.5584769 loss:180.82036 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:529.10651\n",
      "I0710 11:27:25.914000 140295626643200 summary_utils.py:349] Steps/second: 0.177256, Examples/second: 25.120336\n",
      "I0710 11:27:25.914762 140295626643200 trainer.py:508] step:  7163, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:75.092644 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3096699 log_pplx:3.2004371 loss:134.5784 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.11096\n",
      "I0710 11:27:28.120835 140295626643200 summary_utils.py:349] Steps/second: 0.177297, Examples/second: 25.151150\n",
      "I0710 11:27:28.121591 140295626643200 trainer.py:508] step:  7164, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:11.445356 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3091532 log_pplx:3.6237657 loss:26.852673 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:529.11548\n",
      "I0710 11:27:32.039781 140295626643200 summary_utils.py:349] Steps/second: 0.177317, Examples/second: 25.155295\n",
      "I0710 11:27:32.040537 140295626643200 trainer.py:508] step:  7165, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:41.105736 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3109159 log_pplx:3.530556 loss:88.396301 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.11993\n",
      "I0710 11:27:38.414110 140295626643200 summary_utils.py:349] Steps/second: 0.177308, Examples/second: 25.149876\n",
      "I0710 11:27:38.414860 140295626643200 trainer.py:508] step:  7166, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:80.127357 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.30911 log_pplx:3.1166081 loss:124.74223 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.12433\n",
      "I0710 11:27:41.521215 140295626643200 summary_utils.py:349] Steps/second: 0.177339, Examples/second: 25.161869\n",
      "I0710 11:27:41.522040 140295626643200 trainer.py:508] step:  7167, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:21.692488 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3106457 log_pplx:3.5101116 loss:55.064877 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.12927\n",
      "I0710 11:27:47.704518 140295626643200 summary_utils.py:349] Steps/second: 0.177332, Examples/second: 25.156774\n",
      "I0710 11:27:47.705320 140295626643200 trainer.py:508] step:  7168, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:71.961342 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.309269 log_pplx:3.085655 loss:122.65479 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.13391\n",
      "I0710 11:27:51.623281 140295626643200 summary_utils.py:349] Steps/second: 0.177353, Examples/second: 25.160913\n",
      "I0710 11:27:51.624054 140295626643200 trainer.py:508] step:  7169, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:42.3946 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3110155 log_pplx:3.4529126 loss:86.754425 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.13916\n",
      "I0710 11:27:58.485090 140295626643200 summary_utils.py:349] Steps/second: 0.177338, Examples/second: 25.154673\n",
      "I0710 11:27:58.486025 140295626643200 trainer.py:508] step:  7170, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:71.215363 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.309593 log_pplx:3.1392171 loss:129.45346 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.1441\n",
      "I0710 11:28:05.220335 140295626643200 summary_utils.py:349] Steps/second: 0.177325, Examples/second: 25.148653\n",
      "I0710 11:28:05.221160 140295626643200 trainer.py:508] step:  7171, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:70.521736 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3096659 log_pplx:3.0936067 loss:128.6167 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.14923\n",
      "2020-07-10 11:28:05.497272: I lingvo/core/ops/record_yielder.cc:532] Epoch 9: total records 46838\n",
      "2020-07-10 11:28:05.497358: I lingvo/core/ops/record_yielder.cc:485] Epoch 9 /tmp/punctuator_data/train.txt\n",
      "I0710 11:28:08.299804 140295626643200 summary_utils.py:349] Steps/second: 0.177356, Examples/second: 25.160671\n",
      "I0710 11:28:08.300694 140295626643200 trainer.py:508] step:  7172, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:23.375376 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3106781 log_pplx:3.4997523 loss:54.492237 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.1546\n",
      "I0710 11:28:14.729231 140295626643200 summary_utils.py:349] Steps/second: 0.177346, Examples/second: 25.155169\n",
      "I0710 11:28:14.730062 140295626643200 trainer.py:508] step:  7173, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:68.399704 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3092052 log_pplx:3.1024022 loss:122.54488 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.15948\n",
      "I0710 11:28:18.606871 140295626643200 summary_utils.py:349] Steps/second: 0.177367, Examples/second: 25.159370\n",
      "I0710 11:28:18.607691 140295626643200 trainer.py:508] step:  7174, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:40.58839 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3108777 log_pplx:3.5102875 loss:88.239845 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.16412\n",
      "I0710 11:28:29.219976 140295626643200 summary_utils.py:349] Steps/second: 0.177308, Examples/second: 25.144112\n",
      "I0710 11:28:29.220900 140295626643200 trainer.py:508] step:  7175, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:204.34459 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3085684 log_pplx:2.6204123 loss:192.01073 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:529.16858\n",
      "I0710 11:28:35.649345 140295626643200 summary_utils.py:349] Steps/second: 0.177298, Examples/second: 25.138625\n",
      "I0710 11:28:35.650385 140295626643200 trainer.py:508] step:  7176, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:73.880341 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3098181 log_pplx:2.9941289 loss:124.1815 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.1745\n",
      "I0710 11:28:39.527095 140295626643200 summary_utils.py:349] Steps/second: 0.177319, Examples/second: 25.142825\n",
      "I0710 11:28:39.527978 140295626643200 trainer.py:508] step:  7177, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:46.704491 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3111345 log_pplx:3.4404519 loss:86.613373 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.1803\n",
      "I0710 11:28:45.814002 140295626643200 summary_utils.py:349] Steps/second: 0.177312, Examples/second: 25.137583\n",
      "I0710 11:28:45.814788 140295626643200 trainer.py:508] step:  7178, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:89.37796 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3094044 log_pplx:2.9520686 loss:119.15288 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.18561\n",
      "I0710 11:28:48.881202 140295626643200 summary_utils.py:349] Steps/second: 0.177342, Examples/second: 25.149591\n",
      "I0710 11:28:48.882082 140295626643200 trainer.py:508] step:  7179, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:20.744757 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3110223 log_pplx:3.4035568 loss:53.340122 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.19055\n",
      "I0710 11:28:55.390397 140295626643200 summary_utils.py:349] Steps/second: 0.177332, Examples/second: 25.143975\n",
      "I0710 11:28:55.391216 140295626643200 trainer.py:508] step:  7180, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:71.782578 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3092921 log_pplx:2.9882431 loss:119.79118 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.19531\n",
      "I0710 11:29:01.291990 140295626643200 summary_utils.py:349] Steps/second: 0.177329, Examples/second: 25.139389\n",
      "I0710 11:29:01.292758 140295626643200 trainer.py:508] step:  7181, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:76.596024 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3095026 log_pplx:3.0418437 loss:119.2783 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.19995\n",
      "I0710 11:29:05.144598 140295626643200 summary_utils.py:349] Steps/second: 0.177350, Examples/second: 25.143623\n",
      "I0710 11:29:05.145377 140295626643200 trainer.py:508] step:  7182, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:68.973694 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3114637 log_pplx:3.4041586 loss:85.699692 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.20483\n",
      "I0710 11:29:15.830601 140295626643200 summary_utils.py:349] Steps/second: 0.177290, Examples/second: 25.128300\n",
      "I0710 11:29:15.831375 140295626643200 trainer.py:508] step:  7183, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:206.99092 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3087485 log_pplx:2.4000812 loss:180.60612 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:529.21057\n",
      "I0710 11:29:22.018194 140295626643200 summary_utils.py:349] Steps/second: 0.177284, Examples/second: 25.123245\n",
      "I0710 11:29:22.019134 140295626643200 trainer.py:508] step:  7184, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:72.186768 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3096172 log_pplx:2.9745221 loss:117.01028 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.21564\n",
      "I0710 11:29:25.099357 140295626643200 summary_utils.py:349] Steps/second: 0.177314, Examples/second: 25.135203\n",
      "I0710 11:29:25.100121 140295626643200 trainer.py:508] step:  7185, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:22.864355 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3108069 log_pplx:3.4184396 loss:53.065941 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.22137\n",
      "I0710 11:29:31.594010 140295626643200 summary_utils.py:349] Steps/second: 0.177304, Examples/second: 25.129632\n",
      "I0710 11:29:31.594827 140295626643200 trainer.py:508] step:  7186, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:72.671997 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3094435 log_pplx:2.8825026 loss:117.53403 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.22705\n",
      "I0710 11:29:35.457411 140295626643200 summary_utils.py:349] Steps/second: 0.177325, Examples/second: 25.133841\n",
      "I0710 11:29:35.458335 140295626643200 trainer.py:508] step:  7187, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:41.405247 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3114572 log_pplx:3.380105 loss:83.594215 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.23291\n",
      "I0710 11:29:37.635715 140295626643200 summary_utils.py:349] Steps/second: 0.177366, Examples/second: 25.164434\n",
      "I0710 11:29:37.636502 140295626643200 trainer.py:508] step:  7188, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:23.046135 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3095354 log_pplx:3.5126059 loss:26.193609 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:529.2384\n",
      "I0710 11:29:43.860985 140295626643200 summary_utils.py:349] Steps/second: 0.177359, Examples/second: 25.159307\n",
      "I0710 11:29:43.861875 140295626643200 trainer.py:508] step:  7189, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:71.837677 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3096006 log_pplx:2.9806192 loss:119.52281 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.24341\n",
      "I0710 11:29:50.106747 140295626643200 summary_utils.py:349] Steps/second: 0.177352, Examples/second: 25.154151\n",
      "I0710 11:29:50.107552 140295626643200 trainer.py:508] step:  7190, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:66.740837 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3100917 log_pplx:2.9621379 loss:119.04092 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.24835\n",
      "I0710 11:29:53.944621 140295626643200 summary_utils.py:349] Steps/second: 0.177373, Examples/second: 25.158392\n",
      "I0710 11:29:53.945385 140295626643200 trainer.py:508] step:  7191, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:50.450119 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.311604 log_pplx:3.4016469 loss:85.912842 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.2536\n",
      "I0710 11:29:56.991848 140295626643200 summary_utils.py:349] Steps/second: 0.177404, Examples/second: 25.170375\n",
      "I0710 11:29:56.992622 140295626643200 trainer.py:508] step:  7192, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:23.329042 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3108901 log_pplx:3.4296765 loss:53.307346 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.25818\n",
      "I0710 11:30:03.322659 140295626643200 summary_utils.py:349] Steps/second: 0.177396, Examples/second: 25.165075\n",
      "I0710 11:30:03.323454 140295626643200 trainer.py:508] step:  7193, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:82.158554 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3097274 log_pplx:3.0374656 loss:122.29597 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.26257\n",
      "I0710 11:30:12.324371 140295626643200 summary_utils.py:349] Steps/second: 0.177356, Examples/second: 25.152624\n",
      "I0710 11:30:12.325242 140295626643200 trainer.py:508] step:  7194, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:248.84177 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3087635 log_pplx:2.4180348 loss:169.86694 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:529.26752\n",
      "I0710 11:30:18.854269 140295626643200 summary_utils.py:349] Steps/second: 0.177345, Examples/second: 25.147004\n",
      "I0710 11:30:18.855041 140295626643200 trainer.py:508] step:  7195, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:84.343788 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3099444 log_pplx:2.8793557 loss:117.76566 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.27356\n",
      "I0710 11:30:22.702888 140295626643200 summary_utils.py:349] Steps/second: 0.177366, Examples/second: 25.151221\n",
      "I0710 11:30:22.703815 140295626643200 trainer.py:508] step:  7196, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:53.76371 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3117523 log_pplx:3.4500678 loss:87.67485 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.27936\n",
      "I0710 11:30:28.530052 140295626643200 summary_utils.py:349] Steps/second: 0.177364, Examples/second: 25.146782\n",
      "I0710 11:30:28.530831 140295626643200 trainer.py:508] step:  7197, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:153.88614 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3095973 log_pplx:2.9579909 loss:114.80701 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.28473\n",
      "I0710 11:30:31.639252 140295626643200 summary_utils.py:349] Steps/second: 0.177394, Examples/second: 25.158636\n",
      "I0710 11:30:31.640032 140295626643200 trainer.py:508] step:  7198, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:35.207787 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3112057 log_pplx:3.4776864 loss:53.496597 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.28967\n",
      "I0710 11:30:38.158066 140295626643200 summary_utils.py:349] Steps/second: 0.177384, Examples/second: 25.153039\n",
      "I0710 11:30:38.158865 140295626643200 trainer.py:508] step:  7199, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:68.041122 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3097894 log_pplx:2.8815134 loss:114.252 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.29474\n",
      "I0710 11:30:42.055201 140295626643200 summary_utils.py:349] Steps/second: 0.177404, Examples/second: 25.157167\n",
      "I0710 11:30:42.056121 140295626643200 trainer.py:508] step:  7200, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:78.842545 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3118548 log_pplx:3.5349624 loss:88.396149 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.29999\n",
      "I0710 11:30:48.254821 140295626643200 summary_utils.py:349] Steps/second: 0.177398, Examples/second: 25.152109\n",
      "I0710 11:30:48.255827 140295626643200 trainer.py:508] step:  7201, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:76.689095 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3097837 log_pplx:2.9215047 loss:112.98921 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.30579\n",
      "I0710 11:30:58.440365 140295626643200 summary_utils.py:349] Steps/second: 0.177344, Examples/second: 25.137722\n",
      "I0710 11:30:58.441239 140295626643200 trainer.py:508] step:  7202, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:182.42743 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3093654 log_pplx:2.4381263 loss:180.6042 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:529.31189\n",
      "I0710 11:31:04.907290 140295626643200 summary_utils.py:349] Steps/second: 0.177334, Examples/second: 25.132231\n",
      "I0710 11:31:04.908049 140295626643200 trainer.py:508] step:  7203, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:70.351784 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3103503 log_pplx:2.9940667 loss:119.83754 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.31787\n",
      "I0710 11:31:08.102251 140295626643200 summary_utils.py:349] Steps/second: 0.177363, Examples/second: 25.143915\n",
      "I0710 11:31:08.103044 140295626643200 trainer.py:508] step:  7204, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:28.937841 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3118489 log_pplx:3.4801533 loss:55.424152 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.32385\n",
      "I0710 11:31:11.956959 140295626643200 summary_utils.py:349] Steps/second: 0.177384, Examples/second: 25.148109\n",
      "I0710 11:31:11.957713 140295626643200 trainer.py:508] step:  7205, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:41.395542 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3120521 log_pplx:3.4510937 loss:85.97538 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.32928\n",
      "I0710 11:31:18.428385 140295626643200 summary_utils.py:349] Steps/second: 0.177374, Examples/second: 25.142610\n",
      "I0710 11:31:18.429186 140295626643200 trainer.py:508] step:  7206, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:82.525337 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.310106 log_pplx:2.9495609 loss:117.98242 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.33453\n",
      "I0710 11:31:24.598998 140295626643200 summary_utils.py:349] Steps/second: 0.177368, Examples/second: 25.137618\n",
      "I0710 11:31:24.599808 140295626643200 trainer.py:508] step:  7207, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:79.298531 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3101212 log_pplx:2.9610875 loss:117.59219 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.33984\n",
      "I0710 11:31:28.418412 140295626643200 summary_utils.py:349] Steps/second: 0.177389, Examples/second: 25.141868\n",
      "I0710 11:31:28.419243 140295626643200 trainer.py:508] step:  7208, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:44.010941 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3122265 log_pplx:3.3995941 loss:86.115967 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.34552\n",
      "I0710 11:31:34.771765 140295626643200 summary_utils.py:349] Steps/second: 0.177381, Examples/second: 25.136575\n",
      "I0710 11:31:34.772543 140295626643200 trainer.py:508] step:  7209, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:68.089005 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3100183 log_pplx:2.8727086 loss:112.46654 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.35083\n",
      "I0710 11:31:37.829311 140295626643200 summary_utils.py:349] Steps/second: 0.177411, Examples/second: 25.148465\n",
      "I0710 11:31:37.830149 140295626643200 trainer.py:508] step:  7210, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:56.989731 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3117408 log_pplx:3.4981952 loss:55.246887 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.35632\n",
      "I0710 11:31:44.007645 140295626643200 summary_utils.py:349] Steps/second: 0.177405, Examples/second: 25.143464\n",
      "I0710 11:31:44.008429 140295626643200 trainer.py:508] step:  7211, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:80.505722 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3102688 log_pplx:2.94699 loss:121.2318 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.36121\n",
      "I0710 11:31:46.210621 140295626643200 summary_utils.py:349] Steps/second: 0.177445, Examples/second: 25.173753\n",
      "I0710 11:31:46.211397 140295626643200 trainer.py:508] step:  7212, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:12.749187 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3100114 log_pplx:3.4725995 loss:25.311995 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:529.36609\n",
      "I0710 11:31:55.409506 140295626643200 summary_utils.py:349] Steps/second: 0.177403, Examples/second: 25.161053\n",
      "I0710 11:31:55.410433 140295626643200 trainer.py:508] step:  7213, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:145.47618 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3092937 log_pplx:2.3522742 loss:170.18703 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:529.37073\n",
      "I0710 11:31:59.161987 140295626643200 summary_utils.py:349] Steps/second: 0.177426, Examples/second: 25.165400\n",
      "I0710 11:31:59.162775 140295626643200 trainer.py:508] step:  7214, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:48.54356 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3118851 log_pplx:3.4028103 loss:85.920952 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.37671\n",
      "I0710 11:32:05.556967 140295626643200 summary_utils.py:349] Steps/second: 0.177417, Examples/second: 25.160038\n",
      "I0710 11:32:05.557762 140295626643200 trainer.py:508] step:  7215, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:79.971581 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3112119 log_pplx:3.0231621 loss:126.3304 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.38293\n",
      "I0710 11:32:12.050879 140295626643200 summary_utils.py:349] Steps/second: 0.177407, Examples/second: 25.154516\n",
      "I0710 11:32:12.051683 140295626643200 trainer.py:508] step:  7216, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:72.506767 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3105892 log_pplx:2.9131074 loss:117.43465 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.38885\n",
      "I0710 11:32:15.132239 140295626643200 summary_utils.py:349] Steps/second: 0.177437, Examples/second: 25.166333\n",
      "I0710 11:32:15.133116 140295626643200 trainer.py:508] step:  7217, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:31.130487 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3117231 log_pplx:3.4767072 loss:54.25565 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.39447\n",
      "I0710 11:32:21.427897 140295626643200 summary_utils.py:349] Steps/second: 0.177429, Examples/second: 25.161142\n",
      "I0710 11:32:21.428663 140295626643200 trainer.py:508] step:  7218, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:76.396492 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3105222 log_pplx:2.9082379 loss:119.96481 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.40021\n",
      "I0710 11:32:25.203858 140295626643200 summary_utils.py:349] Steps/second: 0.177451, Examples/second: 25.165443\n",
      "I0710 11:32:25.204700 140295626643200 trainer.py:508] step:  7219, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:40.824883 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3125749 log_pplx:3.3661492 loss:84.658653 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.4057\n",
      "I0710 11:32:31.700286 140295626643200 summary_utils.py:349] Steps/second: 0.177441, Examples/second: 25.159921\n",
      "I0710 11:32:31.701074 140295626643200 trainer.py:508] step:  7220, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:66.868599 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3107282 log_pplx:2.8889036 loss:116.24225 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.41101\n",
      "I0710 11:32:35.612550 140295626643200 summary_utils.py:349] Steps/second: 0.177461, Examples/second: 25.163992\n",
      "I0710 11:32:35.613308 140295626643200 trainer.py:508] step:  7221, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:37.094696 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3123155 log_pplx:3.438884 loss:86.552414 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.41614\n",
      "I0710 11:32:41.957229 140295626643200 summary_utils.py:349] Steps/second: 0.177452, Examples/second: 25.158727\n",
      "I0710 11:32:41.958033 140295626643200 trainer.py:508] step:  7222, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:71.338776 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3109478 log_pplx:2.9831367 loss:124.88155 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.4212\n",
      "I0710 11:32:45.071626 140295626643200 summary_utils.py:349] Steps/second: 0.177482, Examples/second: 25.170466\n",
      "I0710 11:32:45.072474 140295626643200 trainer.py:508] step:  7223, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:22.547483 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3116524 log_pplx:3.4247849 loss:52.602558 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.42639\n",
      "I0710 11:32:51.493653 140295626643200 summary_utils.py:349] Steps/second: 0.177473, Examples/second: 25.165073\n",
      "I0710 11:32:51.494604 140295626643200 trainer.py:508] step:  7224, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:69.088219 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3109251 log_pplx:2.8953166 loss:118.63558 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.43121\n",
      "I0710 11:32:57.705986 140295626643200 summary_utils.py:349] Steps/second: 0.177466, Examples/second: 25.160033\n",
      "I0710 11:32:57.706793 140295626643200 trainer.py:508] step:  7225, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:70.319946 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3110062 log_pplx:2.9695332 loss:122.23342 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.43604\n",
      "I0710 11:33:01.628623 140295626643200 summary_utils.py:349] Steps/second: 0.177486, Examples/second: 25.164080\n",
      "I0710 11:33:01.629379 140295626643200 trainer.py:508] step:  7226, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:48.064087 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3124039 log_pplx:3.4079273 loss:86.114059 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.44073\n",
      "I0710 11:33:04.647594 140295626643200 summary_utils.py:349] Steps/second: 0.177517, Examples/second: 25.175960\n",
      "I0710 11:33:04.648710 140295626643200 trainer.py:508] step:  7227, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:22.743462 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3115863 log_pplx:3.3214068 loss:50.599556 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.44501\n",
      "I0710 11:33:15.096632 140295626643200 summary_utils.py:349] Steps/second: 0.177460, Examples/second: 25.161250\n",
      "I0710 11:33:15.097470 140295626643200 trainer.py:508] step:  7228, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:123.77538 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3096499 log_pplx:2.3745103 loss:164.79103 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:529.44904\n",
      "I0710 11:33:21.289926 140295626643200 summary_utils.py:349] Steps/second: 0.177454, Examples/second: 25.156251\n",
      "I0710 11:33:21.290745 140295626643200 trainer.py:508] step:  7229, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:96.628151 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.31082 log_pplx:2.9206154 loss:122.59283 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.45319\n",
      "I0710 11:33:27.669148 140295626643200 summary_utils.py:349] Steps/second: 0.177445, Examples/second: 25.150949\n",
      "I0710 11:33:27.669965 140295626643200 trainer.py:508] step:  7230, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:78.20768 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3110009 log_pplx:2.9819078 loss:121.84822 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.45831\n",
      "I0710 11:33:30.465544 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 11:33:31.649376 140295626643200 summary_utils.py:349] Steps/second: 0.177464, Examples/second: 25.154894\n",
      "I0710 11:33:31.650167 140295626643200 trainer.py:508] step:  7231, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:47.948246 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.312454 log_pplx:3.358397 loss:83.980911 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.46405\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 11:33:35.642326 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 11:33:36.146706 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00007230\n",
      "I0710 11:33:38.275072 140295626643200 summary_utils.py:349] Steps/second: 0.177453, Examples/second: 25.149187\n",
      "I0710 11:33:38.275852 140295626643200 trainer.py:508] step:  7232, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:89.072731 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3108521 log_pplx:2.8926802 loss:119.86543 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.46948\n",
      "I0710 11:33:40.459963 140295626643200 summary_utils.py:349] Steps/second: 0.177493, Examples/second: 25.179278\n",
      "I0710 11:33:40.460787 140295626643200 trainer.py:508] step:  7233, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:13.950451 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.310635 log_pplx:3.508549 loss:25.841286 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:529.47461\n",
      "I0710 11:33:43.501870 140295626643200 summary_utils.py:349] Steps/second: 0.177523, Examples/second: 25.191087\n",
      "I0710 11:33:43.502684 140295626643200 trainer.py:508] step:  7234, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:25.162056 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3122609 log_pplx:3.414047 loss:53.277805 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.47931\n",
      "I0710 11:33:49.786412 140295626643200 summary_utils.py:349] Steps/second: 0.177516, Examples/second: 25.185934\n",
      "I0710 11:33:49.787476 140295626643200 trainer.py:508] step:  7235, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:78.652336 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3113484 log_pplx:3.0505319 loss:125.22434 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.48395\n",
      "I0710 11:33:53.620295 140295626643200 summary_utils.py:349] Steps/second: 0.177537, Examples/second: 25.190107\n",
      "I0710 11:33:53.621186 140295626643200 trainer.py:508] step:  7236, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:51.525852 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3128476 log_pplx:3.4067428 loss:85.445374 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.48871\n",
      "I0710 11:33:59.877604 140295626643200 summary_utils.py:349] Steps/second: 0.177530, Examples/second: 25.185003\n",
      "I0710 11:33:59.878574 140295626643200 base_runner.py:111] step:  7237, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:75.570465 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3108543 log_pplx:2.8774745 loss:118.30017 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.49377\n",
      "I0710 11:34:09.926492 140295626643200 summary_utils.py:349] Steps/second: 0.177478, Examples/second: 25.171002\n",
      "I0710 11:34:09.927293 140295626643200 trainer.py:508] step:  7238, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:200.14754 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3097906 log_pplx:2.3482699 loss:170.42569 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:529.49866\n",
      "I0710 11:34:16.117677 140295626643200 summary_utils.py:349] Steps/second: 0.177472, Examples/second: 25.166021\n",
      "I0710 11:34:16.118515 140295626643200 trainer.py:508] step:  7239, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:67.277939 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.311217 log_pplx:2.9197211 loss:120.21952 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.50269\n",
      "I0710 11:34:19.211328 140295626643200 summary_utils.py:349] Steps/second: 0.177501, Examples/second: 25.177719\n",
      "I0710 11:34:19.212110 140295626643200 trainer.py:508] step:  7240, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:28.849316 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3120648 log_pplx:3.4003468 loss:52.585831 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.50739\n",
      "I0710 11:34:23.118056 140295626643200 summary_utils.py:349] Steps/second: 0.177521, Examples/second: 25.181766\n",
      "I0710 11:34:23.118856 140295626643200 trainer.py:508] step:  7241, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:45.457672 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3128437 log_pplx:3.4054959 loss:86.393173 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.51172\n",
      "I0710 11:34:29.586055 140295626643200 summary_utils.py:349] Steps/second: 0.177512, Examples/second: 25.176327\n",
      "I0710 11:34:29.586837 140295626643200 trainer.py:508] step:  7242, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:116.9662 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.310752 log_pplx:2.9438961 loss:115.91591 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.5163\n",
      "I0710 11:34:35.654979 140295626643200 summary_utils.py:349] Steps/second: 0.177507, Examples/second: 25.171552\n",
      "I0710 11:34:35.655741 140295626643200 trainer.py:508] step:  7243, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:82.821976 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.310798 log_pplx:2.9659841 loss:113.18937 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.52222\n",
      "I0710 11:34:39.526242 140295626643200 summary_utils.py:349] Steps/second: 0.177527, Examples/second: 25.175655\n",
      "I0710 11:34:39.527060 140295626643200 trainer.py:508] step:  7244, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:70.038582 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3130356 log_pplx:3.4535096 loss:87.438553 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.52875\n",
      "I0710 11:34:42.583726 140295626643200 summary_utils.py:349] Steps/second: 0.177557, Examples/second: 25.187393\n",
      "I0710 11:34:42.584751 140295626643200 trainer.py:508] step:  7245, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:42.870651 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3123965 log_pplx:3.4903677 loss:53.541691 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.53436\n",
      "I0710 11:34:49.066925 140295626643200 summary_utils.py:349] Steps/second: 0.177547, Examples/second: 25.181934\n",
      "I0710 11:34:49.067780 140295626643200 trainer.py:508] step:  7246, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:79.576675 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.311088 log_pplx:2.9925542 loss:116.52258 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.53937\n",
      "I0710 11:34:55.518344 140295626643200 summary_utils.py:349] Steps/second: 0.177538, Examples/second: 25.176532\n",
      "I0710 11:34:55.519100 140295626643200 trainer.py:508] step:  7247, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:75.110352 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3112813 log_pplx:2.9012771 loss:118.95235 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.54419\n",
      "I0710 11:35:06.865036 140295626643200 summary_utils.py:349] Steps/second: 0.177471, Examples/second: 25.160449\n",
      "I0710 11:35:06.866041 140295626643200 trainer.py:508] step:  7248, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:196.20721 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3101432 log_pplx:2.4277048 loss:172.60982 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:529.54889\n",
      "I0710 11:35:10.696878 140295626643200 summary_utils.py:349] Steps/second: 0.177492, Examples/second: 25.164611\n",
      "I0710 11:35:10.697664 140295626643200 trainer.py:508] step:  7249, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:38.353554 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3129295 log_pplx:3.389293 loss:84.626404 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.55469\n",
      "I0710 11:35:16.847084 140295626643200 summary_utils.py:349] Steps/second: 0.177486, Examples/second: 25.159720\n",
      "I0710 11:35:16.847866 140295626643200 trainer.py:508] step:  7250, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:83.17765 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3114156 log_pplx:2.9810462 loss:120.17342 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.56024\n",
      "I0710 11:35:19.883084 140295626643200 summary_utils.py:349] Steps/second: 0.177516, Examples/second: 25.171468\n",
      "I0710 11:35:19.883872 140295626643200 trainer.py:508] step:  7251, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:39.191235 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3130237 log_pplx:3.4778905 loss:54.165428 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.56561\n",
      "I0710 11:35:20.551477 140295635035904 trainer.py:345] Write summary @7251\n",
      "I0710 11:35:31.351719 140295626643200 summary_utils.py:349] Steps/second: 0.177449, Examples/second: 25.157829\n",
      "I0710 11:35:31.353031 140295626643200 trainer.py:508] step:  7252, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:90.0047 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3112602 log_pplx:2.9846549 loss:121.96047 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.57111\n",
      "2020-07-10 11:35:34.016576: I lingvo/core/ops/record_batcher.cc:394] 15318 total seconds passed. Total records yielded: 386240. Total records skipped: 170\n",
      "2020-07-10 11:35:34.016649: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 11:35:34.016667: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 11:35:34.016682: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 11:35:34.016696: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 11:35:34.016709: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 11:35:34.016722: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 11:35:34.016735: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 11:35:34.016749: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 11:35:34.016763: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "2020-07-10 11:35:34.016778: I lingvo/core/ops/record_batcher.cc:399] Out-of-range sample: 122\n",
      "I0710 11:35:36.602618 140295626643200 summary_utils.py:349] Steps/second: 0.177453, Examples/second: 25.159652\n",
      "I0710 11:35:36.603927 140295626643200 trainer.py:508] step:  7253, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:42.406925 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3133485 log_pplx:3.4383068 loss:86.15107 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.57654\n",
      "I0710 11:35:45.996886 140295626643200 summary_utils.py:349] Steps/second: 0.177409, Examples/second: 25.149442\n",
      "I0710 11:35:45.997719 140295626643200 trainer.py:508] step:  7254, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:71.894936 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3115451 log_pplx:2.9789994 loss:120.94737 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.58203\n",
      "I0710 11:35:54.803288 140295626643200 summary_utils.py:349] Steps/second: 0.177373, Examples/second: 25.140210\n",
      "I0710 11:35:54.804340 140295626643200 trainer.py:508] step:  7255, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:70.221931 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3113531 log_pplx:2.935591 loss:121.27661 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.58783\n",
      "I0710 11:35:58.779150 140295626643200 summary_utils.py:349] Steps/second: 0.177392, Examples/second: 25.150390\n",
      "I0710 11:35:58.780287 140295626643200 trainer.py:508] step:  7256, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:29.455044 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3127283 log_pplx:3.4712019 loss:54.196846 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.59406\n",
      "I0710 11:36:03.823313 140295626643200 summary_utils.py:349] Steps/second: 0.177399, Examples/second: 25.152551\n",
      "I0710 11:36:03.824477 140295626643200 trainer.py:508] step:  7257, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:51.085541 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3132045 log_pplx:3.3757792 loss:84.626564 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.60016\n",
      "I0710 11:36:13.894830 140295626643200 summary_utils.py:349] Steps/second: 0.177348, Examples/second: 25.141255\n",
      "I0710 11:36:13.896148 140295626643200 trainer.py:508] step:  7258, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:73.338394 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3114702 log_pplx:2.9398415 loss:120.60699 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.60651\n",
      "I0710 11:36:16.824092 140295626643200 summary_utils.py:349] Steps/second: 0.177379, Examples/second: 25.169820\n",
      "I0710 11:36:16.826109 140295626643200 trainer.py:508] step:  7259, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:20.866884 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3114188 log_pplx:3.5226026 loss:26.082397 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:529.61255\n",
      "I0710 11:36:34.206558 140295626643200 summary_utils.py:349] Steps/second: 0.177243, Examples/second: 25.143945\n",
      "I0710 11:36:34.207677 140295626643200 trainer.py:508] step:  7260, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:210.84232 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3102595 log_pplx:2.4359312 loss:174.65628 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:529.6181\n",
      "I0710 11:36:37.503644 140295635035904 trainer.py:354] Write summary done: step 7251\n",
      "I0710 11:36:41.872295 140295626643200 summary_utils.py:349] Steps/second: 0.177220, Examples/second: 25.136610\n",
      "I0710 11:36:41.873211 140295626643200 trainer.py:508] step:  7261, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:76.592384 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3115456 log_pplx:3.0393586 loss:123.66392 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.62305\n",
      "I0710 11:36:45.790619 140295626643200 summary_utils.py:349] Steps/second: 0.177240, Examples/second: 25.140611\n",
      "I0710 11:36:45.791364 140295626643200 trainer.py:508] step:  7262, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:41.182961 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3135192 log_pplx:3.3755355 loss:84.557167 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.62811\n",
      "I0710 11:36:48.875246 140295626643200 summary_utils.py:349] Steps/second: 0.177269, Examples/second: 25.152215\n",
      "I0710 11:36:48.876070 140295626643200 trainer.py:508] step:  7263, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:32.347164 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3126943 log_pplx:3.35513 loss:52.869514 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.63275\n",
      "I0710 11:36:55.332487 140295626643200 summary_utils.py:349] Steps/second: 0.177260, Examples/second: 25.146858\n",
      "I0710 11:36:55.333281 140295626643200 trainer.py:508] step:  7264, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:106.07033 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3117875 log_pplx:3.0353248 loss:121.18533 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.63702\n",
      "I0710 11:37:01.811069 140295626643200 summary_utils.py:349] Steps/second: 0.177250, Examples/second: 25.141471\n",
      "I0710 11:37:01.811895 140295626643200 trainer.py:508] step:  7265, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:126.22476 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3120559 log_pplx:2.9675829 loss:125.93681 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.64258\n",
      "I0710 11:37:05.657425 140295626643200 summary_utils.py:349] Steps/second: 0.177271, Examples/second: 25.145582\n",
      "I0710 11:37:05.658231 140295626643200 trainer.py:508] step:  7266, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:40.557198 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3136501 log_pplx:3.3930545 loss:85.738243 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.64935\n",
      "I0710 11:37:12.079649 140295626643200 summary_utils.py:349] Steps/second: 0.177262, Examples/second: 25.140291\n",
      "I0710 11:37:12.080419 140295626643200 trainer.py:508] step:  7267, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:78.693741 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3118584 log_pplx:2.8712578 loss:116.21416 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.65564\n",
      "I0710 11:37:15.074901 140295626643200 summary_utils.py:349] Steps/second: 0.177292, Examples/second: 25.152022\n",
      "I0710 11:37:15.075976 140295626643200 trainer.py:508] step:  7268, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:30.209549 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3129802 log_pplx:3.4072483 loss:53.411278 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.66174\n",
      "I0710 11:37:21.536452 140295626643200 summary_utils.py:349] Steps/second: 0.177283, Examples/second: 25.146667\n",
      "I0710 11:37:21.537266 140295626643200 trainer.py:508] step:  7269, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:97.941048 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3117071 log_pplx:2.9750495 loss:119.07635 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.6673\n",
      "I0710 11:37:27.978180 140295626643200 summary_utils.py:349] Steps/second: 0.177274, Examples/second: 25.141349\n",
      "I0710 11:37:27.978985 140295626643200 trainer.py:508] step:  7270, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:73.639999 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3117195 log_pplx:2.9135871 loss:119.85769 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.67169\n",
      "I0710 11:37:31.834538 140295626643200 summary_utils.py:349] Steps/second: 0.177294, Examples/second: 25.145437\n",
      "I0710 11:37:31.835298 140295626643200 trainer.py:508] step:  7271, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:38.732506 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3134787 log_pplx:3.3540664 loss:84.753067 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.67603\n",
      "I0710 11:37:42.148281 140295626643200 summary_utils.py:349] Steps/second: 0.177241, Examples/second: 25.131223\n",
      "I0710 11:37:42.149081 140295626643200 trainer.py:508] step:  7272, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:416.27814 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3112395 log_pplx:2.5466158 loss:189.85019 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:529.68018\n",
      "I0710 11:37:48.530787 140295626643200 summary_utils.py:349] Steps/second: 0.177232, Examples/second: 25.126015\n",
      "I0710 11:37:48.531634 140295626643200 trainer.py:508] step:  7273, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:91.768608 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3123447 log_pplx:2.9627469 loss:124.28722 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.68561\n",
      "I0710 11:37:51.633286 140295626643200 summary_utils.py:349] Steps/second: 0.177261, Examples/second: 25.137546\n",
      "I0710 11:37:51.634117 140295626643200 trainer.py:508] step:  7274, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:38.914928 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3134592 log_pplx:3.4497097 loss:54.238598 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.69141\n",
      "I0710 11:37:57.744426 140295626643200 summary_utils.py:349] Steps/second: 0.177256, Examples/second: 25.132780\n",
      "I0710 11:37:57.745180 140295626643200 trainer.py:508] step:  7275, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:232.55229 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3116016 log_pplx:2.993434 loss:122.43144 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.69733\n",
      "I0710 11:38:01.539630 140295626643200 summary_utils.py:349] Steps/second: 0.177277, Examples/second: 25.136962\n",
      "I0710 11:38:01.540375 140295626643200 trainer.py:508] step:  7276, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:41.967693 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3137683 log_pplx:3.4506042 loss:86.459206 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.70264\n",
      "I0710 11:38:07.909157 140295626643200 summary_utils.py:349] Steps/second: 0.177269, Examples/second: 25.131779\n",
      "I0710 11:38:07.909942 140295626643200 trainer.py:508] step:  7277, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:148.86337 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3122965 log_pplx:3.0380833 loss:125.13104 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.70789\n",
      "I0710 11:38:14.305563 140295626643200 summary_utils.py:349] Steps/second: 0.177260, Examples/second: 25.126557\n",
      "I0710 11:38:14.306408 140295626643200 trainer.py:508] step:  7278, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:81.494293 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3125403 log_pplx:3.0666776 loss:129.52881 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.71271\n",
      "I0710 11:38:17.566304 140295626643200 summary_utils.py:349] Steps/second: 0.177287, Examples/second: 25.137811\n",
      "I0710 11:38:17.567203 140295626643200 trainer.py:508] step:  7279, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:43.919296 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3130959 log_pplx:3.4356887 loss:53.105553 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.71759\n",
      "I0710 11:38:21.670008 140295626643200 summary_utils.py:349] Steps/second: 0.177305, Examples/second: 25.141485\n",
      "I0710 11:38:21.670800 140295626643200 trainer.py:508] step:  7280, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:94.287636 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3138689 log_pplx:3.5032332 loss:87.558937 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.72272\n",
      "I0710 11:38:31.223205 140295626643200 summary_utils.py:349] Steps/second: 0.177260, Examples/second: 25.128554\n",
      "I0710 11:38:31.224004 140295626643200 trainer.py:508] step:  7281, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:261.58261 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3109403 log_pplx:2.4766858 loss:172.93457 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:529.72864\n",
      "I0710 11:38:37.562821 140295626643200 summary_utils.py:349] Steps/second: 0.177252, Examples/second: 25.123433\n",
      "I0710 11:38:37.563866 140295626643200 trainer.py:508] step:  7282, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:121.86316 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.312472 log_pplx:3.033577 loss:123.35282 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.7345\n",
      "I0710 11:38:39.776935 140295626643200 summary_utils.py:349] Steps/second: 0.177291, Examples/second: 25.152898\n",
      "I0710 11:38:39.777743 140295626643200 trainer.py:508] step:  7283, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:17.876307 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3119268 log_pplx:3.5292861 loss:26.317999 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:529.74084\n",
      "I0710 11:38:46.030034 140295626643200 summary_utils.py:349] Steps/second: 0.177284, Examples/second: 25.147910\n",
      "I0710 11:38:46.030789 140295626643200 trainer.py:508] step:  7284, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:82.864182 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3124267 log_pplx:3.0262771 loss:121.99679 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.74683\n",
      "I0710 11:38:49.951879 140295626643200 summary_utils.py:349] Steps/second: 0.177304, Examples/second: 25.151870\n",
      "I0710 11:38:49.952696 140295626643200 trainer.py:508] step:  7285, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:69.689568 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3142476 log_pplx:3.4998116 loss:88.238991 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.75281\n",
      "I0710 11:38:56.165036 140295626643200 summary_utils.py:349] Steps/second: 0.177297, Examples/second: 25.146951\n",
      "I0710 11:38:56.165812 140295626643200 trainer.py:508] step:  7286, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:91.966095 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3123059 log_pplx:2.901624 loss:117.37069 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.7583\n",
      "I0710 11:38:59.237039 140295626643200 summary_utils.py:349] Steps/second: 0.177327, Examples/second: 25.158477\n",
      "I0710 11:38:59.237782 140295626643200 trainer.py:508] step:  7287, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:100.31702 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.313863 log_pplx:3.7704964 loss:59.296951 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.76422\n",
      "I0710 11:39:05.701015 140295626643200 summary_utils.py:349] Steps/second: 0.177317, Examples/second: 25.153152\n",
      "I0710 11:39:05.701759 140295626643200 trainer.py:508] step:  7288, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:102.05622 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3125116 log_pplx:3.0068505 loss:126.02462 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.76935\n",
      "I0710 11:39:11.464991 140295626643200 summary_utils.py:349] Steps/second: 0.177316, Examples/second: 25.148965\n",
      "I0710 11:39:11.465836 140295626643200 trainer.py:508] step:  7289, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:75.316116 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3123583 log_pplx:3.0026007 loss:116.95131 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.77527\n",
      "I0710 11:39:15.320879 140295626643200 summary_utils.py:349] Steps/second: 0.177336, Examples/second: 25.153025\n",
      "I0710 11:39:15.321663 140295626643200 trainer.py:508] step:  7290, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:42.162628 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3142502 log_pplx:3.4525378 loss:86.9608 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.78143\n",
      "I0710 11:39:26.469923 140295626643200 summary_utils.py:349] Steps/second: 0.177273, Examples/second: 25.137548\n",
      "I0710 11:39:26.470857 140295626643200 trainer.py:508] step:  7291, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:232.91541 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3114947 log_pplx:2.4839387 loss:184.12196 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:529.78711\n",
      "I0710 11:39:32.831022 140295626643200 summary_utils.py:349] Steps/second: 0.177265, Examples/second: 25.132407\n",
      "I0710 11:39:32.831961 140295626643200 trainer.py:508] step:  7292, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:78.681335 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3125014 log_pplx:3.0227356 loss:121.8918 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.79303\n",
      "I0710 11:39:35.938280 140295626643200 summary_utils.py:349] Steps/second: 0.177294, Examples/second: 25.143852\n",
      "I0710 11:39:35.939114 140295626643200 trainer.py:508] step:  7293, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:26.067698 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3138031 log_pplx:3.425236 loss:53.224957 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.79889\n",
      "I0710 11:39:42.095577 140295626643200 summary_utils.py:349] Steps/second: 0.177288, Examples/second: 25.139041\n",
      "I0710 11:39:42.096408 140295626643200 trainer.py:508] step:  7294, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:78.492645 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3125873 log_pplx:2.9752598 loss:120.34927 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.8045\n",
      "I0710 11:39:45.868638 140295626643200 summary_utils.py:349] Steps/second: 0.177309, Examples/second: 25.143229\n",
      "I0710 11:39:45.869447 140295626643200 trainer.py:508] step:  7295, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:39.125874 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3142037 log_pplx:3.4119873 loss:85.278358 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.80988\n",
      "I0710 11:39:52.146097 140295626643200 summary_utils.py:349] Steps/second: 0.177302, Examples/second: 25.138227\n",
      "I0710 11:39:52.146899 140295626643200 trainer.py:508] step:  7296, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:74.303757 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3123692 log_pplx:3.0067689 loss:117.60225 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.815\n",
      "I0710 11:39:58.362304 140295626643200 summary_utils.py:349] Steps/second: 0.177296, Examples/second: 25.133328\n",
      "I0710 11:39:58.363197 140295626643200 trainer.py:508] step:  7297, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:75.774368 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3120238 log_pplx:2.9276781 loss:116.19222 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.82007\n",
      "I0710 11:40:01.414603 140295626643200 summary_utils.py:349] Steps/second: 0.177325, Examples/second: 25.144843\n",
      "I0710 11:40:01.415403 140295626643200 trainer.py:508] step:  7298, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:38.483589 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3141063 log_pplx:3.4711354 loss:54.792423 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.82495\n",
      "I0710 11:40:05.193070 140295626643200 summary_utils.py:349] Steps/second: 0.177346, Examples/second: 25.149016\n",
      "I0710 11:40:05.193981 140295626643200 trainer.py:508] step:  7299, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:43.565121 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3145845 log_pplx:3.4065778 loss:86.527077 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.8299\n",
      "I0710 11:40:11.526226 140295626643200 summary_utils.py:349] Steps/second: 0.177338, Examples/second: 25.143928\n",
      "I0710 11:40:11.526990 140295626643200 trainer.py:508] step:  7300, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:71.243889 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.312851 log_pplx:2.9804261 loss:119.3288 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.83496\n",
      "I0710 11:40:17.709563 140295626643200 summary_utils.py:349] Steps/second: 0.177332, Examples/second: 25.139085\n",
      "I0710 11:40:17.710710 140295626643200 trainer.py:508] step:  7301, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:70.376717 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3129094 log_pplx:2.9718497 loss:120.91713 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.84045\n",
      "I0710 11:40:28.219729 140295626643200 summary_utils.py:349] Steps/second: 0.177277, Examples/second: 25.124710\n",
      "I0710 11:40:28.220510 140295626643200 trainer.py:508] step:  7302, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:174.0979 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3117498 log_pplx:2.4598653 loss:174.58894 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:529.84601\n",
      "I0710 11:40:31.980638 140295626643200 summary_utils.py:349] Steps/second: 0.177298, Examples/second: 25.128909\n",
      "I0710 11:40:31.981460 140295626643200 trainer.py:508] step:  7303, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:38.957813 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3145584 log_pplx:3.4161489 loss:86.556679 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.85217\n",
      "I0710 11:40:38.127774 140295626643200 summary_utils.py:349] Steps/second: 0.177292, Examples/second: 25.124137\n",
      "I0710 11:40:38.128562 140295626643200 trainer.py:508] step:  7304, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:68.758904 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3125494 log_pplx:2.9513671 loss:116.02562 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.85815\n",
      "I0710 11:40:41.120650 140295626643200 summary_utils.py:349] Steps/second: 0.177322, Examples/second: 25.135720\n",
      "I0710 11:40:41.121449 140295626643200 trainer.py:508] step:  7305, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:26.462843 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3144368 log_pplx:3.4514554 loss:54.239082 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.86407\n",
      "I0710 11:40:47.625125 140295626643200 summary_utils.py:349] Steps/second: 0.177313, Examples/second: 25.130374\n",
      "I0710 11:40:47.625920 140295626643200 trainer.py:508] step:  7306, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:88.975288 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3126068 log_pplx:2.9575465 loss:119.04126 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.86957\n",
      "I0710 11:40:49.823321 140295626643200 summary_utils.py:349] Steps/second: 0.177352, Examples/second: 25.159618\n",
      "I0710 11:40:49.824147 140295626643200 trainer.py:508] step:  7307, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:16.493891 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3129013 log_pplx:3.6203401 loss:26.233322 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:529.87531\n",
      "I0710 11:40:53.600858 140295626643200 summary_utils.py:349] Steps/second: 0.177373, Examples/second: 25.163776\n",
      "I0710 11:40:53.601636 140295626643200 trainer.py:508] step:  7308, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:38.428696 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3142751 log_pplx:3.3792498 loss:84.840294 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.8808\n",
      "I0710 11:40:59.561832 140295626643200 summary_utils.py:349] Steps/second: 0.177369, Examples/second: 25.159297\n",
      "I0710 11:40:59.562671 140295626643200 trainer.py:508] step:  7309, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:69.189835 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.312901 log_pplx:2.9188545 loss:117.41093 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.88593\n",
      "I0710 11:41:09.701767 140295626643200 summary_utils.py:349] Steps/second: 0.177318, Examples/second: 25.145542\n",
      "I0710 11:41:09.702594 140295626643200 trainer.py:508] step:  7310, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:217.03258 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3119615 log_pplx:2.4668384 loss:179.8942 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:529.89124\n",
      "I0710 11:41:12.680183 140295626643200 summary_utils.py:349] Steps/second: 0.177348, Examples/second: 25.157120\n",
      "I0710 11:41:12.681241 140295626643200 trainer.py:508] step:  7311, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:41.36422 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3144246 log_pplx:3.5008702 loss:54.878876 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.89703\n",
      "I0710 11:41:18.825396 140295626643200 summary_utils.py:349] Steps/second: 0.177343, Examples/second: 25.152353\n",
      "I0710 11:41:18.826341 140295626643200 trainer.py:508] step:  7312, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:70.12455 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3132383 log_pplx:2.8997931 loss:117.55037 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.90216\n",
      "I0710 11:41:22.647680 140295626643200 summary_utils.py:349] Steps/second: 0.177363, Examples/second: 25.156433\n",
      "I0710 11:41:22.648473 140295626643200 trainer.py:508] step:  7313, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:54.14513 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3147767 log_pplx:3.4591918 loss:86.890587 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.90771\n",
      "I0710 11:41:29.011463 140295626643200 summary_utils.py:349] Steps/second: 0.177355, Examples/second: 25.151318\n",
      "I0710 11:41:29.012310 140295626643200 trainer.py:508] step:  7314, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:74.380157 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3132993 log_pplx:2.9643459 loss:119.6484 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.91272\n",
      "I0710 11:41:35.315634 140295626643200 summary_utils.py:349] Steps/second: 0.177347, Examples/second: 25.146303\n",
      "I0710 11:41:35.316616 140295626643200 trainer.py:508] step:  7315, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:68.947777 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3126655 log_pplx:2.8868237 loss:113.99345 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.91779\n",
      "I0710 11:41:39.101119 140295626643200 summary_utils.py:349] Steps/second: 0.177368, Examples/second: 25.150439\n",
      "I0710 11:41:39.101921 140295626643200 trainer.py:508] step:  7316, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:43.708103 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3147178 log_pplx:3.3751512 loss:84.695198 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.92328\n",
      "I0710 11:41:42.110658 140295626643200 summary_utils.py:349] Steps/second: 0.177398, Examples/second: 25.161945\n",
      "I0710 11:41:42.111410 140295626643200 trainer.py:508] step:  7317, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:24.951635 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3143389 log_pplx:3.4012742 loss:51.510708 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.92883\n",
      "I0710 11:41:48.284855 140295626643200 summary_utils.py:349] Steps/second: 0.177392, Examples/second: 25.157139\n",
      "I0710 11:41:48.285615 140295626643200 trainer.py:508] step:  7318, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:73.356621 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3132156 log_pplx:2.9709229 loss:119.87675 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.93433\n",
      "I0710 11:41:54.546612 140295626643200 summary_utils.py:349] Steps/second: 0.177385, Examples/second: 25.152195\n",
      "I0710 11:41:54.547603 140295626643200 trainer.py:508] step:  7319, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:71.668274 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3134605 log_pplx:2.9022636 loss:120.22627 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.94\n",
      "I0710 11:42:05.243873 140295626643200 summary_utils.py:349] Steps/second: 0.177328, Examples/second: 25.137600\n",
      "I0710 11:42:05.244684 140295626643200 trainer.py:508] step:  7320, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:145.037 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3125472 log_pplx:2.4459684 loss:177.45502 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:529.94568\n",
      "I0710 11:42:09.040837 140295626643200 summary_utils.py:349] Steps/second: 0.177349, Examples/second: 25.141712\n",
      "I0710 11:42:09.041621 140295626643200 trainer.py:508] step:  7321, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:38.82428 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3150012 log_pplx:3.3715019 loss:85.235779 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.95148\n",
      "I0710 11:42:15.410582 140295626643200 summary_utils.py:349] Steps/second: 0.177340, Examples/second: 25.136609\n",
      "I0710 11:42:15.411373 140295626643200 trainer.py:508] step:  7322, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:83.837921 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3131664 log_pplx:2.9884028 loss:121.70271 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.95691\n",
      "I0710 11:42:18.485273 140295626643200 summary_utils.py:349] Steps/second: 0.177369, Examples/second: 25.147986\n",
      "I0710 11:42:18.486106 140295626643200 trainer.py:508] step:  7323, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:24.758417 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3148333 log_pplx:3.4201424 loss:53.51989 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.96185\n",
      "I0710 11:42:24.456364 140295626643200 summary_utils.py:349] Steps/second: 0.177366, Examples/second: 25.143521\n",
      "I0710 11:42:24.457159 140295626643200 trainer.py:508] step:  7324, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:71.147339 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3130842 log_pplx:2.940479 loss:117.06781 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.96655\n",
      "I0710 11:42:28.322919 140295626643200 summary_utils.py:349] Steps/second: 0.177386, Examples/second: 25.147516\n",
      "I0710 11:42:28.323699 140295626643200 trainer.py:508] step:  7325, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:36.900883 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3151797 log_pplx:3.315995 loss:84.806572 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.97119\n",
      "I0710 11:42:34.568058 140295626643200 summary_utils.py:349] Steps/second: 0.177379, Examples/second: 25.142615\n",
      "I0710 11:42:34.568814 140295626643200 trainer.py:508] step:  7326, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:72.126167 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3134147 log_pplx:2.9428968 loss:118.08374 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.97565\n",
      "I0710 11:42:40.518858 140295626643200 summary_utils.py:349] Steps/second: 0.177375, Examples/second: 25.138189\n",
      "I0710 11:42:40.519822 140295626643200 trainer.py:508] step:  7327, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:72.270531 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3130528 log_pplx:2.953403 loss:115.25655 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:529.98053\n",
      "I0710 11:42:42.693613 140295626643200 summary_utils.py:349] Steps/second: 0.177414, Examples/second: 25.167260\n",
      "I0710 11:42:42.694370 140295626643200 trainer.py:508] step:  7328, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:16.452665 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3132914 log_pplx:3.5423791 loss:25.592306 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:529.98602\n",
      "I0710 11:42:46.489861 140295626643200 summary_utils.py:349] Steps/second: 0.177435, Examples/second: 25.171357\n",
      "I0710 11:42:46.490663 140295626643200 trainer.py:508] step:  7329, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:37.548122 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3156029 log_pplx:3.394089 loss:85.594681 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:529.99115\n",
      "I0710 11:42:49.552225 140295626643200 summary_utils.py:349] Steps/second: 0.177464, Examples/second: 25.182725\n",
      "I0710 11:42:49.553011 140295626643200 trainer.py:508] step:  7330, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:22.30315 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3149748 log_pplx:3.3580675 loss:53.322445 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:529.99622\n",
      "I0710 11:42:56.069619 140295626643200 summary_utils.py:349] Steps/second: 0.177454, Examples/second: 25.177382\n",
      "I0710 11:42:56.070435 140295626643200 trainer.py:508] step:  7331, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:73.107803 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3138318 log_pplx:2.9596503 loss:122.34456 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.0011\n",
      "I0710 11:43:07.042804 140295626643200 summary_utils.py:349] Steps/second: 0.177394, Examples/second: 25.162386\n",
      "I0710 11:43:07.043590 140295626643200 trainer.py:508] step:  7332, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:181.55299 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3125873 log_pplx:2.3578997 loss:162.87192 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.00623\n",
      "I0710 11:43:13.516442 140295626643200 summary_utils.py:349] Steps/second: 0.177384, Examples/second: 25.157127\n",
      "I0710 11:43:13.517224 140295626643200 trainer.py:508] step:  7333, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:68.660088 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3137828 log_pplx:2.98365 loss:121.73291 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.01215\n",
      "I0710 11:43:17.457012 140295626643200 summary_utils.py:349] Steps/second: 0.177404, Examples/second: 25.160987\n",
      "I0710 11:43:17.457795 140295626643200 trainer.py:508] step:  7334, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:45.816753 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3154931 log_pplx:3.4342077 loss:87.014236 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.01788\n",
      "I0710 11:43:23.692871 140295626643200 summary_utils.py:349] Steps/second: 0.177397, Examples/second: 25.156112\n",
      "I0710 11:43:23.693654 140295626643200 trainer.py:508] step:  7335, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:76.793716 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.313771 log_pplx:2.9241776 loss:122.04785 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.02313\n",
      "I0710 11:43:26.825394 140295626643200 summary_utils.py:349] Steps/second: 0.177425, Examples/second: 25.167344\n",
      "I0710 11:43:26.826240 140295626643200 trainer.py:508] step:  7336, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:23.749977 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3151702 log_pplx:3.4011369 loss:54.059475 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.0282\n",
      "I0710 11:43:33.574464 140295626643200 summary_utils.py:349] Steps/second: 0.177412, Examples/second: 25.161651\n",
      "I0710 11:43:33.575294 140295626643200 base_runner.py:111] step:  7337, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:81.320068 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3140979 log_pplx:3.0057368 loss:122.48379 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.03302\n",
      "I0710 11:43:37.520730 140295626643200 summary_utils.py:349] Steps/second: 0.177431, Examples/second: 25.165496\n",
      "I0710 11:43:37.521471 140295626643200 trainer.py:508] step:  7338, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:38.268543 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3152145 log_pplx:3.3038199 loss:83.524696 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.03815\n",
      "I0710 11:43:37.888134 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 11:43:42.828705 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 11:43:43.323466 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00007338\n",
      "I0710 11:43:43.968693 140295626643200 summary_utils.py:349] Steps/second: 0.177422, Examples/second: 25.160287\n",
      "I0710 11:43:43.969438 140295626643200 trainer.py:508] step:  7339, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:68.93634 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3136964 log_pplx:2.9073844 loss:117.89445 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.04279\n",
      "I0710 11:43:50.558586 140295626643200 summary_utils.py:349] Steps/second: 0.177412, Examples/second: 25.154856\n",
      "I0710 11:43:50.559360 140295626643200 trainer.py:508] step:  7340, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:67.668686 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.313764 log_pplx:2.8716164 loss:116.19277 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.04773\n",
      "I0710 11:44:01.411919 140295626643200 summary_utils.py:349] Steps/second: 0.177353, Examples/second: 25.140118\n",
      "I0710 11:44:01.412776 140295626643200 trainer.py:508] step:  7341, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:254.7807 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3132685 log_pplx:2.4975929 loss:186.94482 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.05261\n",
      "I0710 11:44:04.532275 140295626643200 summary_utils.py:349] Steps/second: 0.177381, Examples/second: 25.151346\n",
      "I0710 11:44:04.533291 140295626643200 trainer.py:508] step:  7342, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:30.86318 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3151076 log_pplx:3.4339952 loss:52.972069 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.05756\n",
      "I0710 11:44:08.429194 140295626643200 summary_utils.py:349] Steps/second: 0.177401, Examples/second: 25.155265\n",
      "I0710 11:44:08.429981 140295626643200 trainer.py:508] step:  7343, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:53.047962 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3154011 log_pplx:3.3794007 loss:84.337173 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.06238\n",
      "I0710 11:44:14.877229 140295626643200 summary_utils.py:349] Steps/second: 0.177392, Examples/second: 25.150070\n",
      "I0710 11:44:14.878036 140295626643200 trainer.py:508] step:  7344, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:116.72424 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3139604 log_pplx:2.9713223 loss:122.49276 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.06738\n",
      "I0710 11:44:21.507860 140295626643200 summary_utils.py:349] Steps/second: 0.177381, Examples/second: 25.144590\n",
      "I0710 11:44:21.508863 140295626643200 trainer.py:508] step:  7345, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:99.920403 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3140566 log_pplx:2.9921663 loss:122.2674 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.07227\n",
      "I0710 11:44:25.415551 140295626643200 summary_utils.py:349] Steps/second: 0.177400, Examples/second: 25.148489\n",
      "I0710 11:44:25.416359 140295626643200 trainer.py:508] step:  7346, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:45.895374 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3156085 log_pplx:3.3939557 loss:84.954956 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.07782\n",
      "I0710 11:44:28.516634 140295626643200 summary_utils.py:349] Steps/second: 0.177428, Examples/second: 25.159729\n",
      "I0710 11:44:28.517426 140295626643200 trainer.py:508] step:  7347, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:33.852219 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3148866 log_pplx:3.3641944 loss:52.118729 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.08331\n",
      "I0710 11:44:34.818086 140295626643200 summary_utils.py:349] Steps/second: 0.177421, Examples/second: 25.154772\n",
      "I0710 11:44:34.818853 140295626643200 trainer.py:508] step:  7348, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:72.358147 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3136629 log_pplx:2.912415 loss:112.67406 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.08899\n",
      "I0710 11:44:41.244349 140295626643200 summary_utils.py:349] Steps/second: 0.177412, Examples/second: 25.149620\n",
      "I0710 11:44:41.245134 140295626643200 trainer.py:508] step:  7349, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:74.512741 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3139614 log_pplx:2.9493744 loss:118.41738 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.09454\n",
      "I0710 11:44:52.178956 140295626643200 summary_utils.py:349] Steps/second: 0.177353, Examples/second: 25.134804\n",
      "I0710 11:44:52.179713 140295626643200 trainer.py:508] step:  7350, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:224.54855 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3126601 log_pplx:2.4986334 loss:176.15366 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.09967\n",
      "I0710 11:44:55.984746 140295626643200 summary_utils.py:349] Steps/second: 0.177373, Examples/second: 25.138860\n",
      "I0710 11:44:55.985567 140295626643200 trainer.py:508] step:  7351, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:46.872116 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3158326 log_pplx:3.4153984 loss:87.434189 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.10443\n",
      "I0710 11:44:57.958567 140295635035904 trainer.py:345] Write summary @7351\n",
      "I0710 11:44:59.935437 140295626643200 summary_utils.py:349] Steps/second: 0.177392, Examples/second: 25.164866\n",
      "I0710 11:44:59.936708 140295626643200 trainer.py:508] step:  7352, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:12.465951 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3138514 log_pplx:3.4451029 loss:25.125029 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:530.10907\n",
      "I0710 11:45:09.957293 140295626643200 summary_utils.py:349] Steps/second: 0.177343, Examples/second: 25.154021\n",
      "I0710 11:45:09.958352 140295626643200 trainer.py:508] step:  7353, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:84.928139 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3141111 log_pplx:2.9169509 loss:117.51665 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.11353\n",
      "I0710 11:45:14.035226 140295626643200 summary_utils.py:349] Steps/second: 0.177361, Examples/second: 25.163681\n",
      "I0710 11:45:14.036235 140295626643200 trainer.py:508] step:  7354, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:29.847132 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3155062 log_pplx:3.4955189 loss:54.412666 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.11853\n",
      "I0710 11:45:23.454234 140295626643200 summary_utils.py:349] Steps/second: 0.177318, Examples/second: 25.153801\n",
      "I0710 11:45:23.455056 140295626643200 trainer.py:508] step:  7355, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:86.684624 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3139533 log_pplx:2.8982871 loss:117.12703 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.12305\n",
      "I0710 11:45:28.654213 140295626643200 summary_utils.py:349] Steps/second: 0.177323, Examples/second: 25.155638\n",
      "I0710 11:45:28.654960 140295626643200 trainer.py:508] step:  7356, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:45.7696 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3156406 log_pplx:3.4565017 loss:87.039032 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.12799\n",
      "I0710 11:45:38.063467 140295626643200 summary_utils.py:349] Steps/second: 0.177281, Examples/second: 25.145787\n",
      "I0710 11:45:38.064862 140295626643200 trainer.py:508] step:  7357, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:71.072136 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3141793 log_pplx:2.9211922 loss:117.35891 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.13269\n",
      "I0710 11:45:48.122519 140295626643200 summary_utils.py:349] Steps/second: 0.177232, Examples/second: 25.134921\n",
      "I0710 11:45:48.124457 140295626643200 trainer.py:508] step:  7358, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:239.83887 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3140975 log_pplx:3.0080855 loss:119.27059 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.13788\n",
      "I0710 11:45:53.817637 140295635035904 trainer.py:354] Write summary done: step 7351\n",
      "I0710 11:45:54.504323 140295626643200 summary_utils.py:349] Steps/second: 0.177224, Examples/second: 25.134896\n",
      "I0710 11:45:54.505095 140295626643200 trainer.py:508] step:  7359, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:67.899559 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3156989 log_pplx:3.3259804 loss:83.295021 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.14392\n",
      "I0710 11:46:04.352075 140295626643200 summary_utils.py:349] Steps/second: 0.177177, Examples/second: 25.121871\n",
      "I0710 11:46:04.352938 140295626643200 trainer.py:508] step:  7360, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:158.71417 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3134636 log_pplx:2.449481 loss:177.46489 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.14948\n",
      "I0710 11:46:07.388056 140295626643200 summary_utils.py:349] Steps/second: 0.177206, Examples/second: 25.133150\n",
      "I0710 11:46:07.388864 140295626643200 trainer.py:508] step:  7361, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:47.804146 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3155591 log_pplx:3.4391603 loss:52.554668 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.15564\n",
      "I0710 11:46:13.558680 140295626643200 summary_utils.py:349] Steps/second: 0.177200, Examples/second: 25.128440\n",
      "I0710 11:46:13.559424 140295626643200 trainer.py:508] step:  7362, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:97.604996 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3137813 log_pplx:2.8738227 loss:113.98301 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.16138\n",
      "I0710 11:46:19.865680 140295626643200 summary_utils.py:349] Steps/second: 0.177193, Examples/second: 25.123519\n",
      "I0710 11:46:19.866487 140295626643200 trainer.py:508] step:  7363, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:76.706856 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.314296 log_pplx:2.9810467 loss:120.62061 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.16669\n",
      "I0710 11:46:23.934374 140295626643200 summary_utils.py:349] Steps/second: 0.177210, Examples/second: 25.127140\n",
      "I0710 11:46:23.935279 140295626643200 trainer.py:508] step:  7364, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:51.185543 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3162209 log_pplx:3.3502502 loss:84.698524 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.17218\n",
      "I0710 11:46:30.213084 140295626643200 summary_utils.py:349] Steps/second: 0.177203, Examples/second: 25.122268\n",
      "I0710 11:46:30.213911 140295626643200 trainer.py:508] step:  7365, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:75.118484 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3144784 log_pplx:2.8994672 loss:118.51573 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.17786\n",
      "I0710 11:46:36.541013 140295626643200 summary_utils.py:349] Steps/second: 0.177196, Examples/second: 25.117321\n",
      "I0710 11:46:36.541798 140295626643200 trainer.py:508] step:  7366, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:75.239105 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3144125 log_pplx:3.0442934 loss:121.16288 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.18384\n",
      "I0710 11:46:39.597604 140295626643200 summary_utils.py:349] Steps/second: 0.177224, Examples/second: 25.128545\n",
      "I0710 11:46:39.598431 140295626643200 trainer.py:508] step:  7367, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:34.859665 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3159835 log_pplx:3.4708793 loss:53.513905 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.18994\n",
      "I0710 11:46:45.991190 140295626643200 summary_utils.py:349] Steps/second: 0.177216, Examples/second: 25.123496\n",
      "I0710 11:46:45.992075 140295626643200 trainer.py:508] step:  7368, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:77.684074 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3146222 log_pplx:3.0273974 loss:123.13938 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.19598\n",
      "I0710 11:46:49.766554 140295626643200 summary_utils.py:349] Steps/second: 0.177237, Examples/second: 25.127573\n",
      "I0710 11:46:49.767385 140295626643200 trainer.py:508] step:  7369, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:43.019485 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3160474 log_pplx:3.3238885 loss:83.034889 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.20197\n",
      "I0710 11:46:59.784161 140295626643200 summary_utils.py:349] Steps/second: 0.177188, Examples/second: 25.114331\n",
      "I0710 11:46:59.785005 140295626643200 trainer.py:508] step:  7370, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:149.60396 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3134698 log_pplx:2.4242258 loss:174.66548 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.20789\n",
      "I0710 11:47:06.204417 140295626643200 summary_utils.py:349] Steps/second: 0.177180, Examples/second: 25.109252\n",
      "I0710 11:47:06.205178 140295626643200 trainer.py:508] step:  7371, steps/sec: 0.18, examples/sec: 25.11 grad_norm/all/loss:107.70544 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3149993 log_pplx:3.075074 loss:126.8468 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.21381\n",
      "I0710 11:47:09.270452 140295626643200 summary_utils.py:349] Steps/second: 0.177208, Examples/second: 25.120442\n",
      "I0710 11:47:09.271445 140295626643200 trainer.py:508] step:  7372, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:22.430979 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3159969 log_pplx:3.3596563 loss:52.573372 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.21912\n",
      "I0710 11:47:15.206455 140295626643200 summary_utils.py:349] Steps/second: 0.177205, Examples/second: 25.116123\n",
      "I0710 11:47:15.207234 140295626643200 trainer.py:508] step:  7373, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:78.028099 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3142394 log_pplx:2.9812191 loss:117.57185 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.2243\n",
      "I0710 11:47:18.943189 140295626643200 summary_utils.py:349] Steps/second: 0.177226, Examples/second: 25.120255\n",
      "I0710 11:47:18.943907 140295626643200 trainer.py:508] step:  7374, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:40.964775 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3162805 log_pplx:3.3564756 loss:85.002739 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.22986\n",
      "I0710 11:47:21.112543 140295626643200 summary_utils.py:349] Steps/second: 0.177265, Examples/second: 25.148832\n",
      "I0710 11:47:21.113293 140295626643200 trainer.py:508] step:  7375, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:10.858841 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3148334 log_pplx:3.5206394 loss:25.978466 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:530.23541\n",
      "I0710 11:47:27.475306 140295626643200 summary_utils.py:349] Steps/second: 0.177257, Examples/second: 25.143836\n",
      "I0710 11:47:27.476112 140295626643200 trainer.py:508] step:  7376, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:80.99472 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3149197 log_pplx:3.0637603 loss:126.99287 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.2406\n",
      "I0710 11:47:33.591514 140295626643200 summary_utils.py:349] Steps/second: 0.177251, Examples/second: 25.139231\n",
      "I0710 11:47:33.592290 140295626643200 trainer.py:508] step:  7377, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:71.031906 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3149079 log_pplx:2.8952417 loss:117.65538 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.24634\n",
      "I0710 11:47:36.555912 140295626643200 summary_utils.py:349] Steps/second: 0.177281, Examples/second: 25.150555\n",
      "I0710 11:47:36.556653 140295626643200 trainer.py:508] step:  7378, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:26.44589 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3159536 log_pplx:3.4632814 loss:53.058548 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.25208\n",
      "I0710 11:47:40.374274 140295626643200 summary_utils.py:349] Steps/second: 0.177301, Examples/second: 25.154546\n",
      "I0710 11:47:40.375371 140295626643200 trainer.py:508] step:  7379, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:38.337673 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3167326 log_pplx:3.3802526 loss:85.330246 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.2572\n",
      "I0710 11:47:46.774573 140295626643200 summary_utils.py:349] Steps/second: 0.177293, Examples/second: 25.149495\n",
      "I0710 11:47:46.775301 140295626643200 trainer.py:508] step:  7380, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:75.543633 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3150682 log_pplx:2.9800529 loss:120.84115 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.2619\n",
      "I0710 11:47:55.656019 140295626643200 summary_utils.py:349] Steps/second: 0.177257, Examples/second: 25.138066\n",
      "I0710 11:47:55.656806 140295626643200 trainer.py:508] step:  7381, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:151.18013 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3139021 log_pplx:2.3569212 loss:169.168 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.26672\n",
      "I0710 11:48:01.818756 140295626643200 summary_utils.py:349] Steps/second: 0.177251, Examples/second: 25.133398\n",
      "I0710 11:48:01.819471 140295626643200 trainer.py:508] step:  7382, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:72.535431 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3147731 log_pplx:2.9145575 loss:115.52578 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.27191\n",
      "I0710 11:48:05.655883 140295626643200 summary_utils.py:349] Steps/second: 0.177271, Examples/second: 25.137357\n",
      "I0710 11:48:05.656719 140295626643200 trainer.py:508] step:  7383, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:43.203247 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3165348 log_pplx:3.3611834 loss:84.134613 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.27722\n",
      "I0710 11:48:12.030795 140295626643200 summary_utils.py:349] Steps/second: 0.177263, Examples/second: 25.132361\n",
      "I0710 11:48:12.031588 140295626643200 trainer.py:508] step:  7384, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:79.815308 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3151966 log_pplx:3.005506 loss:124.84121 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.28217\n",
      "I0710 11:48:15.087961 140295626643200 summary_utils.py:349] Steps/second: 0.177291, Examples/second: 25.143514\n",
      "I0710 11:48:15.088874 140295626643200 trainer.py:508] step:  7385, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:25.789127 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3166343 log_pplx:3.4239156 loss:54.635529 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.28711\n",
      "I0710 11:48:21.298066 140295626643200 summary_utils.py:349] Steps/second: 0.177285, Examples/second: 25.138776\n",
      "I0710 11:48:21.298823 140295626643200 trainer.py:508] step:  7386, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:87.368683 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.315298 log_pplx:2.926713 loss:119.73915 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.29193\n",
      "I0710 11:48:25.112776 140295626643200 summary_utils.py:349] Steps/second: 0.177305, Examples/second: 25.142764\n",
      "I0710 11:48:25.113613 140295626643200 trainer.py:508] step:  7387, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:40.298588 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3167286 log_pplx:3.3282702 loss:83.081947 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.29724\n",
      "I0710 11:48:31.285509 140295626643200 summary_utils.py:349] Steps/second: 0.177299, Examples/second: 25.138087\n",
      "I0710 11:48:31.286579 140295626643200 trainer.py:508] step:  7388, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:67.226128 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3150179 log_pplx:2.9619799 loss:118.22005 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.30273\n",
      "I0710 11:48:37.567422 140295626643200 summary_utils.py:349] Steps/second: 0.177292, Examples/second: 25.133244\n",
      "I0710 11:48:37.568283 140295626643200 trainer.py:508] step:  7389, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:66.546227 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3150156 log_pplx:2.9384358 loss:118.23531 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.30841\n",
      "I0710 11:48:40.897570 140295626643200 summary_utils.py:349] Steps/second: 0.177318, Examples/second: 25.143953\n",
      "I0710 11:48:40.898424 140295626643200 trainer.py:508] step:  7390, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:21.458328 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3163245 log_pplx:3.3111532 loss:52.047192 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.31403\n",
      "I0710 11:48:45.009334 140295626643200 summary_utils.py:349] Steps/second: 0.177335, Examples/second: 25.147471\n",
      "I0710 11:48:45.010371 140295626643200 trainer.py:508] step:  7391, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:45.669853 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3171078 log_pplx:3.4632795 loss:88.313637 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.31921\n",
      "I0710 11:48:51.349681 140295626643200 summary_utils.py:349] Steps/second: 0.177327, Examples/second: 25.142536\n",
      "I0710 11:48:51.350488 140295626643200 trainer.py:508] step:  7392, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:79.460083 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3154101 log_pplx:2.967608 loss:126.16044 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.32416\n",
      "I0710 11:49:02.356549 140295626643200 summary_utils.py:349] Steps/second: 0.177268, Examples/second: 25.127842\n",
      "I0710 11:49:02.357442 140295626643200 trainer.py:508] step:  7393, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:128.15857 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3139522 log_pplx:2.3688409 loss:169.19449 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.32898\n",
      "I0710 11:49:08.833544 140295626643200 summary_utils.py:349] Steps/second: 0.177259, Examples/second: 25.122708\n",
      "I0710 11:49:08.834339 140295626643200 trainer.py:508] step:  7394, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:67.196411 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.315449 log_pplx:2.8822937 loss:119.47107 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.33374\n",
      "I0710 11:49:12.713770 140295626643200 summary_utils.py:349] Steps/second: 0.177278, Examples/second: 25.126586\n",
      "I0710 11:49:12.714622 140295626643200 trainer.py:508] step:  7395, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:40.214111 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3167232 log_pplx:3.3222842 loss:85.050476 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.33881\n",
      "I0710 11:49:15.785940 140295626643200 summary_utils.py:349] Steps/second: 0.177306, Examples/second: 25.137675\n",
      "I0710 11:49:15.786722 140295626643200 trainer.py:508] step:  7396, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:22.287725 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3163162 log_pplx:3.3439765 loss:51.779388 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.34351\n",
      "I0710 11:49:22.208894 140295626643200 summary_utils.py:349] Steps/second: 0.177298, Examples/second: 25.132625\n",
      "I0710 11:49:22.209673 140295626643200 trainer.py:508] step:  7397, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:70.856148 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3154095 log_pplx:2.9228971 loss:117.02549 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.3479\n",
      "I0710 11:49:24.426394 140295626643200 summary_utils.py:349] Steps/second: 0.177335, Examples/second: 25.160906\n",
      "I0710 11:49:24.427163 140295626643200 trainer.py:508] step:  7398, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:10.654543 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3150069 log_pplx:3.4241266 loss:25.119181 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:530.35266\n",
      "I0710 11:49:30.819453 140295626643200 summary_utils.py:349] Steps/second: 0.177327, Examples/second: 25.155896\n",
      "I0710 11:49:30.820238 140295626643200 trainer.py:508] step:  7399, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:74.336266 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3149743 log_pplx:2.8839278 loss:115.21291 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.35712\n",
      "I0710 11:49:34.729992 140295626643200 summary_utils.py:349] Steps/second: 0.177346, Examples/second: 25.159714\n",
      "I0710 11:49:34.730915 140295626643200 trainer.py:508] step:  7400, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:37.755543 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3169082 log_pplx:3.3243001 loss:83.66848 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.36218\n",
      "I0710 11:49:40.983154 140295626643200 summary_utils.py:349] Steps/second: 0.177339, Examples/second: 25.154926\n",
      "I0710 11:49:40.983919 140295626643200 trainer.py:508] step:  7401, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:66.767296 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3150154 log_pplx:2.9491849 loss:116.1979 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.36694\n",
      "I0710 11:49:44.015492 140295626643200 summary_utils.py:349] Steps/second: 0.177368, Examples/second: 25.166052\n",
      "I0710 11:49:44.016281 140295626643200 trainer.py:508] step:  7402, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:21.908327 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3170087 log_pplx:3.383961 loss:53.125546 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.37213\n",
      "I0710 11:49:50.565210 140295626643200 summary_utils.py:349] Steps/second: 0.177358, Examples/second: 25.160802\n",
      "I0710 11:49:50.566025 140295626643200 trainer.py:508] step:  7403, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:84.050423 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3153429 log_pplx:2.9480419 loss:121.49618 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.37714\n",
      "I0710 11:49:54.429658 140295626643200 summary_utils.py:349] Steps/second: 0.177377, Examples/second: 25.164686\n",
      "I0710 11:49:54.430594 140295626643200 trainer.py:508] step:  7404, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:39.848793 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3175666 log_pplx:3.3837459 loss:85.270401 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.38177\n",
      "I0710 11:50:00.731453 140295626643200 summary_utils.py:349] Steps/second: 0.177370, Examples/second: 25.159826\n",
      "I0710 11:50:00.732236 140295626643200 trainer.py:508] step:  7405, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:72.519325 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3157266 log_pplx:2.9419971 loss:123.74776 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.38623\n",
      "I0710 11:50:10.670521 140295626643200 summary_utils.py:349] Steps/second: 0.177323, Examples/second: 25.146842\n",
      "I0710 11:50:10.671425 140295626643200 trainer.py:508] step:  7406, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:160.47141 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.314353 log_pplx:2.3753896 loss:168.35573 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.39111\n",
      "I0710 11:50:16.968662 140295626643200 summary_utils.py:349] Steps/second: 0.177316, Examples/second: 25.142000\n",
      "I0710 11:50:16.969497 140295626643200 trainer.py:508] step:  7407, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:74.611893 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3156598 log_pplx:2.8909271 loss:117.55232 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.39581\n",
      "I0710 11:50:20.082175 140295626643200 summary_utils.py:349] Steps/second: 0.177343, Examples/second: 25.152978\n",
      "I0710 11:50:20.083052 140295626643200 trainer.py:508] step:  7408, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:36.224087 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3169625 log_pplx:3.3905582 loss:53.017204 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.40094\n",
      "I0710 11:50:23.967249 140295626643200 summary_utils.py:349] Steps/second: 0.177363, Examples/second: 25.156824\n",
      "I0710 11:50:23.968082 140295626643200 trainer.py:508] step:  7409, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:42.297039 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3176771 log_pplx:3.380578 loss:84.64122 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.40619\n",
      "I0710 11:50:30.342180 140295626643200 summary_utils.py:349] Steps/second: 0.177354, Examples/second: 25.151862\n",
      "I0710 11:50:30.343045 140295626643200 trainer.py:508] step:  7410, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:76.97039 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3155717 log_pplx:2.9157104 loss:120.12728 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.4115\n",
      "I0710 11:50:36.916722 140295626643200 summary_utils.py:349] Steps/second: 0.177344, Examples/second: 25.146595\n",
      "I0710 11:50:36.917690 140295626643200 trainer.py:508] step:  7411, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:73.918419 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3153619 log_pplx:2.9597588 loss:117.24345 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.41675\n",
      "I0710 11:50:43.053149 140295626643200 summary_utils.py:349] Steps/second: 0.177339, Examples/second: 25.142011\n",
      "I0710 11:50:43.053973 140295626643200 trainer.py:508] step:  7412, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:64.237572 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3150495 log_pplx:2.8542814 loss:110.71045 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.42157\n",
      "I0710 11:50:46.140443 140295626643200 summary_utils.py:349] Steps/second: 0.177367, Examples/second: 25.153012\n",
      "I0710 11:50:46.141282 140295626643200 trainer.py:508] step:  7413, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:40.542675 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3170767 log_pplx:3.4301531 loss:53.421951 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.42645\n",
      "I0710 11:50:50.022032 140295626643200 summary_utils.py:349] Steps/second: 0.177386, Examples/second: 25.156858\n",
      "I0710 11:50:50.022846 140295626643200 trainer.py:508] step:  7414, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:47.340126 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3171974 log_pplx:3.3206789 loss:83.162254 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.43079\n",
      "I0710 11:50:56.449152 140295626643200 summary_utils.py:349] Steps/second: 0.177377, Examples/second: 25.151823\n",
      "I0710 11:50:56.449991 140295626643200 trainer.py:508] step:  7415, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:80.300064 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3157134 log_pplx:2.9702427 loss:119.55227 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.43512\n",
      "I0710 11:51:06.153063 140295626643200 summary_utils.py:349] Steps/second: 0.177333, Examples/second: 25.139253\n",
      "I0710 11:51:06.153866 140295626643200 trainer.py:508] step:  7416, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:142.14127 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3145952 log_pplx:2.3958738 loss:168.31015 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.44025\n",
      "I0710 11:51:12.639795 140295626643200 summary_utils.py:349] Steps/second: 0.177324, Examples/second: 25.134138\n",
      "I0710 11:51:12.640562 140295626643200 trainer.py:508] step:  7417, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:70.00457 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.315991 log_pplx:2.9351633 loss:121.29562 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.44604\n",
      "I0710 11:51:16.458034 140295626643200 summary_utils.py:349] Steps/second: 0.177343, Examples/second: 25.138080\n",
      "I0710 11:51:16.458813 140295626643200 trainer.py:508] step:  7418, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:41.201317 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3174255 log_pplx:3.3120925 loss:82.678116 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.45227\n",
      "I0710 11:51:22.828869 140295626643200 summary_utils.py:349] Steps/second: 0.177335, Examples/second: 25.133148\n",
      "I0710 11:51:22.829638 140295626643200 trainer.py:508] step:  7419, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:75.516136 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3160146 log_pplx:2.8878243 loss:123.74327 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.45795\n",
      "I0710 11:51:25.956665 140295626643200 summary_utils.py:349] Steps/second: 0.177363, Examples/second: 25.144061\n",
      "I0710 11:51:25.957432 140295626643200 trainer.py:508] step:  7420, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:21.201233 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3170323 log_pplx:3.3141019 loss:50.954315 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.4635\n",
      "I0710 11:51:28.213497 140295626643200 summary_utils.py:349] Steps/second: 0.177400, Examples/second: 25.172065\n",
      "I0710 11:51:28.214344 140295626643200 trainer.py:508] step:  7421, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:10.258301 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3158411 log_pplx:3.4451137 loss:25.602848 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:530.46875\n",
      "I0710 11:51:34.805004 140295626643200 summary_utils.py:349] Steps/second: 0.177389, Examples/second: 25.166782\n",
      "I0710 11:51:34.805800 140295626643200 trainer.py:508] step:  7422, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:88.782562 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3160424 log_pplx:2.9449196 loss:119.41648 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.47375\n",
      "I0710 11:51:38.682400 140295626643200 summary_utils.py:349] Steps/second: 0.177409, Examples/second: 25.170619\n",
      "I0710 11:51:38.683155 140295626643200 trainer.py:508] step:  7423, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:51.145023 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3176124 log_pplx:3.3341942 loss:83.417374 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.47797\n",
      "I0710 11:51:45.038225 140295626643200 summary_utils.py:349] Steps/second: 0.177401, Examples/second: 25.165704\n",
      "I0710 11:51:45.038998 140295626643200 trainer.py:508] step:  7424, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:71.280457 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3161283 log_pplx:2.9787786 loss:119.70967 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.48242\n",
      "I0710 11:51:48.116685 140295626643200 summary_utils.py:349] Steps/second: 0.177429, Examples/second: 25.176672\n",
      "I0710 11:51:48.117505 140295626643200 trainer.py:508] step:  7425, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:28.637487 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3171196 log_pplx:3.3368728 loss:51.304424 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.48694\n",
      "I0710 11:51:54.543028 140295626643200 summary_utils.py:349] Steps/second: 0.177420, Examples/second: 25.171649\n",
      "I0710 11:51:54.543802 140295626643200 trainer.py:508] step:  7426, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:72.441162 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3157113 log_pplx:2.91044 loss:117.21797 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.49146\n",
      "I0710 11:51:58.404798 140295626643200 summary_utils.py:349] Steps/second: 0.177439, Examples/second: 25.175504\n",
      "I0710 11:51:58.405583 140295626643200 trainer.py:508] step:  7427, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:37.509979 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3176903 log_pplx:3.3552377 loss:83.838997 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.4964\n",
      "I0710 11:52:04.917740 140295626643200 summary_utils.py:349] Steps/second: 0.177430, Examples/second: 25.170351\n",
      "I0710 11:52:04.918548 140295626643200 trainer.py:508] step:  7428, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:75.584518 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.316422 log_pplx:2.9888785 loss:124.26262 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.50134\n",
      "I0710 11:52:15.768474 140295626643200 summary_utils.py:349] Steps/second: 0.177373, Examples/second: 25.156053\n",
      "I0710 11:52:15.769324 140295626643200 trainer.py:508] step:  7429, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:142.13742 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3157867 log_pplx:2.3894644 loss:188.94687 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.50653\n",
      "I0710 11:52:22.204177 140295626643200 summary_utils.py:349] Steps/second: 0.177364, Examples/second: 25.151032\n",
      "I0710 11:52:22.205068 140295626643200 trainer.py:508] step:  7430, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:80.4907 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3160293 log_pplx:3.0085046 loss:119.13678 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.51184\n",
      "I0710 11:52:26.064306 140295626643200 summary_utils.py:349] Steps/second: 0.177384, Examples/second: 25.154889\n",
      "I0710 11:52:26.065085 140295626643200 trainer.py:508] step:  7431, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:54.203705 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3177741 log_pplx:3.3473957 loss:82.868958 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.51715\n",
      "I0710 11:52:29.065036 140295626643200 summary_utils.py:349] Steps/second: 0.177412, Examples/second: 25.165951\n",
      "I0710 11:52:29.065822 140295626643200 trainer.py:508] step:  7432, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:25.385448 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3174223 log_pplx:3.4177592 loss:53.829708 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.52209\n",
      "I0710 11:52:35.604535 140295626643200 summary_utils.py:349] Steps/second: 0.177402, Examples/second: 25.160770\n",
      "I0710 11:52:35.605390 140295626643200 trainer.py:508] step:  7433, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:94.899048 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3159478 log_pplx:2.9474888 loss:116.72056 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.52661\n",
      "I0710 11:52:41.704641 140295626643200 summary_utils.py:349] Steps/second: 0.177397, Examples/second: 25.156271\n",
      "I0710 11:52:41.705710 140295626643200 trainer.py:508] step:  7434, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:106.12236 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3160759 log_pplx:3.0008583 loss:120.10935 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.53204\n",
      "I0710 11:52:45.476966 140295626643200 summary_utils.py:349] Steps/second: 0.177418, Examples/second: 25.160256\n",
      "I0710 11:52:45.477761 140295626643200 trainer.py:508] step:  7435, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:36.588245 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3178939 log_pplx:3.3702087 loss:83.791809 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.53839\n",
      "I0710 11:52:52.091338 140295626643200 summary_utils.py:349] Steps/second: 0.177407, Examples/second: 25.154968\n",
      "I0710 11:52:52.092109 140295626643200 trainer.py:508] step:  7436, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:70.948944 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3168787 log_pplx:2.9121916 loss:125.58828 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.54449\n",
      "I0710 11:52:55.197710 140295626643200 summary_utils.py:349] Steps/second: 0.177435, Examples/second: 25.165850\n",
      "I0710 11:52:55.198506 140295626643200 base_runner.py:111] step:  7437, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:39.98003 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.317493 log_pplx:3.3359482 loss:52.593315 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.55042\n",
      "I0710 11:53:05.249665 140295626643200 summary_utils.py:349] Steps/second: 0.177387, Examples/second: 25.152828\n",
      "I0710 11:53:05.250500 140295626643200 trainer.py:508] step:  7438, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:138.56789 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3149862 log_pplx:2.3231204 loss:163.77998 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.5556\n",
      "I0710 11:53:11.500432 140295626643200 summary_utils.py:349] Steps/second: 0.177380, Examples/second: 25.148108\n",
      "I0710 11:53:11.501249 140295626643200 trainer.py:508] step:  7439, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:108.156 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3161184 log_pplx:2.9883902 loss:118.56439 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.56073\n",
      "I0710 11:53:15.389144 140295626643200 summary_utils.py:349] Steps/second: 0.177399, Examples/second: 25.151909\n",
      "I0710 11:53:15.389973 140295626643200 trainer.py:508] step:  7440, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:46.268597 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.317999 log_pplx:3.3764892 loss:85.48848 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.565\n",
      "I0710 11:53:21.611667 140295626643200 summary_utils.py:349] Steps/second: 0.177393, Examples/second: 25.147235\n",
      "I0710 11:53:21.612442 140295626643200 trainer.py:508] step:  7441, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:68.459381 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3163043 log_pplx:2.9736791 loss:117.86919 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.56891\n",
      "I0710 11:53:28.017671 140295626643200 summary_utils.py:349] Steps/second: 0.177384, Examples/second: 25.142284\n",
      "I0710 11:53:28.018499 140295626643200 trainer.py:508] step:  7442, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:79.793686 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3164666 log_pplx:2.9373953 loss:123.11359 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.57318\n",
      "I0710 11:53:31.964122 140295626643200 summary_utils.py:349] Steps/second: 0.177403, Examples/second: 25.145994\n",
      "I0710 11:53:31.964892 140295626643200 trainer.py:508] step:  7443, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:64.800392 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3178309 log_pplx:3.3624163 loss:84.354614 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.578\n",
      "I0710 11:53:34.176273 140295626643200 summary_utils.py:349] Steps/second: 0.177440, Examples/second: 25.173850\n",
      "I0710 11:53:34.177057 140295626643200 trainer.py:508] step:  7444, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:13.413559 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3162265 log_pplx:3.4566298 loss:25.269855 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:530.5835\n",
      "I0710 11:53:37.247860 140295626643200 summary_utils.py:349] Steps/second: 0.177467, Examples/second: 25.184755\n",
      "I0710 11:53:37.248631 140295626643200 trainer.py:508] step:  7445, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:32.758671 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.317505 log_pplx:3.3181634 loss:51.807423 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.58868\n",
      "I0710 11:53:43.890044 140295626643200 summary_utils.py:349] Steps/second: 0.177457, Examples/second: 25.179430\n",
      "I0710 11:53:43.890912 140295626643200 trainer.py:508] step:  7446, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:69.701248 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3165325 log_pplx:2.8899081 loss:114.69322 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.59412\n",
      "I0710 11:53:44.232156 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 11:53:47.939913 140295626643200 summary_utils.py:349] Steps/second: 0.177474, Examples/second: 25.182969\n",
      "I0710 11:53:47.940751 140295626643200 trainer.py:508] step:  7447, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:46.517956 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3181689 log_pplx:3.2924798 loss:83.382042 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.59973\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 11:53:49.141066 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 11:53:49.627334 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00007446\n",
      "I0710 11:53:54.619929 140295626643200 summary_utils.py:349] Steps/second: 0.177462, Examples/second: 25.177591\n",
      "I0710 11:53:54.620798 140295626643200 trainer.py:508] step:  7448, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:67.771263 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3162868 log_pplx:2.9609365 loss:116.66091 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.60492\n",
      "I0710 11:54:05.062246 140295626643200 summary_utils.py:349] Steps/second: 0.177410, Examples/second: 25.164010\n",
      "I0710 11:54:05.063051 140295626643200 trainer.py:508] step:  7449, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:171.07347 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3157985 log_pplx:2.4184451 loss:179.08585 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.61005\n",
      "I0710 11:54:11.180049 140295626643200 summary_utils.py:349] Steps/second: 0.177405, Examples/second: 25.159507\n",
      "I0710 11:54:11.181092 140295626643200 trainer.py:508] step:  7450, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:93.423615 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.316905 log_pplx:2.8888328 loss:115.26443 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.61652\n",
      "I0710 11:54:14.257021 140295635035904 trainer.py:345] Write summary @7451\n",
      "I0710 11:54:14.257295 140295626643200 summary_utils.py:349] Steps/second: 0.177433, Examples/second: 25.170380\n",
      "I0710 11:54:14.258910 140295626643200 trainer.py:508] step:  7451, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:32.783127 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3180131 log_pplx:3.3554182 loss:52.913372 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.62231\n",
      "I0710 11:54:22.150875 140295626643200 summary_utils.py:349] Steps/second: 0.177408, Examples/second: 25.168026\n",
      "I0710 11:54:22.152264 140295626643200 trainer.py:508] step:  7452, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:47.803303 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3185881 log_pplx:3.4809377 loss:87.523827 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.62762\n",
      "I0710 11:54:32.122115 140295626643200 summary_utils.py:349] Steps/second: 0.177362, Examples/second: 25.157628\n",
      "I0710 11:54:32.123270 140295626643200 trainer.py:508] step:  7453, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:91.969543 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3167393 log_pplx:3.0409555 loss:123.53882 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.63257\n",
      "I0710 11:54:41.365651 140295626643200 summary_utils.py:349] Steps/second: 0.177323, Examples/second: 25.148354\n",
      "I0710 11:54:41.367108 140295626643200 trainer.py:508] step:  7454, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:85.71743 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3168344 log_pplx:3.075671 loss:126.29475 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.63763\n",
      "I0710 11:54:52.152930 140295626643200 summary_utils.py:349] Steps/second: 0.177267, Examples/second: 25.136734\n",
      "I0710 11:54:52.154130 140295626643200 trainer.py:508] step:  7455, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:152.51155 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3166517 log_pplx:2.9702642 loss:119.10759 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.64301\n",
      "I0710 11:54:58.119695 140295626643200 summary_utils.py:349] Steps/second: 0.177264, Examples/second: 25.137342\n",
      "I0710 11:54:58.120891 140295626643200 trainer.py:508] step:  7456, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:44.445343 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3186332 log_pplx:3.3430517 loss:83.847916 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.64832\n",
      "I0710 11:55:01.542357 140295635035904 trainer.py:354] Write summary done: step 7451\n",
      "I0710 11:55:02.835027 140295626643200 summary_utils.py:349] Steps/second: 0.177274, Examples/second: 25.145688\n",
      "I0710 11:55:02.835770 140295626643200 trainer.py:508] step:  7457, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:32.677311 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3184847 log_pplx:3.4042628 loss:53.723522 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.65381\n",
      "I0710 11:55:09.098633 140295626643200 summary_utils.py:349] Steps/second: 0.177267, Examples/second: 25.140985\n",
      "I0710 11:55:09.099440 140295626643200 trainer.py:508] step:  7458, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:93.231354 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3168942 log_pplx:3.0352213 loss:123.00236 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.659\n",
      "I0710 11:55:15.189060 140295626643200 summary_utils.py:349] Steps/second: 0.177262, Examples/second: 25.136549\n",
      "I0710 11:55:15.189914 140295626643200 trainer.py:508] step:  7459, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:81.506737 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3164159 log_pplx:2.9575257 loss:115.63926 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.66418\n",
      "I0710 11:55:19.070607 140295626643200 summary_utils.py:349] Steps/second: 0.177281, Examples/second: 25.140335\n",
      "I0710 11:55:19.071447 140295626643200 trainer.py:508] step:  7460, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:40.032402 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3188572 log_pplx:3.3863101 loss:85.610146 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.66974\n",
      "I0710 11:55:29.796493 140295626643200 summary_utils.py:349] Steps/second: 0.177227, Examples/second: 25.126416\n",
      "I0710 11:55:29.797298 140295626643200 trainer.py:508] step:  7461, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:159.43951 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3156358 log_pplx:2.3742669 loss:168.39487 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.67499\n",
      "I0710 11:55:32.856158 140295626643200 summary_utils.py:349] Steps/second: 0.177254, Examples/second: 25.137271\n",
      "I0710 11:55:32.856972 140295626643200 trainer.py:508] step:  7462, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:25.925663 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3183645 log_pplx:3.4159865 loss:54.215443 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.67993\n",
      "I0710 11:55:39.103324 140295626643200 summary_utils.py:349] Steps/second: 0.177248, Examples/second: 25.132604\n",
      "I0710 11:55:39.104121 140295626643200 trainer.py:508] step:  7463, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:70.617783 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3168137 log_pplx:2.9447727 loss:117.93816 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.68481\n",
      "I0710 11:55:45.554953 140295626643200 summary_utils.py:349] Steps/second: 0.177239, Examples/second: 25.127630\n",
      "I0710 11:55:45.555765 140295626643200 trainer.py:508] step:  7464, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:69.750977 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3170711 log_pplx:2.9231098 loss:117.36285 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.69\n",
      "I0710 11:55:49.350208 140295626643200 summary_utils.py:349] Steps/second: 0.177259, Examples/second: 25.131543\n",
      "I0710 11:55:49.350977 140295626643200 trainer.py:508] step:  7465, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:43.86599 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3188316 log_pplx:3.296021 loss:83.080322 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.69556\n",
      "I0710 11:55:55.742818 140295626643200 summary_utils.py:349] Steps/second: 0.177251, Examples/second: 25.126662\n",
      "I0710 11:55:55.743610 140295626643200 trainer.py:508] step:  7466, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:76.030663 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3169475 log_pplx:2.9226837 loss:117.49188 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.70062\n",
      "I0710 11:55:57.944302 140295626643200 summary_utils.py:349] Steps/second: 0.177288, Examples/second: 25.154295\n",
      "I0710 11:55:57.945116 140295626643200 trainer.py:508] step:  7467, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:13.991765 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3170813 log_pplx:3.4629543 loss:25.688087 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:530.70599\n",
      "I0710 11:56:04.191194 140295626643200 summary_utils.py:349] Steps/second: 0.177281, Examples/second: 25.149629\n",
      "I0710 11:56:04.192001 140295626643200 trainer.py:508] step:  7468, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:77.639992 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.31678 log_pplx:3.0158904 loss:118.93916 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.711\n",
      "I0710 11:56:07.252328 140295626643200 summary_utils.py:349] Steps/second: 0.177309, Examples/second: 25.160455\n",
      "I0710 11:56:07.253139 140295626643200 trainer.py:508] step:  7469, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:25.553463 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3181899 log_pplx:3.3892889 loss:51.819046 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.716\n",
      "I0710 11:56:11.177071 140295626643200 summary_utils.py:349] Steps/second: 0.177327, Examples/second: 25.164158\n",
      "I0710 11:56:11.178028 140295626643200 trainer.py:508] step:  7470, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:39.504433 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3188944 log_pplx:3.3498511 loss:84.039398 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.72095\n",
      "I0710 11:56:17.510358 140295626643200 summary_utils.py:349] Steps/second: 0.177320, Examples/second: 25.159361\n",
      "I0710 11:56:17.511190 140295626643200 trainer.py:508] step:  7471, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:72.590195 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3171284 log_pplx:3.0141749 loss:122.86531 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.72565\n",
      "I0710 11:56:27.144808 140295626643200 summary_utils.py:349] Steps/second: 0.177277, Examples/second: 25.147136\n",
      "I0710 11:56:27.145568 140295626643200 trainer.py:508] step:  7472, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:176.32503 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3164092 log_pplx:2.4051437 loss:179.12308 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.73047\n",
      "I0710 11:56:33.402146 140295626643200 summary_utils.py:349] Steps/second: 0.177271, Examples/second: 25.142466\n",
      "I0710 11:56:33.403000 140295626643200 trainer.py:508] step:  7473, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:66.313911 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3169651 log_pplx:2.9142339 loss:116.64223 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.73584\n",
      "I0710 11:56:37.291427 140295626643200 summary_utils.py:349] Steps/second: 0.177289, Examples/second: 25.146221\n",
      "I0710 11:56:37.292195 140295626643200 trainer.py:508] step:  7474, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:39.410381 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3189943 log_pplx:3.3613784 loss:86.387413 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.74097\n",
      "I0710 11:56:43.763217 140295626643200 summary_utils.py:349] Steps/second: 0.177280, Examples/second: 25.141228\n",
      "I0710 11:56:43.764277 140295626643200 trainer.py:508] step:  7475, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:72.461174 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3170849 log_pplx:3.038327 loss:121.19127 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.74597\n",
      "I0710 11:56:46.864481 140295626643200 summary_utils.py:349] Steps/second: 0.177308, Examples/second: 25.151969\n",
      "I0710 11:56:46.865245 140295626643200 trainer.py:508] step:  7476, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:21.290596 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3187501 log_pplx:3.3625493 loss:52.881336 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.75098\n",
      "I0710 11:56:53.251151 140295626643200 summary_utils.py:349] Steps/second: 0.177300, Examples/second: 25.147106\n",
      "I0710 11:56:53.251943 140295626643200 trainer.py:508] step:  7477, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:70.002007 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3173115 log_pplx:2.8643572 loss:114.61009 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.7558\n",
      "I0710 11:56:59.287112 140295626643200 summary_utils.py:349] Steps/second: 0.177295, Examples/second: 25.142778\n",
      "I0710 11:56:59.287904 140295626643200 trainer.py:508] step:  7478, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:70.483337 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3174552 log_pplx:2.9498694 loss:118.43726 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.7605\n",
      "I0710 11:57:03.227267 140295626643200 summary_utils.py:349] Steps/second: 0.177313, Examples/second: 25.146451\n",
      "I0710 11:57:03.228080 140295626643200 trainer.py:508] step:  7479, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:39.67329 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3192346 log_pplx:3.3588476 loss:83.84523 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.76532\n",
      "I0710 11:57:13.318104 140295626643200 summary_utils.py:349] Steps/second: 0.177266, Examples/second: 25.133576\n",
      "I0710 11:57:13.318872 140295626643200 trainer.py:508] step:  7480, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:191.03563 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.316678 log_pplx:2.5129628 loss:185.14255 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.77039\n",
      "I0710 11:57:16.403037 140295626643200 summary_utils.py:349] Steps/second: 0.177293, Examples/second: 25.144323\n",
      "I0710 11:57:16.403826 140295626643200 trainer.py:508] step:  7481, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:23.339409 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3188082 log_pplx:3.3876395 loss:53.196526 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.77631\n",
      "I0710 11:57:22.619123 140295626643200 summary_utils.py:349] Steps/second: 0.177287, Examples/second: 25.139730\n",
      "I0710 11:57:22.619926 140295626643200 trainer.py:508] step:  7482, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:75.747101 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.317746 log_pplx:2.9840996 loss:122.27348 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.7821\n",
      "I0710 11:57:28.983492 140295626643200 summary_utils.py:349] Steps/second: 0.177279, Examples/second: 25.134916\n",
      "I0710 11:57:28.984272 140295626643200 trainer.py:508] step:  7483, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:74.02005 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3173833 log_pplx:3.0464935 loss:123.00217 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.78833\n",
      "I0710 11:57:32.801049 140295626643200 summary_utils.py:349] Steps/second: 0.177299, Examples/second: 25.138769\n",
      "I0710 11:57:32.802050 140295626643200 trainer.py:508] step:  7484, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:43.885384 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3191485 log_pplx:3.4074957 loss:86.230942 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.79462\n",
      "I0710 11:57:39.216935 140295626643200 summary_utils.py:349] Steps/second: 0.177291, Examples/second: 25.133881\n",
      "I0710 11:57:39.217688 140295626643200 trainer.py:508] step:  7485, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:74.075775 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3175825 log_pplx:3.0047545 loss:120.71603 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.80066\n",
      "I0710 11:57:42.291418 140295626643200 summary_utils.py:349] Steps/second: 0.177318, Examples/second: 25.144627\n",
      "I0710 11:57:42.292200 140295626643200 trainer.py:508] step:  7486, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:22.732487 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3192301 log_pplx:3.4253197 loss:54.082588 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.80682\n",
      "I0710 11:57:48.138765 140295626643200 summary_utils.py:349] Steps/second: 0.177316, Examples/second: 25.140598\n",
      "I0710 11:57:48.139521 140295626643200 trainer.py:508] step:  7487, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:64.99614 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3173569 log_pplx:2.9061859 loss:112.869 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.81262\n",
      "I0710 11:57:51.977143 140295626643200 summary_utils.py:349] Steps/second: 0.177335, Examples/second: 25.144414\n",
      "I0710 11:57:51.977921 140295626643200 trainer.py:508] step:  7488, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:37.455769 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3196613 log_pplx:3.3555114 loss:85.376793 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.81836\n",
      "I0710 11:57:58.001624 140295626643200 summary_utils.py:349] Steps/second: 0.177331, Examples/second: 25.140120\n",
      "I0710 11:57:58.002614 140295626643200 trainer.py:508] step:  7489, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:68.157837 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3175499 log_pplx:2.9255702 loss:115.41374 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.82404\n",
      "I0710 11:58:00.196646 140295626643200 summary_utils.py:349] Steps/second: 0.177367, Examples/second: 25.167558\n",
      "I0710 11:58:00.197451 140295626643200 trainer.py:508] step:  7490, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:10.454305 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3175714 log_pplx:3.4805532 loss:25.383564 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:530.82947\n",
      "I0710 11:58:09.163703 140295626643200 summary_utils.py:349] Steps/second: 0.177332, Examples/second: 25.156412\n",
      "I0710 11:58:09.164529 140295626643200 trainer.py:508] step:  7491, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:162.57808 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3164786 log_pplx:2.3785157 loss:165.96094 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.83453\n",
      "I0710 11:58:15.586047 140295626643200 summary_utils.py:349] Steps/second: 0.177324, Examples/second: 25.151517\n",
      "I0710 11:58:15.586865 140295626643200 trainer.py:508] step:  7492, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:80.869186 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3177923 log_pplx:2.9664223 loss:121.43791 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.83813\n",
      "I0710 11:58:19.408936 140295626643200 summary_utils.py:349] Steps/second: 0.177343, Examples/second: 25.155348\n",
      "I0710 11:58:19.409746 140295626643200 trainer.py:508] step:  7493, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:41.279263 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3195517 log_pplx:3.2722657 loss:81.66349 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.84229\n",
      "I0710 11:58:22.478274 140295626643200 summary_utils.py:349] Steps/second: 0.177370, Examples/second: 25.166072\n",
      "I0710 11:58:22.479110 140295626643200 trainer.py:508] step:  7494, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:22.889706 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3190358 log_pplx:3.3282826 loss:51.757389 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.84656\n",
      "I0710 11:58:28.883059 140295626643200 summary_utils.py:349] Steps/second: 0.177362, Examples/second: 25.161205\n",
      "I0710 11:58:28.883857 140295626643200 trainer.py:508] step:  7495, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:92.93187 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3177383 log_pplx:3.014724 loss:122.66158 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.85071\n",
      "I0710 11:58:32.813490 140295626643200 summary_utils.py:349] Steps/second: 0.177380, Examples/second: 25.164868\n",
      "I0710 11:58:32.814299 140295626643200 trainer.py:508] step:  7496, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:49.729012 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3198383 log_pplx:3.4675217 loss:88.07505 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.85583\n",
      "I0710 11:58:38.970294 140295626643200 summary_utils.py:349] Steps/second: 0.177375, Examples/second: 25.160377\n",
      "I0710 11:58:38.971084 140295626643200 trainer.py:508] step:  7497, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:84.507751 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3177571 log_pplx:2.9672713 loss:116.57668 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.86047\n",
      "I0710 11:58:45.401937 140295626643200 summary_utils.py:349] Steps/second: 0.177366, Examples/second: 25.155476\n",
      "I0710 11:58:45.402925 140295626643200 trainer.py:508] step:  7498, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:70.448563 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3177884 log_pplx:2.9777427 loss:119.92859 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.86511\n",
      "I0710 11:58:48.472827 140295626643200 summary_utils.py:349] Steps/second: 0.177394, Examples/second: 25.166181\n",
      "I0710 11:58:48.473575 140295626643200 trainer.py:508] step:  7499, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:23.086159 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3191252 log_pplx:3.3391221 loss:52.004223 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.87\n",
      "2020-07-10 11:58:51.893390: I lingvo/core/ops/record_yielder.cc:532] Epoch 10: total records 46838\n",
      "2020-07-10 11:58:51.893463: I lingvo/core/ops/record_yielder.cc:485] Epoch 10 /tmp/punctuator_data/train.txt\n",
      "I0710 11:58:55.081094 140295626643200 summary_utils.py:349] Steps/second: 0.177383, Examples/second: 25.161015\n",
      "I0710 11:58:55.081870 140295626643200 trainer.py:508] step:  7500, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:76.904167 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3184103 log_pplx:2.9672542 loss:125.06976 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.87457\n",
      "I0710 11:58:58.886694 140295626643200 summary_utils.py:349] Steps/second: 0.177403, Examples/second: 25.164860\n",
      "I0710 11:58:58.887770 140295626643200 trainer.py:508] step:  7501, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:41.495483 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3197675 log_pplx:3.3756905 loss:84.434456 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.87982\n",
      "I0710 11:59:05.754979 140295626643200 summary_utils.py:349] Steps/second: 0.177390, Examples/second: 25.159306\n",
      "I0710 11:59:05.755769 140295626643200 trainer.py:508] step:  7502, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:75.036972 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3177685 log_pplx:2.973372 loss:119.38088 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.88501\n",
      "I0710 11:59:15.558738 140295626643200 summary_utils.py:349] Steps/second: 0.177346, Examples/second: 25.146949\n",
      "I0710 11:59:15.559506 140295626643200 trainer.py:508] step:  7503, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:162.71527 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3172179 log_pplx:2.3793609 loss:176.07269 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.89056\n",
      "I0710 11:59:21.929806 140295626643200 summary_utils.py:349] Steps/second: 0.177338, Examples/second: 25.142155\n",
      "I0710 11:59:21.930666 140295626643200 trainer.py:508] step:  7504, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:70.635994 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3180612 log_pplx:2.8359795 loss:116.34605 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.8974\n",
      "I0710 11:59:25.005619 140295626643200 summary_utils.py:349] Steps/second: 0.177365, Examples/second: 25.152832\n",
      "I0710 11:59:25.006419 140295626643200 trainer.py:508] step:  7505, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:32.464489 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3189951 log_pplx:3.3321929 loss:50.37339 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.90405\n",
      "I0710 11:59:28.851237 140295626643200 summary_utils.py:349] Steps/second: 0.177384, Examples/second: 25.156612\n",
      "I0710 11:59:28.852003 140295626643200 trainer.py:508] step:  7506, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:56.446514 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3199956 log_pplx:3.3187132 loss:83.071548 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.91016\n",
      "I0710 11:59:35.245923 140295626643200 summary_utils.py:349] Steps/second: 0.177376, Examples/second: 25.151782\n",
      "I0710 11:59:35.246704 140295626643200 trainer.py:508] step:  7507, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:78.367493 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3180592 log_pplx:2.8072209 loss:113.06081 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.91553\n",
      "I0710 11:59:41.557105 140295626643200 summary_utils.py:349] Steps/second: 0.177369, Examples/second: 25.147082\n",
      "I0710 11:59:41.557869 140295626643200 trainer.py:508] step:  7508, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:74.543198 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3183669 log_pplx:2.7922728 loss:114.06435 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.92041\n",
      "I0710 11:59:45.406195 140295626643200 summary_utils.py:349] Steps/second: 0.177388, Examples/second: 25.150854\n",
      "I0710 11:59:45.406998 140295626643200 trainer.py:508] step:  7509, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:59.161636 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3199857 log_pplx:3.2750306 loss:82.469376 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.92542\n",
      "I0710 11:59:51.736344 140295626643200 summary_utils.py:349] Steps/second: 0.177380, Examples/second: 25.146128\n",
      "I0710 11:59:51.737089 140295626643200 trainer.py:508] step:  7510, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:81.440132 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3182281 log_pplx:2.8520303 loss:114.15251 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.93109\n",
      "I0710 11:59:54.777061 140295626643200 summary_utils.py:349] Steps/second: 0.177408, Examples/second: 25.156838\n",
      "I0710 11:59:54.777868 140295626643200 trainer.py:508] step:  7511, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:39.034855 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.319478 log_pplx:3.3190122 loss:51.63916 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.93738\n",
      "I0710 12:00:01.005710 140295626643200 summary_utils.py:349] Steps/second: 0.177402, Examples/second: 25.152265\n",
      "I0710 12:00:01.006582 140295626643200 trainer.py:508] step:  7512, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:82.06942 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3183199 log_pplx:2.8052905 loss:116.98061 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.94379\n",
      "I0710 12:00:09.814845 140295626643200 summary_utils.py:349] Steps/second: 0.177368, Examples/second: 25.141443\n",
      "I0710 12:00:09.815634 140295626643200 trainer.py:508] step:  7513, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:287.34842 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3170077 log_pplx:2.2670176 loss:159.08798 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.94977\n",
      "I0710 12:00:16.190452 140295626643200 summary_utils.py:349] Steps/second: 0.177360, Examples/second: 25.136660\n",
      "I0710 12:00:16.191230 140295626643200 trainer.py:508] step:  7514, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:69.63578 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3183444 log_pplx:2.8580253 loss:115.03552 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.95441\n",
      "I0710 12:00:19.994797 140295626643200 summary_utils.py:349] Steps/second: 0.177380, Examples/second: 25.140494\n",
      "I0710 12:00:19.995598 140295626643200 trainer.py:508] step:  7515, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:38.073071 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3198793 log_pplx:3.2618673 loss:81.281662 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.95966\n",
      "I0710 12:00:22.195498 140295626643200 summary_utils.py:349] Steps/second: 0.177416, Examples/second: 25.167692\n",
      "I0710 12:00:22.196291 140295626643200 trainer.py:508] step:  7516, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:16.445934 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3181332 log_pplx:3.512094 loss:26.10062 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:530.96478\n",
      "I0710 12:00:28.579338 140295626643200 summary_utils.py:349] Steps/second: 0.177408, Examples/second: 25.162889\n",
      "I0710 12:00:28.580209 140295626643200 trainer.py:508] step:  7517, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:99.684464 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3184009 log_pplx:2.7688997 loss:113.31722 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.9696\n",
      "I0710 12:00:31.654140 140295626643200 summary_utils.py:349] Steps/second: 0.177435, Examples/second: 25.173522\n",
      "I0710 12:00:31.654884 140295626643200 trainer.py:508] step:  7518, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:34.91449 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3198218 log_pplx:3.3521571 loss:52.45602 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:530.97552\n",
      "I0710 12:00:38.147175 140295626643200 summary_utils.py:349] Steps/second: 0.177426, Examples/second: 25.168556\n",
      "I0710 12:00:38.147932 140295626643200 trainer.py:508] step:  7519, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:456.91647 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3183279 log_pplx:2.8340123 loss:116.30078 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.9809\n",
      "I0710 12:00:41.915899 140295626643200 summary_utils.py:349] Steps/second: 0.177446, Examples/second: 25.172432\n",
      "I0710 12:00:41.916689 140295626643200 trainer.py:508] step:  7520, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:78.389229 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3199987 log_pplx:3.3105719 loss:82.433243 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:530.98749\n",
      "I0710 12:00:48.226354 140295626643200 summary_utils.py:349] Steps/second: 0.177439, Examples/second: 25.167743\n",
      "I0710 12:00:48.227377 140295626643200 trainer.py:508] step:  7521, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:88.074173 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3181708 log_pplx:2.8189521 loss:113.03998 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:530.99323\n",
      "I0710 12:00:57.216879 140295626643200 summary_utils.py:349] Steps/second: 0.177403, Examples/second: 25.156672\n",
      "I0710 12:00:57.217687 140295626643200 trainer.py:508] step:  7522, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:475.19278 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.317145 log_pplx:2.4246414 loss:177.30191 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:530.99933\n",
      "I0710 12:01:00.284839 140295626643200 summary_utils.py:349] Steps/second: 0.177430, Examples/second: 25.167298\n",
      "I0710 12:01:00.285606 140295626643200 trainer.py:508] step:  7523, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:27.424526 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3198956 log_pplx:3.2913885 loss:51.94223 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.00647\n",
      "I0710 12:01:06.818468 140295626643200 summary_utils.py:349] Steps/second: 0.177421, Examples/second: 25.162283\n",
      "I0710 12:01:06.819330 140295626643200 trainer.py:508] step:  7524, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:460.5954 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3185787 log_pplx:2.9645274 loss:120.80449 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.01337\n",
      "I0710 12:01:10.630825 140295626643200 summary_utils.py:349] Steps/second: 0.177440, Examples/second: 25.166088\n",
      "I0710 12:01:10.631596 140295626643200 trainer.py:508] step:  7525, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:49.558449 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3208772 log_pplx:3.3270168 loss:85.296394 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.01813\n",
      "I0710 12:01:16.650662 140295626643200 summary_utils.py:349] Steps/second: 0.177436, Examples/second: 25.161844\n",
      "I0710 12:01:16.651468 140295626643200 trainer.py:508] step:  7526, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:77.183861 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3185492 log_pplx:2.8082602 loss:109.41683 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.02295\n",
      "I0710 12:01:23.197651 140295626643200 summary_utils.py:349] Steps/second: 0.177426, Examples/second: 25.156815\n",
      "I0710 12:01:23.198550 140295626643200 trainer.py:508] step:  7527, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:104.97076 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3189092 log_pplx:2.8216298 loss:116.74492 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.02795\n",
      "I0710 12:01:26.979088 140295626643200 summary_utils.py:349] Steps/second: 0.177446, Examples/second: 25.160664\n",
      "I0710 12:01:26.979852 140295626643200 trainer.py:508] step:  7528, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:90.417923 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.320449 log_pplx:3.4221368 loss:86.344795 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.03369\n",
      "I0710 12:01:33.209869 140295626643200 summary_utils.py:349] Steps/second: 0.177440, Examples/second: 25.156111\n",
      "I0710 12:01:33.210652 140295626643200 trainer.py:508] step:  7529, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:112.61098 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3187056 log_pplx:2.8547506 loss:117.61571 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.03998\n",
      "I0710 12:01:36.272624 140295626643200 summary_utils.py:349] Steps/second: 0.177467, Examples/second: 25.166722\n",
      "I0710 12:01:36.273483 140295626643200 trainer.py:508] step:  7530, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:42.72163 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3198285 log_pplx:3.3462408 loss:51.278526 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.04688\n",
      "I0710 12:01:42.665321 140295626643200 summary_utils.py:349] Steps/second: 0.177459, Examples/second: 25.161928\n",
      "I0710 12:01:42.666079 140295626643200 trainer.py:508] step:  7531, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:94.225838 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3187953 log_pplx:2.9096971 loss:115.11489 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.05371\n",
      "I0710 12:01:46.468141 140295626643200 summary_utils.py:349] Steps/second: 0.177478, Examples/second: 25.165739\n",
      "I0710 12:01:46.468944 140295626643200 trainer.py:508] step:  7532, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:48.467403 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3205627 log_pplx:3.2563248 loss:81.082497 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.06049\n",
      "I0710 12:01:56.894478 140295626643200 summary_utils.py:349] Steps/second: 0.177428, Examples/second: 25.152569\n",
      "I0710 12:01:56.895230 140295626643200 trainer.py:508] step:  7533, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:593.80121 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3177329 log_pplx:2.5659893 loss:182.89087 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:531.06683\n",
      "I0710 12:02:03.292674 140295626643200 summary_utils.py:349] Steps/second: 0.177420, Examples/second: 25.147778\n",
      "I0710 12:02:03.293438 140295626643200 trainer.py:508] step:  7534, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:87.786705 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3189704 log_pplx:2.8662112 loss:117.47884 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.07239\n",
      "I0710 12:02:06.339798 140295626643200 summary_utils.py:349] Steps/second: 0.177447, Examples/second: 25.158395\n",
      "I0710 12:02:06.340504 140295626643200 trainer.py:508] step:  7535, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:31.419113 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3201623 log_pplx:3.3289642 loss:52.236134 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.07794\n",
      "I0710 12:02:12.502142 140295626643200 summary_utils.py:349] Steps/second: 0.177441, Examples/second: 25.153955\n",
      "I0710 12:02:12.502919 140295626643200 trainer.py:508] step:  7536, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:123.58336 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3186773 log_pplx:2.935205 loss:116.96792 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.08331\n",
      "I0710 12:02:16.260818 140295626643200 summary_utils.py:349] Steps/second: 0.177461, Examples/second: 25.157827\n",
      "I0710 12:02:16.261564 140295626643200 base_runner.py:111] step:  7537, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:64.09201 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3204886 log_pplx:3.312866 loss:82.573189 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.08893\n",
      "I0710 12:02:22.334516 140295626643200 summary_utils.py:349] Steps/second: 0.177456, Examples/second: 25.153522\n",
      "I0710 12:02:22.335339 140295626643200 trainer.py:508] step:  7538, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:209.55936 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3188539 log_pplx:3.0107682 loss:121.22105 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.09454\n",
      "I0710 12:02:24.511728 140295626643200 summary_utils.py:349] Steps/second: 0.177493, Examples/second: 25.180556\n",
      "I0710 12:02:24.512483 140295626643200 trainer.py:508] step:  7539, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:14.372738 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3186384 log_pplx:3.3814096 loss:24.812412 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:531.10132\n",
      "I0710 12:02:30.857387 140295626643200 summary_utils.py:349] Steps/second: 0.177485, Examples/second: 25.175840\n",
      "I0710 12:02:30.858339 140295626643200 trainer.py:508] step:  7540, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:96.457741 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3193376 log_pplx:2.8390069 loss:119.09634 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.10779\n",
      "I0710 12:02:33.919330 140295626643200 summary_utils.py:349] Steps/second: 0.177512, Examples/second: 25.186413\n",
      "I0710 12:02:33.920137 140295626643200 trainer.py:508] step:  7541, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:51.304405 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3201561 log_pplx:3.3543489 loss:51.599319 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.11481\n",
      "I0710 12:02:37.767913 140295626643200 summary_utils.py:349] Steps/second: 0.177531, Examples/second: 25.190139\n",
      "I0710 12:02:37.768806 140295626643200 trainer.py:508] step:  7542, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:73.507309 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3208063 log_pplx:3.4067039 loss:84.614014 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.12122\n",
      "I0710 12:02:44.054719 140295626643200 summary_utils.py:349] Steps/second: 0.177524, Examples/second: 25.185511\n",
      "I0710 12:02:44.055431 140295626643200 trainer.py:508] step:  7543, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:89.398468 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3190504 log_pplx:2.8995278 loss:115.54619 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.12695\n",
      "I0710 12:02:55.178168 140295626643200 summary_utils.py:349] Steps/second: 0.177467, Examples/second: 25.171338\n",
      "I0710 12:02:55.179054 140295626643200 trainer.py:508] step:  7544, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:872.63391 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3186629 log_pplx:2.9890673 loss:224.47896 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:531.13281\n",
      "I0710 12:03:01.329040 140295626643200 summary_utils.py:349] Steps/second: 0.177461, Examples/second: 25.166923\n",
      "I0710 12:03:01.329788 140295626643200 trainer.py:508] step:  7545, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:81.721245 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3188678 log_pplx:2.8667204 loss:115.3855 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.1416\n",
      "I0710 12:03:05.132171 140295626643200 summary_utils.py:349] Steps/second: 0.177480, Examples/second: 25.170715\n",
      "I0710 12:03:05.132953 140295626643200 trainer.py:508] step:  7546, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:70.90313 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3209558 log_pplx:3.4674616 loss:86.903244 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.15039\n",
      "I0710 12:03:11.488540 140295626643200 summary_utils.py:349] Steps/second: 0.177473, Examples/second: 25.165998\n",
      "I0710 12:03:11.489333 140295626643200 trainer.py:508] step:  7547, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:131.13814 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3190455 log_pplx:3.0097411 loss:121.02921 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.15881\n",
      "I0710 12:03:14.562497 140295626643200 summary_utils.py:349] Steps/second: 0.177500, Examples/second: 25.176529\n",
      "I0710 12:03:14.563271 140295626643200 trainer.py:508] step:  7548, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:24.396358 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3201677 log_pplx:3.3162522 loss:51.699856 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.16669\n",
      "I0710 12:03:21.080279 140295626643200 summary_utils.py:349] Steps/second: 0.177490, Examples/second: 25.171574\n",
      "I0710 12:03:21.081072 140295626643200 trainer.py:508] step:  7549, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:162.38242 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3192562 log_pplx:3.0196369 loss:119.08694 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.17432\n",
      "I0710 12:03:24.920496 140295626643200 summary_utils.py:349] Steps/second: 0.177509, Examples/second: 25.175305\n",
      "I0710 12:03:24.921396 140295626643200 trainer.py:508] step:  7550, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:46.323673 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3211381 log_pplx:3.3694029 loss:84.635193 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.1814\n",
      "I0710 12:03:31.389558 140295626643200 summary_utils.py:349] Steps/second: 0.177500, Examples/second: 25.170425\n",
      "I0710 12:03:31.390458 140295626643200 trainer.py:508] step:  7551, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:153.22575 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.319165 log_pplx:3.0393512 loss:121.08014 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.18835\n",
      "I0710 12:03:31.965316 140295635035904 trainer.py:345] Write summary @7551\n",
      "I0710 12:03:42.701772 140295626643200 summary_utils.py:349] Steps/second: 0.177441, Examples/second: 25.158376\n",
      "I0710 12:03:42.703066 140295626643200 trainer.py:508] step:  7552, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:103.77541 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3194958 log_pplx:2.9412367 loss:122.02457 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.19476\n",
      "I0710 12:03:46.973862 140295626643200 summary_utils.py:349] Steps/second: 0.177455, Examples/second: 25.167115\n",
      "I0710 12:03:46.975379 140295626643200 trainer.py:508] step:  7553, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:40.531364 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3209946 log_pplx:3.3300662 loss:52.110329 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.20093\n",
      "I0710 12:03:56.225941 140295626643200 summary_utils.py:349] Steps/second: 0.177418, Examples/second: 25.158127\n",
      "I0710 12:03:56.227074 140295626643200 trainer.py:508] step:  7554, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:109.54733 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3192902 log_pplx:2.8843117 loss:112.70448 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.20697\n",
      "I0710 12:04:01.754920 140295626643200 summary_utils.py:349] Steps/second: 0.177419, Examples/second: 25.159355\n",
      "I0710 12:04:01.756115 140295626643200 trainer.py:508] step:  7555, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:75.654037 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3212521 log_pplx:3.3913252 loss:85.461403 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.21277\n",
      "I0710 12:04:15.809830 140295635035904 trainer.py:354] Write summary done: step 7551\n",
      "I0710 12:04:15.816208 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 12:04:19.220496 140295626643200 summary_utils.py:349] Steps/second: 0.177295, Examples/second: 25.135899\n",
      "I0710 12:04:19.221426 140295626643200 trainer.py:508] step:  7556, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:297.39746 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3184665 log_pplx:2.5856771 loss:184.16487 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:531.21881\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 12:04:21.167790 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 12:04:21.689515 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00007555\n",
      "I0710 12:04:25.852732 140295626643200 summary_utils.py:349] Steps/second: 0.177285, Examples/second: 25.130809\n",
      "I0710 12:04:25.853483 140295626643200 trainer.py:508] step:  7557, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:87.77623 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.319517 log_pplx:2.8295493 loss:115.19802 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.22461\n",
      "I0710 12:04:32.051288 140295626643200 summary_utils.py:349] Steps/second: 0.177279, Examples/second: 25.126362\n",
      "I0710 12:04:32.052334 140295626643200 trainer.py:508] step:  7558, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:100.42354 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3195109 log_pplx:2.9035754 loss:116.68744 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.23059\n",
      "I0710 12:04:35.092655 140295626643200 summary_utils.py:349] Steps/second: 0.177306, Examples/second: 25.136898\n",
      "I0710 12:04:35.093440 140295626643200 trainer.py:508] step:  7559, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:25.071218 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3208905 log_pplx:3.3843577 loss:53.435837 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.23682\n",
      "I0710 12:04:38.969770 140295626643200 summary_utils.py:349] Steps/second: 0.177325, Examples/second: 25.140567\n",
      "I0710 12:04:38.970571 140295626643200 trainer.py:508] step:  7560, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:41.927933 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3214395 log_pplx:3.2983978 loss:81.779648 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.24268\n",
      "I0710 12:04:45.161575 140295626643200 summary_utils.py:349] Steps/second: 0.177319, Examples/second: 25.136130\n",
      "I0710 12:04:45.162366 140295626643200 trainer.py:508] step:  7561, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:87.262421 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3197721 log_pplx:2.9177783 loss:118.3159 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.24847\n",
      "I0710 12:04:51.486788 140295626643200 summary_utils.py:349] Steps/second: 0.177312, Examples/second: 25.131500\n",
      "I0710 12:04:51.487645 140295626643200 trainer.py:508] step:  7562, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:87.505157 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3194131 log_pplx:2.8435829 loss:111.78835 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.25427\n",
      "I0710 12:04:55.445527 140295626643200 summary_utils.py:349] Steps/second: 0.177329, Examples/second: 25.135046\n",
      "I0710 12:04:55.446404 140295626643200 trainer.py:508] step:  7563, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:49.200279 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3216161 log_pplx:3.3097095 loss:82.970284 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.26013\n",
      "I0710 12:04:57.655718 140295626643200 summary_utils.py:349] Steps/second: 0.177365, Examples/second: 25.161791\n",
      "I0710 12:04:57.656522 140295626643200 trainer.py:508] step:  7564, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:14.38368 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3196892 log_pplx:3.3680756 loss:24.964542 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:531.26562\n",
      "I0710 12:05:04.041763 140295626643200 summary_utils.py:349] Steps/second: 0.177357, Examples/second: 25.157065\n",
      "I0710 12:05:04.042734 140295626643200 trainer.py:508] step:  7565, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:75.71067 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3201503 log_pplx:2.8421032 loss:116.56177 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.27081\n",
      "I0710 12:05:07.177981 140295626643200 summary_utils.py:349] Steps/second: 0.177383, Examples/second: 25.167437\n",
      "I0710 12:05:07.178794 140295626643200 trainer.py:508] step:  7566, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:37.162571 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3210833 log_pplx:3.3900433 loss:51.751129 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.27618\n",
      "I0710 12:05:13.706148 140295626643200 summary_utils.py:349] Steps/second: 0.177374, Examples/second: 25.162502\n",
      "I0710 12:05:13.707083 140295626643200 trainer.py:508] step:  7567, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:75.810623 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3199853 log_pplx:2.8919113 loss:113.54366 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.28113\n",
      "I0710 12:05:23.013924 140295626643200 summary_utils.py:349] Steps/second: 0.177336, Examples/second: 25.151140\n",
      "I0710 12:05:23.014735 140295626643200 trainer.py:508] step:  7568, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:209.8701 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3186804 log_pplx:2.4901159 loss:178.35454 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:531.28613\n",
      "I0710 12:05:26.832171 140295626643200 summary_utils.py:349] Steps/second: 0.177355, Examples/second: 25.154881\n",
      "I0710 12:05:26.833039 140295626643200 trainer.py:508] step:  7569, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:46.212284 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3215083 log_pplx:3.2535684 loss:81.766235 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.29181\n",
      "I0710 12:05:33.359793 140295626643200 summary_utils.py:349] Steps/second: 0.177345, Examples/second: 25.149958\n",
      "I0710 12:05:33.360671 140295626643200 trainer.py:508] step:  7570, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:74.754295 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3200731 log_pplx:2.82286 loss:120.11271 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.29712\n",
      "I0710 12:05:39.615722 140295626643200 summary_utils.py:349] Steps/second: 0.177339, Examples/second: 25.145437\n",
      "I0710 12:05:39.616478 140295626643200 trainer.py:508] step:  7571, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:67.313385 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3201782 log_pplx:2.8344712 loss:117.06364 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.30267\n",
      "I0710 12:05:42.730584 140295626643200 summary_utils.py:349] Steps/second: 0.177365, Examples/second: 25.155821\n",
      "I0710 12:05:42.731364 140295626643200 trainer.py:508] step:  7572, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:23.708652 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3216846 log_pplx:3.3466761 loss:52.827801 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.30811\n",
      "I0710 12:05:46.580698 140295626643200 summary_utils.py:349] Steps/second: 0.177384, Examples/second: 25.159510\n",
      "I0710 12:05:46.581468 140295626643200 trainer.py:508] step:  7573, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:38.535023 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3217529 log_pplx:3.2555456 loss:81.327599 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.31335\n",
      "I0710 12:05:53.027346 140295626643200 summary_utils.py:349] Steps/second: 0.177375, Examples/second: 25.154710\n",
      "I0710 12:05:53.028170 140295626643200 trainer.py:508] step:  7574, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:69.982147 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3203735 log_pplx:2.8690386 loss:117.63058 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.31818\n",
      "I0710 12:05:59.514358 140295626643200 summary_utils.py:349] Steps/second: 0.177367, Examples/second: 25.149853\n",
      "I0710 12:05:59.515178 140295626643200 trainer.py:508] step:  7575, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:66.732742 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3203307 log_pplx:2.7674954 loss:114.33215 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.32312\n",
      "I0710 12:06:03.336231 140295626643200 summary_utils.py:349] Steps/second: 0.177385, Examples/second: 25.153582\n",
      "I0710 12:06:03.337004 140295626643200 trainer.py:508] step:  7576, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:35.93079 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3216747 log_pplx:3.212858 loss:80.823448 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.32819\n",
      "I0710 12:06:09.825704 140295626643200 summary_utils.py:349] Steps/second: 0.177377, Examples/second: 25.148726\n",
      "I0710 12:06:09.826515 140295626643200 trainer.py:508] step:  7577, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:65.87149 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.320276 log_pplx:2.7727067 loss:112.77986 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.33307\n",
      "I0710 12:06:12.885362 140295626643200 summary_utils.py:349] Steps/second: 0.177403, Examples/second: 25.159171\n",
      "I0710 12:06:12.886185 140295626643200 trainer.py:508] step:  7578, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:29.751795 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3213967 log_pplx:3.3648636 loss:52.063377 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.33807\n",
      "I0710 12:06:19.367148 140295626643200 summary_utils.py:349] Steps/second: 0.177394, Examples/second: 25.154327\n",
      "I0710 12:06:19.367929 140295626643200 trainer.py:508] step:  7579, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:63.204639 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3202363 log_pplx:2.7181816 loss:111.68327 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.3429\n",
      "I0710 12:06:29.781234 140295626643200 summary_utils.py:349] Steps/second: 0.177345, Examples/second: 25.141391\n",
      "I0710 12:06:29.782051 140295626643200 trainer.py:508] step:  7580, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:150.32899 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3191836 log_pplx:2.3994868 loss:172.70306 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:531.34778\n",
      "I0710 12:06:36.289344 140295626643200 summary_utils.py:349] Steps/second: 0.177336, Examples/second: 25.136519\n",
      "I0710 12:06:36.290156 140295626643200 trainer.py:508] step:  7581, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:66.323303 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3204584 log_pplx:2.8315518 loss:113.26207 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.35217\n",
      "I0710 12:06:40.180272 140295626643200 summary_utils.py:349] Steps/second: 0.177354, Examples/second: 25.140142\n",
      "I0710 12:06:40.181272 140295626643200 trainer.py:508] step:  7582, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:37.439587 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3222872 log_pplx:3.2216322 loss:81.688522 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.35687\n",
      "I0710 12:06:46.857338 140295626643200 summary_utils.py:349] Steps/second: 0.177343, Examples/second: 25.135026\n",
      "I0710 12:06:46.858152 140295626643200 trainer.py:508] step:  7583, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:81.73587 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3202149 log_pplx:2.8077059 loss:115.88805 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.36139\n",
      "I0710 12:06:49.888861 140295626643200 summary_utils.py:349] Steps/second: 0.177370, Examples/second: 25.145493\n",
      "I0710 12:06:49.889645 140295626643200 trainer.py:508] step:  7584, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:21.553835 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3217623 log_pplx:3.3391151 loss:52.199764 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.36603\n",
      "I0710 12:06:56.430141 140295626643200 summary_utils.py:349] Steps/second: 0.177361, Examples/second: 25.140577\n",
      "I0710 12:06:56.431017 140295626643200 trainer.py:508] step:  7585, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:68.516098 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3205514 log_pplx:2.8370016 loss:116.45891 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.37042\n",
      "I0710 12:07:00.291655 140295626643200 summary_utils.py:349] Steps/second: 0.177379, Examples/second: 25.144237\n",
      "I0710 12:07:00.292462 140295626643200 trainer.py:508] step:  7586, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:35.962044 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3221081 log_pplx:3.2005908 loss:81.134972 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.37482\n",
      "I0710 12:07:02.512859 140295626643200 summary_utils.py:349] Steps/second: 0.177415, Examples/second: 25.170771\n",
      "I0710 12:07:02.513659 140295626643200 trainer.py:508] step:  7587, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:11.010002 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3202056 log_pplx:3.4721384 loss:25.817245 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:531.37921\n",
      "I0710 12:07:08.927643 140295626643200 summary_utils.py:349] Steps/second: 0.177407, Examples/second: 25.166034\n",
      "I0710 12:07:08.928413 140295626643200 trainer.py:508] step:  7588, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:66.874573 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3202864 log_pplx:2.7498631 loss:112.02254 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.38342\n",
      "I0710 12:07:12.025691 140295626643200 summary_utils.py:349] Steps/second: 0.177433, Examples/second: 25.176384\n",
      "I0710 12:07:12.026467 140295626643200 trainer.py:508] step:  7589, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:22.04875 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3215286 log_pplx:3.2526081 loss:50.910934 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.38757\n",
      "I0710 12:07:18.498617 140295626643200 summary_utils.py:349] Steps/second: 0.177424, Examples/second: 25.171563\n",
      "I0710 12:07:18.499567 140295626643200 trainer.py:508] step:  7590, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:64.484207 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3202587 log_pplx:2.8126311 loss:112.36462 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.39154\n",
      "I0710 12:07:22.445026 140295626643200 summary_utils.py:349] Steps/second: 0.177442, Examples/second: 25.175087\n",
      "I0710 12:07:22.445880 140295626643200 trainer.py:508] step:  7591, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:37.966213 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3222163 log_pplx:3.1867058 loss:80.563911 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.39563\n",
      "I0710 12:07:28.831447 140295626643200 summary_utils.py:349] Steps/second: 0.177434, Examples/second: 25.170396\n",
      "I0710 12:07:28.832156 140295626643200 trainer.py:508] step:  7592, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:73.021385 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3203816 log_pplx:2.7445896 loss:109.16604 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.39978\n",
      "I0710 12:07:35.083871 140295626643200 summary_utils.py:349] Steps/second: 0.177427, Examples/second: 25.165903\n",
      "I0710 12:07:35.084695 140295626643200 trainer.py:508] step:  7593, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:65.078644 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3210704 log_pplx:2.7835286 loss:115.1685 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.40411\n",
      "I0710 12:07:44.754838 140295626643200 summary_utils.py:349] Steps/second: 0.177386, Examples/second: 25.154102\n",
      "I0710 12:07:44.755616 140295626643200 trainer.py:508] step:  7594, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:141.60483 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3197711 log_pplx:2.3389561 loss:168.40485 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:531.40857\n",
      "I0710 12:07:48.692163 140295626643200 summary_utils.py:349] Steps/second: 0.177403, Examples/second: 25.157638\n",
      "I0710 12:07:48.692990 140295626643200 trainer.py:508] step:  7595, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:34.832153 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3223953 log_pplx:3.2496655 loss:81.241638 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.41333\n",
      "I0710 12:07:51.807859 140295626643200 summary_utils.py:349] Steps/second: 0.177429, Examples/second: 25.167941\n",
      "I0710 12:07:51.808635 140295626643200 trainer.py:508] step:  7596, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:29.178391 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3217309 log_pplx:3.2590964 loss:50.719688 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.41809\n",
      "I0710 12:07:57.767621 140295626643200 summary_utils.py:349] Steps/second: 0.177426, Examples/second: 25.163882\n",
      "I0710 12:07:57.768405 140295626643200 trainer.py:508] step:  7597, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:67.603912 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3205096 log_pplx:2.8156571 loss:111.67599 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.42242\n",
      "I0710 12:08:04.176633 140295626643200 summary_utils.py:349] Steps/second: 0.177418, Examples/second: 25.159171\n",
      "I0710 12:08:04.177420 140295626643200 trainer.py:508] step:  7598, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:67.165268 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3208628 log_pplx:2.8148925 loss:115.26985 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.42725\n",
      "I0710 12:08:08.096705 140295626643200 summary_utils.py:349] Steps/second: 0.177436, Examples/second: 25.162728\n",
      "I0710 12:08:08.097589 140295626643200 trainer.py:508] step:  7599, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:34.506729 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3224055 log_pplx:3.2304964 loss:81.691185 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.43188\n",
      "I0710 12:08:14.610264 140295626643200 summary_utils.py:349] Steps/second: 0.177427, Examples/second: 25.157868\n",
      "I0710 12:08:14.611043 140295626643200 trainer.py:508] step:  7600, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:65.459297 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3206723 log_pplx:2.7596109 loss:112.07468 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.43646\n",
      "I0710 12:08:17.646802 140295626643200 summary_utils.py:349] Steps/second: 0.177453, Examples/second: 25.168270\n",
      "I0710 12:08:17.647578 140295626643200 trainer.py:508] step:  7601, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:21.09157 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3220931 log_pplx:3.2757332 loss:51.144943 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.44086\n",
      "I0710 12:08:24.013011 140295626643200 summary_utils.py:349] Steps/second: 0.177446, Examples/second: 25.163625\n",
      "I0710 12:08:24.013784 140295626643200 trainer.py:508] step:  7602, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:67.122307 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3208271 log_pplx:2.8076203 loss:113.7437 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.44501\n",
      "I0710 12:08:27.896950 140295626643200 summary_utils.py:349] Steps/second: 0.177464, Examples/second: 25.167229\n",
      "I0710 12:08:27.897734 140295626643200 trainer.py:508] step:  7603, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:34.968197 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3224281 log_pplx:3.2139871 loss:80.771507 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.44922\n",
      "I0710 12:08:34.278079 140295626643200 summary_utils.py:349] Steps/second: 0.177456, Examples/second: 25.162566\n",
      "I0710 12:08:34.279126 140295626643200 trainer.py:508] step:  7604, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:67.977493 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3208219 log_pplx:2.7632825 loss:113.12187 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.45319\n",
      "I0710 12:08:45.057882 140295626643200 summary_utils.py:349] Steps/second: 0.177403, Examples/second: 25.149195\n",
      "I0710 12:08:45.058705 140295626643200 trainer.py:508] step:  7605, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:143.69981 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3200582 log_pplx:2.3099315 loss:167.41228 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:531.45715\n",
      "I0710 12:08:51.109554 140295626643200 summary_utils.py:349] Steps/second: 0.177399, Examples/second: 25.145021\n",
      "I0710 12:08:51.110368 140295626643200 trainer.py:508] step:  7606, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:65.051651 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3203821 log_pplx:2.8011205 loss:109.87396 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.46143\n",
      "I0710 12:08:54.163177 140295626643200 summary_utils.py:349] Steps/second: 0.177426, Examples/second: 25.155379\n",
      "I0710 12:08:54.164238 140295626643200 trainer.py:508] step:  7607, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:20.496754 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3221381 log_pplx:3.2249994 loss:50.592178 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.46582\n",
      "I0710 12:08:58.047823 140295626643200 summary_utils.py:349] Steps/second: 0.177444, Examples/second: 25.158978\n",
      "I0710 12:08:58.048718 140295626643200 trainer.py:508] step:  7608, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:35.899265 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3226177 log_pplx:3.3020241 loss:82.571243 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.47009\n",
      "I0710 12:09:04.434514 140295626643200 summary_utils.py:349] Steps/second: 0.177436, Examples/second: 25.154317\n",
      "I0710 12:09:04.435293 140295626643200 trainer.py:508] step:  7609, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:65.911766 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3207903 log_pplx:2.7529268 loss:109.56649 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.47437\n",
      "I0710 12:09:06.623900 140295626643200 summary_utils.py:349] Steps/second: 0.177471, Examples/second: 25.180706\n",
      "I0710 12:09:06.624713 140295626643200 trainer.py:508] step:  7610, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:10.715155 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3202956 log_pplx:3.3908861 loss:24.431599 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:531.47876\n",
      "I0710 12:09:12.961972 140295626643200 summary_utils.py:349] Steps/second: 0.177464, Examples/second: 25.176111\n",
      "I0710 12:09:12.962752 140295626643200 trainer.py:508] step:  7611, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:64.727997 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.320967 log_pplx:2.8151793 loss:113.5573 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.48291\n",
      "I0710 12:09:16.912843 140295626643200 summary_utils.py:349] Steps/second: 0.177481, Examples/second: 25.179604\n",
      "I0710 12:09:16.913706 140295626643200 trainer.py:508] step:  7612, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:34.967846 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3226451 log_pplx:3.173934 loss:79.745094 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.48718\n",
      "I0710 12:09:23.581716 140295626643200 summary_utils.py:349] Steps/second: 0.177471, Examples/second: 25.174532\n",
      "I0710 12:09:23.582596 140295626643200 trainer.py:508] step:  7613, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:64.306717 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3209109 log_pplx:2.7312286 loss:110.92202 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.49139\n",
      "I0710 12:09:26.851818 140295626643200 summary_utils.py:349] Steps/second: 0.177495, Examples/second: 25.184550\n",
      "I0710 12:09:26.852702 140295626643200 trainer.py:508] step:  7614, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:20.314838 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3222492 log_pplx:3.2182076 loss:50.121071 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.49573\n",
      "I0710 12:09:37.727446 140295626643200 summary_utils.py:349] Steps/second: 0.177441, Examples/second: 25.171066\n",
      "I0710 12:09:37.728225 140295626643200 trainer.py:508] step:  7615, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:157.44766 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3200934 log_pplx:2.3366275 loss:169.98965 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:531.49988\n",
      "I0710 12:09:43.542226 140295626643200 summary_utils.py:349] Steps/second: 0.177439, Examples/second: 25.167242\n",
      "I0710 12:09:43.543033 140295626643200 trainer.py:508] step:  7616, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:62.548573 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3206135 log_pplx:2.7933009 loss:110.1259 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.50415\n",
      "I0710 12:09:47.479132 140295626643200 summary_utils.py:349] Steps/second: 0.177457, Examples/second: 25.170752\n",
      "I0710 12:09:47.479837 140295626643200 trainer.py:508] step:  7617, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:37.500427 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3227891 log_pplx:3.2499692 loss:82.285156 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.50854\n",
      "I0710 12:09:54.035660 140295626643200 summary_utils.py:349] Steps/second: 0.177447, Examples/second: 25.165854\n",
      "I0710 12:09:54.036444 140295626643200 trainer.py:508] step:  7618, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:70.045982 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3210059 log_pplx:2.7622216 loss:113.11298 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.51294\n",
      "I0710 12:09:57.169718 140295626643200 summary_utils.py:349] Steps/second: 0.177473, Examples/second: 25.176054\n",
      "I0710 12:09:57.170511 140295626643200 trainer.py:508] step:  7619, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:21.582409 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3222334 log_pplx:3.2042115 loss:49.86554 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.51752\n",
      "I0710 12:10:03.655888 140295626643200 summary_utils.py:349] Steps/second: 0.177464, Examples/second: 25.171259\n",
      "I0710 12:10:03.656778 140295626643200 trainer.py:508] step:  7620, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:64.378601 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3211781 log_pplx:2.7822905 loss:110.6656 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.52191\n",
      "I0710 12:10:07.563871 140295626643200 summary_utils.py:349] Steps/second: 0.177482, Examples/second: 25.174806\n",
      "I0710 12:10:07.564688 140295626643200 trainer.py:508] step:  7621, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:36.20155 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3229228 log_pplx:3.2521005 loss:82.4814 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.52618\n",
      "I0710 12:10:13.849602 140295626643200 summary_utils.py:349] Steps/second: 0.177475, Examples/second: 25.170305\n",
      "I0710 12:10:13.850514 140295626643200 trainer.py:508] step:  7622, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:67.26931 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3211774 log_pplx:2.7508192 loss:111.88959 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.53046\n",
      "I0710 12:10:20.079244 140295626643200 summary_utils.py:349] Steps/second: 0.177469, Examples/second: 25.165888\n",
      "I0710 12:10:20.079997 140295626643200 trainer.py:508] step:  7623, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:68.916435 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3212152 log_pplx:2.7401555 loss:112.86015 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.53497\n",
      "I0710 12:10:30.993145 140295626643200 summary_utils.py:349] Steps/second: 0.177415, Examples/second: 25.152402\n",
      "I0710 12:10:30.994059 140295626643200 trainer.py:508] step:  7624, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:159.09515 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3206812 log_pplx:2.366219 loss:182.90874 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:531.53979\n",
      "I0710 12:10:34.053657 140295626643200 summary_utils.py:349] Steps/second: 0.177442, Examples/second: 25.162689\n",
      "I0710 12:10:34.054520 140295626643200 trainer.py:508] step:  7625, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:26.584774 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3223794 log_pplx:3.3325496 loss:51.055702 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.54517\n",
      "I0710 12:10:37.954009 140295626643200 summary_utils.py:349] Steps/second: 0.177459, Examples/second: 25.166243\n",
      "I0710 12:10:37.954770 140295626643200 trainer.py:508] step:  7626, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:47.380535 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3232406 log_pplx:3.2653806 loss:82.695755 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.55035\n",
      "I0710 12:10:44.009524 140295626643200 summary_utils.py:349] Steps/second: 0.177455, Examples/second: 25.162085\n",
      "I0710 12:10:44.010360 140295626643200 trainer.py:508] step:  7627, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:75.089722 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.321372 log_pplx:2.7499146 loss:110.61532 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.55511\n",
      "I0710 12:10:50.598487 140295626643200 summary_utils.py:349] Steps/second: 0.177445, Examples/second: 25.157160\n",
      "I0710 12:10:50.599294 140295626643200 trainer.py:508] step:  7628, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:69.447617 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3211453 log_pplx:2.7862203 loss:111.41398 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.55963\n",
      "I0710 12:10:54.483423 140295626643200 summary_utils.py:349] Steps/second: 0.177463, Examples/second: 25.160734\n",
      "I0710 12:10:54.484455 140295626643200 trainer.py:508] step:  7629, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:37.933666 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3232076 log_pplx:3.1909525 loss:80.751053 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.56396\n",
      "I0710 12:11:00.679901 140295626643200 summary_utils.py:349] Steps/second: 0.177457, Examples/second: 25.156379\n",
      "I0710 12:11:00.680729 140295626643200 trainer.py:508] step:  7630, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:72.481575 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3212206 log_pplx:2.808665 loss:110.55608 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.56836\n",
      "I0710 12:11:03.734674 140295626643200 summary_utils.py:349] Steps/second: 0.177484, Examples/second: 25.166656\n",
      "I0710 12:11:03.735621 140295626643200 trainer.py:508] step:  7631, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:23.411861 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3225415 log_pplx:3.2746463 loss:50.155811 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.57318\n",
      "I0710 12:11:10.104588 140295626643200 summary_utils.py:349] Steps/second: 0.177476, Examples/second: 25.162050\n",
      "I0710 12:11:10.105373 140295626643200 trainer.py:508] step:  7632, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:69.946945 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3214768 log_pplx:2.7708719 loss:112.80913 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.57788\n",
      "I0710 12:11:16.545148 140295626643200 summary_utils.py:349] Steps/second: 0.177468, Examples/second: 25.157347\n",
      "I0710 12:11:16.545967 140295626643200 trainer.py:508] step:  7633, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:68.572701 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3214246 log_pplx:2.7378361 loss:111.77214 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.5827\n",
      "I0710 12:11:18.766242 140295626643200 summary_utils.py:349] Steps/second: 0.177503, Examples/second: 25.183489\n",
      "I0710 12:11:18.767180 140295626643200 trainer.py:508] step:  7634, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:10.996756 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3212306 log_pplx:3.3862419 loss:24.589937 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:531.58765\n",
      "I0710 12:11:22.707057 140295626643200 summary_utils.py:349] Steps/second: 0.177520, Examples/second: 25.186971\n",
      "I0710 12:11:22.707841 140295626643200 trainer.py:508] step:  7635, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:36.621258 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3229768 log_pplx:3.1804974 loss:79.53231 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.59229\n",
      "I0710 12:11:32.463774 140295626643200 summary_utils.py:349] Steps/second: 0.177478, Examples/second: 25.175188\n",
      "I0710 12:11:32.464628 140295626643200 trainer.py:508] step:  7636, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:190.86754 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3206542 log_pplx:2.4122484 loss:177.66209 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:531.59674\n",
      "I0710 12:11:38.990721 140295626643200 summary_utils.py:349] Steps/second: 0.177469, Examples/second: 25.170361\n",
      "I0710 12:11:38.991649 140295626643200 base_runner.py:111] step:  7637, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:63.28175 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3217357 log_pplx:2.7486119 loss:112.65873 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.60028\n",
      "I0710 12:11:42.070654 140295626643200 summary_utils.py:349] Steps/second: 0.177495, Examples/second: 25.180576\n",
      "I0710 12:11:42.071469 140295626643200 trainer.py:508] step:  7638, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:22.646851 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3228134 log_pplx:3.2931519 loss:51.982914 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.604\n",
      "I0710 12:11:48.467031 140295626643200 summary_utils.py:349] Steps/second: 0.177487, Examples/second: 25.175938\n",
      "I0710 12:11:48.467950 140295626643200 trainer.py:508] step:  7639, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:72.658775 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3218884 log_pplx:2.7532113 loss:111.88362 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.60779\n",
      "I0710 12:11:52.401762 140295626643200 summary_utils.py:349] Steps/second: 0.177504, Examples/second: 25.179424\n",
      "I0710 12:11:52.402631 140295626643200 trainer.py:508] step:  7640, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:35.571827 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3230509 log_pplx:3.1775589 loss:78.843185 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.61218\n",
      "I0710 12:11:58.677774 140295626643200 summary_utils.py:349] Steps/second: 0.177498, Examples/second: 25.174962\n",
      "I0710 12:11:58.678769 140295626643200 trainer.py:508] step:  7641, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:81.422401 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3215901 log_pplx:2.8212314 loss:112.95506 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.61664\n",
      "I0710 12:12:04.941317 140295626643200 summary_utils.py:349] Steps/second: 0.177491, Examples/second: 25.170522\n",
      "I0710 12:12:04.942176 140295626643200 trainer.py:508] step:  7642, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:66.407242 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.32164 log_pplx:2.7767889 loss:112.66821 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.6217\n",
      "I0710 12:12:08.828981 140295626643200 summary_utils.py:349] Steps/second: 0.177509, Examples/second: 25.174073\n",
      "I0710 12:12:08.829824 140295626643200 trainer.py:508] step:  7643, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:58.458248 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3237015 log_pplx:3.1986763 loss:82.006065 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.62708\n",
      "I0710 12:12:11.927217 140295626643200 summary_utils.py:349] Steps/second: 0.177535, Examples/second: 25.184245\n",
      "I0710 12:12:11.927999 140295626643200 trainer.py:508] step:  7644, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:33.316799 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3230041 log_pplx:3.234302 loss:50.700211 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.63196\n",
      "I0710 12:12:18.294336 140295626643200 summary_utils.py:349] Steps/second: 0.177527, Examples/second: 25.179655\n",
      "I0710 12:12:18.295321 140295626643200 trainer.py:508] step:  7645, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:65.999481 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3217647 log_pplx:2.7536061 loss:112.24387 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.63629\n",
      "I0710 12:12:27.962779 140295626643200 summary_utils.py:349] Steps/second: 0.177487, Examples/second: 25.168040\n",
      "I0710 12:12:27.963567 140295626643200 trainer.py:508] step:  7646, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:150.82573 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3206466 log_pplx:2.3297246 loss:171.40948 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:531.64087\n",
      "I0710 12:12:34.315121 140295626643200 summary_utils.py:349] Steps/second: 0.177479, Examples/second: 25.163482\n",
      "I0710 12:12:34.316010 140295626643200 trainer.py:508] step:  7647, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:72.553741 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3216722 log_pplx:2.8148303 loss:109.98949 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.646\n",
      "I0710 12:12:38.238867 140295626643200 summary_utils.py:349] Steps/second: 0.177497, Examples/second: 25.166978\n",
      "I0710 12:12:38.239667 140295626643200 trainer.py:508] step:  7648, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:36.149509 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3234524 log_pplx:3.2325509 loss:81.419868 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.65082\n",
      "I0710 12:12:44.506617 140295626643200 summary_utils.py:349] Steps/second: 0.177490, Examples/second: 25.162544\n",
      "I0710 12:12:44.507473 140295626643200 trainer.py:508] step:  7649, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:63.434978 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3217307 log_pplx:2.7425447 loss:108.09054 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.6554\n",
      "I0710 12:12:47.623106 140295626643200 summary_utils.py:349] Steps/second: 0.177516, Examples/second: 25.172670\n",
      "I0710 12:12:47.623862 140295626643200 trainer.py:508] step:  7650, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:24.926754 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3230561 log_pplx:3.2801218 loss:51.597855 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.65973\n",
      "I0710 12:12:53.858530 140295626643200 summary_utils.py:349] Steps/second: 0.177510, Examples/second: 25.168283\n",
      "I0710 12:12:53.859308 140295626643200 trainer.py:508] step:  7651, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:67.776703 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3216468 log_pplx:2.7287481 loss:109.49101 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.66406\n",
      "I0710 12:12:56.266439 140295635035904 trainer.py:345] Write summary @7651\n",
      "2020-07-10 12:12:57.933504: I lingvo/core/ops/record_batcher.cc:394] 17521 total seconds passed. Total records yielded: 5229. Total records skipped: 0\n",
      "I0710 12:13:01.066416 140295626643200 summary_utils.py:349] Steps/second: 0.177494, Examples/second: 25.167063\n",
      "I0710 12:13:01.067844 140295626643200 trainer.py:508] step:  7652, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:39.542332 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3235058 log_pplx:3.2065413 loss:79.762718 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.66864\n",
      "I0710 12:13:10.302619 140295626643200 summary_utils.py:349] Steps/second: 0.177457, Examples/second: 25.158381\n",
      "I0710 12:13:10.304567 140295626643200 trainer.py:508] step:  7653, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:68.899231 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3221276 log_pplx:2.7762167 loss:116.6011 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.67334\n",
      "I0710 12:13:19.845372 140295626643200 summary_utils.py:349] Steps/second: 0.177418, Examples/second: 25.149269\n",
      "I0710 12:13:19.847039 140295626643200 trainer.py:508] step:  7654, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:70.078316 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3218191 log_pplx:2.7710655 loss:113.5444 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.67822\n",
      "I0710 12:13:35.652680 140295626643200 summary_utils.py:349] Steps/second: 0.177315, Examples/second: 25.128937\n",
      "I0710 12:13:35.653714 140295626643200 trainer.py:508] step:  7655, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:118.95018 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3203324 log_pplx:2.2433798 loss:155.01755 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:531.68304\n",
      "I0710 12:13:41.581535 140295626643200 summary_utils.py:349] Steps/second: 0.177312, Examples/second: 25.129563\n",
      "I0710 12:13:41.582380 140295626643200 trainer.py:508] step:  7656, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:36.848675 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.323796 log_pplx:3.1762321 loss:79.167587 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.68805\n",
      "I0710 12:13:46.069991 140295626643200 summary_utils.py:349] Steps/second: 0.177324, Examples/second: 25.137702\n",
      "I0710 12:13:46.070988 140295626643200 trainer.py:508] step:  7657, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:21.390503 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.32344 log_pplx:3.2766712 loss:51.825157 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.69318\n",
      "I0710 12:13:48.955234 140295626643200 summary_utils.py:349] Steps/second: 0.177352, Examples/second: 25.162675\n",
      "I0710 12:13:48.956435 140295626643200 trainer.py:508] step:  7658, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:13.254 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.321893 log_pplx:3.4067707 loss:24.945278 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:531.69824\n",
      "I0710 12:13:51.488638 140295635035904 trainer.py:354] Write summary done: step 7651\n",
      "I0710 12:13:56.243489 140295626643200 summary_utils.py:349] Steps/second: 0.177335, Examples/second: 25.156802\n",
      "I0710 12:13:56.244275 140295626643200 trainer.py:508] step:  7659, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:76.081856 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3222262 log_pplx:2.7589083 loss:112.97729 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.70288\n",
      "I0710 12:14:02.617743 140295626643200 summary_utils.py:349] Steps/second: 0.177328, Examples/second: 25.152239\n",
      "I0710 12:14:02.618533 140295626643200 trainer.py:508] step:  7660, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:71.269302 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3220899 log_pplx:2.8458564 loss:115.86192 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.7077\n",
      "I0710 12:14:06.460839 140295626643200 summary_utils.py:349] Steps/second: 0.177346, Examples/second: 25.155835\n",
      "I0710 12:14:06.461613 140295626643200 trainer.py:508] step:  7661, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:36.134544 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3238659 log_pplx:3.1597447 loss:80.395767 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.71259\n",
      "I0710 12:14:12.642581 140295626643200 summary_utils.py:349] Steps/second: 0.177340, Examples/second: 25.151550\n",
      "I0710 12:14:12.643369 140295626643200 trainer.py:508] step:  7662, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:62.143909 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3220448 log_pplx:2.7155552 loss:107.19655 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.71735\n",
      "I0710 12:14:15.656222 140295626643200 summary_utils.py:349] Steps/second: 0.177367, Examples/second: 25.161775\n",
      "I0710 12:14:15.656997 140295626643200 trainer.py:508] step:  7663, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:20.949699 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3234751 log_pplx:3.2557714 loss:50.858719 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.72211\n",
      "I0710 12:14:21.526107 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 12:14:26.365969 140295626643200 summary_utils.py:349] Steps/second: 0.177316, Examples/second: 25.148762\n",
      "I0710 12:14:26.386917 140295626643200 trainer.py:508] step:  7664, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:162.86343 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3212652 log_pplx:2.3494999 loss:174.86154 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:531.7265\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 12:14:26.959827 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 12:14:27.424602 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00007663\n",
      "I0710 12:14:32.848656 140295626643200 summary_utils.py:349] Steps/second: 0.177307, Examples/second: 25.144056\n",
      "I0710 12:14:32.849511 140295626643200 trainer.py:508] step:  7665, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:72.073425 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3223312 log_pplx:2.8025639 loss:115.46564 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.73077\n",
      "I0710 12:14:36.964932 140295626643200 summary_utils.py:349] Steps/second: 0.177322, Examples/second: 25.147258\n",
      "I0710 12:14:36.965728 140295626643200 trainer.py:508] step:  7666, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:38.081352 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3240179 log_pplx:3.2586789 loss:81.71138 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.73541\n",
      "I0710 12:14:43.583939 140295626643200 summary_utils.py:349] Steps/second: 0.177313, Examples/second: 25.142360\n",
      "I0710 12:14:43.584705 140295626643200 trainer.py:508] step:  7667, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:71.418854 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3217309 log_pplx:2.7100677 loss:109.04634 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.74005\n",
      "I0710 12:14:49.937240 140295626643200 summary_utils.py:349] Steps/second: 0.177305, Examples/second: 25.137845\n",
      "I0710 12:14:49.938101 140295626643200 trainer.py:508] step:  7668, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:69.631271 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3223534 log_pplx:2.8334308 loss:115.74565 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.74524\n",
      "I0710 12:14:53.012697 140295626643200 summary_utils.py:349] Steps/second: 0.177331, Examples/second: 25.147962\n",
      "I0710 12:14:53.013633 140295626643200 trainer.py:508] step:  7669, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:24.266571 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.323682 log_pplx:3.2821109 loss:51.590683 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.75073\n",
      "I0710 12:14:56.911530 140295626643200 summary_utils.py:349] Steps/second: 0.177349, Examples/second: 25.151469\n",
      "I0710 12:14:56.912316 140295626643200 trainer.py:508] step:  7670, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:48.473824 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3240695 log_pplx:3.2516594 loss:82.876663 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.75574\n",
      "I0710 12:15:02.992331 140295626643200 summary_utils.py:349] Steps/second: 0.177344, Examples/second: 25.147341\n",
      "I0710 12:15:02.993275 140295626643200 trainer.py:508] step:  7671, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:66.905617 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3219509 log_pplx:2.7620592 loss:109.34302 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.76025\n",
      "I0710 12:15:09.417367 140295626643200 summary_utils.py:349] Steps/second: 0.177336, Examples/second: 25.142727\n",
      "I0710 12:15:09.418167 140295626643200 trainer.py:508] step:  7672, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:70.241997 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3224186 log_pplx:2.8114192 loss:114.42478 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.76489\n",
      "I0710 12:15:13.280885 140295626643200 summary_utils.py:349] Steps/second: 0.177354, Examples/second: 25.146282\n",
      "I0710 12:15:13.281678 140295626643200 trainer.py:508] step:  7673, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:36.964497 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3241026 log_pplx:3.212328 loss:80.609352 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.76929\n",
      "I0710 12:15:16.395509 140295626643200 summary_utils.py:349] Steps/second: 0.177379, Examples/second: 25.156329\n",
      "I0710 12:15:16.396351 140295626643200 trainer.py:508] step:  7674, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:24.540497 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3239498 log_pplx:3.3031161 loss:51.366039 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.7738\n",
      "I0710 12:15:26.000811 140295626643200 summary_utils.py:349] Steps/second: 0.177340, Examples/second: 25.144934\n",
      "I0710 12:15:26.001630 140295626643200 trainer.py:508] step:  7675, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:137.76242 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3211644 log_pplx:2.2753804 loss:158.36646 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:531.77838\n",
      "I0710 12:15:32.492108 140295626643200 summary_utils.py:349] Steps/second: 0.177331, Examples/second: 25.140232\n",
      "I0710 12:15:32.492942 140295626643200 trainer.py:508] step:  7676, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:69.808632 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3222281 log_pplx:2.8131981 loss:110.84001 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.78326\n",
      "I0710 12:15:38.764137 140295626643200 summary_utils.py:349] Steps/second: 0.177325, Examples/second: 25.135846\n",
      "I0710 12:15:38.765009 140295626643200 trainer.py:508] step:  7677, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:64.998085 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3223965 log_pplx:2.7378404 loss:107.97357 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.78876\n",
      "I0710 12:15:42.624904 140295626643200 summary_utils.py:349] Steps/second: 0.177343, Examples/second: 25.139400\n",
      "I0710 12:15:42.625728 140295626643200 trainer.py:508] step:  7678, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:55.827103 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3240877 log_pplx:3.2308676 loss:81.034203 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.79456\n",
      "I0710 12:15:49.104211 140295626643200 summary_utils.py:349] Steps/second: 0.177334, Examples/second: 25.134722\n",
      "I0710 12:15:49.105218 140295626643200 trainer.py:508] step:  7679, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:87.468468 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3221431 log_pplx:2.7550857 loss:110.99552 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.79974\n",
      "I0710 12:15:52.126329 140295626643200 summary_utils.py:349] Steps/second: 0.177360, Examples/second: 25.144882\n",
      "I0710 12:15:52.127124 140295626643200 trainer.py:508] step:  7680, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:24.380587 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3234565 log_pplx:3.1695893 loss:48.670528 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.80438\n",
      "I0710 12:15:54.353634 140295626643200 summary_utils.py:349] Steps/second: 0.177395, Examples/second: 25.170610\n",
      "I0710 12:15:54.354503 140295626643200 trainer.py:508] step:  7681, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:10.758079 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3225172 log_pplx:3.4211128 loss:25.290844 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:531.80872\n",
      "I0710 12:16:00.873090 140295626643200 summary_utils.py:349] Steps/second: 0.177386, Examples/second: 25.165867\n",
      "I0710 12:16:00.874086 140295626643200 trainer.py:508] step:  7682, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:66.593475 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3221989 log_pplx:2.6886847 loss:107.1777 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.81274\n",
      "I0710 12:16:04.814922 140295626643200 summary_utils.py:349] Steps/second: 0.177403, Examples/second: 25.169296\n",
      "I0710 12:16:04.815941 140295626643200 trainer.py:508] step:  7683, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:71.450493 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3244216 log_pplx:3.257437 loss:83.34967 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.81708\n",
      "I0710 12:16:15.757750 140295626643200 summary_utils.py:349] Steps/second: 0.177350, Examples/second: 25.156028\n",
      "I0710 12:16:15.758588 140295626643200 trainer.py:508] step:  7684, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:127.32838 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3215113 log_pplx:2.2876389 loss:165.79665 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:531.82202\n",
      "I0710 12:16:22.043740 140295626643200 summary_utils.py:349] Steps/second: 0.177343, Examples/second: 25.151627\n",
      "I0710 12:16:22.044527 140295626643200 trainer.py:508] step:  7685, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:66.634583 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3225442 log_pplx:2.7352064 loss:109.54502 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.82678\n",
      "I0710 12:16:28.460689 140295626643200 summary_utils.py:349] Steps/second: 0.177335, Examples/second: 25.147043\n",
      "I0710 12:16:28.461493 140295626643200 trainer.py:508] step:  7686, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:64.062386 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3224261 log_pplx:2.7758586 loss:109.26473 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.83197\n",
      "I0710 12:16:31.577571 140295626643200 summary_utils.py:349] Steps/second: 0.177361, Examples/second: 25.157044\n",
      "I0710 12:16:31.578379 140295626643200 trainer.py:508] step:  7687, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:21.46694 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.323887 log_pplx:3.2005908 loss:50.009232 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.83746\n",
      "I0710 12:16:35.538564 140295626643200 summary_utils.py:349] Steps/second: 0.177377, Examples/second: 25.160441\n",
      "I0710 12:16:35.539550 140295626643200 trainer.py:508] step:  7688, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:40.680195 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3245171 log_pplx:3.2047484 loss:80.098686 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.84271\n",
      "I0710 12:16:41.977307 140295626643200 summary_utils.py:349] Steps/second: 0.177369, Examples/second: 25.155827\n",
      "I0710 12:16:41.978104 140295626643200 trainer.py:508] step:  7689, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:66.512108 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3223935 log_pplx:2.7140162 loss:109.00167 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.84766\n",
      "I0710 12:16:48.480281 140295626643200 summary_utils.py:349] Steps/second: 0.177361, Examples/second: 25.151125\n",
      "I0710 12:16:48.481354 140295626643200 trainer.py:508] step:  7690, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:66.85733 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3225564 log_pplx:2.7790844 loss:109.66962 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.85266\n",
      "I0710 12:16:52.425165 140295626643200 summary_utils.py:349] Steps/second: 0.177378, Examples/second: 25.154543\n",
      "I0710 12:16:52.426048 140295626643200 trainer.py:508] step:  7691, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:35.183594 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3246094 log_pplx:3.1629534 loss:80.951836 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.85767\n",
      "I0710 12:16:55.519914 140295626643200 summary_utils.py:349] Steps/second: 0.177403, Examples/second: 25.164561\n",
      "I0710 12:16:55.520757 140295626643200 trainer.py:508] step:  7692, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:22.559647 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3239809 log_pplx:3.2520545 loss:50.292519 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.86249\n",
      "I0710 12:17:01.787042 140295626643200 summary_utils.py:349] Steps/second: 0.177397, Examples/second: 25.160193\n",
      "I0710 12:17:01.788044 140295626643200 trainer.py:508] step:  7693, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:85.582001 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3231421 log_pplx:2.7810676 loss:120.66357 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.86707\n",
      "I0710 12:17:12.717188 140295626643200 summary_utils.py:349] Steps/second: 0.177344, Examples/second: 25.146991\n",
      "I0710 12:17:12.718140 140295626643200 trainer.py:508] step:  7694, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:140.16451 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3219776 log_pplx:2.3308303 loss:171.19948 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:531.87201\n",
      "I0710 12:17:19.156956 140295626643200 summary_utils.py:349] Steps/second: 0.177336, Examples/second: 25.142390\n",
      "I0710 12:17:19.157754 140295626643200 trainer.py:508] step:  7695, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:71.705025 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3231511 log_pplx:2.7838795 loss:116.01817 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.87775\n",
      "I0710 12:17:23.006447 140295626643200 summary_utils.py:349] Steps/second: 0.177354, Examples/second: 25.145939\n",
      "I0710 12:17:23.007272 140295626643200 trainer.py:508] step:  7696, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:42.080315 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3248717 log_pplx:3.2320843 loss:82.377739 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.88293\n",
      "I0710 12:17:29.178953 140295626643200 summary_utils.py:349] Steps/second: 0.177348, Examples/second: 25.141718\n",
      "I0710 12:17:29.179741 140295626643200 trainer.py:508] step:  7697, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:67.153435 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3227564 log_pplx:2.7698376 loss:107.40045 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.88776\n",
      "I0710 12:17:32.203224 140295626643200 summary_utils.py:349] Steps/second: 0.177374, Examples/second: 25.151816\n",
      "I0710 12:17:32.203985 140295626643200 trainer.py:508] step:  7698, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:22.052702 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3243895 log_pplx:3.2450953 loss:50.413059 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.89233\n",
      "I0710 12:17:38.503797 140295626643200 summary_utils.py:349] Steps/second: 0.177368, Examples/second: 25.147414\n",
      "I0710 12:17:38.504582 140295626643200 trainer.py:508] step:  7699, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:71.430542 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3228637 log_pplx:2.7889631 loss:111.73283 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.89661\n",
      "I0710 12:17:44.818228 140295626643200 summary_utils.py:349] Steps/second: 0.177361, Examples/second: 25.142996\n",
      "I0710 12:17:44.819076 140295626643200 trainer.py:508] step:  7700, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:68.806679 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3230894 log_pplx:2.8005888 loss:114.40405 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.90143\n",
      "I0710 12:17:48.734704 140295626643200 summary_utils.py:349] Steps/second: 0.177378, Examples/second: 25.146446\n",
      "I0710 12:17:48.735445 140295626643200 trainer.py:508] step:  7701, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:41.235714 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3243704 log_pplx:3.164959 loss:79.7174 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.90662\n",
      "I0710 12:17:58.353868 140295626643200 summary_utils.py:349] Steps/second: 0.177339, Examples/second: 25.135134\n",
      "I0710 12:17:58.354682 140295626643200 trainer.py:508] step:  7702, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:136.58379 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3219006 log_pplx:2.3283756 loss:163.74301 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:531.91187\n",
      "I0710 12:18:04.774212 140295626643200 summary_utils.py:349] Steps/second: 0.177331, Examples/second: 25.130576\n",
      "I0710 12:18:04.775024 140295626643200 trainer.py:508] step:  7703, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:66.749939 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3230557 log_pplx:2.6615505 loss:106.66162 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.91718\n",
      "I0710 12:18:07.860562 140295626643200 summary_utils.py:349] Steps/second: 0.177356, Examples/second: 25.140568\n",
      "I0710 12:18:07.861318 140295626643200 trainer.py:508] step:  7704, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:22.955843 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3243331 log_pplx:3.2805002 loss:50.322357 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.92255\n",
      "I0710 12:18:11.782090 140295626643200 summary_utils.py:349] Steps/second: 0.177373, Examples/second: 25.144007\n",
      "I0710 12:18:11.782829 140295626643200 trainer.py:508] step:  7705, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:38.550789 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3247589 log_pplx:3.2123058 loss:80.829636 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.92761\n",
      "I0710 12:18:18.398165 140295626643200 summary_utils.py:349] Steps/second: 0.177364, Examples/second: 25.139174\n",
      "I0710 12:18:18.398925 140295626643200 trainer.py:508] step:  7706, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:70.316231 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3230275 log_pplx:2.7665672 loss:113.84424 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.93231\n",
      "I0710 12:18:20.603316 140295626643200 summary_utils.py:349] Steps/second: 0.177398, Examples/second: 25.164724\n",
      "I0710 12:18:20.604059 140295626643200 trainer.py:508] step:  7707, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:10.194487 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3229935 log_pplx:3.312712 loss:24.47007 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:531.93695\n",
      "I0710 12:18:26.914876 140295626643200 summary_utils.py:349] Steps/second: 0.177391, Examples/second: 25.160314\n",
      "I0710 12:18:26.915696 140295626643200 trainer.py:508] step:  7708, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:73.074516 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3230541 log_pplx:2.7158322 loss:107.81854 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.94128\n",
      "I0710 12:18:32.981708 140295626643200 summary_utils.py:349] Steps/second: 0.177387, Examples/second: 25.156251\n",
      "I0710 12:18:32.982503 140295626643200 trainer.py:508] step:  7709, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:68.764473 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3236235 log_pplx:2.7805054 loss:115.87756 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.94586\n",
      "I0710 12:18:36.041791 140295626643200 summary_utils.py:349] Steps/second: 0.177412, Examples/second: 25.166261\n",
      "I0710 12:18:36.042843 140295626643200 trainer.py:508] step:  7710, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:24.608271 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3245201 log_pplx:3.2083218 loss:49.879379 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.95117\n",
      "I0710 12:18:39.876895 140295626643200 summary_utils.py:349] Steps/second: 0.177430, Examples/second: 25.169810\n",
      "I0710 12:18:39.877688 140295626643200 trainer.py:508] step:  7711, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:45.673866 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3251579 log_pplx:3.2912338 loss:81.97229 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.95618\n",
      "I0710 12:18:46.248894 140295626643200 summary_utils.py:349] Steps/second: 0.177423, Examples/second: 25.165318\n",
      "I0710 12:18:46.249721 140295626643200 trainer.py:508] step:  7712, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:77.484344 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3234694 log_pplx:2.8113863 loss:111.61204 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.96094\n",
      "I0710 12:18:52.625759 140295626643200 summary_utils.py:349] Steps/second: 0.177415, Examples/second: 25.160822\n",
      "I0710 12:18:52.626617 140295626643200 trainer.py:508] step:  7713, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:63.760777 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3236622 log_pplx:2.7434623 loss:113.8194 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.96533\n",
      "I0710 12:19:02.835814 140295626643200 summary_utils.py:349] Steps/second: 0.177370, Examples/second: 25.148713\n",
      "I0710 12:19:02.836627 140295626643200 trainer.py:508] step:  7714, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:132.29181 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3222276 log_pplx:2.2170467 loss:162.50955 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:531.96967\n",
      "I0710 12:19:06.689248 140295626643200 summary_utils.py:349] Steps/second: 0.177388, Examples/second: 25.152235\n",
      "I0710 12:19:06.690309 140295626643200 trainer.py:508] step:  7715, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:38.283894 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3252903 log_pplx:3.2681069 loss:82.88736 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.97363\n",
      "I0710 12:19:13.013521 140295626643200 summary_utils.py:349] Steps/second: 0.177381, Examples/second: 25.147823\n",
      "I0710 12:19:13.014442 140295626643200 trainer.py:508] step:  7716, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:66.578064 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3231953 log_pplx:2.7416255 loss:110.4875 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.97797\n",
      "I0710 12:19:16.111818 140295626643200 summary_utils.py:349] Steps/second: 0.177406, Examples/second: 25.157758\n",
      "I0710 12:19:16.112866 140295626643200 trainer.py:508] step:  7717, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:22.152515 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3247961 log_pplx:3.2080722 loss:51.266499 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:531.98273\n",
      "I0710 12:19:22.411082 140295626643200 summary_utils.py:349] Steps/second: 0.177400, Examples/second: 25.153382\n",
      "I0710 12:19:22.411868 140295626643200 trainer.py:508] step:  7718, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:67.608925 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3237884 log_pplx:2.7346175 loss:113.14481 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.9873\n",
      "I0710 12:19:26.282320 140295626643200 summary_utils.py:349] Steps/second: 0.177417, Examples/second: 25.156873\n",
      "I0710 12:19:26.283090 140295626643200 trainer.py:508] step:  7719, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:36.262444 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.324849 log_pplx:3.1927514 loss:80.696793 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:531.99219\n",
      "I0710 12:19:32.790358 140295626643200 summary_utils.py:349] Steps/second: 0.177408, Examples/second: 25.152207\n",
      "I0710 12:19:32.791559 140295626643200 trainer.py:508] step:  7720, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:70.907837 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.323045 log_pplx:2.6666317 loss:104.09863 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:531.99689\n",
      "I0710 12:19:39.039981 140295626643200 summary_utils.py:349] Steps/second: 0.177402, Examples/second: 25.147906\n",
      "I0710 12:19:39.040784 140295626643200 trainer.py:508] step:  7721, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:70.991989 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3236331 log_pplx:2.8199735 loss:112.37593 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.00171\n",
      "I0710 12:19:42.075564 140295626643200 summary_utils.py:349] Steps/second: 0.177428, Examples/second: 25.157915\n",
      "I0710 12:19:42.076510 140295626643200 trainer.py:508] step:  7722, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:21.948706 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3249691 log_pplx:3.2781982 loss:51.055374 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.00647\n",
      "I0710 12:19:46.089144 140295626643200 summary_utils.py:349] Steps/second: 0.177444, Examples/second: 25.161202\n",
      "I0710 12:19:46.090206 140295626643200 trainer.py:508] step:  7723, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:43.561863 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3250213 log_pplx:3.1428468 loss:78.767593 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.01105\n",
      "I0710 12:19:52.852526 140295626643200 summary_utils.py:349] Steps/second: 0.177433, Examples/second: 25.156182\n",
      "I0710 12:19:52.853319 140295626643200 trainer.py:508] step:  7724, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:75.702866 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3233706 log_pplx:2.7143278 loss:110.9821 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.01593\n",
      "I0710 12:20:03.568292 140295626643200 summary_utils.py:349] Steps/second: 0.177383, Examples/second: 25.143408\n",
      "I0710 12:20:03.569085 140295626643200 trainer.py:508] step:  7725, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:139.65932 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3224823 log_pplx:2.2297149 loss:157.02766 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.02118\n",
      "I0710 12:20:09.949738 140295626643200 summary_utils.py:349] Steps/second: 0.177376, Examples/second: 25.138934\n",
      "I0710 12:20:09.950771 140295626643200 trainer.py:508] step:  7726, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:66.316086 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3240546 log_pplx:2.7087851 loss:112.82088 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.02679\n",
      "I0710 12:20:13.813561 140295626643200 summary_utils.py:349] Steps/second: 0.177393, Examples/second: 25.142430\n",
      "I0710 12:20:13.814660 140295626643200 trainer.py:508] step:  7727, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:41.606998 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3253539 log_pplx:3.228838 loss:81.326355 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.03265\n",
      "I0710 12:20:16.006851 140295626643200 summary_utils.py:349] Steps/second: 0.177427, Examples/second: 25.167832\n",
      "I0710 12:20:16.007700 140295626643200 trainer.py:508] step:  7728, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:11.841475 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3234272 log_pplx:3.3336594 loss:24.533649 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:532.03827\n",
      "I0710 12:20:19.083519 140295626643200 summary_utils.py:349] Steps/second: 0.177452, Examples/second: 25.177759\n",
      "I0710 12:20:19.084342 140295626643200 trainer.py:508] step:  7729, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:24.965288 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3250033 log_pplx:3.2200723 loss:49.65955 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.04358\n",
      "I0710 12:20:25.383620 140295626643200 summary_utils.py:349] Steps/second: 0.177446, Examples/second: 25.173390\n",
      "I0710 12:20:25.384440 140295626643200 trainer.py:508] step:  7730, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:73.312439 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3232887 log_pplx:2.7525172 loss:108.72441 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.04834\n",
      "I0710 12:20:31.967757 140295626643200 summary_utils.py:349] Steps/second: 0.177436, Examples/second: 25.168626\n",
      "I0710 12:20:31.968548 140295626643200 trainer.py:508] step:  7731, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:66.22451 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3240179 log_pplx:2.7273638 loss:112.43557 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.05273\n",
      "I0710 12:20:35.886325 140295626643200 summary_utils.py:349] Steps/second: 0.177453, Examples/second: 25.172035\n",
      "I0710 12:20:35.887140 140295626643200 trainer.py:508] step:  7732, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:36.434776 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.326007 log_pplx:3.2508485 loss:82.75441 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.05725\n",
      "I0710 12:20:42.135941 140295626643200 summary_utils.py:349] Steps/second: 0.177447, Examples/second: 25.167742\n",
      "I0710 12:20:42.136723 140295626643200 trainer.py:508] step:  7733, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:62.685104 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3236878 log_pplx:2.7113135 loss:105.91068 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.06171\n",
      "I0710 12:20:48.577506 140295626643200 summary_utils.py:349] Steps/second: 0.177439, Examples/second: 25.163185\n",
      "I0710 12:20:48.578299 140295626643200 trainer.py:508] step:  7734, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:64.008926 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3239397 log_pplx:2.7821023 loss:112.39693 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.06635\n",
      "I0710 12:20:51.673826 140295626643200 summary_utils.py:349] Steps/second: 0.177464, Examples/second: 25.173067\n",
      "I0710 12:20:51.674969 140295626643200 trainer.py:508] step:  7735, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:23.600025 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.324916 log_pplx:3.2292728 loss:50.015881 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.07117\n",
      "I0710 12:20:55.481188 140295626643200 summary_utils.py:349] Steps/second: 0.177482, Examples/second: 25.176626\n",
      "I0710 12:20:55.482221 140295626643200 trainer.py:508] step:  7736, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:41.253563 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3256423 log_pplx:3.2123275 loss:80.007034 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.07574\n",
      "I0710 12:21:01.806266 140295626643200 summary_utils.py:349] Steps/second: 0.177475, Examples/second: 25.172231\n",
      "I0710 12:21:01.807092 140295626643200 base_runner.py:111] step:  7737, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:64.89901 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3242521 log_pplx:2.7767293 loss:115.68547 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.08014\n",
      "I0710 12:21:11.061771 140295626643200 summary_utils.py:349] Steps/second: 0.177440, Examples/second: 25.161534\n",
      "I0710 12:21:11.062588 140295626643200 trainer.py:508] step:  7738, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:131.82735 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3229895 log_pplx:2.2449112 loss:163.31729 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.08478\n",
      "I0710 12:21:17.509664 140295626643200 summary_utils.py:349] Steps/second: 0.177432, Examples/second: 25.156977\n",
      "I0710 12:21:17.510586 140295626643200 trainer.py:508] step:  7739, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:64.84082 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3240342 log_pplx:2.7881038 loss:117.69285 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.08972\n",
      "I0710 12:21:20.507705 140295626643200 summary_utils.py:349] Steps/second: 0.177458, Examples/second: 25.166982\n",
      "I0710 12:21:20.508452 140295626643200 trainer.py:508] step:  7740, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:21.973669 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3248298 log_pplx:3.2009614 loss:48.839672 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.09497\n",
      "I0710 12:21:24.361450 140295626643200 summary_utils.py:349] Steps/second: 0.177475, Examples/second: 25.170472\n",
      "I0710 12:21:24.362347 140295626643200 trainer.py:508] step:  7741, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:35.132122 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3256644 log_pplx:3.1940887 loss:80.451111 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.10004\n",
      "I0710 12:21:30.453254 140295626643200 summary_utils.py:349] Steps/second: 0.177471, Examples/second: 25.166411\n",
      "I0710 12:21:30.454146 140295626643200 trainer.py:508] step:  7742, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:66.101997 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3237078 log_pplx:2.7573786 loss:109.74367 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.10516\n",
      "I0710 12:21:36.881573 140295626643200 summary_utils.py:349] Steps/second: 0.177463, Examples/second: 25.161885\n",
      "I0710 12:21:36.882406 140295626643200 trainer.py:508] step:  7743, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:68.301369 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3241347 log_pplx:2.7287946 loss:111.36894 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.1106\n",
      "I0710 12:21:43.396589 140295626643200 summary_utils.py:349] Steps/second: 0.177454, Examples/second: 25.157240\n",
      "I0710 12:21:43.397351 140295626643200 trainer.py:508] step:  7744, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:68.616974 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3241926 log_pplx:2.83324 loss:114.63998 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.11609\n",
      "I0710 12:21:47.264041 140295626643200 summary_utils.py:349] Steps/second: 0.177472, Examples/second: 25.160709\n",
      "I0710 12:21:47.264926 140295626643200 trainer.py:508] step:  7745, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:36.700218 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.325896 log_pplx:3.1628332 loss:79.940598 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.1217\n",
      "I0710 12:21:50.344269 140295626643200 summary_utils.py:349] Steps/second: 0.177497, Examples/second: 25.170583\n",
      "I0710 12:21:50.345231 140295626643200 trainer.py:508] step:  7746, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:25.866226 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3255184 log_pplx:3.2857547 loss:51.583778 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.12701\n",
      "I0710 12:21:56.598718 140295626643200 summary_utils.py:349] Steps/second: 0.177491, Examples/second: 25.166301\n",
      "I0710 12:21:56.599503 140295626643200 trainer.py:508] step:  7747, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:69.007225 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3239762 log_pplx:2.7483122 loss:109.10799 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.13202\n",
      "I0710 12:22:03.076130 140295626643200 summary_utils.py:349] Steps/second: 0.177482, Examples/second: 25.161713\n",
      "I0710 12:22:03.077003 140295626643200 trainer.py:508] step:  7748, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:74.947578 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3238864 log_pplx:2.7322123 loss:108.33223 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.13708\n",
      "I0710 12:22:11.947536 140295626643200 summary_utils.py:349] Steps/second: 0.177451, Examples/second: 25.151591\n",
      "I0710 12:22:11.948336 140295626643200 trainer.py:508] step:  7749, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:119.1473 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3226188 log_pplx:2.2585218 loss:155.95093 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.1416\n",
      "I0710 12:22:15.898895 140295626643200 summary_utils.py:349] Steps/second: 0.177467, Examples/second: 25.154939\n",
      "I0710 12:22:15.899647 140295626643200 trainer.py:508] step:  7750, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:38.51825 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3260547 log_pplx:3.2121859 loss:81.44899 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.14655\n",
      "I0710 12:22:22.446745 140295626643200 summary_utils.py:349] Steps/second: 0.177458, Examples/second: 25.150262\n",
      "I0710 12:22:22.447531 140295626643200 trainer.py:508] step:  7751, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:64.194298 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3238629 log_pplx:2.6826193 loss:105.72874 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.15155\n",
      "I0710 12:22:25.541687 140295626643200 summary_utils.py:349] Steps/second: 0.177483, Examples/second: 25.160097\n",
      "I0710 12:22:25.542580 140295626643200 trainer.py:508] step:  7752, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:25.136917 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3258291 log_pplx:3.238862 loss:50.936161 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.1568\n",
      "I0710 12:22:27.753697 140295626643200 summary_utils.py:349] Steps/second: 0.177517, Examples/second: 25.185286\n",
      "I0710 12:22:27.754556 140295626643200 trainer.py:508] step:  7753, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:13.154205 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3240062 log_pplx:3.4009311 loss:24.650108 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:532.16187\n",
      "I0710 12:22:31.985159 140295635035904 trainer.py:345] Write summary @7753\n",
      "I0710 12:22:37.926574 140295626643200 summary_utils.py:349] Steps/second: 0.177472, Examples/second: 25.175566\n",
      "I0710 12:22:37.927875 140295626643200 trainer.py:508] step:  7754, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:72.700783 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3241498 log_pplx:2.7836783 loss:111.83427 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.1665\n",
      "I0710 12:22:43.133580 140295626643200 summary_utils.py:349] Steps/second: 0.177476, Examples/second: 25.177160\n",
      "I0710 12:22:43.135624 140295626643200 trainer.py:508] step:  7755, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:43.481197 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3261876 log_pplx:3.1609309 loss:79.457901 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.17175\n",
      "I0710 12:22:52.143289 140295626643200 summary_utils.py:349] Steps/second: 0.177443, Examples/second: 25.169066\n",
      "I0710 12:22:52.144706 140295626643200 trainer.py:508] step:  7756, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:66.794388 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.324246 log_pplx:2.7456317 loss:111.71291 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.17731\n",
      "I0710 12:22:56.195797 140295626643200 summary_utils.py:349] Steps/second: 0.177459, Examples/second: 25.177553\n",
      "I0710 12:22:56.197641 140295626643200 trainer.py:508] step:  7757, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:25.992159 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3255123 log_pplx:3.2125423 loss:50.195972 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.18274\n",
      "I0710 12:23:05.101439 140295626643200 summary_utils.py:349] Steps/second: 0.177427, Examples/second: 25.169609\n",
      "I0710 12:23:05.102348 140295626643200 trainer.py:508] step:  7758, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:75.779617 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3245598 log_pplx:2.8152075 loss:113.55843 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.18768\n",
      "I0710 12:23:10.806927 140295626643200 summary_utils.py:349] Steps/second: 0.177426, Examples/second: 25.170512\n",
      "I0710 12:23:10.808438 140295626643200 trainer.py:508] step:  7759, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:39.967659 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3260665 log_pplx:3.2338002 loss:80.481201 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.1922\n",
      "I0710 12:23:20.817770 140295626643200 summary_utils.py:349] Steps/second: 0.177383, Examples/second: 25.161046\n",
      "I0710 12:23:20.818988 140295626643200 trainer.py:508] step:  7760, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:69.330437 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3243228 log_pplx:2.8028586 loss:112.32456 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.19641\n",
      "I0710 12:23:29.288652 140295635035904 trainer.py:354] Write summary done: step 7753\n",
      "I0710 12:23:35.075911 140295626643200 summary_utils.py:349] Steps/second: 0.177299, Examples/second: 25.143517\n",
      "I0710 12:23:35.076677 140295626643200 trainer.py:508] step:  7761, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:122.68985 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3236709 log_pplx:2.2520678 loss:170.81932 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.20074\n",
      "I0710 12:23:41.575476 140295626643200 summary_utils.py:349] Steps/second: 0.177291, Examples/second: 25.138931\n",
      "I0710 12:23:41.576663 140295626643200 trainer.py:508] step:  7762, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:77.639725 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3244436 log_pplx:2.7878957 loss:114.26888 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.20538\n",
      "I0710 12:23:44.682305 140295626643200 summary_utils.py:349] Steps/second: 0.177316, Examples/second: 25.148709\n",
      "I0710 12:23:44.683117 140295626643200 trainer.py:508] step:  7763, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:27.666931 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3259994 log_pplx:3.1600749 loss:49.228043 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.21063\n",
      "I0710 12:23:48.541667 140295626643200 summary_utils.py:349] Steps/second: 0.177333, Examples/second: 25.152168\n",
      "I0710 12:23:48.542488 140295626643200 trainer.py:508] step:  7764, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:42.156357 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3264605 log_pplx:3.1198795 loss:78.835464 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.216\n",
      "I0710 12:23:55.172155 140295626643200 summary_utils.py:349] Steps/second: 0.177323, Examples/second: 25.147401\n",
      "I0710 12:23:55.172930 140295626643200 trainer.py:508] step:  7765, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:79.856453 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3243778 log_pplx:2.7743485 loss:109.89889 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.22095\n",
      "I0710 12:24:01.610688 140295626643200 summary_utils.py:349] Steps/second: 0.177316, Examples/second: 25.142903\n",
      "I0710 12:24:01.611666 140295626643200 trainer.py:508] step:  7766, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:71.759338 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3247101 log_pplx:2.7989495 loss:116.54126 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.22552\n",
      "I0710 12:24:05.482870 140295626643200 summary_utils.py:349] Steps/second: 0.177333, Examples/second: 25.146343\n",
      "I0710 12:24:05.483668 140295626643200 trainer.py:508] step:  7767, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:41.580029 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.326353 log_pplx:3.2124875 loss:81.416489 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.23004\n",
      "I0710 12:24:08.489527 140295626643200 summary_utils.py:349] Steps/second: 0.177358, Examples/second: 25.156245\n",
      "I0710 12:24:08.490394 140295626643200 trainer.py:508] step:  7768, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:27.095795 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3260263 log_pplx:3.1820209 loss:50.340557 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.23486\n",
      "I0710 12:24:15.065105 140295626643200 summary_utils.py:349] Steps/second: 0.177349, Examples/second: 25.151558\n",
      "I0710 12:24:15.065886 140295626643200 trainer.py:508] step:  7769, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:77.71183 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3247548 log_pplx:2.8005691 loss:112.7229 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.23975\n",
      "I0710 12:24:21.586642 140295626643200 summary_utils.py:349] Steps/second: 0.177341, Examples/second: 25.146949\n",
      "I0710 12:24:21.587412 140295626643200 trainer.py:508] step:  7770, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:75.998039 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3248676 log_pplx:2.7595701 loss:112.349 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.24536\n",
      "I0710 12:24:25.463471 140295626643200 summary_utils.py:349] Steps/second: 0.177358, Examples/second: 25.150377\n",
      "I0710 12:24:25.464419 140295626643200 trainer.py:508] step:  7771, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:43.39262 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3268765 log_pplx:3.2114816 loss:81.631844 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.25116\n",
      "I0710 12:24:29.347539 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 12:24:34.438630 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 12:24:34.968310 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00007771\n",
      "I0710 12:24:36.313221 140295626643200 summary_utils.py:349] Steps/second: 0.177307, Examples/second: 25.137613\n",
      "I0710 12:24:36.313987 140295626643200 trainer.py:508] step:  7772, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:242.62852 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3238393 log_pplx:2.2764416 loss:165.72493 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.25635\n",
      "I0710 12:24:42.595236 140295626643200 summary_utils.py:349] Steps/second: 0.177301, Examples/second: 25.133344\n",
      "I0710 12:24:42.595977 140295626643200 trainer.py:508] step:  7773, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:81.08506 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3250104 log_pplx:2.7339971 loss:114.5203 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.26202\n",
      "I0710 12:24:44.788413 140295626643200 summary_utils.py:349] Steps/second: 0.177334, Examples/second: 25.158374\n",
      "I0710 12:24:44.789326 140295626643200 trainer.py:508] step:  7774, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:20.641726 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3247102 log_pplx:3.4090559 loss:25.115156 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:532.26776\n",
      "I0710 12:24:51.220410 140295626643200 summary_utils.py:349] Steps/second: 0.177327, Examples/second: 25.153893\n",
      "I0710 12:24:51.221234 140295626643200 trainer.py:508] step:  7775, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:97.724144 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3249512 log_pplx:2.8676696 loss:116.10477 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.27307\n",
      "I0710 12:24:54.393595 140295626643200 summary_utils.py:349] Steps/second: 0.177351, Examples/second: 25.163540\n",
      "I0710 12:24:54.394482 140295626643200 trainer.py:508] step:  7776, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:38.58004 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.32583 log_pplx:3.2701542 loss:49.397213 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.27795\n",
      "I0710 12:24:58.490274 140295626643200 summary_utils.py:349] Steps/second: 0.177366, Examples/second: 25.166655\n",
      "I0710 12:24:58.491351 140295626643200 trainer.py:508] step:  7777, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:41.897942 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3270772 log_pplx:3.2639663 loss:81.395157 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.28229\n",
      "I0710 12:25:05.336452 140295626643200 summary_utils.py:349] Steps/second: 0.177354, Examples/second: 25.161605\n",
      "I0710 12:25:05.337440 140295626643200 trainer.py:508] step:  7778, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:100.81915 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3247993 log_pplx:2.8212457 loss:112.81458 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.28687\n",
      "I0710 12:25:11.902596 140295626643200 summary_utils.py:349] Steps/second: 0.177345, Examples/second: 25.156943\n",
      "I0710 12:25:11.903487 140295626643200 trainer.py:508] step:  7779, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:71.745872 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3249655 log_pplx:2.7838666 loss:113.96452 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.29248\n",
      "I0710 12:25:15.839566 140295626643200 summary_utils.py:349] Steps/second: 0.177361, Examples/second: 25.160277\n",
      "I0710 12:25:15.840502 140295626643200 trainer.py:508] step:  7780, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:42.787079 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3266752 log_pplx:3.1718283 loss:80.405846 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.29852\n",
      "I0710 12:25:22.368452 140295626643200 summary_utils.py:349] Steps/second: 0.177353, Examples/second: 25.155670\n",
      "I0710 12:25:22.369278 140295626643200 trainer.py:508] step:  7781, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:73.556953 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3251562 log_pplx:2.8187442 loss:113.94773 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.30463\n",
      "I0710 12:25:25.490161 140295626643200 summary_utils.py:349] Steps/second: 0.177377, Examples/second: 25.165371\n",
      "I0710 12:25:25.490987 140295626643200 trainer.py:508] step:  7782, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:29.037134 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3262678 log_pplx:3.1987312 loss:50.017662 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.31024\n",
      "I0710 12:25:36.603905 140295626643200 summary_utils.py:349] Steps/second: 0.177324, Examples/second: 25.152277\n",
      "I0710 12:25:36.604687 140295626643200 trainer.py:508] step:  7783, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:920.39105 grad_scale_all/loss:0 has_nan_or_inf/loss:0 l2_loss/loss:1.3242242 log_pplx:2.5679865 loss:190.48041 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.31573\n",
      "I0710 12:25:42.702634 140295626643200 summary_utils.py:349] Steps/second: 0.177320, Examples/second: 25.148269\n",
      "I0710 12:25:42.703475 140295626643200 trainer.py:508] step:  7784, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:103.25395 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3250185 log_pplx:2.7623749 loss:114.39685 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.32086\n",
      "I0710 12:25:46.567810 140295626643200 summary_utils.py:349] Steps/second: 0.177337, Examples/second: 25.151698\n",
      "I0710 12:25:46.568574 140295626643200 trainer.py:508] step:  7785, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:53.737701 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3267453 log_pplx:3.2354462 loss:82.03878 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.32526\n",
      "I0710 12:25:52.985847 140295626643200 summary_utils.py:349] Steps/second: 0.177329, Examples/second: 25.147254\n",
      "I0710 12:25:52.986661 140295626643200 trainer.py:508] step:  7786, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:64.72451 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3249933 log_pplx:2.7707233 loss:111.83334 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.32886\n",
      "I0710 12:25:59.196151 140295626643200 summary_utils.py:349] Steps/second: 0.177324, Examples/second: 25.143098\n",
      "I0710 12:25:59.197034 140295626643200 trainer.py:508] step:  7787, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:79.434395 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3250073 log_pplx:2.7659922 loss:110.88171 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.33276\n",
      "I0710 12:26:02.290759 140295626643200 summary_utils.py:349] Steps/second: 0.177348, Examples/second: 25.152818\n",
      "I0710 12:26:02.291584 140295626643200 trainer.py:508] step:  7788, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:42.038322 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3267132 log_pplx:3.3213706 loss:51.675854 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.33722\n",
      "I0710 12:26:06.117236 140295626643200 summary_utils.py:349] Steps/second: 0.177366, Examples/second: 25.156295\n",
      "I0710 12:26:06.117986 140295626643200 trainer.py:508] step:  7789, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:42.545521 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3268975 log_pplx:3.1009042 loss:77.716415 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.34186\n",
      "I0710 12:26:12.483650 140295626643200 summary_utils.py:349] Steps/second: 0.177359, Examples/second: 25.151925\n",
      "I0710 12:26:12.484459 140295626643200 trainer.py:508] step:  7790, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:71.493576 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3252137 log_pplx:2.7972758 loss:113.84912 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.34674\n",
      "I0710 12:26:19.027630 140295626643200 summary_utils.py:349] Steps/second: 0.177350, Examples/second: 25.147315\n",
      "I0710 12:26:19.028450 140295626643200 trainer.py:508] step:  7791, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:67.122879 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3254681 log_pplx:2.7111442 loss:111.5636 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.35175\n",
      "I0710 12:26:28.960247 140295626643200 summary_utils.py:349] Steps/second: 0.177309, Examples/second: 25.135888\n",
      "I0710 12:26:28.961095 140295626643200 trainer.py:508] step:  7792, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:306.74814 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3242215 log_pplx:2.3800683 loss:171.90042 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.35687\n",
      "I0710 12:26:32.794873 140295626643200 summary_utils.py:349] Steps/second: 0.177326, Examples/second: 25.139352\n",
      "I0710 12:26:32.795678 140295626643200 trainer.py:508] step:  7793, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:49.499859 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3271122 log_pplx:3.2249227 loss:80.562599 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.36139\n",
      "I0710 12:26:39.140835 140295626643200 summary_utils.py:349] Steps/second: 0.177319, Examples/second: 25.135022\n",
      "I0710 12:26:39.141612 140295626643200 trainer.py:508] step:  7794, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:121.00521 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3254117 log_pplx:2.8282504 loss:116.13503 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.36572\n",
      "I0710 12:26:42.236499 140295626643200 summary_utils.py:349] Steps/second: 0.177344, Examples/second: 25.144721\n",
      "I0710 12:26:42.237255 140295626643200 trainer.py:508] step:  7795, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:44.414028 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3268794 log_pplx:3.3204849 loss:51.934456 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.37097\n",
      "I0710 12:26:48.702816 140295626643200 summary_utils.py:349] Steps/second: 0.177336, Examples/second: 25.140227\n",
      "I0710 12:26:48.703591 140295626643200 trainer.py:508] step:  7796, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:108.53239 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3251988 log_pplx:2.8352997 loss:115.43214 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.37579\n",
      "I0710 12:26:50.891859 140295626643200 summary_utils.py:349] Steps/second: 0.177369, Examples/second: 25.165090\n",
      "I0710 12:26:50.892638 140295626643200 trainer.py:508] step:  7797, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:11.375857 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.325099 log_pplx:3.3289633 loss:24.401564 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:532.38177\n",
      "I0710 12:26:54.753965 140295626643200 summary_utils.py:349] Steps/second: 0.177386, Examples/second: 25.168506\n",
      "I0710 12:26:54.754723 140295626643200 trainer.py:508] step:  7798, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:41.333477 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3273561 log_pplx:3.2514851 loss:82.851906 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.38739\n",
      "I0710 12:27:00.935973 140295626643200 summary_utils.py:349] Steps/second: 0.177381, Examples/second: 25.164396\n",
      "I0710 12:27:00.936774 140295626643200 trainer.py:508] step:  7799, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:69.856285 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3250118 log_pplx:2.765599 loss:107.65096 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.39288\n",
      "I0710 12:27:04.098388 140295626643200 summary_utils.py:349] Steps/second: 0.177405, Examples/second: 25.173988\n",
      "I0710 12:27:04.099485 140295626643200 trainer.py:508] step:  7800, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:22.918165 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3269434 log_pplx:3.2649436 loss:51.754463 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.39832\n",
      "I0710 12:27:10.225605 140295626643200 summary_utils.py:349] Steps/second: 0.177400, Examples/second: 25.169952\n",
      "I0710 12:27:10.226430 140295626643200 trainer.py:508] step:  7801, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:78.820694 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.325587 log_pplx:2.8376076 loss:112.26286 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.40363\n",
      "I0710 12:27:16.808666 140295626643200 summary_utils.py:349] Steps/second: 0.177391, Examples/second: 25.165296\n",
      "I0710 12:27:16.809448 140295626643200 trainer.py:508] step:  7802, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:73.082886 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3257736 log_pplx:2.7729952 loss:118.92682 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.40851\n",
      "I0710 12:27:20.627377 140295626643200 summary_utils.py:349] Steps/second: 0.177409, Examples/second: 25.168767\n",
      "I0710 12:27:20.628154 140295626643200 trainer.py:508] step:  7803, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:37.37801 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3271997 log_pplx:3.2164547 loss:80.330956 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.4137\n",
      "I0710 12:27:31.868031 140295626643200 summary_utils.py:349] Steps/second: 0.177355, Examples/second: 25.155580\n",
      "I0710 12:27:31.869172 140295626643200 trainer.py:508] step:  7804, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:528.22253 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.32496 log_pplx:2.5505862 loss:185.49141 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.41882\n",
      "I0710 12:27:38.455091 140295626643200 summary_utils.py:349] Steps/second: 0.177345, Examples/second: 25.150930\n",
      "I0710 12:27:38.455972 140295626643200 trainer.py:508] step:  7805, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:78.971687 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3258314 log_pplx:2.8261685 loss:115.30769 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.42676\n",
      "I0710 12:27:41.547520 140295626643200 summary_utils.py:349] Steps/second: 0.177370, Examples/second: 25.160600\n",
      "I0710 12:27:41.548333 140295626643200 trainer.py:508] step:  7806, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:24.915663 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3270179 log_pplx:3.2549 loss:50.425518 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.43427\n",
      "I0710 12:27:47.847277 140295626643200 summary_utils.py:349] Steps/second: 0.177364, Examples/second: 25.156342\n",
      "I0710 12:27:47.848050 140295626643200 trainer.py:508] step:  7807, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:211.23302 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.325734 log_pplx:2.9344594 loss:119.46918 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.44153\n",
      "I0710 12:27:51.656161 140295626643200 summary_utils.py:349] Steps/second: 0.177381, Examples/second: 25.159822\n",
      "I0710 12:27:51.656923 140295626643200 trainer.py:508] step:  7808, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:45.262302 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3273528 log_pplx:3.2862704 loss:81.704895 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.44751\n",
      "I0710 12:27:57.847735 140295626643200 summary_utils.py:349] Steps/second: 0.177376, Examples/second: 25.155714\n",
      "I0710 12:27:57.848599 140295626643200 trainer.py:508] step:  7809, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:168.8575 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3257545 log_pplx:2.9622328 loss:123.52511 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.45343\n",
      "I0710 12:28:01.719062 140295626643200 summary_utils.py:349] Steps/second: 0.177393, Examples/second: 25.159108\n",
      "I0710 12:28:01.719978 140295626643200 trainer.py:508] step:  7810, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:58.288113 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.32776 log_pplx:3.2122221 loss:81.73098 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.4588\n",
      "I0710 12:28:08.039991 140295626643200 summary_utils.py:349] Steps/second: 0.177386, Examples/second: 25.154826\n",
      "I0710 12:28:08.040774 140295626643200 trainer.py:508] step:  7811, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:85.568405 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.326098 log_pplx:2.8094869 loss:116.55857 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.4646\n",
      "I0710 12:28:19.061333 140295626643200 summary_utils.py:349] Steps/second: 0.177335, Examples/second: 25.141980\n",
      "I0710 12:28:19.062165 140295626643200 trainer.py:508] step:  7812, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:735.10144 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3247257 log_pplx:2.6511927 loss:190.15678 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.47083\n",
      "I0710 12:28:22.155033 140295626643200 summary_utils.py:349] Steps/second: 0.177359, Examples/second: 25.151628\n",
      "I0710 12:28:22.156062 140295626643200 trainer.py:508] step:  7813, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:43.722015 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3273566 log_pplx:3.2928634 loss:52.608639 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.47632\n",
      "I0710 12:28:28.736482 140295626643200 summary_utils.py:349] Steps/second: 0.177350, Examples/second: 25.146999\n",
      "I0710 12:28:28.737286 140295626643200 trainer.py:508] step:  7814, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:235.11052 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3257695 log_pplx:3.0872979 loss:122.44995 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.48218\n",
      "I0710 12:28:32.601449 140295626643200 summary_utils.py:349] Steps/second: 0.177367, Examples/second: 25.150398\n",
      "I0710 12:28:32.602257 140295626643200 trainer.py:508] step:  7815, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:73.882881 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3274255 log_pplx:3.2588127 loss:81.612892 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.48914\n",
      "I0710 12:28:38.848426 140295626643200 summary_utils.py:349] Steps/second: 0.177361, Examples/second: 25.146227\n",
      "I0710 12:28:38.849174 140295626643200 trainer.py:508] step:  7816, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:311.32214 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3258762 log_pplx:2.9774551 loss:118.53996 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.49658\n",
      "I0710 12:28:44.779289 140295626643200 summary_utils.py:349] Steps/second: 0.177358, Examples/second: 25.142488\n",
      "I0710 12:28:44.780056 140295626643200 trainer.py:508] step:  7817, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:228.46985 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3255178 log_pplx:2.9821093 loss:115.25852 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.50452\n",
      "I0710 12:28:47.790452 140295626643200 summary_utils.py:349] Steps/second: 0.177384, Examples/second: 25.152235\n",
      "I0710 12:28:47.791424 140295626643200 trainer.py:508] step:  7818, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:83.358902 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.327155 log_pplx:3.4647288 loss:53.446148 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.51245\n",
      "I0710 12:28:54.172645 140295626643200 summary_utils.py:349] Steps/second: 0.177376, Examples/second: 25.147883\n",
      "I0710 12:28:54.173415 140295626643200 trainer.py:508] step:  7819, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:135.83138 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3262104 log_pplx:2.9654686 loss:119.39718 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.51984\n",
      "I0710 12:28:58.013879 140295626643200 summary_utils.py:349] Steps/second: 0.177394, Examples/second: 25.151309\n",
      "I0710 12:28:58.014717 140295626643200 trainer.py:508] step:  7820, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:135.79877 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3275436 log_pplx:3.4864526 loss:87.793228 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.52722\n",
      "I0710 12:29:00.199195 140295626643200 summary_utils.py:349] Steps/second: 0.177427, Examples/second: 25.176002\n",
      "I0710 12:29:00.199960 140295626643200 trainer.py:508] step:  7821, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:12.46758 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3256437 log_pplx:3.3598883 loss:24.15576 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:532.53363\n",
      "I0710 12:29:06.356036 140295626643200 summary_utils.py:349] Steps/second: 0.177422, Examples/second: 25.171951\n",
      "I0710 12:29:06.356985 140295626643200 trainer.py:508] step:  7822, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:102.80415 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3263537 log_pplx:2.9574594 loss:118.70503 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.53986\n",
      "I0710 12:29:17.601218 140295626643200 summary_utils.py:349] Steps/second: 0.177368, Examples/second: 25.158831\n",
      "I0710 12:29:17.602115 140295626643200 trainer.py:508] step:  7823, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:342.62042 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3250353 log_pplx:2.6335409 loss:190.5367 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.54639\n",
      "I0710 12:29:20.711389 140295626643200 summary_utils.py:349] Steps/second: 0.177392, Examples/second: 25.168424\n",
      "I0710 12:29:20.712161 140295626643200 trainer.py:508] step:  7824, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:29.786955 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3273648 log_pplx:3.2693305 loss:51.325935 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.55408\n",
      "I0710 12:29:24.569637 140295626643200 summary_utils.py:349] Steps/second: 0.177409, Examples/second: 25.171817\n",
      "I0710 12:29:24.570410 140295626643200 trainer.py:508] step:  7825, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:53.172241 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3278756 log_pplx:3.2984285 loss:82.172096 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.56158\n",
      "I0710 12:29:31.205523 140295626643200 summary_utils.py:349] Steps/second: 0.177400, Examples/second: 25.167123\n",
      "I0710 12:29:31.206328 140295626643200 trainer.py:508] step:  7826, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:86.388931 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3261981 log_pplx:2.9072638 loss:118.18027 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.56879\n",
      "I0710 12:29:37.164996 140295626643200 summary_utils.py:349] Steps/second: 0.177397, Examples/second: 25.163349\n",
      "I0710 12:29:37.166125 140295626643200 trainer.py:508] step:  7827, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:97.720886 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3258606 log_pplx:2.8763845 loss:108.79924 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.57587\n",
      "I0710 12:29:41.117470 140295626643200 summary_utils.py:349] Steps/second: 0.177413, Examples/second: 25.166613\n",
      "I0710 12:29:41.118296 140295626643200 trainer.py:508] step:  7828, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:64.422356 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3279023 log_pplx:3.3094738 loss:83.005737 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.5827\n",
      "I0710 12:29:47.248835 140295626643200 summary_utils.py:349] Steps/second: 0.177408, Examples/second: 25.162609\n",
      "I0710 12:29:47.249625 140295626643200 trainer.py:508] step:  7829, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:75.06456 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3264862 log_pplx:2.8644209 loss:113.89655 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.58905\n",
      "I0710 12:29:50.337015 140295626643200 summary_utils.py:349] Steps/second: 0.177432, Examples/second: 25.172215\n",
      "I0710 12:29:50.337794 140295626643200 trainer.py:508] step:  7830, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:35.367386 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3277028 log_pplx:3.310879 loss:52.314476 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.59503\n",
      "2020-07-10 12:29:50.552410: I lingvo/core/ops/record_yielder.cc:532] Epoch 11: total records 46838\n",
      "2020-07-10 12:29:50.552482: I lingvo/core/ops/record_yielder.cc:485] Epoch 11 /tmp/punctuator_data/train.txt\n",
      "I0710 12:29:56.767785 140295626643200 summary_utils.py:349] Steps/second: 0.177425, Examples/second: 25.167805\n",
      "I0710 12:29:56.768588 140295626643200 trainer.py:508] step:  7831, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:80.492233 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3261473 log_pplx:2.87552 loss:114.69729 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.60083\n",
      "I0710 12:30:03.340973 140295626643200 summary_utils.py:349] Steps/second: 0.177416, Examples/second: 25.163205\n",
      "I0710 12:30:03.341789 140295626643200 trainer.py:508] step:  7832, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:95.778557 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.326629 log_pplx:2.9694755 loss:124.12407 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.60632\n",
      "I0710 12:30:07.460322 140295626643200 summary_utils.py:349] Steps/second: 0.177430, Examples/second: 25.166238\n",
      "I0710 12:30:07.461204 140295626643200 trainer.py:508] step:  7833, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:44.052818 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3286572 log_pplx:3.3004186 loss:83.005531 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.61139\n",
      "I0710 12:30:15.988557 140295626643200 summary_utils.py:349] Steps/second: 0.177403, Examples/second: 25.156842\n",
      "I0710 12:30:15.989360 140295626643200 trainer.py:508] step:  7834, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:133.96414 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.325274 log_pplx:2.3322384 loss:159.4668 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.61658\n",
      "I0710 12:30:22.307352 140295626643200 summary_utils.py:349] Steps/second: 0.177396, Examples/second: 25.152595\n",
      "I0710 12:30:22.308160 140295626643200 trainer.py:508] step:  7835, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:65.020859 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3262565 log_pplx:2.7953796 loss:109.15956 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.62158\n",
      "I0710 12:30:28.870234 140295626643200 summary_utils.py:349] Steps/second: 0.177387, Examples/second: 25.148021\n",
      "I0710 12:30:28.871036 140295626643200 trainer.py:508] step:  7836, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:72.978653 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3267778 log_pplx:2.6869769 loss:110.03172 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.62659\n",
      "I0710 12:30:31.970577 140295626643200 summary_utils.py:349] Steps/second: 0.177411, Examples/second: 25.157592\n",
      "I0710 12:30:31.971505 140295626643200 base_runner.py:111] step:  7837, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:26.701767 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3280261 log_pplx:3.2428861 loss:50.454746 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.63208\n",
      "I0710 12:30:35.917505 140295626643200 summary_utils.py:349] Steps/second: 0.177428, Examples/second: 25.160855\n",
      "I0710 12:30:35.918320 140295626643200 trainer.py:508] step:  7838, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:38.062531 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3286992 log_pplx:3.1364832 loss:79.882301 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.63745\n",
      "I0710 12:30:42.175809 140295626643200 summary_utils.py:349] Steps/second: 0.177422, Examples/second: 25.156692\n",
      "I0710 12:30:42.176559 140295626643200 trainer.py:508] step:  7839, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:70.207375 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3265566 log_pplx:2.6910272 loss:105.35372 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.64264\n",
      "I0710 12:30:48.459338 140295626643200 summary_utils.py:349] Steps/second: 0.177415, Examples/second: 25.152499\n",
      "I0710 12:30:48.460142 140295626643200 trainer.py:508] step:  7840, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:72.432686 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3266029 log_pplx:2.7110348 loss:107.42476 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.64795\n",
      "I0710 12:30:52.340761 140295626643200 summary_utils.py:349] Steps/second: 0.177432, Examples/second: 25.155848\n",
      "I0710 12:30:52.341524 140295626643200 trainer.py:508] step:  7841, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:38.921818 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3282125 log_pplx:3.1021695 loss:77.050133 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.65314\n",
      "I0710 12:30:54.572856 140295626643200 summary_utils.py:349] Steps/second: 0.177465, Examples/second: 25.180326\n",
      "I0710 12:30:54.573641 140295626643200 trainer.py:508] step:  7842, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:11.508904 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3269986 log_pplx:3.3217318 loss:24.809185 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:532.65808\n",
      "I0710 12:31:00.789482 140295626643200 summary_utils.py:349] Steps/second: 0.177459, Examples/second: 25.176217\n",
      "I0710 12:31:00.790395 140295626643200 trainer.py:508] step:  7843, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:62.100262 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.326509 log_pplx:2.677536 loss:108.37325 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.66278\n",
      "I0710 12:31:03.832936 140295626643200 summary_utils.py:349] Steps/second: 0.177484, Examples/second: 25.185844\n",
      "I0710 12:31:03.833755 140295626643200 trainer.py:508] step:  7844, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:28.521421 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3279908 log_pplx:3.2024791 loss:49.600903 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.66766\n",
      "I0710 12:31:10.161614 140295626643200 summary_utils.py:349] Steps/second: 0.177477, Examples/second: 25.181585\n",
      "I0710 12:31:10.162433 140295626643200 trainer.py:508] step:  7845, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:64.950661 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.32688 log_pplx:2.6542342 loss:107.59602 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.67212\n",
      "I0710 12:31:19.177019 140295626643200 summary_utils.py:349] Steps/second: 0.177445, Examples/second: 25.171556\n",
      "I0710 12:31:19.177816 140295626643200 trainer.py:508] step:  7846, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:153.69826 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3254845 log_pplx:2.2606778 loss:159.20822 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.67682\n",
      "I0710 12:31:25.520789 140295626643200 summary_utils.py:349] Steps/second: 0.177438, Examples/second: 25.167284\n",
      "I0710 12:31:25.521845 140295626643200 trainer.py:508] step:  7847, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:68.013077 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3270363 log_pplx:2.6921935 loss:108.89922 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.68176\n",
      "I0710 12:31:29.344837 140295626643200 summary_utils.py:349] Steps/second: 0.177455, Examples/second: 25.170702\n",
      "I0710 12:31:29.345645 140295626643200 trainer.py:508] step:  7848, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:39.609928 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.328714 log_pplx:3.117346 loss:78.342812 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.68719\n",
      "I0710 12:31:35.751916 140295626643200 summary_utils.py:349] Steps/second: 0.177448, Examples/second: 25.166347\n",
      "I0710 12:31:35.752749 140295626643200 trainer.py:508] step:  7849, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:67.216515 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3266126 log_pplx:2.6660073 loss:104.34086 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.6925\n",
      "I0710 12:31:38.855404 140295626643200 summary_utils.py:349] Steps/second: 0.177472, Examples/second: 25.175877\n",
      "I0710 12:31:38.856287 140295626643200 trainer.py:508] step:  7850, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:20.424698 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3281945 log_pplx:3.2005861 loss:50.109177 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.698\n",
      "I0710 12:31:45.148325 140295626643200 summary_utils.py:349] Steps/second: 0.177466, Examples/second: 25.171677\n",
      "I0710 12:31:45.149093 140295626643200 trainer.py:508] step:  7851, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:66.653641 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3270018 log_pplx:2.6876459 loss:109.25281 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.70325\n",
      "I0710 12:31:48.876453 140295626643200 summary_utils.py:349] Steps/second: 0.177484, Examples/second: 25.175219\n",
      "I0710 12:31:48.877240 140295626643200 trainer.py:508] step:  7852, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:35.628994 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3285313 log_pplx:3.1294646 loss:77.884544 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.70862\n",
      "I0710 12:31:55.191254 140295626643200 summary_utils.py:349] Steps/second: 0.177478, Examples/second: 25.170992\n",
      "I0710 12:31:55.191995 140295626643200 trainer.py:508] step:  7853, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:66.4561 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3268493 log_pplx:2.6873033 loss:105.61102 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.71368\n",
      "I0710 12:31:59.747266 140295635035904 trainer.py:345] Write summary @7853\n",
      "I0710 12:32:04.873956 140295626643200 summary_utils.py:349] Steps/second: 0.177439, Examples/second: 25.162234\n",
      "I0710 12:32:04.875714 140295626643200 trainer.py:508] step:  7854, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:62.341015 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3272042 log_pplx:2.6547589 loss:103.86743 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.71863\n",
      "I0710 12:32:09.013527 140295626643200 summary_utils.py:349] Steps/second: 0.177453, Examples/second: 25.170355\n",
      "I0710 12:32:09.014968 140295626643200 trainer.py:508] step:  7855, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:27.255415 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3286563 log_pplx:3.2117722 loss:49.744831 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.72339\n",
      "I0710 12:32:14.578585 140295626643200 summary_utils.py:349] Steps/second: 0.177454, Examples/second: 25.171420\n",
      "I0710 12:32:14.579800 140295626643200 trainer.py:508] step:  7856, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:35.068569 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3285514 log_pplx:3.1371784 loss:78.252998 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.72784\n",
      "I0710 12:32:29.254795 140295626643200 summary_utils.py:349] Steps/second: 0.177368, Examples/second: 25.153821\n",
      "I0710 12:32:29.256829 140295626643200 trainer.py:508] step:  7857, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:127.69901 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3260056 log_pplx:2.2210677 loss:162.08241 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.73218\n",
      "I0710 12:32:39.243377 140295626643200 summary_utils.py:349] Steps/second: 0.177327, Examples/second: 25.144677\n",
      "I0710 12:32:39.244466 140295626643200 trainer.py:508] step:  7858, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:64.067276 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3270222 log_pplx:2.6338651 loss:105.42046 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.73688\n",
      "I0710 12:32:45.733045 140295635035904 trainer.py:354] Write summary done: step 7853\n",
      "I0710 12:32:48.738110 140295626643200 summary_utils.py:349] Steps/second: 0.177291, Examples/second: 25.136205\n",
      "I0710 12:32:48.738888 140295626643200 trainer.py:508] step:  7859, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:68.543861 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3274403 log_pplx:2.7423048 loss:112.91441 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.74164\n",
      "I0710 12:32:52.603157 140295626643200 summary_utils.py:349] Steps/second: 0.177307, Examples/second: 25.139558\n",
      "I0710 12:32:52.603951 140295626643200 trainer.py:508] step:  7860, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:34.521065 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3290535 log_pplx:3.0830765 loss:77.211807 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.7467\n",
      "I0710 12:32:58.946610 140295626643200 summary_utils.py:349] Steps/second: 0.177301, Examples/second: 25.135319\n",
      "I0710 12:32:58.947439 140295626643200 trainer.py:508] step:  7861, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:100.17696 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3271866 log_pplx:2.6697493 loss:107.05695 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.75153\n",
      "I0710 12:33:02.031855 140295626643200 summary_utils.py:349] Steps/second: 0.177325, Examples/second: 25.144836\n",
      "I0710 12:33:02.032616 140295626643200 trainer.py:508] step:  7862, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:21.970644 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3284823 log_pplx:3.1474745 loss:48.847332 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.75604\n",
      "I0710 12:33:11.325855 140295626643200 summary_utils.py:349] Steps/second: 0.177290, Examples/second: 25.134511\n",
      "I0710 12:33:11.326649 140295626643200 trainer.py:508] step:  7863, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:126.12761 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3262403 log_pplx:2.1983085 loss:157.45384 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.76056\n",
      "I0710 12:33:17.511234 140295626643200 summary_utils.py:349] Steps/second: 0.177285, Examples/second: 25.130490\n",
      "I0710 12:33:17.512171 140295626643200 trainer.py:508] step:  7864, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:61.609779 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3270891 log_pplx:2.6009836 loss:105.43738 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.76489\n",
      "I0710 12:33:21.282611 140295626643200 summary_utils.py:349] Steps/second: 0.177303, Examples/second: 25.133965\n",
      "I0710 12:33:21.283408 140295626643200 trainer.py:508] step:  7865, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:48.293827 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3286185 log_pplx:3.0379708 loss:75.474586 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.76923\n",
      "I0710 12:33:27.147333 140295626643200 summary_utils.py:349] Steps/second: 0.177301, Examples/second: 25.130375\n",
      "I0710 12:33:27.148125 140295626643200 trainer.py:508] step:  7866, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:62.278393 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3274162 log_pplx:2.6821418 loss:107.62095 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.77338\n",
      "I0710 12:33:30.212740 140295626643200 summary_utils.py:349] Steps/second: 0.177325, Examples/second: 25.139905\n",
      "I0710 12:33:30.213519 140295626643200 trainer.py:508] step:  7867, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:22.452507 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3287649 log_pplx:3.1637068 loss:50.668743 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.77771\n",
      "I0710 12:33:36.656992 140295626643200 summary_utils.py:349] Steps/second: 0.177317, Examples/second: 25.135540\n",
      "I0710 12:33:36.657766 140295626643200 trainer.py:508] step:  7868, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:67.373604 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3278632 log_pplx:2.6731358 loss:112.43876 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.78186\n",
      "I0710 12:33:38.860492 140295626643200 summary_utils.py:349] Steps/second: 0.177350, Examples/second: 25.159843\n",
      "I0710 12:33:38.861311 140295626643200 trainer.py:508] step:  7869, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:12.195983 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3270918 log_pplx:3.3333089 loss:24.023262 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:532.78625\n",
      "I0710 12:33:42.685668 140295626643200 summary_utils.py:349] Steps/second: 0.177367, Examples/second: 25.163236\n",
      "I0710 12:33:42.686403 140295626643200 trainer.py:508] step:  7870, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:36.733948 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3291962 log_pplx:3.0619123 loss:77.007095 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.79041\n",
      "I0710 12:33:48.927850 140295626643200 summary_utils.py:349] Steps/second: 0.177361, Examples/second: 25.159136\n",
      "I0710 12:33:48.928626 140295626643200 trainer.py:508] step:  7871, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:67.17894 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3276957 log_pplx:2.6887479 loss:109.49927 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.79431\n",
      "I0710 12:33:55.217412 140295626643200 summary_utils.py:349] Steps/second: 0.177355, Examples/second: 25.154975\n",
      "I0710 12:33:55.218242 140295626643200 trainer.py:508] step:  7872, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:62.687164 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3275281 log_pplx:2.6160934 loss:106.6058 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.79834\n",
      "I0710 12:33:59.090801 140295626643200 summary_utils.py:349] Steps/second: 0.177372, Examples/second: 25.158302\n",
      "I0710 12:33:59.091572 140295626643200 trainer.py:508] step:  7873, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:36.447666 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3295568 log_pplx:3.1231055 loss:79.9515 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.80249\n",
      "I0710 12:34:02.126649 140295626643200 summary_utils.py:349] Steps/second: 0.177396, Examples/second: 25.167851\n",
      "I0710 12:34:02.127421 140295626643200 trainer.py:508] step:  7874, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:22.85618 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3288317 log_pplx:3.1669977 loss:49.843102 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.8064\n",
      "I0710 12:34:08.433358 140295626643200 summary_utils.py:349] Steps/second: 0.177390, Examples/second: 25.163667\n",
      "I0710 12:34:08.434411 140295626643200 trainer.py:508] step:  7875, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:62.668285 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3273251 log_pplx:2.6440024 loss:104.5703 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.81006\n",
      "I0710 12:34:19.120626 140295626643200 summary_utils.py:349] Steps/second: 0.177342, Examples/second: 25.151508\n",
      "I0710 12:34:19.121494 140295626643200 trainer.py:508] step:  7876, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:139.13649 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3264884 log_pplx:2.2903821 loss:165.99545 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.81378\n",
      "I0710 12:34:25.308962 140295626643200 summary_utils.py:349] Steps/second: 0.177337, Examples/second: 25.147492\n",
      "I0710 12:34:25.309714 140295626643200 trainer.py:508] step:  7877, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:63.196075 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3276137 log_pplx:2.7020767 loss:109.94075 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.81854\n",
      "I0710 12:34:29.092522 140295626643200 summary_utils.py:349] Steps/second: 0.177355, Examples/second: 25.150934\n",
      "I0710 12:34:29.093486 140295626643200 trainer.py:508] step:  7878, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:35.065285 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3291948 log_pplx:3.0609128 loss:77.135002 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.82367\n",
      "I0710 12:34:35.268117 140295626643200 summary_utils.py:349] Steps/second: 0.177350, Examples/second: 25.146937\n",
      "I0710 12:34:35.268880 140295626643200 trainer.py:508] step:  7879, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:63.826611 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.327606 log_pplx:2.5857778 loss:103.68967 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.82874\n",
      "I0710 12:34:35.835503 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 12:34:38.655884 140295626643200 summary_utils.py:349] Steps/second: 0.177371, Examples/second: 25.156000\n",
      "I0710 12:34:38.656692 140295626643200 trainer.py:508] step:  7880, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:23.228701 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3286897 log_pplx:3.0908625 loss:47.546158 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.83356\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 12:34:41.528356 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 12:34:41.997010 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00007879\n",
      "I0710 12:34:45.453168 140295626643200 summary_utils.py:349] Steps/second: 0.177360, Examples/second: 25.151174\n",
      "I0710 12:34:45.453951 140295626643200 trainer.py:508] step:  7881, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:68.868607 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3278863 log_pplx:2.6384962 loss:108.47516 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.83826\n",
      "I0710 12:34:49.274996 140295626643200 summary_utils.py:349] Steps/second: 0.177377, Examples/second: 25.154561\n",
      "I0710 12:34:49.275777 140295626643200 trainer.py:508] step:  7882, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:34.580959 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3295094 log_pplx:3.1189027 loss:77.719162 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.84271\n",
      "I0710 12:34:55.657161 140295626643200 summary_utils.py:349] Steps/second: 0.177370, Examples/second: 25.150291\n",
      "I0710 12:34:55.657982 140295626643200 trainer.py:508] step:  7883, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:64.911163 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3276131 log_pplx:2.689409 loss:107.44191 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.84711\n",
      "I0710 12:35:04.581924 140295626643200 summary_utils.py:349] Steps/second: 0.177339, Examples/second: 25.140518\n",
      "I0710 12:35:04.582740 140295626643200 trainer.py:508] step:  7884, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:120.25186 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3266546 log_pplx:2.1929858 loss:155.20857 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.85114\n",
      "I0710 12:35:11.125919 140295626643200 summary_utils.py:349] Steps/second: 0.177331, Examples/second: 25.136041\n",
      "I0710 12:35:11.126687 140295626643200 trainer.py:508] step:  7885, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:62.438442 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3275472 log_pplx:2.5752554 loss:103.94375 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.85492\n",
      "I0710 12:35:14.185113 140295626643200 summary_utils.py:349] Steps/second: 0.177355, Examples/second: 25.145526\n",
      "I0710 12:35:14.185934 140295626643200 trainer.py:508] step:  7886, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:21.143476 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3289874 log_pplx:3.1597364 loss:48.97591 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.85889\n",
      "I0710 12:35:18.274435 140295626643200 summary_utils.py:349] Steps/second: 0.177369, Examples/second: 25.148553\n",
      "I0710 12:35:18.275268 140295626643200 trainer.py:508] step:  7887, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:42.630062 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.329443 log_pplx:3.1302471 loss:77.845337 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.86298\n",
      "I0710 12:35:24.963957 140295626643200 summary_utils.py:349] Steps/second: 0.177360, Examples/second: 25.143883\n",
      "I0710 12:35:24.964811 140295626643200 trainer.py:508] step:  7888, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:72.836121 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3277154 log_pplx:2.6674602 loss:105.26465 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.86731\n",
      "I0710 12:35:31.399387 140295626643200 summary_utils.py:349] Steps/second: 0.177352, Examples/second: 25.139555\n",
      "I0710 12:35:31.400184 140295626643200 trainer.py:508] step:  7889, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:61.141235 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3272949 log_pplx:2.5608001 loss:100.60744 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.87201\n",
      "I0710 12:35:33.608400 140295626643200 summary_utils.py:349] Steps/second: 0.177384, Examples/second: 25.163703\n",
      "I0710 12:35:33.609244 140295626643200 trainer.py:508] step:  7890, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:11.440793 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3274869 log_pplx:3.2443936 loss:24.02879 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:532.87671\n",
      "I0710 12:35:37.571440 140295626643200 summary_utils.py:349] Steps/second: 0.177400, Examples/second: 25.166891\n",
      "I0710 12:35:37.572229 140295626643200 trainer.py:508] step:  7891, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:42.732014 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3294965 log_pplx:3.0008781 loss:75.322037 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.88104\n",
      "I0710 12:35:40.675157 140295626643200 summary_utils.py:349] Steps/second: 0.177424, Examples/second: 25.176299\n",
      "I0710 12:35:40.675976 140295626643200 trainer.py:508] step:  7892, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:21.621361 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3292238 log_pplx:3.1003356 loss:48.733391 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.88501\n",
      "I0710 12:35:47.248643 140295626643200 summary_utils.py:349] Steps/second: 0.177415, Examples/second: 25.171779\n",
      "I0710 12:35:47.249444 140295626643200 trainer.py:508] step:  7893, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:69.933914 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3279668 log_pplx:2.6503568 loss:107.77013 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.88861\n",
      "I0710 12:35:53.459138 140295626643200 summary_utils.py:349] Steps/second: 0.177410, Examples/second: 25.167745\n",
      "I0710 12:35:53.459931 140295626643200 trainer.py:508] step:  7894, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:68.272324 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3279954 log_pplx:2.6575325 loss:107.09856 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.89209\n",
      "I0710 12:36:03.083310 140295626643200 summary_utils.py:349] Steps/second: 0.177372, Examples/second: 25.157064\n",
      "I0710 12:36:03.084188 140295626643200 trainer.py:508] step:  7895, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:128.96724 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3268495 log_pplx:2.2286463 loss:158.17818 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.89575\n",
      "I0710 12:36:09.574986 140295626643200 summary_utils.py:349] Steps/second: 0.177364, Examples/second: 25.152665\n",
      "I0710 12:36:09.576021 140295626643200 trainer.py:508] step:  7896, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:62.420502 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3277695 log_pplx:2.662873 loss:105.91576 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.8996\n",
      "I0710 12:36:13.485021 140295626643200 summary_utils.py:349] Steps/second: 0.177380, Examples/second: 25.155919\n",
      "I0710 12:36:13.485835 140295626643200 trainer.py:508] step:  7897, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:40.032566 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3295939 log_pplx:3.0900147 loss:77.095871 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.90381\n",
      "I0710 12:36:16.505249 140295626643200 summary_utils.py:349] Steps/second: 0.177405, Examples/second: 25.165422\n",
      "I0710 12:36:16.506165 140295626643200 trainer.py:508] step:  7898, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:27.52269 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3291031 log_pplx:3.166827 loss:48.999226 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.90833\n",
      "I0710 12:36:23.077227 140295626643200 summary_utils.py:349] Steps/second: 0.177396, Examples/second: 25.160916\n",
      "I0710 12:36:23.078056 140295626643200 trainer.py:508] step:  7899, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:64.180824 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.328222 log_pplx:2.6477942 loss:107.16947 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.91284\n",
      "I0710 12:36:29.232331 140295626643200 summary_utils.py:349] Steps/second: 0.177391, Examples/second: 25.156967\n",
      "I0710 12:36:29.233091 140295626643200 trainer.py:508] step:  7900, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:63.416943 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3280267 log_pplx:2.6568503 loss:108.1006 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.9176\n",
      "I0710 12:36:33.074604 140295626643200 summary_utils.py:349] Steps/second: 0.177408, Examples/second: 25.160307\n",
      "I0710 12:36:33.075576 140295626643200 trainer.py:508] step:  7901, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:36.230953 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3301617 log_pplx:3.148428 loss:80.284912 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.92249\n",
      "I0710 12:36:39.500228 140295626643200 summary_utils.py:349] Steps/second: 0.177401, Examples/second: 25.156002\n",
      "I0710 12:36:39.501031 140295626643200 trainer.py:508] step:  7902, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:63.752518 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3280053 log_pplx:2.6400206 loss:105.93082 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.92706\n",
      "I0710 12:36:42.579867 140295626643200 summary_utils.py:349] Steps/second: 0.177425, Examples/second: 25.165412\n",
      "I0710 12:36:42.580657 140295626643200 trainer.py:508] step:  7903, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:23.649857 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3290882 log_pplx:3.0960937 loss:48.001549 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.93164\n",
      "I0710 12:36:48.776280 140295626643200 summary_utils.py:349] Steps/second: 0.177419, Examples/second: 25.161411\n",
      "I0710 12:36:48.777333 140295626643200 trainer.py:508] step:  7904, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:61.971794 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3283714 log_pplx:2.6448083 loss:105.46172 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.93579\n",
      "I0710 12:36:52.709044 140295626643200 summary_utils.py:349] Steps/second: 0.177435, Examples/second: 25.164627\n",
      "I0710 12:36:52.709816 140295626643200 trainer.py:508] step:  7905, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:39.190292 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3299247 log_pplx:3.0678484 loss:76.715378 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.94006\n",
      "I0710 12:37:02.693174 140295626643200 summary_utils.py:349] Steps/second: 0.177395, Examples/second: 25.153504\n",
      "I0710 12:37:02.693966 140295626643200 trainer.py:508] step:  7906, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:155.79375 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3270756 log_pplx:2.2403369 loss:163.20854 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.94409\n",
      "I0710 12:37:09.032680 140295626643200 summary_utils.py:349] Steps/second: 0.177388, Examples/second: 25.149321\n",
      "I0710 12:37:09.033455 140295626643200 trainer.py:508] step:  7907, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:65.297005 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.327828 log_pplx:2.6283028 loss:101.5839 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.94836\n",
      "I0710 12:37:15.729272 140295626643200 summary_utils.py:349] Steps/second: 0.177378, Examples/second: 25.144669\n",
      "I0710 12:37:15.730363 140295626643200 trainer.py:508] step:  7908, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:66.150795 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3285464 log_pplx:2.6019938 loss:109.44637 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.95288\n",
      "I0710 12:37:18.864852 140295626643200 summary_utils.py:349] Steps/second: 0.177402, Examples/second: 25.153989\n",
      "I0710 12:37:18.865656 140295626643200 trainer.py:508] step:  7909, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:21.141161 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3295326 log_pplx:3.152494 loss:48.925228 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.95764\n",
      "I0710 12:37:22.786064 140295626643200 summary_utils.py:349] Steps/second: 0.177418, Examples/second: 25.157217\n",
      "I0710 12:37:22.786843 140295626643200 trainer.py:508] step:  7910, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:37.571152 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.330317 log_pplx:3.0695226 loss:78.292007 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.96222\n",
      "I0710 12:37:29.155765 140295626643200 summary_utils.py:349] Steps/second: 0.177411, Examples/second: 25.152998\n",
      "I0710 12:37:29.156799 140295626643200 trainer.py:508] step:  7911, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:68.152061 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3284787 log_pplx:2.6158068 loss:107.21539 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.96667\n",
      "I0710 12:37:35.575825 140295626643200 summary_utils.py:349] Steps/second: 0.177403, Examples/second: 25.148714\n",
      "I0710 12:37:35.576600 140295626643200 trainer.py:508] step:  7912, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:64.781334 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3282027 log_pplx:2.5583167 loss:104.28339 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.97101\n",
      "I0710 12:37:39.413539 140295626643200 summary_utils.py:349] Steps/second: 0.177420, Examples/second: 25.152051\n",
      "I0710 12:37:39.414324 140295626643200 trainer.py:508] step:  7913, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:35.490406 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3300897 log_pplx:3.0509028 loss:75.73867 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:532.97565\n",
      "I0710 12:37:50.099346 140295626643200 summary_utils.py:349] Steps/second: 0.177373, Examples/second: 25.140035\n",
      "I0710 12:37:50.100162 140295626643200 trainer.py:508] step:  7914, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:139.86867 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3277093 log_pplx:2.2515533 loss:164.08194 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:532.98035\n",
      "I0710 12:37:52.275624 140295626643200 summary_utils.py:349] Steps/second: 0.177405, Examples/second: 25.164051\n",
      "I0710 12:37:52.276406 140295626643200 trainer.py:508] step:  7915, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:10.148509 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3281441 log_pplx:3.2063997 loss:23.784971 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:532.98499\n",
      "I0710 12:37:58.775937 140295626643200 summary_utils.py:349] Steps/second: 0.177397, Examples/second: 25.159663\n",
      "I0710 12:37:58.776726 140295626643200 trainer.py:508] step:  7916, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:68.994354 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3284978 log_pplx:2.6549823 loss:107.99141 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.98938\n",
      "I0710 12:38:01.858584 140295626643200 summary_utils.py:349] Steps/second: 0.177421, Examples/second: 25.169030\n",
      "I0710 12:38:01.859300 140295626643200 trainer.py:508] step:  7917, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:24.703817 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3302938 log_pplx:3.2376292 loss:51.043247 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:532.99432\n",
      "I0710 12:38:08.358992 140295626643200 summary_utils.py:349] Steps/second: 0.177413, Examples/second: 25.164643\n",
      "I0710 12:38:08.359776 140295626643200 trainer.py:508] step:  7918, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:70.192352 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.328564 log_pplx:2.5742867 loss:107.76608 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:532.99927\n",
      "I0710 12:38:12.262793 140295626643200 summary_utils.py:349] Steps/second: 0.177429, Examples/second: 25.167883\n",
      "I0710 12:38:12.263531 140295626643200 trainer.py:508] step:  7919, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:38.394032 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3302305 log_pplx:3.1246095 loss:78.798729 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.00421\n",
      "I0710 12:38:18.557429 140295626643200 summary_utils.py:349] Steps/second: 0.177423, Examples/second: 25.163770\n",
      "I0710 12:38:18.558431 140295626643200 trainer.py:508] step:  7920, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:63.695152 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.328582 log_pplx:2.5968685 loss:103.45274 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.00909\n",
      "I0710 12:38:24.577811 140295626643200 summary_utils.py:349] Steps/second: 0.177420, Examples/second: 25.160021\n",
      "I0710 12:38:24.578653 140295626643200 trainer.py:508] step:  7921, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:65.777802 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3287501 log_pplx:2.5906303 loss:106.7016 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.01392\n",
      "I0710 12:38:27.635081 140295626643200 summary_utils.py:349] Steps/second: 0.177444, Examples/second: 25.169409\n",
      "I0710 12:38:27.635826 140295626643200 trainer.py:508] step:  7922, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:24.674913 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3299239 log_pplx:3.2099273 loss:49.96703 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.01868\n",
      "I0710 12:38:31.444434 140295626643200 summary_utils.py:349] Steps/second: 0.177461, Examples/second: 25.172769\n",
      "I0710 12:38:31.445204 140295626643200 trainer.py:508] step:  7923, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:34.919918 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3303699 log_pplx:3.1693835 loss:78.977081 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.02307\n",
      "I0710 12:38:37.532380 140295626643200 summary_utils.py:349] Steps/second: 0.177456, Examples/second: 25.168932\n",
      "I0710 12:38:37.533150 140295626643200 trainer.py:508] step:  7924, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:65.856819 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.32855 log_pplx:2.5668995 loss:105.37123 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.02734\n",
      "I0710 12:38:43.502501 140295626643200 summary_utils.py:349] Steps/second: 0.177453, Examples/second: 25.165252\n",
      "I0710 12:38:43.503491 140295626643200 trainer.py:508] step:  7925, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:70.833588 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3285682 log_pplx:2.6111069 loss:103.23665 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.03186\n",
      "I0710 12:38:55.018128 140295626643200 summary_utils.py:349] Steps/second: 0.177399, Examples/second: 25.152176\n",
      "I0710 12:38:55.019103 140295626643200 trainer.py:508] step:  7926, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:127.76633 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.327574 log_pplx:2.2832191 loss:170.55647 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:533.0365\n",
      "I0710 12:38:58.856630 140295626643200 summary_utils.py:349] Steps/second: 0.177415, Examples/second: 25.155497\n",
      "I0710 12:38:58.857477 140295626643200 trainer.py:508] step:  7927, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:34.055756 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3304733 log_pplx:3.0959191 loss:78.442848 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.04132\n",
      "I0710 12:39:01.904004 140295626643200 summary_utils.py:349] Steps/second: 0.177439, Examples/second: 25.164881\n",
      "I0710 12:39:01.904765 140295626643200 trainer.py:508] step:  7928, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:20.853132 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3299836 log_pplx:3.0915399 loss:48.64344 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.04608\n",
      "I0710 12:39:07.902312 140295626643200 summary_utils.py:349] Steps/second: 0.177436, Examples/second: 25.161170\n",
      "I0710 12:39:07.903200 140295626643200 trainer.py:508] step:  7929, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:65.179489 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3285279 log_pplx:2.6304052 loss:103.27628 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.05066\n",
      "I0710 12:39:13.897766 140295626643200 summary_utils.py:349] Steps/second: 0.177433, Examples/second: 25.157465\n",
      "I0710 12:39:13.898580 140295626643200 trainer.py:508] step:  7930, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:66.165634 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3287185 log_pplx:2.6003768 loss:107.81813 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.05518\n",
      "I0710 12:39:17.762007 140295626643200 summary_utils.py:349] Steps/second: 0.177449, Examples/second: 25.160747\n",
      "I0710 12:39:17.762814 140295626643200 trainer.py:508] step:  7931, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:34.14114 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3306354 log_pplx:3.0563862 loss:77.479393 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.05988\n",
      "I0710 12:39:24.211303 140295626643200 summary_utils.py:349] Steps/second: 0.177441, Examples/second: 25.156448\n",
      "I0710 12:39:24.212331 140295626643200 trainer.py:508] step:  7932, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:61.120438 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3283302 log_pplx:2.6122148 loss:104.97838 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.06451\n",
      "I0710 12:39:27.261306 140295626643200 summary_utils.py:349] Steps/second: 0.177465, Examples/second: 25.165816\n",
      "I0710 12:39:27.262129 140295626643200 trainer.py:508] step:  7933, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:23.217522 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3301824 log_pplx:3.1678984 loss:49.58503 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.06909\n",
      "I0710 12:39:33.562014 140295626643200 summary_utils.py:349] Steps/second: 0.177459, Examples/second: 25.161712\n",
      "I0710 12:39:33.562761 140295626643200 trainer.py:508] step:  7934, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:68.731812 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3291583 log_pplx:2.6550534 loss:110.41703 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.07361\n",
      "I0710 12:39:37.499275 140295626643200 summary_utils.py:349] Steps/second: 0.177475, Examples/second: 25.164894\n",
      "I0710 12:39:37.500113 140295626643200 trainer.py:508] step:  7935, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:35.630013 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3307122 log_pplx:3.0856543 loss:77.681351 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.07843\n",
      "I0710 12:39:39.697316 140295626643200 summary_utils.py:349] Steps/second: 0.177507, Examples/second: 25.188744\n",
      "I0710 12:39:39.698282 140295626643200 trainer.py:508] step:  7936, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:9.8287325 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3283604 log_pplx:3.1717997 loss:23.23715 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:533.08319\n",
      "I0710 12:39:46.136068 140295626643200 summary_utils.py:349] Steps/second: 0.177499, Examples/second: 25.184453\n",
      "I0710 12:39:46.136808 140295626643200 base_runner.py:111] step:  7937, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:64.829826 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3290861 log_pplx:2.6384234 loss:109.29668 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.08759\n",
      "I0710 12:39:56.320874 140295626643200 summary_utils.py:349] Steps/second: 0.177457, Examples/second: 25.173157\n",
      "I0710 12:39:56.321746 140295626643200 trainer.py:508] step:  7938, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:141.01521 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3278602 log_pplx:2.2773647 loss:164.8812 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:533.09204\n",
      "I0710 12:40:02.739890 140295626643200 summary_utils.py:349] Steps/second: 0.177450, Examples/second: 25.168901\n",
      "I0710 12:40:02.740863 140295626643200 trainer.py:508] step:  7939, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:64.443481 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3288915 log_pplx:2.6163468 loss:106.68154 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.09595\n",
      "I0710 12:40:05.855401 140295626643200 summary_utils.py:349] Steps/second: 0.177473, Examples/second: 25.178163\n",
      "I0710 12:40:05.856172 140295626643200 trainer.py:508] step:  7940, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:31.337477 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3303753 log_pplx:3.1260326 loss:48.490139 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.10065\n",
      "I0710 12:40:09.737799 140295626643200 summary_utils.py:349] Steps/second: 0.177490, Examples/second: 25.181409\n",
      "I0710 12:40:09.738966 140295626643200 trainer.py:508] step:  7941, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:40.195148 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3308307 log_pplx:3.1115906 loss:78.023132 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.10504\n",
      "I0710 12:40:16.162114 140295626643200 summary_utils.py:349] Steps/second: 0.177482, Examples/second: 25.177146\n",
      "I0710 12:40:16.162902 140295626643200 trainer.py:508] step:  7942, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:70.628792 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3290846 log_pplx:2.6566203 loss:107.62632 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.10919\n",
      "I0710 12:40:22.627245 140295626643200 summary_utils.py:349] Steps/second: 0.177475, Examples/second: 25.172833\n",
      "I0710 12:40:22.628039 140295626643200 trainer.py:508] step:  7943, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:72.474136 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3290012 log_pplx:2.6665537 loss:105.79553 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.11395\n",
      "I0710 12:40:26.571416 140295626643200 summary_utils.py:349] Steps/second: 0.177490, Examples/second: 25.175996\n",
      "I0710 12:40:26.572532 140295626643200 trainer.py:508] step:  7944, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:36.047195 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3307861 log_pplx:3.0599179 loss:76.555321 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.1192\n",
      "I0710 12:40:29.791556 140295626643200 summary_utils.py:349] Steps/second: 0.177513, Examples/second: 25.185108\n",
      "I0710 12:40:29.792572 140295626643200 trainer.py:508] step:  7945, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:24.255749 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3302799 log_pplx:3.1110389 loss:47.892986 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.12439\n",
      "I0710 12:40:36.476181 140295626643200 summary_utils.py:349] Steps/second: 0.177503, Examples/second: 25.180507\n",
      "I0710 12:40:36.477228 140295626643200 trainer.py:508] step:  7946, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:63.946262 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3289065 log_pplx:2.6191788 loss:107.94292 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.12946\n",
      "I0710 12:40:45.592972 140295626643200 summary_utils.py:349] Steps/second: 0.177471, Examples/second: 25.170641\n",
      "I0710 12:40:45.593768 140295626643200 trainer.py:508] step:  7947, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:117.26676 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.327931 log_pplx:2.2149343 loss:157.1496 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:533.13477\n",
      "I0710 12:40:52.019049 140295626643200 summary_utils.py:349] Steps/second: 0.177463, Examples/second: 25.166388\n",
      "I0710 12:40:52.019847 140295626643200 trainer.py:508] step:  7948, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:65.881081 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3295724 log_pplx:2.6275856 loss:107.86239 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.14038\n",
      "I0710 12:40:55.852003 140295626643200 summary_utils.py:349] Steps/second: 0.177480, Examples/second: 25.169693\n",
      "I0710 12:40:55.852764 140295626643200 trainer.py:508] step:  7949, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:42.23386 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.331175 log_pplx:3.0845799 loss:78.425438 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.14587\n",
      "I0710 12:41:02.246085 140295626643200 summary_utils.py:349] Steps/second: 0.177473, Examples/second: 25.165485\n",
      "I0710 12:41:02.247168 140295626643200 trainer.py:508] step:  7950, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:68.958824 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3291584 log_pplx:2.565124 loss:102.57289 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.15082\n",
      "I0710 12:41:05.265590 140295626643200 summary_utils.py:349] Steps/second: 0.177497, Examples/second: 25.174844\n",
      "I0710 12:41:05.266366 140295626643200 trainer.py:508] step:  7951, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:23.771248 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3300887 log_pplx:3.1271315 loss:47.444443 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.1557\n",
      "I0710 12:41:11.715486 140295626643200 summary_utils.py:349] Steps/second: 0.177490, Examples/second: 25.170562\n",
      "I0710 12:41:11.716263 140295626643200 trainer.py:508] step:  7952, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:59.966595 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.329011 log_pplx:2.5724063 loss:101.70651 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.1604\n",
      "I0710 12:41:15.610028 140295626643200 summary_utils.py:349] Steps/second: 0.177506, Examples/second: 25.173783\n",
      "I0710 12:41:15.610803 140295626643200 trainer.py:508] step:  7953, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:37.614037 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3309659 log_pplx:3.0341446 loss:76.100143 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.1651\n",
      "I0710 12:41:16.191359 140295635035904 trainer.py:345] Write summary @7953\n",
      "I0710 12:41:26.642437 140295626643200 summary_utils.py:349] Steps/second: 0.177456, Examples/second: 25.163514\n",
      "I0710 12:41:26.643622 140295626643200 trainer.py:508] step:  7954, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:66.508102 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3297495 log_pplx:2.6174777 loss:112.19164 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.16974\n",
      "I0710 12:41:35.936310 140295626643200 summary_utils.py:349] Steps/second: 0.177422, Examples/second: 25.155527\n",
      "I0710 12:41:35.937440 140295626643200 trainer.py:508] step:  7955, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:65.481544 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.329182 log_pplx:2.5952334 loss:105.17184 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.17444\n",
      "I0710 12:41:50.826495 140295626643200 summary_utils.py:349] Steps/second: 0.177337, Examples/second: 25.138174\n",
      "I0710 12:41:50.827573 140295626643200 trainer.py:508] step:  7956, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:116.79419 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3283167 log_pplx:2.1371098 loss:151.57451 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:533.17957\n",
      "I0710 12:41:54.839376 140295626643200 summary_utils.py:349] Steps/second: 0.177352, Examples/second: 25.146218\n",
      "I0710 12:41:54.840642 140295626643200 trainer.py:508] step:  7957, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:35.261303 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3307999 log_pplx:3.1875679 loss:49.394855 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.18494\n",
      "I0710 12:42:00.110713 140295626643200 summary_utils.py:349] Steps/second: 0.177355, Examples/second: 25.147641\n",
      "I0710 12:42:00.111917 140295626643200 trainer.py:508] step:  7958, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:34.035381 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3309069 log_pplx:2.9736466 loss:73.653503 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.18976\n",
      "I0710 12:42:09.756233 140295626643200 summary_utils.py:349] Steps/second: 0.177318, Examples/second: 25.139217\n",
      "I0710 12:42:09.757094 140295626643200 trainer.py:508] step:  7959, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:68.38974 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3292466 log_pplx:2.6422393 loss:104.46754 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.19458\n",
      "I0710 12:42:20.217960 140295626643200 summary_utils.py:349] Steps/second: 0.177274, Examples/second: 25.129740\n",
      "I0710 12:42:20.219149 140295626643200 trainer.py:508] step:  7960, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:67.886108 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3295285 log_pplx:2.5962188 loss:107.90535 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.19983\n",
      "I0710 12:42:23.144987 140295626643200 summary_utils.py:349] Steps/second: 0.177299, Examples/second: 25.152443\n",
      "I0710 12:42:23.146979 140295626643200 trainer.py:508] step:  7961, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:10.34116 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3295834 log_pplx:3.2602537 loss:24.101683 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:533.2052\n",
      "I0710 12:42:29.196261 140295626643200 summary_utils.py:349] Steps/second: 0.177295, Examples/second: 25.152847\n",
      "I0710 12:42:29.198097 140295626643200 trainer.py:508] step:  7962, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:34.060963 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3312538 log_pplx:3.0482125 loss:76.643494 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.21027\n",
      "I0710 12:42:35.099060 140295635035904 trainer.py:354] Write summary done: step 7953\n",
      "I0710 12:42:38.378359 140295626643200 summary_utils.py:349] Steps/second: 0.177263, Examples/second: 25.145036\n",
      "I0710 12:42:38.379140 140295626643200 trainer.py:508] step:  7963, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:65.623085 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3293819 log_pplx:2.6452136 loss:106.56905 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.21515\n",
      "I0710 12:42:41.411228 140295626643200 summary_utils.py:349] Steps/second: 0.177287, Examples/second: 25.154334\n",
      "I0710 12:42:41.412107 140295626643200 trainer.py:508] step:  7964, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:20.854877 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3304113 log_pplx:3.0697906 loss:47.281971 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.22028\n",
      "I0710 12:42:47.885803 140295626643200 summary_utils.py:349] Steps/second: 0.177279, Examples/second: 25.150049\n",
      "I0710 12:42:47.886632 140295626643200 trainer.py:508] step:  7965, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:58.754105 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3295788 log_pplx:2.5891719 loss:102.0781 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.22534\n",
      "I0710 12:42:51.730599 140295626643200 summary_utils.py:349] Steps/second: 0.177296, Examples/second: 25.153322\n",
      "I0710 12:42:51.731407 140295626643200 trainer.py:508] step:  7966, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:36.183811 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3314717 log_pplx:3.0734763 loss:76.356674 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.23004\n",
      "I0710 12:42:57.943490 140295626643200 summary_utils.py:349] Steps/second: 0.177290, Examples/second: 25.149380\n",
      "I0710 12:42:57.944268 140295626643200 trainer.py:508] step:  7967, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:63.987259 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3297125 log_pplx:2.6362331 loss:108.90938 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.23444\n",
      "I0710 12:43:04.335464 140295626643200 summary_utils.py:349] Steps/second: 0.177283, Examples/second: 25.145207\n",
      "I0710 12:43:04.336258 140295626643200 trainer.py:508] step:  7968, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:62.156204 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3298318 log_pplx:2.6292081 loss:106.68011 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.23889\n",
      "I0710 12:43:07.414202 140295626643200 summary_utils.py:349] Steps/second: 0.177307, Examples/second: 25.154433\n",
      "I0710 12:43:07.414954 140295626643200 trainer.py:508] step:  7969, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:21.409754 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3311092 log_pplx:3.144388 loss:49.131065 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.24304\n",
      "I0710 12:43:17.017462 140295626643200 summary_utils.py:349] Steps/second: 0.177271, Examples/second: 25.144025\n",
      "I0710 12:43:17.018289 140295626643200 trainer.py:508] step:  7970, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:106.02953 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3285223 log_pplx:2.1561654 loss:149.58395 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:533.24701\n",
      "I0710 12:43:23.423531 140295626643200 summary_utils.py:349] Steps/second: 0.177264, Examples/second: 25.139840\n",
      "I0710 12:43:23.424344 140295626643200 trainer.py:508] step:  7971, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:67.570129 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3296423 log_pplx:2.5759146 loss:107.67323 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.2511\n",
      "I0710 12:43:27.252830 140295626643200 summary_utils.py:349] Steps/second: 0.177280, Examples/second: 25.143129\n",
      "I0710 12:43:27.253636 140295626643200 trainer.py:508] step:  7972, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:37.462036 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3316658 log_pplx:3.0675285 loss:77.665985 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.25513\n",
      "I0710 12:43:33.831807 140295626643200 summary_utils.py:349] Steps/second: 0.177272, Examples/second: 25.138722\n",
      "I0710 12:43:33.832617 140295626643200 trainer.py:508] step:  7973, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:60.982117 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3294507 log_pplx:2.6042788 loss:104.98497 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.25934\n",
      "I0710 12:43:40.303052 140295626643200 summary_utils.py:349] Steps/second: 0.177264, Examples/second: 25.134458\n",
      "I0710 12:43:40.304098 140295626643200 trainer.py:508] step:  7974, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:69.567261 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3297445 log_pplx:2.6642327 loss:106.46941 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.26398\n",
      "I0710 12:43:44.106843 140295626643200 summary_utils.py:349] Steps/second: 0.177281, Examples/second: 25.137778\n",
      "I0710 12:43:44.107668 140295626643200 trainer.py:508] step:  7975, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:37.076405 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3318191 log_pplx:3.0991263 loss:77.904282 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.26935\n",
      "I0710 12:43:47.127805 140295626643200 summary_utils.py:349] Steps/second: 0.177305, Examples/second: 25.147062\n",
      "I0710 12:43:47.128607 140295626643200 trainer.py:508] step:  7976, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:21.798441 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3310512 log_pplx:3.1275291 loss:48.440052 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.2749\n",
      "I0710 12:43:53.622292 140295626643200 summary_utils.py:349] Steps/second: 0.177297, Examples/second: 25.142768\n",
      "I0710 12:43:53.623113 140295626643200 trainer.py:508] step:  7977, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:64.968887 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3296031 log_pplx:2.5866473 loss:104.11253 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.28015\n",
      "I0710 12:43:57.440160 140295626643200 summary_utils.py:349] Steps/second: 0.177314, Examples/second: 25.146066\n",
      "I0710 12:43:57.440930 140295626643200 trainer.py:508] step:  7978, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:45.842381 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.331606 log_pplx:3.0683341 loss:77.130264 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.28528\n",
      "I0710 12:44:03.813431 140295626643200 summary_utils.py:349] Steps/second: 0.177307, Examples/second: 25.141931\n",
      "I0710 12:44:03.814357 140295626643200 trainer.py:508] step:  7979, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:62.671124 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3294301 log_pplx:2.5679877 loss:101.82072 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.28961\n",
      "I0710 12:44:15.041718 140295626643200 summary_utils.py:349] Steps/second: 0.177256, Examples/second: 25.129457\n",
      "I0710 12:44:15.042539 140295626643200 trainer.py:508] step:  7980, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:125.10015 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3290063 log_pplx:2.2207949 loss:160.17484 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:533.29382\n",
      "I0710 12:44:21.301630 140295626643200 summary_utils.py:349] Steps/second: 0.177250, Examples/second: 25.125478\n",
      "I0710 12:44:21.302442 140295626643200 trainer.py:508] step:  7981, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:61.871094 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3294525 log_pplx:2.6146574 loss:101.0892 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.29871\n",
      "I0710 12:44:24.409105 140295626643200 summary_utils.py:349] Steps/second: 0.177273, Examples/second: 25.134634\n",
      "I0710 12:44:24.409926 140295626643200 trainer.py:508] step:  7982, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:20.576174 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3314637 log_pplx:3.1159215 loss:48.260269 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.30402\n",
      "I0710 12:44:28.250036 140295626643200 summary_utils.py:349] Steps/second: 0.177290, Examples/second: 25.137899\n",
      "I0710 12:44:28.250809 140295626643200 trainer.py:508] step:  7983, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:37.239922 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.331359 log_pplx:3.0074611 loss:75.618851 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.3092\n",
      "I0710 12:44:34.596848 140295626643200 summary_utils.py:349] Steps/second: 0.177283, Examples/second: 25.133808\n",
      "I0710 12:44:34.597671 140295626643200 trainer.py:508] step:  7984, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:62.645748 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3300471 log_pplx:2.5726836 loss:103.93642 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.31439\n",
      "I0710 12:44:36.798996 140295626643200 summary_utils.py:349] Steps/second: 0.177315, Examples/second: 25.157291\n",
      "I0710 12:44:36.799822 140295626643200 trainer.py:508] step:  7985, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:17.544518 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.329877 log_pplx:3.3494639 loss:24.349031 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:533.31927\n",
      "I0710 12:44:43.060595 140295626643200 summary_utils.py:349] Steps/second: 0.177309, Examples/second: 25.153306\n",
      "I0710 12:44:43.061375 140295626643200 trainer.py:508] step:  7986, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:65.671638 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3302209 log_pplx:2.6788375 loss:107.25395 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.32379\n",
      "I0710 12:44:45.215424 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 12:44:47.099105 140295626643200 summary_utils.py:349] Steps/second: 0.177324, Examples/second: 25.156308\n",
      "I0710 12:44:47.099892 140295626643200 trainer.py:508] step:  7987, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:38.036613 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3319259 log_pplx:3.0694277 loss:77.656517 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.32849\n",
      "I0710 12:44:50.263485 140295626643200 summary_utils.py:349] Steps/second: 0.177346, Examples/second: 25.165373\n",
      "I0710 12:44:50.284584 140295626643200 trainer.py:508] step:  7988, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:22.286478 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3316864 log_pplx:3.1332445 loss:48.859032 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.33289\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 12:44:50.838884 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 12:44:51.338278 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00007986\n",
      "I0710 12:44:56.901174 140295626643200 summary_utils.py:349] Steps/second: 0.177337, Examples/second: 25.160902\n",
      "I0710 12:44:56.902055 140295626643200 trainer.py:508] step:  7989, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:65.774429 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3301208 log_pplx:2.635128 loss:106.22861 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.33704\n",
      "I0710 12:45:03.277195 140295626643200 summary_utils.py:349] Steps/second: 0.177330, Examples/second: 25.156771\n",
      "I0710 12:45:03.278036 140295626643200 trainer.py:508] step:  7990, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:68.52726 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.33057 log_pplx:2.6381273 loss:109.4823 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.34088\n",
      "I0710 12:45:14.184251 140295626643200 summary_utils.py:349] Steps/second: 0.177282, Examples/second: 25.144741\n",
      "I0710 12:45:14.185047 140295626643200 trainer.py:508] step:  7991, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:193.41811 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3286952 log_pplx:2.1192539 loss:149.93721 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:533.34528\n",
      "I0710 12:45:20.245826 140295626643200 summary_utils.py:349] Steps/second: 0.177279, Examples/second: 25.141025\n",
      "I0710 12:45:20.246588 140295626643200 trainer.py:508] step:  7992, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:64.504852 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.330053 log_pplx:2.5841858 loss:103.49664 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.34985\n",
      "I0710 12:45:24.115075 140295626643200 summary_utils.py:349] Steps/second: 0.177295, Examples/second: 25.144243\n",
      "I0710 12:45:24.115859 140295626643200 trainer.py:508] step:  7993, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:38.09491 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3321054 log_pplx:3.0721557 loss:78.45517 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.35437\n",
      "I0710 12:45:27.237208 140295626643200 summary_utils.py:349] Steps/second: 0.177318, Examples/second: 25.153347\n",
      "I0710 12:45:27.238304 140295626643200 trainer.py:508] step:  7994, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:22.991945 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3313153 log_pplx:3.1386309 loss:48.330006 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.35901\n",
      "I0710 12:45:33.758278 140295626643200 summary_utils.py:349] Steps/second: 0.177310, Examples/second: 25.149039\n",
      "I0710 12:45:33.759045 140295626643200 trainer.py:508] step:  7995, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:95.969147 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3301295 log_pplx:2.578896 loss:103.41374 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.36359\n",
      "I0710 12:45:40.613794 140295626643200 summary_utils.py:349] Steps/second: 0.177299, Examples/second: 25.144302\n",
      "I0710 12:45:40.614644 140295626643200 trainer.py:508] step:  7996, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:73.872009 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3301371 log_pplx:2.5552351 loss:101.47478 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.36774\n",
      "I0710 12:45:44.651039 140295626643200 summary_utils.py:349] Steps/second: 0.177313, Examples/second: 25.147299\n",
      "I0710 12:45:44.651858 140295626643200 trainer.py:508] step:  7997, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:41.93808 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3321615 log_pplx:3.1328537 loss:79.02623 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.37189\n",
      "I0710 12:45:50.946994 140295626643200 summary_utils.py:349] Steps/second: 0.177307, Examples/second: 25.143287\n",
      "I0710 12:45:50.947801 140295626643200 trainer.py:508] step:  7998, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:71.041008 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3301778 log_pplx:2.6093802 loss:107.53909 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.37634\n",
      "I0710 12:45:54.028180 140295626643200 summary_utils.py:349] Steps/second: 0.177330, Examples/second: 25.152431\n",
      "I0710 12:45:54.028980 140295626643200 trainer.py:508] step:  7999, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:33.953045 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.331965 log_pplx:3.1773129 loss:49.769623 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.38159\n",
      "I0710 12:46:04.232526 140295626643200 summary_utils.py:349] Steps/second: 0.177289, Examples/second: 25.141339\n",
      "I0710 12:46:04.233311 140295626643200 trainer.py:508] step:  8000, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:328.92078 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.329869 log_pplx:2.2956614 loss:175.04417 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:533.38696\n",
      "I0710 12:46:10.654667 140295626643200 summary_utils.py:349] Steps/second: 0.177282, Examples/second: 25.137170\n",
      "I0710 12:46:10.655583 140295626643200 trainer.py:508] step:  8001, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:105.65749 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3306202 log_pplx:2.7740049 loss:112.38187 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.39197\n",
      "I0710 12:46:14.507523 140295626643200 summary_utils.py:349] Steps/second: 0.177298, Examples/second: 25.140401\n",
      "I0710 12:46:14.508308 140295626643200 trainer.py:508] step:  8002, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:52.522461 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.331936 log_pplx:3.0816307 loss:76.828903 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.39819\n",
      "I0710 12:46:20.469836 140295626643200 summary_utils.py:349] Steps/second: 0.177295, Examples/second: 25.136826\n",
      "I0710 12:46:20.470640 140295626643200 trainer.py:508] step:  8003, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:103.15687 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3302677 log_pplx:2.6791582 loss:105.92722 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.40485\n",
      "I0710 12:46:26.706802 140295626643200 summary_utils.py:349] Steps/second: 0.177290, Examples/second: 25.132900\n",
      "I0710 12:46:26.707552 140295626643200 trainer.py:508] step:  8004, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:75.397964 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3305812 log_pplx:2.6472943 loss:109.33324 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.41193\n",
      "I0710 12:46:30.542588 140295626643200 summary_utils.py:349] Steps/second: 0.177306, Examples/second: 25.136151\n",
      "I0710 12:46:30.543620 140295626643200 trainer.py:508] step:  8005, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:83.503693 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3323462 log_pplx:3.1511261 loss:79.605324 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.41858\n",
      "I0710 12:46:33.646538 140295626643200 summary_utils.py:349] Steps/second: 0.177329, Examples/second: 25.145249\n",
      "I0710 12:46:33.647428 140295626643200 trainer.py:508] step:  8006, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:58.53791 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3319385 log_pplx:3.237015 loss:51.147369 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.42401\n",
      "I0710 12:46:40.036930 140295626643200 summary_utils.py:349] Steps/second: 0.177322, Examples/second: 25.141126\n",
      "I0710 12:46:40.037726 140295626643200 trainer.py:508] step:  8007, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:118.26105 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3306118 log_pplx:2.7393289 loss:110.60042 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.42853\n",
      "I0710 12:46:42.242315 140295626643200 summary_utils.py:349] Steps/second: 0.177353, Examples/second: 25.164454\n",
      "I0710 12:46:42.243083 140295626643200 trainer.py:508] step:  8008, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:10.134889 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3305199 log_pplx:3.2505629 loss:23.699902 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:533.43414\n",
      "I0710 12:46:48.796143 140295626643200 summary_utils.py:349] Steps/second: 0.177345, Examples/second: 25.160116\n",
      "I0710 12:46:48.796915 140295626643200 trainer.py:508] step:  8009, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:80.159485 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3308291 log_pplx:2.6303048 loss:109.91388 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.43945\n",
      "I0710 12:46:52.678628 140295626643200 summary_utils.py:349] Steps/second: 0.177361, Examples/second: 25.163299\n",
      "I0710 12:46:52.679411 140295626643200 trainer.py:508] step:  8010, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:52.256901 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3325031 log_pplx:3.0917845 loss:78.376747 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.4455\n",
      "I0710 12:46:58.600511 140295626643200 summary_utils.py:349] Steps/second: 0.177358, Examples/second: 25.159775\n",
      "I0710 12:46:58.601277 140295626643200 trainer.py:508] step:  8011, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:63.889297 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3305286 log_pplx:2.642082 loss:103.83384 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.45184\n",
      "I0710 12:47:01.654154 140295626643200 summary_utils.py:349] Steps/second: 0.177382, Examples/second: 25.168921\n",
      "I0710 12:47:01.654878 140295626643200 trainer.py:508] step:  8012, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:24.224598 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3321003 log_pplx:3.134213 loss:49.351608 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.45837\n",
      "I0710 12:47:12.348492 140295626643200 summary_utils.py:349] Steps/second: 0.177336, Examples/second: 25.157230\n",
      "I0710 12:47:12.349213 140295626643200 trainer.py:508] step:  8013, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:336.96323 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3299587 log_pplx:2.5177388 loss:185.93503 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:533.4646\n",
      "I0710 12:47:18.689132 140295626643200 summary_utils.py:349] Steps/second: 0.177330, Examples/second: 25.153175\n",
      "I0710 12:47:18.689876 140295626643200 trainer.py:508] step:  8014, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:114.6495 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3314053 log_pplx:2.6990163 loss:114.37081 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.47034\n",
      "I0710 12:47:25.409473 140295626643200 summary_utils.py:349] Steps/second: 0.177320, Examples/second: 25.148635\n",
      "I0710 12:47:25.410276 140295626643200 trainer.py:508] step:  8015, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:111.39576 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3306258 log_pplx:2.6830184 loss:107.65613 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.47546\n",
      "I0710 12:47:29.264571 140295626643200 summary_utils.py:349] Steps/second: 0.177336, Examples/second: 25.151849\n",
      "I0710 12:47:29.265367 140295626643200 trainer.py:508] step:  8016, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:46.974308 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3325193 log_pplx:3.0697501 loss:77.587944 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.48004\n",
      "I0710 12:47:35.621704 140295626643200 summary_utils.py:349] Steps/second: 0.177330, Examples/second: 25.147778\n",
      "I0710 12:47:35.622482 140295626643200 trainer.py:508] step:  8017, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:84.59301 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3307062 log_pplx:2.6342323 loss:104.80952 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.48444\n",
      "I0710 12:47:38.703199 140295626643200 summary_utils.py:349] Steps/second: 0.177353, Examples/second: 25.156872\n",
      "I0710 12:47:38.704079 140295626643200 trainer.py:508] step:  8018, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:26.151375 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3325341 log_pplx:3.1525295 loss:49.221321 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.48895\n",
      "I0710 12:47:44.864276 140295626643200 summary_utils.py:349] Steps/second: 0.177348, Examples/second: 25.153053\n",
      "I0710 12:47:44.865128 140295626643200 trainer.py:508] step:  8019, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:98.8993 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3306959 log_pplx:2.6917808 loss:108.84888 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.49365\n",
      "I0710 12:47:48.639383 140295626643200 summary_utils.py:349] Steps/second: 0.177365, Examples/second: 25.156365\n",
      "I0710 12:47:48.640138 140295626643200 trainer.py:508] step:  8020, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:54.556679 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3331245 log_pplx:3.1128945 loss:77.530533 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.49933\n",
      "I0710 12:47:54.831512 140295626643200 summary_utils.py:349] Steps/second: 0.177360, Examples/second: 25.152508\n",
      "I0710 12:47:54.832291 140295626643200 trainer.py:508] step:  8021, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:87.544556 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3313351 log_pplx:2.6938672 loss:111.69447 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.50555\n",
      "I0710 12:47:57.880933 140295626643200 summary_utils.py:349] Steps/second: 0.177383, Examples/second: 25.161634\n",
      "I0710 12:47:57.881690 140295626643200 trainer.py:508] step:  8022, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:24.696819 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3320589 log_pplx:3.0970628 loss:48.28273 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.51245\n",
      "I0710 12:48:03.955355 140295626643200 summary_utils.py:349] Steps/second: 0.177379, Examples/second: 25.157928\n",
      "I0710 12:48:03.956151 140295626643200 trainer.py:508] step:  8023, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:69.083809 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3309745 log_pplx:2.6904664 loss:107.2487 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.51904\n",
      "I0710 12:48:07.865036 140295626643200 summary_utils.py:349] Steps/second: 0.177395, Examples/second: 25.161063\n",
      "I0710 12:48:07.865859 140295626643200 trainer.py:508] step:  8024, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:63.381886 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3327413 log_pplx:3.0990002 loss:77.184479 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.52539\n",
      "I0710 12:48:18.913244 140295626643200 summary_utils.py:349] Steps/second: 0.177346, Examples/second: 25.148963\n",
      "I0710 12:48:18.914028 140295626643200 trainer.py:508] step:  8025, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:256.93661 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3299229 log_pplx:2.346283 loss:166.9967 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:533.53088\n",
      "I0710 12:48:25.404295 140295626643200 summary_utils.py:349] Steps/second: 0.177338, Examples/second: 25.144732\n",
      "I0710 12:48:25.405378 140295626643200 trainer.py:508] step:  8026, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:72.794777 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3310456 log_pplx:2.6261287 loss:105.57037 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.53723\n",
      "I0710 12:48:31.589706 140295626643200 summary_utils.py:349] Steps/second: 0.177333, Examples/second: 25.140894\n",
      "I0710 12:48:31.590474 140295626643200 trainer.py:508] step:  8027, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:69.432472 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3310953 log_pplx:2.6756375 loss:108.1292 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.54309\n",
      "I0710 12:48:35.479485 140295626643200 summary_utils.py:349] Steps/second: 0.177349, Examples/second: 25.144055\n",
      "I0710 12:48:35.480316 140295626643200 trainer.py:508] step:  8028, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:37.921997 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3331423 log_pplx:3.055099 loss:78.019592 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.54883\n",
      "I0710 12:48:38.470352 140295626643200 summary_utils.py:349] Steps/second: 0.177373, Examples/second: 25.153238\n",
      "I0710 12:48:38.471106 140295626643200 trainer.py:508] step:  8029, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:20.020269 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3326236 log_pplx:3.113167 loss:48.606754 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.5545\n",
      "I0710 12:48:44.815886 140295626643200 summary_utils.py:349] Steps/second: 0.177367, Examples/second: 25.149195\n",
      "I0710 12:48:44.816712 140295626643200 trainer.py:508] step:  8030, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:92.459877 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3315387 log_pplx:2.6497328 loss:112.41492 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.56\n",
      "I0710 12:48:47.011223 140295626643200 summary_utils.py:349] Steps/second: 0.177398, Examples/second: 25.172388\n",
      "I0710 12:48:47.012026 140295626643200 trainer.py:508] step:  8031, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:11.588512 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3311123 log_pplx:3.277297 loss:23.978039 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:533.56592\n",
      "I0710 12:48:53.039437 140295626643200 summary_utils.py:349] Steps/second: 0.177394, Examples/second: 25.168746\n",
      "I0710 12:48:53.040206 140295626643200 trainer.py:508] step:  8032, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:83.914589 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3315572 log_pplx:2.6706033 loss:110.59636 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.57147\n",
      "I0710 12:48:56.805601 140295626643200 summary_utils.py:349] Steps/second: 0.177411, Examples/second: 25.172055\n",
      "I0710 12:48:56.806368 140295626643200 trainer.py:508] step:  8033, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:43.167206 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3331003 log_pplx:3.0542724 loss:78.704788 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.57751\n",
      "I0710 12:49:05.871762 140295626643200 summary_utils.py:349] Steps/second: 0.177380, Examples/second: 25.162508\n",
      "I0710 12:49:05.872617 140295626643200 trainer.py:508] step:  8034, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:243.36429 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3303267 log_pplx:2.320667 loss:166.10173 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:533.58356\n",
      "I0710 12:49:11.996639 140295626643200 summary_utils.py:349] Steps/second: 0.177376, Examples/second: 25.158750\n",
      "I0710 12:49:11.997396 140295626643200 trainer.py:508] step:  8035, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:68.271835 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3311061 log_pplx:2.7019498 loss:107.57137 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.59009\n",
      "I0710 12:49:18.425423 140295626643200 summary_utils.py:349] Steps/second: 0.177369, Examples/second: 25.154607\n",
      "I0710 12:49:18.426194 140295626643200 trainer.py:508] step:  8036, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:99.871506 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3317086 log_pplx:2.6732185 loss:113.24424 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.59644\n",
      "I0710 12:49:21.420344 140295626643200 summary_utils.py:349] Steps/second: 0.177392, Examples/second: 25.163763\n",
      "I0710 12:49:21.421116 140295626643200 base_runner.py:111] step:  8037, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:44.457375 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3326699 log_pplx:3.123807 loss:50.005318 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.60211\n",
      "I0710 12:49:25.301605 140295626643200 summary_utils.py:349] Steps/second: 0.177408, Examples/second: 25.166922\n",
      "I0710 12:49:25.302487 140295626643200 trainer.py:508] step:  8038, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:54.684639 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3330593 log_pplx:3.0789859 loss:77.186325 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.60724\n",
      "I0710 12:49:31.741383 140295626643200 summary_utils.py:349] Steps/second: 0.177401, Examples/second: 25.162765\n",
      "I0710 12:49:31.742200 140295626643200 trainer.py:508] step:  8039, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:71.203865 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.331727 log_pplx:2.620199 loss:108.41073 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.61218\n",
      "I0710 12:49:37.946077 140295626643200 summary_utils.py:349] Steps/second: 0.177396, Examples/second: 25.158909\n",
      "I0710 12:49:37.946813 140295626643200 trainer.py:508] step:  8040, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:65.664528 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3314922 log_pplx:2.5821183 loss:102.89742 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.61737\n",
      "I0710 12:49:41.753721 140295626643200 summary_utils.py:349] Steps/second: 0.177412, Examples/second: 25.162160\n",
      "I0710 12:49:41.754602 140295626643200 trainer.py:508] step:  8041, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:40.095852 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3333815 log_pplx:3.0315647 loss:76.528061 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.62286\n",
      "I0710 12:49:48.048068 140295626643200 summary_utils.py:349] Steps/second: 0.177406, Examples/second: 25.158193\n",
      "I0710 12:49:48.049129 140295626643200 trainer.py:508] step:  8042, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:69.987267 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3318279 log_pplx:2.6329496 loss:108.34587 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.62842\n",
      "I0710 12:49:51.091778 140295626643200 summary_utils.py:349] Steps/second: 0.177430, Examples/second: 25.167273\n",
      "I0710 12:49:51.092561 140295626643200 trainer.py:508] step:  8043, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:26.771677 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3330309 log_pplx:3.0760579 loss:47.967278 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.63397\n",
      "I0710 12:50:00.058036 140295626643200 summary_utils.py:349] Steps/second: 0.177400, Examples/second: 25.157882\n",
      "I0710 12:50:00.058814 140295626643200 trainer.py:508] step:  8044, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:162.61926 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3304816 log_pplx:2.2721844 loss:157.80321 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:533.6394\n",
      "I0710 12:50:06.121349 140295626643200 summary_utils.py:349] Steps/second: 0.177396, Examples/second: 25.154214\n",
      "I0710 12:50:06.122189 140295626643200 trainer.py:508] step:  8045, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:68.138107 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3318423 log_pplx:2.5991414 loss:105.39519 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.64447\n",
      "I0710 12:50:09.926951 140295626643200 summary_utils.py:349] Steps/second: 0.177412, Examples/second: 25.157463\n",
      "I0710 12:50:09.927774 140295626643200 trainer.py:508] step:  8046, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:44.469963 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3334868 log_pplx:3.0477972 loss:77.395004 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.64948\n",
      "I0710 12:50:16.316174 140295626643200 summary_utils.py:349] Steps/second: 0.177406, Examples/second: 25.153383\n",
      "I0710 12:50:16.316955 140295626643200 trainer.py:508] step:  8047, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:71.047562 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3319473 log_pplx:2.5878682 loss:108.52872 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.65424\n",
      "I0710 12:50:22.360223 140295626643200 summary_utils.py:349] Steps/second: 0.177402, Examples/second: 25.149743\n",
      "I0710 12:50:22.361026 140295626643200 trainer.py:508] step:  8048, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:67.46125 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.331532 log_pplx:2.6090386 loss:104.03542 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.659\n",
      "I0710 12:50:25.404828 140295626643200 summary_utils.py:349] Steps/second: 0.177425, Examples/second: 25.158808\n",
      "I0710 12:50:25.405581 140295626643200 trainer.py:508] step:  8049, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:22.511751 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3333653 log_pplx:3.1475577 loss:48.713371 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.66364\n",
      "I0710 12:50:36.229797 140295626643200 summary_utils.py:349] Steps/second: 0.177379, Examples/second: 25.147076\n",
      "I0710 12:50:36.230635 140295626643200 trainer.py:508] step:  8050, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:138.08847 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3307985 log_pplx:2.2920067 loss:172.3589 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:533.66827\n",
      "I0710 12:50:42.670542 140295626643200 summary_utils.py:349] Steps/second: 0.177371, Examples/second: 25.142939\n",
      "I0710 12:50:42.671359 140295626643200 trainer.py:508] step:  8051, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:71.703537 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3321173 log_pplx:2.6786232 loss:110.42625 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.67316\n",
      "I0710 12:50:46.566288 140295626643200 summary_utils.py:349] Steps/second: 0.177387, Examples/second: 25.146070\n",
      "I0710 12:50:46.567200 140295626643200 trainer.py:508] step:  8052, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:48.641754 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3335581 log_pplx:3.1169093 loss:78.565598 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.67902\n",
      "I0710 12:50:53.408760 140295626643200 summary_utils.py:349] Steps/second: 0.177376, Examples/second: 25.141426\n",
      "I0710 12:50:53.409676 140295626643200 trainer.py:508] step:  8053, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:66.747368 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3319615 log_pplx:2.6221542 loss:106.16445 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.68512\n",
      "I0710 12:50:55.535381 140295635035904 trainer.py:345] Write summary @8053\n",
      "I0710 12:50:59.516936 140295626643200 summary_utils.py:349] Steps/second: 0.177372, Examples/second: 25.146592\n",
      "I0710 12:50:59.519084 140295626643200 trainer.py:508] step:  8054, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:21.888577 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3332658 log_pplx:3.1153786 loss:47.959789 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.69147\n",
      "I0710 12:51:02.336036 140295626643200 summary_utils.py:349] Steps/second: 0.177397, Examples/second: 25.168835\n",
      "I0710 12:51:02.337864 140295626643200 trainer.py:508] step:  8055, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:9.9575033 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3317813 log_pplx:3.2724891 loss:23.744722 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:533.69751\n",
      "I0710 12:51:12.372692 140295626643200 summary_utils.py:349] Steps/second: 0.177358, Examples/second: 25.160136\n",
      "I0710 12:51:12.374595 140295626643200 trainer.py:508] step:  8056, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:69.843948 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3318937 log_pplx:2.5970898 loss:104.95491 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.70312\n",
      "I0710 12:51:18.004698 140295626643200 summary_utils.py:349] Steps/second: 0.177358, Examples/second: 25.161058\n",
      "I0710 12:51:18.006472 140295626643200 trainer.py:508] step:  8057, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:49.713966 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3336163 log_pplx:3.1080027 loss:79.292923 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.70837\n",
      "I0710 12:51:28.158561 140295626643200 summary_utils.py:349] Steps/second: 0.177318, Examples/second: 25.152220\n",
      "I0710 12:51:28.159792 140295626643200 trainer.py:508] step:  8058, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:64.498169 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3323953 log_pplx:2.6122856 loss:108.14862 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.71289\n",
      "I0710 12:51:38.176420 140295626643200 summary_utils.py:349] Steps/second: 0.177279, Examples/second: 25.143564\n",
      "I0710 12:51:38.177856 140295626643200 trainer.py:508] step:  8059, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:62.470459 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3323758 log_pplx:2.6258743 loss:106.01968 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.71741\n",
      "I0710 12:51:41.064680 140295635035904 trainer.py:354] Write summary done: step 8053\n",
      "I0710 12:51:42.647218 140295626643200 summary_utils.py:349] Steps/second: 0.177289, Examples/second: 25.150790\n",
      "I0710 12:51:42.647993 140295626643200 trainer.py:508] step:  8060, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:21.293276 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3335681 log_pplx:3.126286 loss:49.800755 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.72192\n",
      "I0710 12:51:46.527683 140295626643200 summary_utils.py:349] Steps/second: 0.177305, Examples/second: 25.153930\n",
      "I0710 12:51:46.528440 140295626643200 trainer.py:508] step:  8061, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:33.728073 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3337317 log_pplx:3.0386186 loss:76.497231 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.7262\n",
      "I0710 12:51:53.132310 140295626643200 summary_utils.py:349] Steps/second: 0.177296, Examples/second: 25.149598\n",
      "I0710 12:51:53.133197 140295626643200 trainer.py:508] step:  8062, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:72.406364 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3321406 log_pplx:2.6219676 loss:107.04181 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.73047\n",
      "I0710 12:52:02.944065 140295626643200 summary_utils.py:349] Steps/second: 0.177259, Examples/second: 25.139203\n",
      "I0710 12:52:02.945092 140295626643200 trainer.py:508] step:  8063, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:164.9135 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.331418 log_pplx:2.3801048 loss:178.38885 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:533.73547\n",
      "I0710 12:52:09.254069 140295626643200 summary_utils.py:349] Steps/second: 0.177253, Examples/second: 25.135252\n",
      "I0710 12:52:09.254802 140295626643200 trainer.py:508] step:  8064, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:59.372734 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3320569 log_pplx:2.6151042 loss:102.54478 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.74103\n",
      "I0710 12:52:15.883896 140295626643200 summary_utils.py:349] Steps/second: 0.177244, Examples/second: 25.130899\n",
      "I0710 12:52:15.884734 140295626643200 trainer.py:508] step:  8065, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:64.406364 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3325859 log_pplx:2.601105 loss:109.37646 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.74658\n",
      "I0710 12:52:19.763891 140295626643200 summary_utils.py:349] Steps/second: 0.177260, Examples/second: 25.134038\n",
      "I0710 12:52:19.764640 140295626643200 trainer.py:508] step:  8066, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:37.151855 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3342848 log_pplx:3.0000432 loss:75.732338 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.75226\n",
      "I0710 12:52:22.845106 140295626643200 summary_utils.py:349] Steps/second: 0.177283, Examples/second: 25.143005\n",
      "I0710 12:52:22.845941 140295626643200 trainer.py:508] step:  8067, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:23.075125 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3336043 log_pplx:3.1605561 loss:48.692314 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.75769\n",
      "I0710 12:52:29.486258 140295626643200 summary_utils.py:349] Steps/second: 0.177274, Examples/second: 25.138638\n",
      "I0710 12:52:29.487083 140295626643200 trainer.py:508] step:  8068, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:66.859528 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.332136 log_pplx:2.6583178 loss:107.62864 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.76282\n",
      "I0710 12:52:35.928167 140295626643200 summary_utils.py:349] Steps/second: 0.177267, Examples/second: 25.134526\n",
      "I0710 12:52:35.928948 140295626643200 trainer.py:508] step:  8069, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:74.986412 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3323566 log_pplx:2.6034071 loss:106.60952 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.76758\n",
      "I0710 12:52:39.812037 140295626643200 summary_utils.py:349] Steps/second: 0.177282, Examples/second: 25.137656\n",
      "I0710 12:52:39.812802 140295626643200 trainer.py:508] step:  8070, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:37.591724 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.334439 log_pplx:3.0553861 loss:77.874153 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.77161\n",
      "I0710 12:52:46.296393 140295626643200 summary_utils.py:349] Steps/second: 0.177275, Examples/second: 25.133493\n",
      "I0710 12:52:46.297254 140295626643200 trainer.py:508] step:  8071, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:68.443039 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3321831 log_pplx:2.5523734 loss:102.82875 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.77582\n",
      "I0710 12:52:49.377365 140295626643200 summary_utils.py:349] Steps/second: 0.177298, Examples/second: 25.142448\n",
      "I0710 12:52:49.378454 140295626643200 trainer.py:508] step:  8072, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:23.773575 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3334343 log_pplx:3.0796125 loss:47.565575 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.78027\n",
      "I0710 12:52:55.791367 140295626643200 summary_utils.py:349] Steps/second: 0.177291, Examples/second: 25.138374\n",
      "I0710 12:52:55.792170 140295626643200 trainer.py:508] step:  8073, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:60.085587 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3320755 log_pplx:2.6189265 loss:103.15297 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.78479\n",
      "I0710 12:52:59.626226 140295626643200 summary_utils.py:349] Steps/second: 0.177307, Examples/second: 25.141562\n",
      "I0710 12:52:59.626979 140295626643200 trainer.py:508] step:  8074, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:35.212635 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3342428 log_pplx:3.0752878 loss:77.708679 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.78943\n",
      "I0710 12:53:09.260499 140295626643200 summary_utils.py:349] Steps/second: 0.177271, Examples/second: 25.131430\n",
      "I0710 12:53:09.261277 140295626643200 trainer.py:508] step:  8075, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:193.45367 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3320379 log_pplx:2.2591846 loss:164.92049 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:533.79401\n",
      "I0710 12:53:15.475787 140295626643200 summary_utils.py:349] Steps/second: 0.177266, Examples/second: 25.127613\n",
      "I0710 12:53:15.476601 140295626643200 trainer.py:508] step:  8076, steps/sec: 0.18, examples/sec: 25.13 grad_norm/all/loss:64.682808 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3322296 log_pplx:2.5648839 loss:104.26253 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.79852\n",
      "I0710 12:53:21.443777 140295626643200 summary_utils.py:349] Steps/second: 0.177263, Examples/second: 25.124110\n",
      "I0710 12:53:21.444736 140295626643200 trainer.py:508] step:  8077, steps/sec: 0.18, examples/sec: 25.12 grad_norm/all/loss:66.352211 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3320141 log_pplx:2.6552284 loss:104.78194 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.80347\n",
      "I0710 12:53:23.646136 140295626643200 summary_utils.py:349] Steps/second: 0.177294, Examples/second: 25.146975\n",
      "I0710 12:53:23.646898 140295626643200 trainer.py:508] step:  8078, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:15.098537 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3323301 log_pplx:3.2454929 loss:23.903814 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:533.80884\n",
      "I0710 12:53:26.791798 140295626643200 summary_utils.py:349] Steps/second: 0.177316, Examples/second: 25.155830\n",
      "I0710 12:53:26.792605 140295626643200 trainer.py:508] step:  8079, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:30.329399 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.33442 log_pplx:3.1752224 loss:49.637657 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.8139\n",
      "I0710 12:53:30.667691 140295626643200 summary_utils.py:349] Steps/second: 0.177332, Examples/second: 25.158958\n",
      "I0710 12:53:30.668662 140295626643200 trainer.py:508] step:  8080, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:42.01778 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3341513 log_pplx:3.0262806 loss:76.564903 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.8183\n",
      "I0710 12:53:37.045142 140295626643200 summary_utils.py:349] Steps/second: 0.177325, Examples/second: 25.154933\n",
      "I0710 12:53:37.046196 140295626643200 trainer.py:508] step:  8081, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:63.158508 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3328967 log_pplx:2.5710216 loss:107.21159 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.82245\n",
      "I0710 12:53:43.307266 140295626643200 summary_utils.py:349] Steps/second: 0.177319, Examples/second: 25.151055\n",
      "I0710 12:53:43.308021 140295626643200 trainer.py:508] step:  8082, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:64.632904 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3327498 log_pplx:2.5932772 loss:104.54148 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.82672\n",
      "I0710 12:53:47.092823 140295626643200 summary_utils.py:349] Steps/second: 0.177336, Examples/second: 25.154295\n",
      "I0710 12:53:47.093538 140295626643200 trainer.py:508] step:  8083, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:33.36977 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3340081 log_pplx:3.0260656 loss:75.746201 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.83167\n",
      "I0710 12:53:50.098012 140295626643200 summary_utils.py:349] Steps/second: 0.177359, Examples/second: 25.163316\n",
      "I0710 12:53:50.099091 140295626643200 trainer.py:508] step:  8084, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:23.172863 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3342743 log_pplx:3.1015613 loss:49.18882 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.83661\n",
      "I0710 12:53:56.327548 140295626643200 summary_utils.py:349] Steps/second: 0.177354, Examples/second: 25.159479\n",
      "I0710 12:53:56.328272 140295626643200 trainer.py:508] step:  8085, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:63.951847 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3328648 log_pplx:2.6233399 loss:107.55692 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.84143\n",
      "I0710 12:54:02.508398 140295626643200 summary_utils.py:349] Steps/second: 0.177349, Examples/second: 25.155706\n",
      "I0710 12:54:02.509194 140295626643200 trainer.py:508] step:  8086, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:60.889233 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3324423 log_pplx:2.6429987 loss:105.19135 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.84656\n",
      "I0710 12:54:06.203782 140295626643200 summary_utils.py:349] Steps/second: 0.177366, Examples/second: 25.159056\n",
      "I0710 12:54:06.204593 140295626643200 trainer.py:508] step:  8087, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:35.238777 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3346403 log_pplx:3.0222652 loss:76.368858 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.85162\n",
      "I0710 12:54:15.865131 140295626643200 summary_utils.py:349] Steps/second: 0.177331, Examples/second: 25.148915\n",
      "I0710 12:54:15.865890 140295626643200 trainer.py:508] step:  8088, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:167.27066 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3318694 log_pplx:2.2299156 loss:160.10791 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:533.85651\n",
      "I0710 12:54:21.848215 140295626643200 summary_utils.py:349] Steps/second: 0.177328, Examples/second: 25.145397\n",
      "I0710 12:54:21.849035 140295626643200 trainer.py:508] step:  8089, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:64.621857 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3327976 log_pplx:2.643249 loss:105.33347 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.86206\n",
      "I0710 12:54:24.887949 140295626643200 summary_utils.py:349] Steps/second: 0.177351, Examples/second: 25.154360\n",
      "I0710 12:54:24.889124 140295626643200 trainer.py:508] step:  8090, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:21.583849 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3338864 log_pplx:3.1257532 loss:48.156136 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.86792\n",
      "I0710 12:54:31.107989 140295626643200 summary_utils.py:349] Steps/second: 0.177346, Examples/second: 25.150544\n",
      "I0710 12:54:31.109085 140295626643200 trainer.py:508] step:  8091, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:70.611023 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3329601 log_pplx:2.60074 loss:105.59004 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.87347\n",
      "I0710 12:54:34.931515 140295626643200 summary_utils.py:349] Steps/second: 0.177362, Examples/second: 25.153729\n",
      "I0710 12:54:34.932295 140295626643200 trainer.py:508] step:  8092, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:37.468933 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3343158 log_pplx:2.963593 loss:75.312309 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.87878\n",
      "I0710 12:54:41.231309 140295626643200 summary_utils.py:349] Steps/second: 0.177356, Examples/second: 25.149816\n",
      "I0710 12:54:41.232097 140295626643200 trainer.py:508] step:  8093, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:68.988602 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3329751 log_pplx:2.6382344 loss:109.2559 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.88385\n",
      "I0710 12:54:47.284230 140295626643200 summary_utils.py:349] Steps/second: 0.177352, Examples/second: 25.146214\n",
      "I0710 12:54:47.285040 140295626643200 trainer.py:508] step:  8094, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:62.187286 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3324506 log_pplx:2.6038139 loss:101.22326 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.88849\n",
      "I0710 12:54:50.375206 140295626643200 summary_utils.py:349] Steps/second: 0.177375, Examples/second: 25.155102\n",
      "I0710 12:54:50.376380 140295626643200 trainer.py:508] step:  8095, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:24.494404 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3339061 log_pplx:3.0809784 loss:47.418182 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.89288\n",
      "I0710 12:54:51.227378 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 12:54:54.421586 140295626643200 summary_utils.py:349] Steps/second: 0.177389, Examples/second: 25.158003\n",
      "I0710 12:54:54.422588 140295626643200 trainer.py:508] step:  8096, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:43.381672 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3345497 log_pplx:3.0182621 loss:75.796104 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.89734\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 12:54:56.053562 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 12:54:56.556707 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00008095\n",
      "I0710 12:55:00.669670 140295626643200 summary_utils.py:349] Steps/second: 0.177383, Examples/second: 25.154157\n",
      "I0710 12:55:00.670616 140295626643200 trainer.py:508] step:  8097, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:63.388668 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3327141 log_pplx:2.5777886 loss:102.24154 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.90204\n",
      "I0710 12:55:06.835973 140295626643200 summary_utils.py:349] Steps/second: 0.177379, Examples/second: 25.150416\n",
      "I0710 12:55:06.836714 140295626643200 trainer.py:508] step:  8098, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:61.209824 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3330175 log_pplx:2.5983679 loss:103.18768 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.90692\n",
      "I0710 12:55:10.518885 140295626643200 summary_utils.py:349] Steps/second: 0.177396, Examples/second: 25.153771\n",
      "I0710 12:55:10.519621 140295626643200 trainer.py:508] step:  8099, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:35.324169 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3347483 log_pplx:3.084425 loss:76.821457 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.91205\n",
      "I0710 12:55:20.648324 140295626643200 summary_utils.py:349] Steps/second: 0.177356, Examples/second: 25.143080\n",
      "I0710 12:55:20.649342 140295626643200 trainer.py:508] step:  8100, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:118.68991 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3321767 log_pplx:2.1849997 loss:155.51735 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:533.91687\n",
      "I0710 12:55:26.731492 140295626643200 summary_utils.py:349] Steps/second: 0.177352, Examples/second: 25.139449\n",
      "I0710 12:55:26.732287 140295626643200 trainer.py:508] step:  8101, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:66.72448 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3331091 log_pplx:2.6374059 loss:105.26546 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.92059\n",
      "I0710 12:55:28.945419 140295626643200 summary_utils.py:349] Steps/second: 0.177383, Examples/second: 25.162156\n",
      "I0710 12:55:28.946197 140295626643200 trainer.py:508] step:  8102, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:11.928818 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3329999 log_pplx:3.2879415 loss:24.235725 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:533.92438\n",
      "I0710 12:55:32.060563 140295626643200 summary_utils.py:349] Steps/second: 0.177405, Examples/second: 25.170992\n",
      "I0710 12:55:32.061340 140295626643200 trainer.py:508] step:  8103, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:43.021999 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3349202 log_pplx:3.1380041 loss:49.631947 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.92804\n",
      "I0710 12:55:38.572946 140295626643200 summary_utils.py:349] Steps/second: 0.177397, Examples/second: 25.166818\n",
      "I0710 12:55:38.573739 140295626643200 trainer.py:508] step:  8104, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:63.129135 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3327508 log_pplx:2.6223762 loss:102.79714 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.93115\n",
      "I0710 12:55:42.435478 140295626643200 summary_utils.py:349] Steps/second: 0.177413, Examples/second: 25.169940\n",
      "I0710 12:55:42.436362 140295626643200 trainer.py:508] step:  8105, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:35.8993 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3350437 log_pplx:3.0618873 loss:77.140427 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.93481\n",
      "I0710 12:55:48.899486 140295626643200 summary_utils.py:349] Steps/second: 0.177405, Examples/second: 25.165829\n",
      "I0710 12:55:48.900289 140295626643200 trainer.py:508] step:  8106, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:66.731918 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3329571 log_pplx:2.5801609 loss:102.72266 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.93835\n",
      "I0710 12:55:51.990336 140295626643200 summary_utils.py:349] Steps/second: 0.177428, Examples/second: 25.174687\n",
      "I0710 12:55:51.991169 140295626643200 trainer.py:508] step:  8107, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:23.392424 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3344456 log_pplx:3.125165 loss:47.561104 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.94269\n",
      "I0710 12:55:58.561208 140295626643200 summary_utils.py:349] Steps/second: 0.177420, Examples/second: 25.170443\n",
      "I0710 12:55:58.562158 140295626643200 trainer.py:508] step:  8108, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:61.307842 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3328539 log_pplx:2.6176941 loss:105.00227 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.94696\n",
      "I0710 12:56:02.601195 140295626643200 summary_utils.py:349] Steps/second: 0.177434, Examples/second: 25.173339\n",
      "I0710 12:56:02.602292 140295626643200 trainer.py:508] step:  8109, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:36.270405 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3353006 log_pplx:3.1176882 loss:79.578995 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.9516\n",
      "I0710 12:56:09.022610 140295626643200 summary_utils.py:349] Steps/second: 0.177427, Examples/second: 25.169285\n",
      "I0710 12:56:09.023579 140295626643200 trainer.py:508] step:  8110, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:60.982605 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3334026 log_pplx:2.6188464 loss:104.55746 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.95599\n",
      "I0710 12:56:15.523205 140295626643200 summary_utils.py:349] Steps/second: 0.177419, Examples/second: 25.165134\n",
      "I0710 12:56:15.523981 140295626643200 trainer.py:508] step:  8111, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:61.846527 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3332456 log_pplx:2.5734663 loss:105.99465 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.96057\n",
      "I0710 12:56:26.448284 140295626643200 summary_utils.py:349] Steps/second: 0.177373, Examples/second: 25.153478\n",
      "I0710 12:56:26.449063 140295626643200 trainer.py:508] step:  8112, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:137.1725 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3324785 log_pplx:2.2450371 loss:170.84734 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:533.96509\n",
      "I0710 12:56:30.228136 140295626643200 summary_utils.py:349] Steps/second: 0.177389, Examples/second: 25.156699\n",
      "I0710 12:56:30.228997 140295626643200 trainer.py:508] step:  8113, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:35.419846 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3349732 log_pplx:2.9871161 loss:74.827255 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.97107\n",
      "I0710 12:56:33.286088 140295626643200 summary_utils.py:349] Steps/second: 0.177412, Examples/second: 25.165580\n",
      "I0710 12:56:33.286904 140295626643200 trainer.py:508] step:  8114, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:23.110136 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3348515 log_pplx:3.0821092 loss:48.410789 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.97675\n",
      "I0710 12:56:39.566258 140295626643200 summary_utils.py:349] Steps/second: 0.177406, Examples/second: 25.161711\n",
      "I0710 12:56:39.567028 140295626643200 trainer.py:508] step:  8115, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:81.264854 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.333365 log_pplx:2.633194 loss:108.22427 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.98212\n",
      "I0710 12:56:45.968126 140295626643200 summary_utils.py:349] Steps/second: 0.177399, Examples/second: 25.157692\n",
      "I0710 12:56:45.968924 140295626643200 trainer.py:508] step:  8116, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:83.424919 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3333488 log_pplx:2.6648505 loss:108.19293 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.98669\n",
      "I0710 12:56:49.757407 140295626643200 summary_utils.py:349] Steps/second: 0.177416, Examples/second: 25.160897\n",
      "I0710 12:56:49.758213 140295626643200 trainer.py:508] step:  8117, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:38.555832 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3355304 log_pplx:3.114912 loss:78.865677 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:533.9906\n",
      "I0710 12:56:55.707704 140295626643200 summary_utils.py:349] Steps/second: 0.177413, Examples/second: 25.157443\n",
      "I0710 12:56:55.708512 140295626643200 trainer.py:508] step:  8118, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:61.841755 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3334901 log_pplx:2.6632102 loss:104.69743 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:533.99475\n",
      "I0710 12:56:58.748562 140295626643200 summary_utils.py:349] Steps/second: 0.177436, Examples/second: 25.166334\n",
      "I0710 12:56:58.749283 140295626643200 trainer.py:508] step:  8119, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:27.249283 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3346107 log_pplx:3.0510535 loss:47.565449 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:533.99927\n",
      "I0710 12:57:05.010274 140295626643200 summary_utils.py:349] Steps/second: 0.177430, Examples/second: 25.162492\n",
      "I0710 12:57:05.011061 140295626643200 trainer.py:508] step:  8120, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:78.17659 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3338929 log_pplx:2.5861471 loss:106.74323 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.00391\n",
      "I0710 12:57:08.833618 140295626643200 summary_utils.py:349] Steps/second: 0.177446, Examples/second: 25.165651\n",
      "I0710 12:57:08.834391 140295626643200 trainer.py:508] step:  8121, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:39.455353 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3351461 log_pplx:2.9959769 loss:75.461174 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.00964\n",
      "I0710 12:57:15.249674 140295626643200 summary_utils.py:349] Steps/second: 0.177439, Examples/second: 25.161619\n",
      "I0710 12:57:15.250741 140295626643200 trainer.py:508] step:  8122, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:95.935356 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3335851 log_pplx:2.5952914 loss:105.17418 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.01562\n",
      "I0710 12:57:21.620987 140295626643200 summary_utils.py:349] Steps/second: 0.177433, Examples/second: 25.157645\n",
      "I0710 12:57:21.621774 140295626643200 trainer.py:508] step:  8123, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:66.965775 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3336914 log_pplx:2.6250191 loss:106.41171 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.02179\n",
      "I0710 12:57:31.913348 140295626643200 summary_utils.py:349] Steps/second: 0.177392, Examples/second: 25.146818\n",
      "I0710 12:57:31.914140 140295626643200 trainer.py:508] step:  8124, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:129.00502 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3323874 log_pplx:2.2754145 loss:165.65016 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:534.02759\n",
      "I0710 12:57:34.149358 140295626643200 summary_utils.py:349] Steps/second: 0.177422, Examples/second: 25.169355\n",
      "I0710 12:57:34.150403 140295626643200 trainer.py:508] step:  8125, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:20.505642 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3336449 log_pplx:3.2713983 loss:24.228794 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:534.03204\n",
      "I0710 12:57:40.681722 140295626643200 summary_utils.py:349] Steps/second: 0.177414, Examples/second: 25.165182\n",
      "I0710 12:57:40.682638 140295626643200 trainer.py:508] step:  8126, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:68.898788 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3334086 log_pplx:2.6373796 loss:104.53914 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.03625\n",
      "I0710 12:57:43.726772 140295626643200 summary_utils.py:349] Steps/second: 0.177437, Examples/second: 25.174048\n",
      "I0710 12:57:43.727479 140295626643200 trainer.py:508] step:  8127, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:45.617966 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3352455 log_pplx:3.1496994 loss:50.136814 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.04065\n",
      "I0710 12:57:47.522893 140295626643200 summary_utils.py:349] Steps/second: 0.177453, Examples/second: 25.177232\n",
      "I0710 12:57:47.523669 140295626643200 trainer.py:508] step:  8128, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:45.06982 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3352406 log_pplx:3.0259457 loss:75.156921 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.04456\n",
      "I0710 12:57:53.655475 140295626643200 summary_utils.py:349] Steps/second: 0.177448, Examples/second: 25.173556\n",
      "I0710 12:57:53.656255 140295626643200 trainer.py:508] step:  8129, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:84.749779 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3335198 log_pplx:2.6207891 loss:104.24187 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.04834\n",
      "I0710 12:57:59.328224 140295626643200 summary_utils.py:349] Steps/second: 0.177448, Examples/second: 25.170455\n",
      "I0710 12:57:59.329019 140295626643200 trainer.py:508] step:  8130, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:76.535057 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3337092 log_pplx:2.6875083 loss:105.85423 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.05328\n",
      "I0710 12:58:03.202816 140295626643200 summary_utils.py:349] Steps/second: 0.177463, Examples/second: 25.173540\n",
      "I0710 12:58:03.203970 140295626643200 trainer.py:508] step:  8131, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:35.75518 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3355398 log_pplx:3.0801384 loss:76.425941 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.05902\n",
      "I0710 12:58:06.245452 140295626643200 summary_utils.py:349] Steps/second: 0.177486, Examples/second: 25.182397\n",
      "I0710 12:58:06.246223 140295626643200 trainer.py:508] step:  8132, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:27.9384 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3348174 log_pplx:3.0405512 loss:47.199802 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.06451\n",
      "I0710 12:58:12.326348 140295626643200 summary_utils.py:349] Steps/second: 0.177482, Examples/second: 25.178787\n",
      "I0710 12:58:12.327134 140295626643200 trainer.py:508] step:  8133, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:73.742584 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3336928 log_pplx:2.6079087 loss:105.35952 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.06982\n",
      "I0710 12:58:18.411925 140295626643200 summary_utils.py:349] Steps/second: 0.177478, Examples/second: 25.175174\n",
      "I0710 12:58:18.412715 140295626643200 trainer.py:508] step:  8134, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:82.939728 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.333889 log_pplx:2.6264758 loss:105.51865 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.07465\n",
      "I0710 12:58:22.144014 140295626643200 summary_utils.py:349] Steps/second: 0.177495, Examples/second: 25.178432\n",
      "I0710 12:58:22.144812 140295626643200 trainer.py:508] step:  8135, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:47.364792 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3356072 log_pplx:3.0738411 loss:77.268677 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.07922\n",
      "I0710 12:58:31.897758 140295626643200 summary_utils.py:349] Steps/second: 0.177459, Examples/second: 25.168296\n",
      "I0710 12:58:31.898608 140295626643200 trainer.py:508] step:  8136, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:125.12692 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3323445 log_pplx:2.1768451 loss:152.7601 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:534.08337\n",
      "I0710 12:58:38.125713 140295626643200 summary_utils.py:349] Steps/second: 0.177454, Examples/second: 25.164512\n",
      "I0710 12:58:38.126510 140295626643200 base_runner.py:111] step:  8137, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:73.746201 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3342931 log_pplx:2.6521454 loss:114.04224 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.08832\n",
      "I0710 12:58:41.160731 140295626643200 summary_utils.py:349] Steps/second: 0.177476, Examples/second: 25.173365\n",
      "I0710 12:58:41.161495 140295626643200 trainer.py:508] step:  8138, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:38.742172 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3352537 log_pplx:3.1699538 loss:49.580059 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.09381\n",
      "I0710 12:58:47.716664 140295626643200 summary_utils.py:349] Steps/second: 0.177468, Examples/second: 25.169176\n",
      "I0710 12:58:47.717449 140295626643200 trainer.py:508] step:  8139, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:65.908356 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3336153 log_pplx:2.6192162 loss:104.44126 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.09949\n",
      "I0710 12:58:51.508751 140295626643200 summary_utils.py:349] Steps/second: 0.177484, Examples/second: 25.172356\n",
      "I0710 12:58:51.509819 140295626643200 trainer.py:508] step:  8140, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:35.750191 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3361645 log_pplx:3.0798938 loss:78.595032 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.10486\n",
      "I0710 12:58:57.530434 140295626643200 summary_utils.py:349] Steps/second: 0.177481, Examples/second: 25.168831\n",
      "I0710 12:58:57.531407 140295626643200 trainer.py:508] step:  8141, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:66.634132 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3337039 log_pplx:2.6666493 loss:106.36598 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.11005\n",
      "I0710 12:59:03.625278 140295626643200 summary_utils.py:349] Steps/second: 0.177477, Examples/second: 25.165217\n",
      "I0710 12:59:03.626175 140295626643200 trainer.py:508] step:  8142, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:65.554771 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3340782 log_pplx:2.5447085 loss:103.25156 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.11487\n",
      "I0710 12:59:06.659033 140295626643200 summary_utils.py:349] Steps/second: 0.177500, Examples/second: 25.174060\n",
      "I0710 12:59:06.659772 140295626643200 trainer.py:508] step:  8143, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:22.268978 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3356637 log_pplx:3.1129127 loss:49.064854 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.11981\n",
      "I0710 12:59:17.541498 140295626643200 summary_utils.py:349] Steps/second: 0.177454, Examples/second: 25.162551\n",
      "I0710 12:59:17.542302 140295626643200 trainer.py:508] step:  8144, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:153.6004 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3335807 log_pplx:2.2316155 loss:172.3923 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:534.12433\n",
      "I0710 12:59:23.707288 140295626643200 summary_utils.py:349] Steps/second: 0.177449, Examples/second: 25.158855\n",
      "I0710 12:59:23.708076 140295626643200 trainer.py:508] step:  8145, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:62.939159 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3344898 log_pplx:2.6009383 loss:107.90642 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.12909\n",
      "I0710 12:59:27.473394 140295626643200 summary_utils.py:349] Steps/second: 0.177466, Examples/second: 25.162063\n",
      "I0710 12:59:27.474228 140295626643200 trainer.py:508] step:  8146, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:38.473122 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.335884 log_pplx:2.9908834 loss:75.127258 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.1344\n",
      "I0710 12:59:33.899233 140295626643200 summary_utils.py:349] Steps/second: 0.177459, Examples/second: 25.158048\n",
      "I0710 12:59:33.900044 140295626643200 trainer.py:508] step:  8147, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:75.238441 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3343934 log_pplx:2.633028 loss:111.83787 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.13995\n",
      "I0710 12:59:36.081420 140295626643200 summary_utils.py:349] Steps/second: 0.177489, Examples/second: 25.180515\n",
      "I0710 12:59:36.082520 140295626643200 trainer.py:508] step:  8148, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:14.003574 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3340915 log_pplx:3.2898645 loss:24.314157 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:534.14581\n",
      "I0710 12:59:42.641761 140295626643200 summary_utils.py:349] Steps/second: 0.177481, Examples/second: 25.176329\n",
      "I0710 12:59:42.642622 140295626643200 trainer.py:508] step:  8149, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:66.842377 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3342458 log_pplx:2.6264114 loss:108.14249 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.15131\n",
      "I0710 12:59:46.361473 140295626643200 summary_utils.py:349] Steps/second: 0.177497, Examples/second: 25.179589\n",
      "I0710 12:59:46.362313 140295626643200 trainer.py:508] step:  8150, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:37.924294 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3356252 log_pplx:2.9883907 loss:73.738541 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.15704\n",
      "I0710 12:59:49.368300 140295626643200 summary_utils.py:349] Steps/second: 0.177520, Examples/second: 25.188445\n",
      "I0710 12:59:49.369045 140295626643200 trainer.py:508] step:  8151, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:36.27504 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3353039 log_pplx:3.1091218 loss:48.422142 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.16241\n",
      "I0710 12:59:55.694513 140295626643200 summary_utils.py:349] Steps/second: 0.177514, Examples/second: 25.184548\n",
      "I0710 12:59:55.695387 140295626643200 trainer.py:508] step:  8152, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:64.023514 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3340846 log_pplx:2.6345167 loss:105.67705 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.16711\n",
      "I0710 13:00:04.739176 140295626643200 summary_utils.py:349] Steps/second: 0.177485, Examples/second: 25.175331\n",
      "I0710 13:00:04.739986 140295626643200 trainer.py:508] step:  8153, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:207.55116 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.332683 log_pplx:2.2520976 loss:157.92834 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:534.17133\n",
      "I0710 13:00:08.485242 140295626643200 summary_utils.py:349] Steps/second: 0.177501, Examples/second: 25.178555\n",
      "I0710 13:00:08.486096 140295626643200 trainer.py:508] step:  8154, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:36.987961 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3360075 log_pplx:3.0114601 loss:75.945251 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.17499\n",
      "I0710 13:00:11.505233 140295635035904 trainer.py:345] Write summary @8154\n",
      "I0710 13:00:19.082504 140295626643200 summary_utils.py:349] Steps/second: 0.177458, Examples/second: 25.169391\n",
      "I0710 13:00:19.083672 140295626643200 trainer.py:508] step:  8155, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:79.12999 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3340873 log_pplx:2.6438773 loss:102.91292 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.17889\n",
      "I0710 13:00:27.988464 140295626643200 summary_utils.py:349] Steps/second: 0.177429, Examples/second: 25.162324\n",
      "I0710 13:00:27.989572 140295626643200 trainer.py:508] step:  8156, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:75.398415 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3340155 log_pplx:2.624378 loss:103.85974 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.18372\n",
      "I0710 13:00:31.962068 140295626643200 summary_utils.py:349] Steps/second: 0.177444, Examples/second: 25.169971\n",
      "I0710 13:00:31.963172 140295626643200 trainer.py:508] step:  8157, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:26.129242 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3354155 log_pplx:3.0454178 loss:47.203976 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.1889\n",
      "I0710 13:00:37.329679 140295626643200 summary_utils.py:349] Steps/second: 0.177446, Examples/second: 25.171191\n",
      "I0710 13:00:37.330706 140295626643200 trainer.py:508] step:  8158, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:42.070141 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3362881 log_pplx:3.0363812 loss:75.966454 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.19409\n",
      "I0710 13:00:46.224167 140295626643200 summary_utils.py:349] Steps/second: 0.177418, Examples/second: 25.164143\n",
      "I0710 13:00:46.225836 140295626643200 trainer.py:508] step:  8159, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:69.753014 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3340443 log_pplx:2.6108069 loss:104.13857 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.19873\n",
      "I0710 13:00:56.410656 140295626643200 summary_utils.py:349] Steps/second: 0.177378, Examples/second: 25.155510\n",
      "I0710 13:00:56.412653 140295626643200 trainer.py:508] step:  8160, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:72.803299 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3343985 log_pplx:2.6393244 loss:105.0781 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.20355\n",
      "2020-07-10 13:00:56.436662: I lingvo/core/ops/record_yielder.cc:532] Epoch 12: total records 46838\n",
      "2020-07-10 13:00:56.436751: I lingvo/core/ops/record_yielder.cc:485] Epoch 12 /tmp/punctuator_data/train.txt\n",
      "I0710 13:01:02.258347 140295626643200 summary_utils.py:349] Steps/second: 0.177377, Examples/second: 25.156142\n",
      "I0710 13:01:02.260020 140295626643200 trainer.py:508] step:  8161, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:37.998608 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3364223 log_pplx:3.0119755 loss:76.052383 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.2085\n",
      "I0710 13:01:05.879490 140295635035904 trainer.py:354] Write summary done: step 8154\n",
      "I0710 13:01:06.902153 140295626643200 summary_utils.py:349] Steps/second: 0.177385, Examples/second: 25.162952\n",
      "I0710 13:01:06.903148 140295626643200 trainer.py:508] step:  8162, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:24.901821 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3358285 log_pplx:3.1215994 loss:49.470036 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.2132\n",
      "I0710 13:01:13.409030 140295626643200 summary_utils.py:349] Steps/second: 0.177378, Examples/second: 25.158856\n",
      "I0710 13:01:13.410112 140295626643200 trainer.py:508] step:  8163, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:64.301758 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3343917 log_pplx:2.5500867 loss:102.64099 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.21796\n",
      "I0710 13:01:22.682357 140295626643200 summary_utils.py:349] Steps/second: 0.177346, Examples/second: 25.149404\n",
      "I0710 13:01:22.683085 140295626643200 trainer.py:508] step:  8164, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:379.92847 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3332206 log_pplx:2.3965816 loss:168.47971 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:534.22321\n",
      "I0710 13:01:29.104758 140295626643200 summary_utils.py:349] Steps/second: 0.177339, Examples/second: 25.145419\n",
      "I0710 13:01:29.105543 140295626643200 trainer.py:508] step:  8165, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:71.209373 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3344852 log_pplx:2.6110837 loss:104.47599 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.23163\n",
      "I0710 13:01:32.911570 140295626643200 summary_utils.py:349] Steps/second: 0.177355, Examples/second: 25.148561\n",
      "I0710 13:01:32.912370 140295626643200 trainer.py:508] step:  8166, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:57.801453 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3365638 log_pplx:3.1226768 loss:79.003716 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.23981\n",
      "I0710 13:01:39.298581 140295626643200 summary_utils.py:349] Steps/second: 0.177349, Examples/second: 25.144622\n",
      "I0710 13:01:39.299487 140295626643200 trainer.py:508] step:  8167, steps/sec: 0.18, examples/sec: 25.14 grad_norm/all/loss:140.4875 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3348635 log_pplx:2.5693898 loss:106.05157 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.24719\n",
      "I0710 13:01:42.289840 140295626643200 summary_utils.py:349] Steps/second: 0.177372, Examples/second: 25.153453\n",
      "I0710 13:01:42.290658 140295626643200 trainer.py:508] step:  8168, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:20.808622 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3359612 log_pplx:3.0077038 loss:46.431427 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.25323\n",
      "I0710 13:01:48.844245 140295626643200 summary_utils.py:349] Steps/second: 0.177364, Examples/second: 25.149309\n",
      "I0710 13:01:48.845046 140295626643200 trainer.py:508] step:  8169, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:346.75577 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.334942 log_pplx:2.6895299 loss:110.70778 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.25909\n",
      "I0710 13:01:52.627260 140295626643200 summary_utils.py:349] Steps/second: 0.177380, Examples/second: 25.152475\n",
      "I0710 13:01:52.627989 140295626643200 trainer.py:508] step:  8170, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:54.284031 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.336248 log_pplx:2.9685395 loss:72.859085 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.26385\n",
      "I0710 13:01:58.688213 140295626643200 summary_utils.py:349] Steps/second: 0.177376, Examples/second: 25.148939\n",
      "I0710 13:01:58.688970 140295626643200 trainer.py:508] step:  8171, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:72.948395 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3349701 log_pplx:2.5277414 loss:103.35303 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.26929\n",
      "I0710 13:02:00.860621 140295626643200 summary_utils.py:349] Steps/second: 0.177406, Examples/second: 25.171261\n",
      "I0710 13:02:00.861566 140295626643200 trainer.py:508] step:  8172, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:11.670527 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3346583 log_pplx:3.2747355 loss:23.837772 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:534.2749\n",
      "I0710 13:02:07.191578 140295626643200 summary_utils.py:349] Steps/second: 0.177400, Examples/second: 25.167389\n",
      "I0710 13:02:07.192350 140295626643200 trainer.py:508] step:  8173, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:92.980194 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3347073 log_pplx:2.6022246 loss:104.96724 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.2804\n",
      "I0710 13:02:10.984993 140295626643200 summary_utils.py:349] Steps/second: 0.177416, Examples/second: 25.170537\n",
      "I0710 13:02:10.985961 140295626643200 trainer.py:508] step:  8174, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:64.430473 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3367969 log_pplx:3.0256405 loss:75.546463 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.28656\n",
      "I0710 13:02:13.957859 140295626643200 summary_utils.py:349] Steps/second: 0.177439, Examples/second: 25.179373\n",
      "I0710 13:02:13.958690 140295626643200 trainer.py:508] step:  8175, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:23.334059 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3362167 log_pplx:3.028347 loss:47.353409 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.2934\n",
      "I0710 13:02:20.195383 140295626643200 summary_utils.py:349] Steps/second: 0.177434, Examples/second: 25.175616\n",
      "I0710 13:02:20.196221 140295626643200 trainer.py:508] step:  8176, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:87.708977 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3348684 log_pplx:2.5395408 loss:104.47035 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.2998\n",
      "I0710 13:02:28.537126 140295626643200 summary_utils.py:349] Steps/second: 0.177410, Examples/second: 25.167330\n",
      "I0710 13:02:28.537992 140295626643200 trainer.py:508] step:  8177, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:194.10939 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3334459 log_pplx:2.2941029 loss:156.85928 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:534.30627\n",
      "I0710 13:02:34.791789 140295626643200 summary_utils.py:349] Steps/second: 0.177405, Examples/second: 25.163558\n",
      "I0710 13:02:34.792584 140295626643200 trainer.py:508] step:  8178, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:71.977974 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.334892 log_pplx:2.5454731 loss:100.76892 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.31165\n",
      "I0710 13:02:38.677238 140295626643200 summary_utils.py:349] Steps/second: 0.177420, Examples/second: 25.166590\n",
      "I0710 13:02:38.678061 140295626643200 trainer.py:508] step:  8179, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:47.613129 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3369635 log_pplx:3.0145082 loss:76.229385 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.31696\n",
      "I0710 13:02:45.210123 140295626643200 summary_utils.py:349] Steps/second: 0.177413, Examples/second: 25.162479\n",
      "I0710 13:02:45.211046 140295626643200 trainer.py:508] step:  8180, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:71.006638 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3348821 log_pplx:2.5694292 loss:105.21812 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.32208\n",
      "I0710 13:02:48.246978 140295626643200 summary_utils.py:349] Steps/second: 0.177435, Examples/second: 25.171223\n",
      "I0710 13:02:48.248005 140295626643200 trainer.py:508] step:  8181, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:31.174858 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.33604 log_pplx:3.0776443 loss:47.859779 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.32739\n",
      "I0710 13:02:54.668669 140295626643200 summary_utils.py:349] Steps/second: 0.177428, Examples/second: 25.167249\n",
      "I0710 13:02:54.669645 140295626643200 trainer.py:508] step:  8182, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:74.436516 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3351253 log_pplx:2.5975521 loss:104.64888 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.3324\n",
      "I0710 13:02:58.550177 140295626643200 summary_utils.py:349] Steps/second: 0.177443, Examples/second: 25.170282\n",
      "I0710 13:02:58.550956 140295626643200 trainer.py:508] step:  8183, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:39.528259 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3371173 log_pplx:3.0656629 loss:77.254707 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.33765\n",
      "I0710 13:03:04.701805 140295626643200 summary_utils.py:349] Steps/second: 0.177439, Examples/second: 25.166641\n",
      "I0710 13:03:04.702678 140295626643200 trainer.py:508] step:  8184, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:72.656403 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.335328 log_pplx:2.6052458 loss:104.76344 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.34283\n",
      "I0710 13:03:10.817298 140295626643200 summary_utils.py:349] Steps/second: 0.177435, Examples/second: 25.163046\n",
      "I0710 13:03:10.818167 140295626643200 trainer.py:508] step:  8185, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:77.489647 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3356104 log_pplx:2.5571227 loss:108.80556 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.34863\n",
      "I0710 13:03:14.598462 140295626643200 summary_utils.py:349] Steps/second: 0.177451, Examples/second: 25.166200\n",
      "I0710 13:03:14.599290 140295626643200 trainer.py:508] step:  8186, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:40.433495 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3371509 log_pplx:2.9630945 loss:74.466263 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.35504\n",
      "I0710 13:03:17.645451 140295626643200 summary_utils.py:349] Steps/second: 0.177473, Examples/second: 25.174919\n",
      "I0710 13:03:17.646183 140295626643200 trainer.py:508] step:  8187, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:26.254213 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3364209 log_pplx:3.0980151 loss:47.644089 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.36084\n",
      "I0710 13:03:23.970726 140295626643200 summary_utils.py:349] Steps/second: 0.177467, Examples/second: 25.171067\n",
      "I0710 13:03:23.971492 140295626643200 trainer.py:508] step:  8188, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:71.030563 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.33529 log_pplx:2.5829749 loss:103.60958 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.36621\n",
      "I0710 13:03:33.152301 140295626643200 summary_utils.py:349] Steps/second: 0.177437, Examples/second: 25.161782\n",
      "I0710 13:03:33.153073 140295626643200 trainer.py:508] step:  8189, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:165.69756 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3348213 log_pplx:2.197736 loss:159.50067 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:534.37134\n",
      "I0710 13:03:39.295529 140295626643200 summary_utils.py:349] Steps/second: 0.177432, Examples/second: 25.158160\n",
      "I0710 13:03:39.296277 140295626643200 trainer.py:508] step:  8190, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:75.443016 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3356084 log_pplx:2.6260302 loss:106.48552 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.37726\n",
      "I0710 13:03:45.705399 140295626643200 summary_utils.py:349] Steps/second: 0.177425, Examples/second: 25.154214\n",
      "I0710 13:03:45.706314 140295626643200 trainer.py:508] step:  8191, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:76.8741 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3351883 log_pplx:2.5321352 loss:101.12716 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.383\n",
      "I0710 13:03:49.479978 140295626643200 summary_utils.py:349] Steps/second: 0.177442, Examples/second: 25.157372\n",
      "I0710 13:03:49.480816 140295626643200 trainer.py:508] step:  8192, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:37.109062 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3371677 log_pplx:2.959981 loss:73.999527 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.38861\n",
      "I0710 13:03:52.521621 140295626643200 summary_utils.py:349] Steps/second: 0.177464, Examples/second: 25.166084\n",
      "I0710 13:03:52.522420 140295626643200 trainer.py:508] step:  8193, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:28.847885 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3370905 log_pplx:3.0684607 loss:48.400177 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.39423\n",
      "I0710 13:03:58.518959 140295626643200 summary_utils.py:349] Steps/second: 0.177461, Examples/second: 25.162642\n",
      "I0710 13:03:58.519842 140295626643200 trainer.py:508] step:  8194, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:67.482452 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3351823 log_pplx:2.4796638 loss:97.574768 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.39984\n",
      "I0710 13:04:08.883815 140295626643200 summary_utils.py:349] Steps/second: 0.177420, Examples/second: 25.151933\n",
      "I0710 13:04:08.884806 140295626643200 trainer.py:508] step:  8195, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:135.79994 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3346099 log_pplx:2.212702 loss:163.13147 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:534.40552\n",
      "I0710 13:04:11.073750 140295626643200 summary_utils.py:349] Steps/second: 0.177450, Examples/second: 25.174092\n",
      "I0710 13:04:11.074506 140295626643200 trainer.py:508] step:  8196, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:11.317049 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3356853 log_pplx:3.2119472 loss:23.989229 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:534.41119\n",
      "I0710 13:04:17.342844 140295626643200 summary_utils.py:349] Steps/second: 0.177444, Examples/second: 25.170319\n",
      "I0710 13:04:17.343672 140295626643200 trainer.py:508] step:  8197, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:65.618378 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3359396 log_pplx:2.5462253 loss:106.81416 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.41656\n",
      "I0710 13:04:21.231796 140295626643200 summary_utils.py:349] Steps/second: 0.177459, Examples/second: 25.173330\n",
      "I0710 13:04:21.232736 140295626643200 trainer.py:508] step:  8198, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:39.733353 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3374104 log_pplx:3.0079598 loss:74.785408 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.42206\n",
      "I0710 13:04:24.331166 140295626643200 summary_utils.py:349] Steps/second: 0.177481, Examples/second: 25.181956\n",
      "I0710 13:04:24.331935 140295626643200 trainer.py:508] step:  8199, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:24.178448 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3363923 log_pplx:3.0120261 loss:45.815735 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.42737\n",
      "I0710 13:04:30.702562 140295626643200 summary_utils.py:349] Steps/second: 0.177475, Examples/second: 25.178058\n",
      "I0710 13:04:30.703360 140295626643200 trainer.py:508] step:  8200, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:70.295647 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3358468 log_pplx:2.5224495 loss:104.33482 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.43243\n",
      "I0710 13:04:36.980680 140295626643200 summary_utils.py:349] Steps/second: 0.177469, Examples/second: 25.174277\n",
      "I0710 13:04:36.981538 140295626643200 trainer.py:508] step:  8201, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:65.012314 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3357321 log_pplx:2.484199 loss:102.50426 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.43701\n",
      "I0710 13:04:40.752071 140295626643200 summary_utils.py:349] Steps/second: 0.177485, Examples/second: 25.177427\n",
      "I0710 13:04:40.752881 140295626643200 trainer.py:508] step:  8202, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:34.238113 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3374165 log_pplx:2.9622211 loss:73.833366 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.44141\n",
      "I0710 13:04:46.867689 140295626643200 summary_utils.py:349] Steps/second: 0.177481, Examples/second: 25.173846\n",
      "I0710 13:04:46.868583 140295626643200 trainer.py:508] step:  8203, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:59.632862 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3354524 log_pplx:2.5219581 loss:101.85558 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.44568\n",
      "I0710 13:04:49.928693 140295626643200 summary_utils.py:349] Steps/second: 0.177503, Examples/second: 25.182508\n",
      "I0710 13:04:49.929449 140295626643200 trainer.py:508] step:  8204, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:20.431787 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3365507 log_pplx:2.9393096 loss:45.616711 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.45013\n",
      "I0710 13:04:56.090375 140295635035904 checkpointer.py:116] Save checkpoint\n",
      "I0710 13:04:59.414349 140295626643200 summary_utils.py:349] Steps/second: 0.177470, Examples/second: 25.172886\n",
      "I0710 13:04:59.415116 140295626643200 trainer.py:508] step:  8205, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:126.51724 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3349159 log_pplx:2.1893775 loss:161.08347 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:534.45441\n",
      "WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "W0710 13:05:01.416117 140295635035904 meta_graph.py:437] Issue encountered when serializing __batch_norm_update_dict.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'dict' object has no attribute 'name'\n",
      "I0710 13:05:01.874586 140295635035904 checkpointer.py:118] Save checkpoint done: /tmp/punctuator/train/ckpt-00008204\n",
      "I0710 13:05:05.566944 140295626643200 summary_utils.py:349] Steps/second: 0.177466, Examples/second: 25.169264\n",
      "I0710 13:05:05.567692 140295626643200 trainer.py:508] step:  8206, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:65.16481 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3354354 log_pplx:2.5410423 loss:98.592438 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.4588\n",
      "I0710 13:05:09.316951 140295626643200 summary_utils.py:349] Steps/second: 0.177482, Examples/second: 25.172437\n",
      "I0710 13:05:09.317752 140295626643200 trainer.py:508] step:  8207, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:38.125427 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3373837 log_pplx:2.962605 loss:73.454086 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.46381\n",
      "I0710 13:05:15.380995 140295626643200 summary_utils.py:349] Steps/second: 0.177478, Examples/second: 25.168925\n",
      "I0710 13:05:15.382057 140295626643200 trainer.py:508] step:  8208, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:60.870014 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3356378 log_pplx:2.5326722 loss:102.66821 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.46875\n",
      "I0710 13:05:21.898671 140295626643200 summary_utils.py:349] Steps/second: 0.177471, Examples/second: 25.164863\n",
      "I0710 13:05:21.899468 140295626643200 trainer.py:508] step:  8209, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:60.988842 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3358173 log_pplx:2.5014901 loss:101.71684 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.47375\n",
      "I0710 13:05:24.984504 140295626643200 summary_utils.py:349] Steps/second: 0.177492, Examples/second: 25.173481\n",
      "I0710 13:05:24.985670 140295626643200 trainer.py:508] step:  8210, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:24.157038 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3372095 log_pplx:3.0127838 loss:46.392166 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.4787\n",
      "I0710 13:05:28.864087 140295626643200 summary_utils.py:349] Steps/second: 0.177507, Examples/second: 25.176493\n",
      "I0710 13:05:28.865191 140295626643200 trainer.py:508] step:  8211, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:38.083797 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3378359 log_pplx:2.946383 loss:74.469826 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.48334\n",
      "I0710 13:05:35.267287 140295626643200 summary_utils.py:349] Steps/second: 0.177501, Examples/second: 25.172571\n",
      "I0710 13:05:35.268155 140295626643200 trainer.py:508] step:  8212, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:67.548782 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3361123 log_pplx:2.5445957 loss:106.20506 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.48767\n",
      "I0710 13:05:41.717201 140295626643200 summary_utils.py:349] Steps/second: 0.177494, Examples/second: 25.168594\n",
      "I0710 13:05:41.718320 140295626643200 trainer.py:508] step:  8213, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:63.218334 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.336035 log_pplx:2.4606879 loss:99.811653 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.49194\n",
      "I0710 13:05:45.635244 140295626643200 summary_utils.py:349] Steps/second: 0.177509, Examples/second: 25.171557\n",
      "I0710 13:05:45.636043 140295626643200 trainer.py:508] step:  8214, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:35.316563 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3375082 log_pplx:2.9710739 loss:75.261009 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.49646\n",
      "I0710 13:05:52.090196 140295626643200 summary_utils.py:349] Steps/second: 0.177502, Examples/second: 25.167577\n",
      "I0710 13:05:52.090980 140295626643200 trainer.py:508] step:  8215, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:59.333881 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3359919 log_pplx:2.4352641 loss:99.784935 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.50116\n",
      "I0710 13:05:55.164037 140295626643200 summary_utils.py:349] Steps/second: 0.177523, Examples/second: 25.176197\n",
      "I0710 13:05:55.164832 140295626643200 trainer.py:508] step:  8216, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:24.388372 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3372777 log_pplx:3.0282593 loss:47.671425 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.50586\n",
      "I0710 13:06:01.462337 140295626643200 summary_utils.py:349] Steps/second: 0.177518, Examples/second: 25.172407\n",
      "I0710 13:06:01.463130 140295626643200 trainer.py:508] step:  8217, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:62.285595 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3363354 log_pplx:2.5142319 loss:103.5235 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.51044\n",
      "I0710 13:06:10.891468 140295626643200 summary_utils.py:349] Steps/second: 0.177485, Examples/second: 25.162891\n",
      "I0710 13:06:10.892262 140295626643200 trainer.py:508] step:  8218, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:111.75287 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3346367 log_pplx:2.1210201 loss:153.13765 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:534.5152\n",
      "I0710 13:06:14.722212 140295626643200 summary_utils.py:349] Steps/second: 0.177501, Examples/second: 25.165957\n",
      "I0710 13:06:14.722959 140295626643200 trainer.py:508] step:  8219, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:39.702145 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3382195 log_pplx:2.9767833 loss:76.168442 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.51984\n",
      "I0710 13:06:21.142245 140295626643200 summary_utils.py:349] Steps/second: 0.177494, Examples/second: 25.162026\n",
      "I0710 13:06:21.143102 140295626643200 trainer.py:508] step:  8220, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:60.143604 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3354685 log_pplx:2.4547763 loss:98.405853 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.52423\n",
      "I0710 13:06:23.420001 140295626643200 summary_utils.py:349] Steps/second: 0.177523, Examples/second: 25.183937\n",
      "I0710 13:06:23.420855 140295626643200 trainer.py:508] step:  8221, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:12.900196 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.335979 log_pplx:3.2276838 loss:23.753735 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:534.52863\n",
      "I0710 13:06:26.649249 140295626643200 summary_utils.py:349] Steps/second: 0.177543, Examples/second: 25.192352\n",
      "I0710 13:06:26.650169 140295626643200 trainer.py:508] step:  8222, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:20.341244 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3371983 log_pplx:3.0070488 loss:46.527035 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.53271\n",
      "I0710 13:06:32.824025 140295626643200 summary_utils.py:349] Steps/second: 0.177539, Examples/second: 25.188713\n",
      "I0710 13:06:32.824838 140295626643200 trainer.py:508] step:  8223, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:58.641277 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3357936 log_pplx:2.4483037 loss:96.769203 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.53662\n",
      "I0710 13:06:36.682505 140295626643200 summary_utils.py:349] Steps/second: 0.177554, Examples/second: 25.191738\n",
      "I0710 13:06:36.683545 140295626643200 trainer.py:508] step:  8224, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:35.191689 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3378962 log_pplx:2.9060364 loss:73.540886 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.54095\n",
      "I0710 13:06:42.858625 140295626643200 summary_utils.py:349] Steps/second: 0.177549, Examples/second: 25.188099\n",
      "I0710 13:06:42.859472 140295626643200 trainer.py:508] step:  8225, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:64.09626 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3359901 log_pplx:2.482574 loss:99.178818 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.54529\n",
      "I0710 13:06:49.154419 140295626643200 summary_utils.py:349] Steps/second: 0.177543, Examples/second: 25.184317\n",
      "I0710 13:06:49.155203 140295626643200 trainer.py:508] step:  8226, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:59.711079 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3360115 log_pplx:2.5044513 loss:99.270187 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.54999\n",
      "I0710 13:06:52.954271 140295626643200 summary_utils.py:349] Steps/second: 0.177559, Examples/second: 25.187411\n",
      "I0710 13:06:52.955235 140295626643200 trainer.py:508] step:  8227, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:34.76207 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3381431 log_pplx:2.955759 loss:74.171074 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.55487\n",
      "I0710 13:06:56.031095 140295626643200 summary_utils.py:349] Steps/second: 0.177581, Examples/second: 25.195999\n",
      "I0710 13:06:56.032153 140295626643200 trainer.py:508] step:  8228, steps/sec: 0.18, examples/sec: 25.20 grad_norm/all/loss:21.545191 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3376166 log_pplx:2.9704726 loss:46.216377 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.55951\n",
      "I0710 13:07:02.559014 140295626643200 summary_utils.py:349] Steps/second: 0.177573, Examples/second: 25.191936\n",
      "I0710 13:07:02.559839 140295626643200 trainer.py:508] step:  8229, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:61.472065 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3361754 log_pplx:2.4656277 loss:95.512245 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.56378\n",
      "I0710 13:07:11.959858 140295626643200 summary_utils.py:349] Steps/second: 0.177541, Examples/second: 25.182474\n",
      "I0710 13:07:11.960663 140295626643200 trainer.py:508] step:  8230, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:115.07982 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3352211 log_pplx:2.1848972 loss:157.31258 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:534.56775\n",
      "I0710 13:07:18.061473 140295626643200 summary_utils.py:349] Steps/second: 0.177537, Examples/second: 25.178934\n",
      "I0710 13:07:18.062308 140295626643200 trainer.py:508] step:  8231, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:58.379902 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3363922 log_pplx:2.486901 loss:100.22211 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.57159\n",
      "I0710 13:07:21.871289 140295626643200 summary_utils.py:349] Steps/second: 0.177553, Examples/second: 25.182013\n",
      "I0710 13:07:21.872074 140295626643200 trainer.py:508] step:  8232, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:36.074459 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3381352 log_pplx:2.95014 loss:73.956322 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.57532\n",
      "I0710 13:07:28.122401 140295626643200 summary_utils.py:349] Steps/second: 0.177547, Examples/second: 25.178294\n",
      "I0710 13:07:28.123185 140295626643200 trainer.py:508] step:  8233, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:60.9007 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3364319 log_pplx:2.4637175 loss:99.503395 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.5788\n",
      "I0710 13:07:31.203679 140295626643200 summary_utils.py:349] Steps/second: 0.177569, Examples/second: 25.186864\n",
      "I0710 13:07:31.204479 140295626643200 trainer.py:508] step:  8234, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:20.941839 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3375504 log_pplx:3.0008035 loss:45.867744 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.58246\n",
      "I0710 13:07:37.460510 140295626643200 summary_utils.py:349] Steps/second: 0.177564, Examples/second: 25.183138\n",
      "I0710 13:07:37.461283 140295626643200 trainer.py:508] step:  8235, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:63.154461 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3364913 log_pplx:2.5256023 loss:104.59151 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.58612\n",
      "I0710 13:07:48.260240 140295626643200 summary_utils.py:349] Steps/second: 0.177520, Examples/second: 25.172005\n",
      "I0710 13:07:48.261021 140295626643200 trainer.py:508] step:  8236, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:123.29366 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3354701 log_pplx:2.1566794 loss:153.71733 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:534.59003\n",
      "I0710 13:07:52.018591 140295626643200 summary_utils.py:349] Steps/second: 0.177536, Examples/second: 25.175143\n",
      "I0710 13:07:52.019375 140295626643200 base_runner.py:111] step:  8237, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:37.432907 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3381081 log_pplx:2.9127715 loss:72.691849 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.59424\n",
      "I0710 13:07:58.289259 140295626643200 summary_utils.py:349] Steps/second: 0.177530, Examples/second: 25.171408\n",
      "I0710 13:07:58.290080 140295626643200 trainer.py:508] step:  8238, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:62.67268 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3366792 log_pplx:2.483954 loss:101.99736 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.59869\n",
      "I0710 13:08:04.516147 140295626643200 summary_utils.py:349] Steps/second: 0.177525, Examples/second: 25.167728\n",
      "I0710 13:08:04.516924 140295626643200 trainer.py:508] step:  8239, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:60.978951 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3366351 log_pplx:2.4482634 loss:102.58225 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.6037\n",
      "I0710 13:08:07.579838 140295626643200 summary_utils.py:349] Steps/second: 0.177547, Examples/second: 25.176306\n",
      "I0710 13:08:07.580632 140295626643200 trainer.py:508] step:  8240, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:20.797747 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.337926 log_pplx:2.9831245 loss:46.331654 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.60864\n",
      "I0710 13:08:11.360568 140295626643200 summary_utils.py:349] Steps/second: 0.177563, Examples/second: 25.179413\n",
      "I0710 13:08:11.361341 140295626643200 trainer.py:508] step:  8241, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:40.029205 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3382719 log_pplx:2.9381194 loss:74.242599 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.61328\n",
      "I0710 13:08:17.466265 140295626643200 summary_utils.py:349] Steps/second: 0.177559, Examples/second: 25.175879\n",
      "I0710 13:08:17.467137 140295626643200 trainer.py:508] step:  8242, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:60.50452 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3366925 log_pplx:2.5688105 loss:103.42673 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.61768\n",
      "I0710 13:08:23.682389 140295626643200 summary_utils.py:349] Steps/second: 0.177554, Examples/second: 25.172214\n",
      "I0710 13:08:23.683273 140295626643200 trainer.py:508] step:  8243, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:56.701885 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3362589 log_pplx:2.4461772 loss:96.776886 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.62219\n",
      "I0710 13:08:25.874271 140295626643200 summary_utils.py:349] Steps/second: 0.177583, Examples/second: 25.194098\n",
      "I0710 13:08:25.875112 140295626643200 trainer.py:508] step:  8244, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:10.190709 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3363219 log_pplx:3.1407027 loss:22.990923 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:534.62671\n",
      "I0710 13:08:37.173047 140295626643200 summary_utils.py:349] Steps/second: 0.177535, Examples/second: 25.182384\n",
      "I0710 13:08:37.174264 140295626643200 trainer.py:508] step:  8245, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:140.73363 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3359064 log_pplx:2.1958919 loss:162.93515 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:534.63092\n",
      "I0710 13:08:41.021263 140295626643200 summary_utils.py:349] Steps/second: 0.177550, Examples/second: 25.185404\n",
      "I0710 13:08:41.022072 140295626643200 trainer.py:508] step:  8246, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:38.33625 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3382684 log_pplx:2.8981543 loss:73.105942 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.63477\n",
      "I0710 13:08:47.435621 140295626643200 summary_utils.py:349] Steps/second: 0.177543, Examples/second: 25.181502\n",
      "I0710 13:08:47.436468 140295626643200 trainer.py:508] step:  8247, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:60.258289 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.336681 log_pplx:2.4622757 loss:99.599052 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.63898\n",
      "I0710 13:08:50.468585 140295626643200 summary_utils.py:349] Steps/second: 0.177566, Examples/second: 25.190097\n",
      "I0710 13:08:50.469342 140295626643200 trainer.py:508] step:  8248, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:20.005867 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3380531 log_pplx:3.0011909 loss:47.046017 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.64355\n",
      "I0710 13:08:56.861798 140295626643200 summary_utils.py:349] Steps/second: 0.177559, Examples/second: 25.186220\n",
      "I0710 13:08:56.862840 140295626643200 trainer.py:508] step:  8249, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:64.728539 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3371902 log_pplx:2.4910302 loss:107.33225 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.64807\n",
      "I0710 13:09:00.669833 140295626643200 summary_utils.py:349] Steps/second: 0.177575, Examples/second: 25.189285\n",
      "I0710 13:09:00.670626 140295626643200 trainer.py:508] step:  8250, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:40.257935 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3389182 log_pplx:2.9393358 loss:74.328453 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.65295\n",
      "I0710 13:09:06.978693 140295626643200 summary_utils.py:349] Steps/second: 0.177569, Examples/second: 25.185512\n",
      "I0710 13:09:06.979503 140295626643200 trainer.py:508] step:  8251, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:59.082302 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3366256 log_pplx:2.4677405 loss:99.912643 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.65747\n",
      "I0710 13:09:13.309265 140295626643200 summary_utils.py:349] Steps/second: 0.177563, Examples/second: 25.181715\n",
      "I0710 13:09:13.310082 140295626643200 trainer.py:508] step:  8252, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:62.383293 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.336882 log_pplx:2.5041802 loss:102.95311 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.66211\n",
      "I0710 13:09:16.378449 140295626643200 summary_utils.py:349] Steps/second: 0.177585, Examples/second: 25.190256\n",
      "I0710 13:09:16.379259 140295626643200 trainer.py:508] step:  8253, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:22.681463 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3376776 log_pplx:2.9445622 loss:45.491184 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.66681\n",
      "I0710 13:09:25.275004 140295626643200 summary_utils.py:349] Steps/second: 0.177557, Examples/second: 25.181461\n",
      "I0710 13:09:25.275785 140295626643200 trainer.py:508] step:  8254, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:122.97125 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.336064 log_pplx:2.1751604 loss:160.47246 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:534.67102\n",
      "I0710 13:09:26.349780 140295635035904 trainer.py:345] Write summary @8254\n",
      "I0710 13:09:33.389923 140295626643200 summary_utils.py:349] Steps/second: 0.177536, Examples/second: 25.179344\n",
      "I0710 13:09:33.391124 140295626643200 trainer.py:508] step:  8255, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:37.086575 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3384906 log_pplx:2.885787 loss:72.469322 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.67529\n",
      "I0710 13:09:42.268820 140295626643200 summary_utils.py:349] Steps/second: 0.177508, Examples/second: 25.172491\n",
      "I0710 13:09:42.271136 140295626643200 trainer.py:508] step:  8256, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:61.76857 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3366632 log_pplx:2.4651082 loss:96.447365 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.67981\n",
      "I0710 13:09:51.516499 140295626643200 summary_utils.py:349] Steps/second: 0.177478, Examples/second: 25.165202\n",
      "I0710 13:09:51.517716 140295626643200 trainer.py:508] step:  8257, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:63.123425 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3367656 log_pplx:2.5280008 loss:102.28923 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.68451\n",
      "I0710 13:09:55.453687 140295626643200 summary_utils.py:349] Steps/second: 0.177492, Examples/second: 25.172687\n",
      "I0710 13:09:55.455340 140295626643200 trainer.py:508] step:  8258, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:21.306952 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3383408 log_pplx:2.9835718 loss:46.501762 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.68945\n",
      "I0710 13:10:05.172396 140295626643200 summary_utils.py:349] Steps/second: 0.177458, Examples/second: 25.164837\n",
      "I0710 13:10:05.173583 140295626643200 trainer.py:508] step:  8259, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:69.47007 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3372233 log_pplx:2.5326242 loss:104.66069 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.69409\n",
      "I0710 13:10:11.030557 140295626643200 summary_utils.py:349] Steps/second: 0.177456, Examples/second: 25.165437\n",
      "I0710 13:10:11.031630 140295626643200 trainer.py:508] step:  8260, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:37.636635 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3386155 log_pplx:2.8758707 loss:72.741554 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.6991\n",
      "I0710 13:10:20.211293 140295635035904 trainer.py:354] Write summary done: step 8254\n",
      "I0710 13:10:21.096826 140295626643200 summary_utils.py:349] Steps/second: 0.177418, Examples/second: 25.157179\n",
      "I0710 13:10:21.097581 140295626643200 trainer.py:508] step:  8261, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:62.336857 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3369021 log_pplx:2.5312803 loss:99.447678 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.70361\n",
      "I0710 13:10:27.396971 140295626643200 summary_utils.py:349] Steps/second: 0.177413, Examples/second: 25.153441\n",
      "I0710 13:10:27.397822 140295626643200 trainer.py:508] step:  8262, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:61.622097 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3364904 log_pplx:2.4449646 loss:96.54554 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.70837\n",
      "I0710 13:10:31.239747 140295626643200 summary_utils.py:349] Steps/second: 0.177428, Examples/second: 25.156457\n",
      "I0710 13:10:31.240530 140295626643200 trainer.py:508] step:  8263, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:37.535416 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3386523 log_pplx:2.9557264 loss:73.338951 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.71283\n",
      "I0710 13:10:34.303053 140295626643200 summary_utils.py:349] Steps/second: 0.177449, Examples/second: 25.164977\n",
      "I0710 13:10:34.303797 140295626643200 trainer.py:508] step:  8264, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:21.144518 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3382298 log_pplx:3.0174713 loss:46.664726 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.71729\n",
      "I0710 13:10:40.640753 140295626643200 summary_utils.py:349] Steps/second: 0.177444, Examples/second: 25.161193\n",
      "I0710 13:10:40.641583 140295626643200 trainer.py:508] step:  8265, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:59.558029 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.336828 log_pplx:2.5056157 loss:99.50428 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.72174\n",
      "I0710 13:10:49.528287 140295626643200 summary_utils.py:349] Steps/second: 0.177416, Examples/second: 25.152457\n",
      "I0710 13:10:49.529344 140295626643200 trainer.py:508] step:  8266, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:102.36278 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3359406 log_pplx:2.0693092 loss:143.66179 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:534.72601\n",
      "I0710 13:10:55.789550 140295626643200 summary_utils.py:349] Steps/second: 0.177411, Examples/second: 25.148771\n",
      "I0710 13:10:55.790345 140295626643200 trainer.py:508] step:  8267, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:58.253067 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.33705 log_pplx:2.4567733 loss:97.871712 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.7312\n",
      "I0710 13:10:59.681143 140295626643200 summary_utils.py:349] Steps/second: 0.177426, Examples/second: 25.151726\n",
      "I0710 13:10:59.681934 140295626643200 trainer.py:508] step:  8268, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:37.427811 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3387743 log_pplx:2.8857157 loss:73.477539 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.73639\n",
      "I0710 13:11:01.880331 140295626643200 summary_utils.py:349] Steps/second: 0.177455, Examples/second: 25.173441\n",
      "I0710 13:11:01.881094 140295626643200 trainer.py:508] step:  8269, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:17.61393 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3366582 log_pplx:3.1621168 loss:23.110626 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:534.74133\n",
      "I0710 13:11:08.284014 140295626643200 summary_utils.py:349] Steps/second: 0.177448, Examples/second: 25.169581\n",
      "I0710 13:11:08.284770 140295626643200 trainer.py:508] step:  8270, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:60.951054 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3374864 log_pplx:2.4940794 loss:102.35078 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.74585\n",
      "I0710 13:11:11.320873 140295626643200 summary_utils.py:349] Steps/second: 0.177470, Examples/second: 25.178116\n",
      "I0710 13:11:11.321641 140295626643200 trainer.py:508] step:  8271, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:21.394585 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3387246 log_pplx:2.9900732 loss:46.334454 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.75049\n",
      "I0710 13:11:17.540581 140295626643200 summary_utils.py:349] Steps/second: 0.177465, Examples/second: 25.174476\n",
      "I0710 13:11:17.541388 140295626643200 trainer.py:508] step:  8272, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:78.417603 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3373703 log_pplx:2.6059906 loss:104.79341 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.75482\n",
      "I0710 13:11:21.326812 140295626643200 summary_utils.py:349] Steps/second: 0.177481, Examples/second: 25.177549\n",
      "I0710 13:11:21.327543 140295626643200 trainer.py:508] step:  8273, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:36.508411 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3390011 log_pplx:2.8967428 loss:72.798767 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.75897\n",
      "I0710 13:11:27.476416 140295626643200 summary_utils.py:349] Steps/second: 0.177476, Examples/second: 25.173995\n",
      "I0710 13:11:27.477246 140295626643200 trainer.py:508] step:  8274, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:64.087982 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3373362 log_pplx:2.4855058 loss:101.40865 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.76324\n",
      "I0710 13:11:34.242285 140295626643200 summary_utils.py:349] Steps/second: 0.177467, Examples/second: 25.169707\n",
      "I0710 13:11:34.243086 140295626643200 trainer.py:508] step:  8275, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:58.331181 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.33726 log_pplx:2.4309294 loss:97.996841 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.76813\n",
      "I0710 13:11:44.598771 140295626643200 summary_utils.py:349] Steps/second: 0.177427, Examples/second: 25.159235\n",
      "I0710 13:11:44.599508 140295626643200 trainer.py:508] step:  8276, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:156.97983 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3359466 log_pplx:2.0703003 loss:148.38878 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:534.77332\n",
      "I0710 13:11:47.657250 140295626643200 summary_utils.py:349] Steps/second: 0.177449, Examples/second: 25.167730\n",
      "I0710 13:11:47.658066 140295626643200 trainer.py:508] step:  8277, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:22.068344 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3385531 log_pplx:3.0219276 loss:46.710033 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.77844\n",
      "I0710 13:11:51.465606 140295626643200 summary_utils.py:349] Steps/second: 0.177464, Examples/second: 25.170774\n",
      "I0710 13:11:51.466380 140295626643200 trainer.py:508] step:  8278, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:34.695892 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3393475 log_pplx:2.8470912 loss:72.974518 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.78363\n",
      "I0710 13:11:57.627556 140295626643200 summary_utils.py:349] Steps/second: 0.177460, Examples/second: 25.167212\n",
      "I0710 13:11:57.628296 140295626643200 trainer.py:508] step:  8279, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:65.182373 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3374128 log_pplx:2.4833689 loss:102.78042 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.7887\n",
      "I0710 13:12:03.961149 140295626643200 summary_utils.py:349] Steps/second: 0.177454, Examples/second: 25.163448\n",
      "I0710 13:12:03.961952 140295626643200 trainer.py:508] step:  8280, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:81.641823 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3376675 log_pplx:2.5172329 loss:105.81816 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.79333\n",
      "I0710 13:12:07.800631 140295626643200 summary_utils.py:349] Steps/second: 0.177469, Examples/second: 25.166452\n",
      "I0710 13:12:07.801394 140295626643200 trainer.py:508] step:  8281, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:39.382725 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3390856 log_pplx:2.9612813 loss:73.32872 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.79791\n",
      "I0710 13:12:10.878431 140295626643200 summary_utils.py:349] Steps/second: 0.177490, Examples/second: 25.174914\n",
      "I0710 13:12:10.879503 140295626643200 trainer.py:508] step:  8282, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:21.845655 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3390107 log_pplx:2.9688647 loss:46.724831 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.80219\n",
      "I0710 13:12:17.180640 140295626643200 summary_utils.py:349] Steps/second: 0.177485, Examples/second: 25.171187\n",
      "I0710 13:12:17.181558 140295626643200 trainer.py:508] step:  8283, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:62.36039 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3377006 log_pplx:2.5089447 loss:101.0164 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.80646\n",
      "I0710 13:12:23.608842 140295626643200 summary_utils.py:349] Steps/second: 0.177478, Examples/second: 25.167313\n",
      "I0710 13:12:23.609601 140295626643200 trainer.py:508] step:  8284, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:64.457039 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3374084 log_pplx:2.5117826 loss:100.09453 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.8111\n",
      "I0710 13:12:33.552716 140295626643200 summary_utils.py:349] Steps/second: 0.177442, Examples/second: 25.157358\n",
      "I0710 13:12:33.553520 140295626643200 trainer.py:508] step:  8285, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:127.50121 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3362769 log_pplx:2.1177783 loss:151.95059 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:534.81604\n",
      "I0710 13:12:39.917294 140295626643200 summary_utils.py:349] Steps/second: 0.177436, Examples/second: 25.153567\n",
      "I0710 13:12:39.918066 140295626643200 trainer.py:508] step:  8286, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:59.414944 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3375334 log_pplx:2.5175245 loss:100.63805 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.82135\n",
      "I0710 13:12:43.709824 140295626643200 summary_utils.py:349] Steps/second: 0.177451, Examples/second: 25.156624\n",
      "I0710 13:12:43.710628 140295626643200 trainer.py:508] step:  8287, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:45.929279 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3394729 log_pplx:2.9750307 loss:74.412949 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.82672\n",
      "I0710 13:12:49.900265 140295626643200 summary_utils.py:349] Steps/second: 0.177447, Examples/second: 25.153041\n",
      "I0710 13:12:49.901341 140295626643200 trainer.py:508] step:  8288, steps/sec: 0.18, examples/sec: 25.15 grad_norm/all/loss:65.437668 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3377353 log_pplx:2.492656 loss:101.35762 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.83173\n",
      "I0710 13:12:52.993964 140295626643200 summary_utils.py:349] Steps/second: 0.177468, Examples/second: 25.161469\n",
      "I0710 13:12:52.994703 140295626643200 trainer.py:508] step:  8289, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:20.374163 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3391095 log_pplx:3.0101781 loss:46.681278 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.83673\n",
      "I0710 13:12:59.517941 140295626643200 summary_utils.py:349] Steps/second: 0.177460, Examples/second: 25.157490\n",
      "I0710 13:12:59.518698 140295626643200 trainer.py:508] step:  8290, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:63.900486 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3378413 log_pplx:2.4958265 loss:102.51607 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.84161\n",
      "I0710 13:13:03.235006 140295626643200 summary_utils.py:349] Steps/second: 0.177477, Examples/second: 25.160633\n",
      "I0710 13:13:03.235788 140295626643200 trainer.py:508] step:  8291, steps/sec: 0.18, examples/sec: 25.16 grad_norm/all/loss:38.320358 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3394939 log_pplx:2.953465 loss:74.464233 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.8468\n",
      "I0710 13:13:05.438615 140295626643200 summary_utils.py:349] Steps/second: 0.177505, Examples/second: 25.182215\n",
      "I0710 13:13:05.439373 140295626643200 trainer.py:508] step:  8292, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:10.804708 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3375399 log_pplx:3.2005119 loss:23.591274 lr_schedule/loss:1 num_samples_in_batch:512 var_norm/all/loss:534.85211\n",
      "I0710 13:13:11.730992 140295626643200 summary_utils.py:349] Steps/second: 0.177500, Examples/second: 25.178508\n",
      "I0710 13:13:11.731774 140295626643200 trainer.py:508] step:  8293, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:61.773102 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3379309 log_pplx:2.5067301 loss:103.96664 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.85712\n",
      "I0710 13:13:18.046645 140295626643200 summary_utils.py:349] Steps/second: 0.177494, Examples/second: 25.174775\n",
      "I0710 13:13:18.047535 140295626643200 trainer.py:508] step:  8294, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:67.452301 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3382503 log_pplx:2.5041506 loss:104.92391 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.86188\n",
      "I0710 13:13:21.144064 140295626643200 summary_utils.py:349] Steps/second: 0.177515, Examples/second: 25.183184\n",
      "I0710 13:13:21.145020 140295626643200 trainer.py:508] step:  8295, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:22.008678 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3391614 log_pplx:2.9883072 loss:48.12809 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.86652\n",
      "I0710 13:13:24.993365 140295626643200 summary_utils.py:349] Steps/second: 0.177530, Examples/second: 25.186162\n",
      "I0710 13:13:24.994304 140295626643200 trainer.py:508] step:  8296, steps/sec: 0.18, examples/sec: 25.19 grad_norm/all/loss:39.308781 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3395863 log_pplx:2.9133918 loss:74.38253 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.87097\n",
      "I0710 13:13:31.331956 140295626643200 summary_utils.py:349] Steps/second: 0.177524, Examples/second: 25.182402\n",
      "I0710 13:13:31.332694 140295626643200 trainer.py:508] step:  8297, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:63.524403 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3380456 log_pplx:2.416827 loss:99.089905 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.87561\n",
      "I0710 13:13:40.729186 140295626643200 summary_utils.py:349] Steps/second: 0.177493, Examples/second: 25.173122\n",
      "I0710 13:13:40.730251 140295626643200 trainer.py:508] step:  8298, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:108.48952 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3366207 log_pplx:2.1024492 loss:146.80351 lr_schedule/loss:1 num_samples_in_batch:40 var_norm/all/loss:534.88037\n",
      "I0710 13:13:46.711892 140295626643200 summary_utils.py:349] Steps/second: 0.177490, Examples/second: 25.169791\n",
      "I0710 13:13:46.712715 140295626643200 trainer.py:508] step:  8299, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:68.094017 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3377422 log_pplx:2.5437138 loss:101.71675 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.88538\n",
      "I0710 13:13:50.530353 140295626643200 summary_utils.py:349] Steps/second: 0.177505, Examples/second: 25.172804\n",
      "I0710 13:13:50.531118 140295626643200 trainer.py:508] step:  8300, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:35.983929 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3398122 log_pplx:3.0042286 loss:74.955505 lr_schedule/loss:1 num_samples_in_batch:160 var_norm/all/loss:534.89087\n",
      "I0710 13:13:56.631437 140295626643200 summary_utils.py:349] Steps/second: 0.177501, Examples/second: 25.169334\n",
      "I0710 13:13:56.632209 140295626643200 trainer.py:508] step:  8301, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:62.859428 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3377579 log_pplx:2.4315555 loss:96.319984 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.89618\n",
      "I0710 13:13:59.667674 140295626643200 summary_utils.py:349] Steps/second: 0.177523, Examples/second: 25.177802\n",
      "I0710 13:13:59.668433 140295626643200 trainer.py:508] step:  8302, steps/sec: 0.18, examples/sec: 25.18 grad_norm/all/loss:33.8927 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3394939 log_pplx:3.046442 loss:47.148445 lr_schedule/loss:1 num_samples_in_batch:256 var_norm/all/loss:534.90131\n",
      "I0710 13:14:05.566539 140295626643200 summary_utils.py:349] Steps/second: 0.177521, Examples/second: 25.174572\n",
      "I0710 13:14:05.567489 140295626643200 trainer.py:508] step:  8303, steps/sec: 0.18, examples/sec: 25.17 grad_norm/all/loss:64.370102 grad_scale_all/loss:1 has_nan_or_inf/loss:0 l2_loss/loss:1.3378847 log_pplx:2.5331805 loss:98.825699 lr_schedule/loss:1 num_samples_in_batch:80 var_norm/all/loss:534.90576\n"
     ]
    }
   ],
   "source": [
    "# Start tensorboard (access at http://localhost:6006)\n",
    "import os\n",
    "os.system('lsof -t -i:6006 || tensorboard --logdir=/tmp/punctuator &')\n",
    "\n",
    "!python3 -m lingvo.trainer --model=codelab.RNMTModel --mode=sync --logdir=/tmp/punctuator --saver_max_to_keep=3\\\n",
    "        --noenable_asserts --run_locally=gpu --saver_keep_checkpoint_every_n_hours=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sOQMKZiuwNKr"
   },
   "source": [
    "The following cell evaluates the model. In Lingvo, evaluation is meant to be run alongside training as a separate process that periodically looks for the latest checkpoint and evaluates it. There is only one process in Colab so running this cell will evaluate the current checkpoint then it will block indefinitely waiting for the next checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "leWsSPQ6L__H",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Lingvo does not support eager execution yet. Please disable eager execution with tf.compat.v1.disable_eager_execution() or proceed at your own risk.\n",
      "model_imports.py: Importing codelab\n",
      "model_imports.py: Imported codelab\n",
      "2020-07-09 01:07:14.749753: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-07-09 01:07:14.759201: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n",
      "2020-07-09 01:07:14.760264: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563a5cee4710 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-07-09 01:07:14.760291: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-07-09 01:07:14.763326: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-07-09 01:07:15.194023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.245258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.255073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.269867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.271244: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563a5cfa9a40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-07-09 01:07:15.271278: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2020-07-09 01:07:15.271288: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2020-07-09 01:07:15.271300: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2020-07-09 01:07:15.271312: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2020-07-09 01:07:15.274934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.276005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2020-07-09 01:07:15.276117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.277134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \n",
      "pciBusID: 0000:00:05.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2020-07-09 01:07:15.277204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.278215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 2 with properties: \n",
      "pciBusID: 0000:00:06.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2020-07-09 01:07:15.278283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.279291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 3 with properties: \n",
      "pciBusID: 0000:00:07.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2020-07-09 01:07:15.279540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-07-09 01:07:15.281491: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-07-09 01:07:15.283345: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-07-09 01:07:15.283715: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-07-09 01:07:15.285767: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-07-09 01:07:15.286908: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-07-09 01:07:15.291384: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-07-09 01:07:15.291512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.292634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.293758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.294866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.295930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.297022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.298134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.299586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.300901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2020-07-09 01:07:15.300974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-07-09 01:07:15.307831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-07-09 01:07:15.307858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 2 3 \n",
      "2020-07-09 01:07:15.307866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y Y Y \n",
      "2020-07-09 01:07:15.307871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N Y Y \n",
      "2020-07-09 01:07:15.307876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 2:   Y Y N Y \n",
      "2020-07-09 01:07:15.307882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 3:   Y Y Y N \n",
      "2020-07-09 01:07:15.308571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.309691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.310767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.311841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.312954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.314178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14576 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n",
      "2020-07-09 01:07:15.314865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.316160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 14576 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:05.0, compute capability: 7.0)\n",
      "2020-07-09 01:07:15.316947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.318173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 14576 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:06.0, compute capability: 7.0)\n",
      "2020-07-09 01:07:15.318813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.319868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 14576 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:07.0, compute capability: 7.0)\n",
      "2020-07-09 01:07:15.327816: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:35195}\n",
      "2020-07-09 01:07:15.328659: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:35195\n",
      "I0709 01:07:15.329015 140556593907072 trainer.py:1539] Job evaler_test start\n",
      "model_imports.py: Importing codelab\n",
      "model_imports.py: Imported codelab\n",
      "W0709 01:07:15.346344 140556593907072 trainer.py:1409] Exception configuring dataset test, retrying as Test: 'RNMTModel' object has no attribute 'test'\n",
      "model_imports.py: Importing codelab\n",
      "model_imports.py: Imported codelab\n",
      "W0709 01:07:15.359220 140556593907072 trainer.py:1413] Succeeded after retrying as Test.\n",
      "I0709 01:07:15.365286 140556593907072 base_runner.py:57] ============================================================\n",
      "I0709 01:07:15.377563 140556593907072 base_runner.py:59] allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.377707 140556593907072 base_runner.py:59] cls : type/lingvo.core.base_model/SingleTaskModel\n",
      "I0709 01:07:15.377798 140556593907072 base_runner.py:59] cluster.add_summary : NoneType\n",
      "I0709 01:07:15.377865 140556593907072 base_runner.py:59] cluster.cls : type/lingvo.core.cluster/_Cluster\n",
      "I0709 01:07:15.377938 140556593907072 base_runner.py:59] cluster.controller.cpus_per_replica : 1\n",
      "I0709 01:07:15.378005 140556593907072 base_runner.py:59] cluster.controller.devices_per_split : 1\n",
      "I0709 01:07:15.378075 140556593907072 base_runner.py:59] cluster.controller.gpus_per_replica : 0\n",
      "I0709 01:07:15.378130 140556593907072 base_runner.py:59] cluster.controller.name : '/job:localhost'\n",
      "I0709 01:07:15.378205 140556593907072 base_runner.py:59] cluster.controller.num_tpu_hosts : 0\n",
      "I0709 01:07:15.378271 140556593907072 base_runner.py:59] cluster.controller.replicas : 1\n",
      "I0709 01:07:15.378342 140556593907072 base_runner.py:59] cluster.controller.targets : ''\n",
      "I0709 01:07:15.378407 140556593907072 base_runner.py:59] cluster.controller.tpus_per_replica : 0\n",
      "I0709 01:07:15.378475 140556593907072 base_runner.py:59] cluster.decoder.cpus_per_replica : 1\n",
      "I0709 01:07:15.378540 140556593907072 base_runner.py:59] cluster.decoder.devices_per_split : 1\n",
      "I0709 01:07:15.378608 140556593907072 base_runner.py:59] cluster.decoder.gpus_per_replica : 0\n",
      "I0709 01:07:15.378671 140556593907072 base_runner.py:59] cluster.decoder.name : '/job:localhost'\n",
      "I0709 01:07:15.378738 140556593907072 base_runner.py:59] cluster.decoder.num_tpu_hosts : 0\n",
      "I0709 01:07:15.378813 140556593907072 base_runner.py:59] cluster.decoder.replicas : 1\n",
      "I0709 01:07:15.378876 140556593907072 base_runner.py:59] cluster.decoder.targets : ''\n",
      "I0709 01:07:15.378938 140556593907072 base_runner.py:59] cluster.decoder.tpus_per_replica : 0\n",
      "I0709 01:07:15.379004 140556593907072 base_runner.py:59] cluster.do_eval : True\n",
      "I0709 01:07:15.379065 140556593907072 base_runner.py:59] cluster.evaler.cpus_per_replica : 1\n",
      "I0709 01:07:15.379122 140556593907072 base_runner.py:59] cluster.evaler.devices_per_split : 1\n",
      "I0709 01:07:15.379215 140556593907072 base_runner.py:59] cluster.evaler.gpus_per_replica : 0\n",
      "I0709 01:07:15.379286 140556593907072 base_runner.py:59] cluster.evaler.name : '/job:localhost'\n",
      "I0709 01:07:15.379352 140556593907072 base_runner.py:59] cluster.evaler.num_tpu_hosts : 0\n",
      "I0709 01:07:15.379417 140556593907072 base_runner.py:59] cluster.evaler.replicas : 1\n",
      "I0709 01:07:15.379482 140556593907072 base_runner.py:59] cluster.evaler.targets : ''\n",
      "I0709 01:07:15.379546 140556593907072 base_runner.py:59] cluster.evaler.tpus_per_replica : 0\n",
      "I0709 01:07:15.379601 140556593907072 base_runner.py:59] cluster.input.cpus_per_replica : 1\n",
      "I0709 01:07:15.379661 140556593907072 base_runner.py:59] cluster.input.devices_per_split : 1\n",
      "I0709 01:07:15.379730 140556593907072 base_runner.py:59] cluster.input.gpus_per_replica : 0\n",
      "I0709 01:07:15.379793 140556593907072 base_runner.py:59] cluster.input.name : '/job:localhost'\n",
      "I0709 01:07:15.379857 140556593907072 base_runner.py:59] cluster.input.num_tpu_hosts : 0\n",
      "I0709 01:07:15.379926 140556593907072 base_runner.py:59] cluster.input.replicas : 0\n",
      "I0709 01:07:15.379992 140556593907072 base_runner.py:59] cluster.input.targets : ''\n",
      "I0709 01:07:15.380076 140556593907072 base_runner.py:59] cluster.input.tpus_per_replica : 0\n",
      "I0709 01:07:15.380146 140556593907072 base_runner.py:59] cluster.job : 'evaler'\n",
      "I0709 01:07:15.380219 140556593907072 base_runner.py:59] cluster.logdir : ''\n",
      "I0709 01:07:15.380284 140556593907072 base_runner.py:59] cluster.mode : 'async'\n",
      "I0709 01:07:15.380347 140556593907072 base_runner.py:59] cluster.ps.cpus_per_replica : 1\n",
      "I0709 01:07:15.380413 140556593907072 base_runner.py:59] cluster.ps.devices_per_split : 1\n",
      "I0709 01:07:15.380481 140556593907072 base_runner.py:59] cluster.ps.gpus_per_replica : 0\n",
      "I0709 01:07:15.380538 140556593907072 base_runner.py:59] cluster.ps.name : '/job:localhost'\n",
      "I0709 01:07:15.380604 140556593907072 base_runner.py:59] cluster.ps.num_tpu_hosts : 0\n",
      "I0709 01:07:15.380667 140556593907072 base_runner.py:59] cluster.ps.replicas : 1\n",
      "I0709 01:07:15.380732 140556593907072 base_runner.py:59] cluster.ps.targets : ''\n",
      "I0709 01:07:15.380809 140556593907072 base_runner.py:59] cluster.ps.tpus_per_replica : 0\n",
      "I0709 01:07:15.380864 140556593907072 base_runner.py:59] cluster.split_id : 0\n",
      "I0709 01:07:15.380926 140556593907072 base_runner.py:59] cluster.task : 0\n",
      "I0709 01:07:15.380987 140556593907072 base_runner.py:59] cluster.worker.cpus_per_replica : 1\n",
      "I0709 01:07:15.381049 140556593907072 base_runner.py:59] cluster.worker.devices_per_split : 1\n",
      "I0709 01:07:15.381111 140556593907072 base_runner.py:59] cluster.worker.gpus_per_replica : 0\n",
      "I0709 01:07:15.381171 140556593907072 base_runner.py:59] cluster.worker.name : '/job:localhost'\n",
      "I0709 01:07:15.381242 140556593907072 base_runner.py:59] cluster.worker.num_tpu_hosts : 0\n",
      "I0709 01:07:15.381306 140556593907072 base_runner.py:59] cluster.worker.replicas : 1\n",
      "I0709 01:07:15.381370 140556593907072 base_runner.py:59] cluster.worker.targets : ''\n",
      "I0709 01:07:15.381432 140556593907072 base_runner.py:59] cluster.worker.tpus_per_replica : 0\n",
      "I0709 01:07:15.381525 140556593907072 base_runner.py:59] dtype : float32\n",
      "I0709 01:07:15.381609 140556593907072 base_runner.py:59] fprop_dtype : NoneType\n",
      "I0709 01:07:15.381697 140556593907072 base_runner.py:59] inference_driver_name : NoneType\n",
      "I0709 01:07:15.381749 140556593907072 base_runner.py:59] input.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.381795 140556593907072 base_runner.py:59] input.bucket_adjust_every_n : 0\n",
      "I0709 01:07:15.381859 140556593907072 base_runner.py:59] input.bucket_batch_limit : [16, 16, 16, 16, 4, 4]\n",
      "I0709 01:07:15.381922 140556593907072 base_runner.py:59] input.bucket_upper_bound : [10, 20, 30, 60, 120, 200]\n",
      "I0709 01:07:15.381973 140556593907072 base_runner.py:59] input.cls : type/input_generator/PunctuatorInput\n",
      "I0709 01:07:15.382037 140556593907072 base_runner.py:59] input.dtype : float32\n",
      "I0709 01:07:15.382099 140556593907072 base_runner.py:59] input.file_buffer_size : 1\n",
      "I0709 01:07:15.382159 140556593907072 base_runner.py:59] input.file_buffer_size_in_seconds : 0\n",
      "I0709 01:07:15.382229 140556593907072 base_runner.py:59] input.file_datasource : NoneType\n",
      "I0709 01:07:15.382292 140556593907072 base_runner.py:59] input.file_parallelism : 1\n",
      "I0709 01:07:15.382352 140556593907072 base_runner.py:59] input.file_pattern : 'text:/tmp/punctuator_data/test.txt'\n",
      "I0709 01:07:15.382415 140556593907072 base_runner.py:59] input.file_random_seed : 27182818\n",
      "I0709 01:07:15.382477 140556593907072 base_runner.py:59] input.flush_every_n : 0\n",
      "I0709 01:07:15.382535 140556593907072 base_runner.py:59] input.fprop_dtype : NoneType\n",
      "I0709 01:07:15.382597 140556593907072 base_runner.py:59] input.inference_driver_name : NoneType\n",
      "I0709 01:07:15.382660 140556593907072 base_runner.py:59] input.is_inference : NoneType\n",
      "I0709 01:07:15.382722 140556593907072 base_runner.py:59] input.name : 'input'\n",
      "I0709 01:07:15.382783 140556593907072 base_runner.py:59] input.num_batcher_threads : 1\n",
      "I0709 01:07:15.382845 140556593907072 base_runner.py:59] input.num_partitions : NoneType\n",
      "I0709 01:07:15.382912 140556593907072 base_runner.py:59] input.num_samples : 0\n",
      "I0709 01:07:15.382976 140556593907072 base_runner.py:59] input.pad_to_max_seq_length : False\n",
      "I0709 01:07:15.383029 140556593907072 base_runner.py:59] input.params_init.method : 'xavier'\n",
      "I0709 01:07:15.383084 140556593907072 base_runner.py:59] input.params_init.scale : 1.000001\n",
      "I0709 01:07:15.383145 140556593907072 base_runner.py:59] input.params_init.seed : NoneType\n",
      "I0709 01:07:15.383213 140556593907072 base_runner.py:59] input.random_seed : NoneType\n",
      "I0709 01:07:15.383278 140556593907072 base_runner.py:59] input.remote.max_inflights_per_target : 32\n",
      "I0709 01:07:15.383341 140556593907072 base_runner.py:59] input.remote.shardable_batch : False\n",
      "I0709 01:07:15.383414 140556593907072 base_runner.py:59] input.repeat_count : -1\n",
      "I0709 01:07:15.383471 140556593907072 base_runner.py:59] input.require_sequential_order : False\n",
      "I0709 01:07:15.383537 140556593907072 base_runner.py:59] input.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.383625 140556593907072 base_runner.py:59] input.source_max_length : 202\n",
      "I0709 01:07:15.383692 140556593907072 base_runner.py:59] input.target_max_length : 202\n",
      "I0709 01:07:15.383770 140556593907072 base_runner.py:59] input.tokenizer.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.383834 140556593907072 base_runner.py:59] input.tokenizer.append_eos : True\n",
      "I0709 01:07:15.383897 140556593907072 base_runner.py:59] input.tokenizer.cls : type/lingvo.core.tokenizers/WpmTokenizer\n",
      "I0709 01:07:15.383961 140556593907072 base_runner.py:59] input.tokenizer.dtype : float32\n",
      "I0709 01:07:15.384057 140556593907072 base_runner.py:59] input.tokenizer.fprop_dtype : NoneType\n",
      "I0709 01:07:15.384110 140556593907072 base_runner.py:59] input.tokenizer.inference_driver_name : NoneType\n",
      "I0709 01:07:15.384171 140556593907072 base_runner.py:59] input.tokenizer.is_inference : NoneType\n",
      "I0709 01:07:15.384247 140556593907072 base_runner.py:59] input.tokenizer.merge_prob : 1.0\n",
      "I0709 01:07:15.384313 140556593907072 base_runner.py:59] input.tokenizer.name : 'tokenizer'\n",
      "I0709 01:07:15.384377 140556593907072 base_runner.py:59] input.tokenizer.pad_to_max_length : False\n",
      "I0709 01:07:15.384442 140556593907072 base_runner.py:59] input.tokenizer.params_init.method : 'xavier'\n",
      "I0709 01:07:15.384500 140556593907072 base_runner.py:59] input.tokenizer.params_init.scale : 1.000001\n",
      "I0709 01:07:15.384547 140556593907072 base_runner.py:59] input.tokenizer.params_init.seed : NoneType\n",
      "I0709 01:07:15.384614 140556593907072 base_runner.py:59] input.tokenizer.random_seed : NoneType\n",
      "I0709 01:07:15.384678 140556593907072 base_runner.py:59] input.tokenizer.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.384742 140556593907072 base_runner.py:59] input.tokenizer.target_eos_id : 2\n",
      "I0709 01:07:15.384813 140556593907072 base_runner.py:59] input.tokenizer.target_sos_id : 1\n",
      "I0709 01:07:15.384858 140556593907072 base_runner.py:59] input.tokenizer.target_unk_id : 0\n",
      "I0709 01:07:15.384902 140556593907072 base_runner.py:59] input.tokenizer.target_wb_id : -1\n",
      "I0709 01:07:15.384966 140556593907072 base_runner.py:59] input.tokenizer.vn.global_vn : False\n",
      "I0709 01:07:15.385045 140556593907072 base_runner.py:59] input.tokenizer.vn.per_step_vn : False\n",
      "I0709 01:07:15.385143 140556593907072 base_runner.py:59] input.tokenizer.vn.scale : NoneType\n",
      "I0709 01:07:15.385240 140556593907072 base_runner.py:59] input.tokenizer.vn.seed : NoneType\n",
      "I0709 01:07:15.385324 140556593907072 base_runner.py:59] input.tokenizer.vocab_filepath : 'brown_corpus_wpm.16000.vocab'\n",
      "I0709 01:07:15.385391 140556593907072 base_runner.py:59] input.tokenizer.vocab_size : 16000\n",
      "I0709 01:07:15.385447 140556593907072 base_runner.py:59] input.tokenizer_dict : {}\n",
      "I0709 01:07:15.385517 140556593907072 base_runner.py:59] input.tpu_infeed_parallelism : 1\n",
      "I0709 01:07:15.385582 140556593907072 base_runner.py:59] input.use_chaining : False\n",
      "I0709 01:07:15.385649 140556593907072 base_runner.py:59] input.use_partitioned_infeed_queue : False\n",
      "I0709 01:07:15.385724 140556593907072 base_runner.py:59] input.use_per_host_infeed : False\n",
      "I0709 01:07:15.385787 140556593907072 base_runner.py:59] input.use_within_batch_mixing : False\n",
      "I0709 01:07:15.385853 140556593907072 base_runner.py:59] input.vn.global_vn : False\n",
      "I0709 01:07:15.385913 140556593907072 base_runner.py:59] input.vn.per_step_vn : False\n",
      "I0709 01:07:15.385980 140556593907072 base_runner.py:59] input.vn.scale : NoneType\n",
      "I0709 01:07:15.386046 140556593907072 base_runner.py:59] input.vn.seed : NoneType\n",
      "I0709 01:07:15.386111 140556593907072 base_runner.py:59] is_inference : NoneType\n",
      "I0709 01:07:15.386174 140556593907072 base_runner.py:59] model : 'codelab.RNMTModel@/home/jupyter/lingvo/codelabs/codelab.py:81'\n",
      "I0709 01:07:15.386250 140556593907072 base_runner.py:59] name : ''\n",
      "I0709 01:07:15.386316 140556593907072 base_runner.py:59] params_init.method : 'xavier'\n",
      "I0709 01:07:15.386381 140556593907072 base_runner.py:59] params_init.scale : 1.000001\n",
      "I0709 01:07:15.386447 140556593907072 base_runner.py:59] params_init.seed : NoneType\n",
      "I0709 01:07:15.386499 140556593907072 base_runner.py:59] random_seed : NoneType\n",
      "I0709 01:07:15.386540 140556593907072 base_runner.py:59] skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.386581 140556593907072 base_runner.py:59] task.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.386622 140556593907072 base_runner.py:59] task.cls : type/lingvo.tasks.punctuator.model/RNMTModel\n",
      "I0709 01:07:15.386663 140556593907072 base_runner.py:59] task.decoder.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.386704 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.386745 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.apply_pruning : False\n",
      "I0709 01:07:15.386786 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.apply_pruning_to_projection : False\n",
      "I0709 01:07:15.386828 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.bias_init.method : 'constant'\n",
      "I0709 01:07:15.386869 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.bias_init.scale : 0.0\n",
      "I0709 01:07:15.386910 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.bias_init.seed : 0\n",
      "I0709 01:07:15.386950 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.cell_value_cap : 10.0\n",
      "I0709 01:07:15.386991 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.cls : type/lingvo.core.rnn_cell/LayerNormalizedLSTMCellSimple\n",
      "I0709 01:07:15.389775 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.couple_input_forget_gates : False\n",
      "I0709 01:07:15.389923 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.dtype : float32\n",
      "I0709 01:07:15.390024 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.enable_lstm_bias : True\n",
      "I0709 01:07:15.390112 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.forget_gate_bias : 0.0\n",
      "I0709 01:07:15.390185 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.fprop_dtype : NoneType\n",
      "I0709 01:07:15.390275 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.gradient_pruning : False\n",
      "I0709 01:07:15.390344 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.inference_driver_name : NoneType\n",
      "I0709 01:07:15.390414 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.inputs_arity : 1\n",
      "I0709 01:07:15.390483 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.is_inference : NoneType\n",
      "I0709 01:07:15.390549 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.layer_norm_epsilon : 1e-08\n",
      "I0709 01:07:15.390610 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.name : ''\n",
      "I0709 01:07:15.390657 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.num_hidden_nodes : 0\n",
      "I0709 01:07:15.390697 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.num_input_nodes : 0\n",
      "I0709 01:07:15.390738 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.num_output_nodes : 1024\n",
      "I0709 01:07:15.390795 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.output_nonlinearity : False\n",
      "I0709 01:07:15.390836 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.params_init.method : 'uniform'\n",
      "I0709 01:07:15.390877 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.params_init.scale : 0.04\n",
      "I0709 01:07:15.390935 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.params_init.seed : NoneType\n",
      "I0709 01:07:15.391002 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.qdomain.c_state : NoneType\n",
      "I0709 01:07:15.391071 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.qdomain.default : NoneType\n",
      "I0709 01:07:15.391133 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.qdomain.fullyconnected : NoneType\n",
      "I0709 01:07:15.391198 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.qdomain.m_state : NoneType\n",
      "I0709 01:07:15.391262 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.qdomain.weight : NoneType\n",
      "I0709 01:07:15.391323 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.random_seed : NoneType\n",
      "I0709 01:07:15.391386 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.reset_cell_state : False\n",
      "I0709 01:07:15.391447 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.391516 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.vn.global_vn : False\n",
      "I0709 01:07:15.391580 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.vn.per_step_vn : False\n",
      "I0709 01:07:15.391641 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.vn.scale : NoneType\n",
      "I0709 01:07:15.391702 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.vn.seed : NoneType\n",
      "I0709 01:07:15.391776 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.zero_state_init_params.method : 'zeros'\n",
      "I0709 01:07:15.391837 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.zero_state_init_params.seed : NoneType\n",
      "I0709 01:07:15.391896 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cell_tpl.zo_prob : 0.0\n",
      "I0709 01:07:15.391953 140556593907072 base_runner.py:59] task.decoder.atten_rnn_cls : type/lingvo.core.rnn_layers/FRNNWithAttention\n",
      "I0709 01:07:15.392028 140556593907072 base_runner.py:59] task.decoder.attention.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.392089 140556593907072 base_runner.py:59] task.decoder.attention.atten_dropout_deterministic : True\n",
      "I0709 01:07:15.392174 140556593907072 base_runner.py:59] task.decoder.attention.atten_dropout_prob : 0.3\n",
      "I0709 01:07:15.392225 140556593907072 base_runner.py:59] task.decoder.attention.attention_head_prob_index : -1\n",
      "I0709 01:07:15.392268 140556593907072 base_runner.py:59] task.decoder.attention.cls : type/lingvo.core.attention/MultiHeadedAttention\n",
      "I0709 01:07:15.392311 140556593907072 base_runner.py:59] task.decoder.attention.context_dim : 1024\n",
      "I0709 01:07:15.392381 140556593907072 base_runner.py:59] task.decoder.attention.ctx_post_proj_dim : 0\n",
      "I0709 01:07:15.392431 140556593907072 base_runner.py:59] task.decoder.attention.dtype : float32\n",
      "I0709 01:07:15.392513 140556593907072 base_runner.py:59] task.decoder.attention.enable_ctx_post_proj : False\n",
      "I0709 01:07:15.392588 140556593907072 base_runner.py:59] task.decoder.attention.enable_ctx_pre_proj : False\n",
      "I0709 01:07:15.392647 140556593907072 base_runner.py:59] task.decoder.attention.enable_query_proj : True\n",
      "I0709 01:07:15.392719 140556593907072 base_runner.py:59] task.decoder.attention.enable_source_proj : True\n",
      "I0709 01:07:15.392777 140556593907072 base_runner.py:59] task.decoder.attention.fprop_dtype : NoneType\n",
      "I0709 01:07:15.392845 140556593907072 base_runner.py:59] task.decoder.attention.hidden_dim : 1024\n",
      "I0709 01:07:15.392893 140556593907072 base_runner.py:59] task.decoder.attention.inference_driver_name : NoneType\n",
      "I0709 01:07:15.392935 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.392976 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.atten_dropout_deterministic : False\n",
      "I0709 01:07:15.393017 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.atten_dropout_prob : 0.0\n",
      "I0709 01:07:15.393092 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.cls : type/lingvo.core.attention/AdditiveAttention\n",
      "I0709 01:07:15.393151 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.dtype : float32\n",
      "I0709 01:07:15.393237 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.fprop_dtype : NoneType\n",
      "I0709 01:07:15.393307 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.hidden_dim : 0\n",
      "I0709 01:07:15.393403 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.inference_driver_name : NoneType\n",
      "I0709 01:07:15.393448 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.is_inference : NoneType\n",
      "I0709 01:07:15.393495 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.name : ''\n",
      "I0709 01:07:15.393539 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.packed_input : False\n",
      "I0709 01:07:15.393584 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.params_init.method : 'gaussian_sqrt_dim'\n",
      "I0709 01:07:15.393625 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.params_init.scale : 1.0\n",
      "I0709 01:07:15.393666 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.params_init.seed : NoneType\n",
      "I0709 01:07:15.393707 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.qdomain.default : NoneType\n",
      "I0709 01:07:15.393748 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.qdomain.fullyconnected : NoneType\n",
      "I0709 01:07:15.393789 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.qdomain.softmax : NoneType\n",
      "I0709 01:07:15.393830 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.query_dim : 0\n",
      "I0709 01:07:15.393870 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.random_seed : NoneType\n",
      "I0709 01:07:15.393911 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.same_batch_size : False\n",
      "I0709 01:07:15.393951 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.393992 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.source_dim : 0\n",
      "I0709 01:07:15.394033 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.vn.global_vn : False\n",
      "I0709 01:07:15.394073 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.vn.per_step_vn : False\n",
      "I0709 01:07:15.394113 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.vn.scale : NoneType\n",
      "I0709 01:07:15.394190 140556593907072 base_runner.py:59] task.decoder.attention.inner_atten_params.vn.seed : NoneType\n",
      "I0709 01:07:15.394258 140556593907072 base_runner.py:59] task.decoder.attention.is_inference : NoneType\n",
      "I0709 01:07:15.394305 140556593907072 base_runner.py:59] task.decoder.attention.name : ''\n",
      "I0709 01:07:15.394346 140556593907072 base_runner.py:59] task.decoder.attention.num_attention_heads : 4\n",
      "I0709 01:07:15.394438 140556593907072 base_runner.py:59] task.decoder.attention.num_post_proj : 1\n",
      "I0709 01:07:15.394535 140556593907072 base_runner.py:59] task.decoder.attention.packed_input : False\n",
      "I0709 01:07:15.394614 140556593907072 base_runner.py:59] task.decoder.attention.params_init.method : 'xavier'\n",
      "I0709 01:07:15.394679 140556593907072 base_runner.py:59] task.decoder.attention.params_init.scale : 1.0\n",
      "I0709 01:07:15.394747 140556593907072 base_runner.py:59] task.decoder.attention.params_init.seed : NoneType\n",
      "I0709 01:07:15.394807 140556593907072 base_runner.py:59] task.decoder.attention.proj_init : 'default'\n",
      "I0709 01:07:15.394871 140556593907072 base_runner.py:59] task.decoder.attention.qdomain.atten_context : NoneType\n",
      "I0709 01:07:15.394935 140556593907072 base_runner.py:59] task.decoder.attention.qdomain.default : NoneType\n",
      "I0709 01:07:15.395002 140556593907072 base_runner.py:59] task.decoder.attention.qdomain.fullyconnected : NoneType\n",
      "I0709 01:07:15.395051 140556593907072 base_runner.py:59] task.decoder.attention.qdomain.softmax : NoneType\n",
      "I0709 01:07:15.395105 140556593907072 base_runner.py:59] task.decoder.attention.query_dim : 1024\n",
      "I0709 01:07:15.395169 140556593907072 base_runner.py:59] task.decoder.attention.random_seed : NoneType\n",
      "I0709 01:07:15.395233 140556593907072 base_runner.py:59] task.decoder.attention.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.395297 140556593907072 base_runner.py:59] task.decoder.attention.source_dim : 1024\n",
      "I0709 01:07:15.395361 140556593907072 base_runner.py:59] task.decoder.attention.use_source_vec_as_attention_value : True\n",
      "I0709 01:07:15.395425 140556593907072 base_runner.py:59] task.decoder.attention.vn.global_vn : False\n",
      "I0709 01:07:15.395494 140556593907072 base_runner.py:59] task.decoder.attention.vn.per_step_vn : False\n",
      "I0709 01:07:15.395562 140556593907072 base_runner.py:59] task.decoder.attention.vn.scale : NoneType\n",
      "I0709 01:07:15.395627 140556593907072 base_runner.py:59] task.decoder.attention.vn.seed : NoneType\n",
      "I0709 01:07:15.395689 140556593907072 base_runner.py:59] task.decoder.beam_search.allow_empty_terminated_hyp : True\n",
      "I0709 01:07:15.395755 140556593907072 base_runner.py:59] task.decoder.beam_search.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.395805 140556593907072 base_runner.py:59] task.decoder.beam_search.batch_major_compute : False\n",
      "I0709 01:07:15.395889 140556593907072 base_runner.py:59] task.decoder.beam_search.batch_major_state : True\n",
      "I0709 01:07:15.395954 140556593907072 base_runner.py:59] task.decoder.beam_search.beam_size : 3.0\n",
      "I0709 01:07:15.396023 140556593907072 base_runner.py:59] task.decoder.beam_search.cls : type/lingvo.core.beam_search_helper/BeamSearchHelper\n",
      "I0709 01:07:15.396101 140556593907072 base_runner.py:59] task.decoder.beam_search.coverage_penalty : 0.2\n",
      "I0709 01:07:15.396166 140556593907072 base_runner.py:59] task.decoder.beam_search.dtype : float32\n",
      "I0709 01:07:15.396223 140556593907072 base_runner.py:59] task.decoder.beam_search.ensure_full_beam : False\n",
      "I0709 01:07:15.396287 140556593907072 base_runner.py:59] task.decoder.beam_search.force_eos_in_last_step : False\n",
      "I0709 01:07:15.396343 140556593907072 base_runner.py:59] task.decoder.beam_search.fprop_dtype : NoneType\n",
      "I0709 01:07:15.396408 140556593907072 base_runner.py:59] task.decoder.beam_search.inference_driver_name : NoneType\n",
      "I0709 01:07:15.396461 140556593907072 base_runner.py:59] task.decoder.beam_search.is_inference : NoneType\n",
      "I0709 01:07:15.396533 140556593907072 base_runner.py:59] task.decoder.beam_search.length_normalization : 0.2\n",
      "I0709 01:07:15.396594 140556593907072 base_runner.py:59] task.decoder.beam_search.local_eos_threshold : -100.0\n",
      "I0709 01:07:15.396659 140556593907072 base_runner.py:59] task.decoder.beam_search.merge_paths : False\n",
      "I0709 01:07:15.396711 140556593907072 base_runner.py:59] task.decoder.beam_search.name : 'beam_search'\n",
      "I0709 01:07:15.396790 140556593907072 base_runner.py:59] task.decoder.beam_search.num_hyps_per_beam : 16\n",
      "I0709 01:07:15.396890 140556593907072 base_runner.py:59] task.decoder.beam_search.params_init.method : 'xavier'\n",
      "I0709 01:07:15.396982 140556593907072 base_runner.py:59] task.decoder.beam_search.params_init.scale : 1.000001\n",
      "I0709 01:07:15.397074 140556593907072 base_runner.py:59] task.decoder.beam_search.params_init.seed : NoneType\n",
      "I0709 01:07:15.397156 140556593907072 base_runner.py:59] task.decoder.beam_search.random_seed : NoneType\n",
      "I0709 01:07:15.397230 140556593907072 base_runner.py:59] task.decoder.beam_search.short_seq_limit : 0\n",
      "I0709 01:07:15.397281 140556593907072 base_runner.py:59] task.decoder.beam_search.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.397326 140556593907072 base_runner.py:59] task.decoder.beam_search.target_eoc_id : -1\n",
      "I0709 01:07:15.397369 140556593907072 base_runner.py:59] task.decoder.beam_search.target_eos_id : 2\n",
      "I0709 01:07:15.397410 140556593907072 base_runner.py:59] task.decoder.beam_search.target_seq_len : 0\n",
      "I0709 01:07:15.397451 140556593907072 base_runner.py:59] task.decoder.beam_search.target_seq_length_ratio : 1.0\n",
      "I0709 01:07:15.397499 140556593907072 base_runner.py:59] task.decoder.beam_search.target_sos_id : 1\n",
      "I0709 01:07:15.397542 140556593907072 base_runner.py:59] task.decoder.beam_search.valid_eos_max_logit_delta : 5.0\n",
      "I0709 01:07:15.397583 140556593907072 base_runner.py:59] task.decoder.beam_search.vn.global_vn : False\n",
      "I0709 01:07:15.397623 140556593907072 base_runner.py:59] task.decoder.beam_search.vn.per_step_vn : False\n",
      "I0709 01:07:15.397664 140556593907072 base_runner.py:59] task.decoder.beam_search.vn.scale : NoneType\n",
      "I0709 01:07:15.397704 140556593907072 base_runner.py:59] task.decoder.beam_search.vn.seed : NoneType\n",
      "I0709 01:07:15.397744 140556593907072 base_runner.py:59] task.decoder.bias_only_if_consistent : True\n",
      "I0709 01:07:15.397784 140556593907072 base_runner.py:59] task.decoder.cc_schedule : NoneType\n",
      "I0709 01:07:15.397825 140556593907072 base_runner.py:59] task.decoder.cls : type/lingvo.tasks.mt.decoder/MTDecoderV1\n",
      "I0709 01:07:15.397866 140556593907072 base_runner.py:59] task.decoder.dropout_prob : 0.3\n",
      "I0709 01:07:15.397906 140556593907072 base_runner.py:59] task.decoder.dtype : float32\n",
      "I0709 01:07:15.397947 140556593907072 base_runner.py:59] task.decoder.emb.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.397987 140556593907072 base_runner.py:59] task.decoder.emb.cls : type/lingvo.core.layers/EmbeddingLayer\n",
      "I0709 01:07:15.398028 140556593907072 base_runner.py:59] task.decoder.emb.dtype : float32\n",
      "I0709 01:07:15.398068 140556593907072 base_runner.py:59] task.decoder.emb.embedding_dim : 1024\n",
      "I0709 01:07:15.398109 140556593907072 base_runner.py:59] task.decoder.emb.fprop_dtype : NoneType\n",
      "I0709 01:07:15.398149 140556593907072 base_runner.py:59] task.decoder.emb.inference_driver_name : NoneType\n",
      "I0709 01:07:15.398189 140556593907072 base_runner.py:59] task.decoder.emb.is_inference : NoneType\n",
      "I0709 01:07:15.398230 140556593907072 base_runner.py:59] task.decoder.emb.max_num_shards : 16\n",
      "I0709 01:07:15.398271 140556593907072 base_runner.py:59] task.decoder.emb.name : ''\n",
      "I0709 01:07:15.398311 140556593907072 base_runner.py:59] task.decoder.emb.on_ps : True\n",
      "I0709 01:07:15.398351 140556593907072 base_runner.py:59] task.decoder.emb.params_init.method : 'uniform'\n",
      "I0709 01:07:15.398392 140556593907072 base_runner.py:59] task.decoder.emb.params_init.scale : 0.04\n",
      "I0709 01:07:15.398432 140556593907072 base_runner.py:59] task.decoder.emb.params_init.seed : NoneType\n",
      "I0709 01:07:15.398472 140556593907072 base_runner.py:59] task.decoder.emb.random_seed : NoneType\n",
      "I0709 01:07:15.398517 140556593907072 base_runner.py:59] task.decoder.emb.scale_sqrt_depth : False\n",
      "I0709 01:07:15.398557 140556593907072 base_runner.py:59] task.decoder.emb.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.398599 140556593907072 base_runner.py:59] task.decoder.emb.vn.global_vn : False\n",
      "I0709 01:07:15.398639 140556593907072 base_runner.py:59] task.decoder.emb.vn.per_step_vn : False\n",
      "I0709 01:07:15.398683 140556593907072 base_runner.py:59] task.decoder.emb.vn.scale : 1.0\n",
      "I0709 01:07:15.398724 140556593907072 base_runner.py:59] task.decoder.emb.vn.seed : NoneType\n",
      "I0709 01:07:15.398794 140556593907072 base_runner.py:59] task.decoder.emb.vocab_size : 16000\n",
      "I0709 01:07:15.398839 140556593907072 base_runner.py:59] task.decoder.feed_attention_context_vec_to_softmax : True\n",
      "I0709 01:07:15.398883 140556593907072 base_runner.py:59] task.decoder.fprop_dtype : NoneType\n",
      "I0709 01:07:15.398924 140556593907072 base_runner.py:59] task.decoder.greedy_search.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.398968 140556593907072 base_runner.py:59] task.decoder.greedy_search.cls : type/lingvo.core.beam_search_helper/GreedySearchHelper\n",
      "I0709 01:07:15.399028 140556593907072 base_runner.py:59] task.decoder.greedy_search.dtype : float32\n",
      "I0709 01:07:15.399253 140556593907072 base_runner.py:59] task.decoder.greedy_search.fprop_dtype : NoneType\n",
      "I0709 01:07:15.399375 140556593907072 base_runner.py:59] task.decoder.greedy_search.inference_driver_name : NoneType\n",
      "I0709 01:07:15.399454 140556593907072 base_runner.py:59] task.decoder.greedy_search.is_inference : NoneType\n",
      "I0709 01:07:15.399516 140556593907072 base_runner.py:59] task.decoder.greedy_search.name : 'greedy_search'\n",
      "I0709 01:07:15.399589 140556593907072 base_runner.py:59] task.decoder.greedy_search.params_init.method : 'xavier'\n",
      "I0709 01:07:15.399661 140556593907072 base_runner.py:59] task.decoder.greedy_search.params_init.scale : 1.000001\n",
      "I0709 01:07:15.399709 140556593907072 base_runner.py:59] task.decoder.greedy_search.params_init.seed : NoneType\n",
      "I0709 01:07:15.399779 140556593907072 base_runner.py:59] task.decoder.greedy_search.random_seed : NoneType\n",
      "I0709 01:07:15.399845 140556593907072 base_runner.py:59] task.decoder.greedy_search.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.399904 140556593907072 base_runner.py:59] task.decoder.greedy_search.target_eos_id : 2\n",
      "I0709 01:07:15.399972 140556593907072 base_runner.py:59] task.decoder.greedy_search.target_seq_len : 0\n",
      "I0709 01:07:15.400036 140556593907072 base_runner.py:59] task.decoder.greedy_search.target_sos_id : 1\n",
      "I0709 01:07:15.400115 140556593907072 base_runner.py:59] task.decoder.greedy_search.vn.global_vn : False\n",
      "I0709 01:07:15.400184 140556593907072 base_runner.py:59] task.decoder.greedy_search.vn.per_step_vn : False\n",
      "I0709 01:07:15.400245 140556593907072 base_runner.py:59] task.decoder.greedy_search.vn.scale : NoneType\n",
      "I0709 01:07:15.400310 140556593907072 base_runner.py:59] task.decoder.greedy_search.vn.seed : NoneType\n",
      "I0709 01:07:15.400370 140556593907072 base_runner.py:59] task.decoder.inference_driver_name : NoneType\n",
      "I0709 01:07:15.400435 140556593907072 base_runner.py:59] task.decoder.init_step_ids : False\n",
      "I0709 01:07:15.400498 140556593907072 base_runner.py:59] task.decoder.is_inference : NoneType\n",
      "I0709 01:07:15.400563 140556593907072 base_runner.py:59] task.decoder.label_smoothing.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.400632 140556593907072 base_runner.py:59] task.decoder.label_smoothing.cls : type/lingvo.core.layers/UniformLabelSmoother\n",
      "I0709 01:07:15.400698 140556593907072 base_runner.py:59] task.decoder.label_smoothing.dtype : float32\n",
      "I0709 01:07:15.400764 140556593907072 base_runner.py:59] task.decoder.label_smoothing.fprop_dtype : NoneType\n",
      "I0709 01:07:15.400836 140556593907072 base_runner.py:59] task.decoder.label_smoothing.inference_driver_name : NoneType\n",
      "I0709 01:07:15.400900 140556593907072 base_runner.py:59] task.decoder.label_smoothing.is_inference : NoneType\n",
      "I0709 01:07:15.400972 140556593907072 base_runner.py:59] task.decoder.label_smoothing.name : ''\n",
      "I0709 01:07:15.401036 140556593907072 base_runner.py:59] task.decoder.label_smoothing.num_classes : 16000\n",
      "I0709 01:07:15.401111 140556593907072 base_runner.py:59] task.decoder.label_smoothing.params_init.method : 'xavier'\n",
      "I0709 01:07:15.401177 140556593907072 base_runner.py:59] task.decoder.label_smoothing.params_init.scale : 1.000001\n",
      "I0709 01:07:15.401228 140556593907072 base_runner.py:59] task.decoder.label_smoothing.params_init.seed : NoneType\n",
      "I0709 01:07:15.401292 140556593907072 base_runner.py:59] task.decoder.label_smoothing.random_seed : NoneType\n",
      "I0709 01:07:15.401356 140556593907072 base_runner.py:59] task.decoder.label_smoothing.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.401419 140556593907072 base_runner.py:59] task.decoder.label_smoothing.token_id_uncertainty_larger : NoneType\n",
      "I0709 01:07:15.401483 140556593907072 base_runner.py:59] task.decoder.label_smoothing.uncertainty : 0.1\n",
      "I0709 01:07:15.401547 140556593907072 base_runner.py:59] task.decoder.label_smoothing.uncertainty_larger : 0.1\n",
      "I0709 01:07:15.401611 140556593907072 base_runner.py:59] task.decoder.label_smoothing.vn.global_vn : False\n",
      "I0709 01:07:15.401674 140556593907072 base_runner.py:59] task.decoder.label_smoothing.vn.per_step_vn : False\n",
      "I0709 01:07:15.401738 140556593907072 base_runner.py:59] task.decoder.label_smoothing.vn.scale : NoneType\n",
      "I0709 01:07:15.401801 140556593907072 base_runner.py:59] task.decoder.label_smoothing.vn.seed : NoneType\n",
      "I0709 01:07:15.401870 140556593907072 base_runner.py:59] task.decoder.name : ''\n",
      "I0709 01:07:15.401948 140556593907072 base_runner.py:59] task.decoder.packed_input : False\n",
      "I0709 01:07:15.402079 140556593907072 base_runner.py:59] task.decoder.params_init.method : 'xavier'\n",
      "I0709 01:07:15.402189 140556593907072 base_runner.py:59] task.decoder.params_init.scale : 1.000001\n",
      "I0709 01:07:15.402264 140556593907072 base_runner.py:59] task.decoder.params_init.seed : NoneType\n",
      "I0709 01:07:15.402328 140556593907072 base_runner.py:59] task.decoder.per_example_tensors : False\n",
      "I0709 01:07:15.402401 140556593907072 base_runner.py:59] task.decoder.per_word_avg_loss : False\n",
      "I0709 01:07:15.402475 140556593907072 base_runner.py:59] task.decoder.qdomain.default : NoneType\n",
      "I0709 01:07:15.402554 140556593907072 base_runner.py:59] task.decoder.qlogsoftmax_range_min : -10.0\n",
      "I0709 01:07:15.402623 140556593907072 base_runner.py:59] task.decoder.random_seed : NoneType\n",
      "I0709 01:07:15.402690 140556593907072 base_runner.py:59] task.decoder.residual_start : 2\n",
      "I0709 01:07:15.402753 140556593907072 base_runner.py:59] task.decoder.rnn_cell_dim : 1024\n",
      "I0709 01:07:15.402822 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.402887 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.apply_pruning : False\n",
      "I0709 01:07:15.402951 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.apply_pruning_to_projection : False\n",
      "I0709 01:07:15.403014 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.bias_init.method : 'constant'\n",
      "I0709 01:07:15.403097 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.bias_init.scale : 0.0\n",
      "I0709 01:07:15.403161 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.bias_init.seed : 0\n",
      "I0709 01:07:15.403223 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.cell_value_cap : 10.0\n",
      "I0709 01:07:15.403317 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.cls : type/lingvo.core.rnn_cell/LayerNormalizedLSTMCellSimple\n",
      "I0709 01:07:15.403416 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.couple_input_forget_gates : False\n",
      "I0709 01:07:15.403482 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.dtype : float32\n",
      "I0709 01:07:15.403549 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.enable_lstm_bias : True\n",
      "I0709 01:07:15.403625 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.forget_gate_bias : 0.0\n",
      "I0709 01:07:15.403687 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.fprop_dtype : NoneType\n",
      "I0709 01:07:15.403748 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.gradient_pruning : False\n",
      "I0709 01:07:15.403810 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.inference_driver_name : NoneType\n",
      "I0709 01:07:15.403872 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.inputs_arity : 1\n",
      "I0709 01:07:15.403934 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.is_inference : NoneType\n",
      "I0709 01:07:15.403994 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.layer_norm_epsilon : 1e-08\n",
      "I0709 01:07:15.404077 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.name : ''\n",
      "I0709 01:07:15.404139 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.num_hidden_nodes : 0\n",
      "I0709 01:07:15.404205 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.num_input_nodes : 0\n",
      "I0709 01:07:15.404269 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.num_output_nodes : 1024\n",
      "I0709 01:07:15.404331 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.output_nonlinearity : False\n",
      "I0709 01:07:15.404401 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.params_init.method : 'uniform'\n",
      "I0709 01:07:15.404464 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.params_init.scale : 0.04\n",
      "I0709 01:07:15.404515 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.params_init.seed : NoneType\n",
      "I0709 01:07:15.404580 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.qdomain.c_state : NoneType\n",
      "I0709 01:07:15.404642 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.qdomain.default : NoneType\n",
      "I0709 01:07:15.404687 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.qdomain.fullyconnected : NoneType\n",
      "I0709 01:07:15.404747 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.qdomain.m_state : NoneType\n",
      "I0709 01:07:15.404810 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.qdomain.weight : NoneType\n",
      "I0709 01:07:15.404871 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.random_seed : NoneType\n",
      "I0709 01:07:15.404937 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.reset_cell_state : False\n",
      "I0709 01:07:15.405000 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.405066 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.vn.global_vn : False\n",
      "I0709 01:07:15.405122 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.vn.per_step_vn : False\n",
      "I0709 01:07:15.405185 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.vn.scale : NoneType\n",
      "I0709 01:07:15.405247 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.vn.seed : NoneType\n",
      "I0709 01:07:15.405305 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.zero_state_init_params.method : 'zeros'\n",
      "I0709 01:07:15.405368 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.zero_state_init_params.seed : NoneType\n",
      "I0709 01:07:15.405431 140556593907072 base_runner.py:59] task.decoder.rnn_cell_tpl.zo_prob : 0.0\n",
      "I0709 01:07:15.405492 140556593907072 base_runner.py:59] task.decoder.rnn_layers : 8\n",
      "I0709 01:07:15.405552 140556593907072 base_runner.py:59] task.decoder.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.405615 140556593907072 base_runner.py:59] task.decoder.softmax.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.405677 140556593907072 base_runner.py:59] task.decoder.softmax.apply_pruning : False\n",
      "I0709 01:07:15.405738 140556593907072 base_runner.py:59] task.decoder.softmax.chunk_size : 0\n",
      "I0709 01:07:15.405792 140556593907072 base_runner.py:59] task.decoder.softmax.cls : type/lingvo.core.layers/SimpleFullSoftmax\n",
      "I0709 01:07:15.405855 140556593907072 base_runner.py:59] task.decoder.softmax.dtype : float32\n",
      "I0709 01:07:15.405917 140556593907072 base_runner.py:59] task.decoder.softmax.fprop_dtype : NoneType\n",
      "I0709 01:07:15.405977 140556593907072 base_runner.py:59] task.decoder.softmax.inference_driver_name : NoneType\n",
      "I0709 01:07:15.406038 140556593907072 base_runner.py:59] task.decoder.softmax.input_dim : 0\n",
      "I0709 01:07:15.406106 140556593907072 base_runner.py:59] task.decoder.softmax.is_inference : NoneType\n",
      "I0709 01:07:15.406170 140556593907072 base_runner.py:59] task.decoder.softmax.logits_abs_max : NoneType\n",
      "I0709 01:07:15.406233 140556593907072 base_runner.py:59] task.decoder.softmax.name : ''\n",
      "I0709 01:07:15.406295 140556593907072 base_runner.py:59] task.decoder.softmax.num_classes : 16000\n",
      "I0709 01:07:15.406362 140556593907072 base_runner.py:59] task.decoder.softmax.num_sampled : 0\n",
      "I0709 01:07:15.406424 140556593907072 base_runner.py:59] task.decoder.softmax.num_shards : 16\n",
      "I0709 01:07:15.406486 140556593907072 base_runner.py:59] task.decoder.softmax.params_init.method : 'uniform'\n",
      "I0709 01:07:15.406570 140556593907072 base_runner.py:59] task.decoder.softmax.params_init.scale : 0.04\n",
      "I0709 01:07:15.406635 140556593907072 base_runner.py:59] task.decoder.softmax.params_init.seed : NoneType\n",
      "I0709 01:07:15.406701 140556593907072 base_runner.py:59] task.decoder.softmax.qdomain.default : NoneType\n",
      "I0709 01:07:15.406765 140556593907072 base_runner.py:59] task.decoder.softmax.random_seed : NoneType\n",
      "I0709 01:07:15.406830 140556593907072 base_runner.py:59] task.decoder.softmax.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.406893 140556593907072 base_runner.py:59] task.decoder.softmax.use_num_classes_major_weight : False\n",
      "I0709 01:07:15.406957 140556593907072 base_runner.py:59] task.decoder.softmax.vn.global_vn : False\n",
      "I0709 01:07:15.407044 140556593907072 base_runner.py:59] task.decoder.softmax.vn.per_step_vn : False\n",
      "I0709 01:07:15.407102 140556593907072 base_runner.py:59] task.decoder.softmax.vn.scale : 1.0\n",
      "I0709 01:07:15.407164 140556593907072 base_runner.py:59] task.decoder.softmax.vn.seed : NoneType\n",
      "I0709 01:07:15.407222 140556593907072 base_runner.py:59] task.decoder.source_dim : 1024\n",
      "I0709 01:07:15.407287 140556593907072 base_runner.py:59] task.decoder.target_eos_id : 2\n",
      "I0709 01:07:15.407351 140556593907072 base_runner.py:59] task.decoder.target_seq_len : 300\n",
      "I0709 01:07:15.407415 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.407480 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.cls : type/lingvo.core.target_sequence_sampler/TargetSequenceSampler\n",
      "I0709 01:07:15.407602 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.dtype : float32\n",
      "I0709 01:07:15.407674 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.fprop_dtype : NoneType\n",
      "I0709 01:07:15.407741 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.inference_driver_name : NoneType\n",
      "I0709 01:07:15.407815 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.is_inference : NoneType\n",
      "I0709 01:07:15.407917 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.name : 'target_sequence_sampler'\n",
      "I0709 01:07:15.407988 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.params_init.method : 'xavier'\n",
      "I0709 01:07:15.408076 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.params_init.scale : 1.000001\n",
      "I0709 01:07:15.408146 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.params_init.seed : NoneType\n",
      "I0709 01:07:15.408204 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.random_seed : NoneType\n",
      "I0709 01:07:15.408270 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.408339 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.target_eoc_id : -1\n",
      "I0709 01:07:15.408403 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.target_eos_id : 2\n",
      "I0709 01:07:15.408467 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.target_seq_len : 0\n",
      "I0709 01:07:15.408533 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.target_sos_id : 1\n",
      "I0709 01:07:15.408589 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.temperature : 1.0\n",
      "I0709 01:07:15.408655 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.vn.global_vn : False\n",
      "I0709 01:07:15.408723 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.vn.per_step_vn : False\n",
      "I0709 01:07:15.408787 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.vn.scale : NoneType\n",
      "I0709 01:07:15.408853 140556593907072 base_runner.py:59] task.decoder.target_sequence_sampler.vn.seed : NoneType\n",
      "I0709 01:07:15.408917 140556593907072 base_runner.py:59] task.decoder.target_sos_id : 1\n",
      "I0709 01:07:15.408981 140556593907072 base_runner.py:59] task.decoder.unidi_rnn_type : 'func'\n",
      "I0709 01:07:15.409050 140556593907072 base_runner.py:59] task.decoder.use_prev_atten_ctx : False\n",
      "I0709 01:07:15.409119 140556593907072 base_runner.py:59] task.decoder.use_zero_atten_state : False\n",
      "I0709 01:07:15.409184 140556593907072 base_runner.py:59] task.decoder.vn.global_vn : False\n",
      "I0709 01:07:15.409250 140556593907072 base_runner.py:59] task.decoder.vn.per_step_vn : False\n",
      "I0709 01:07:15.409311 140556593907072 base_runner.py:59] task.decoder.vn.scale : NoneType\n",
      "I0709 01:07:15.409380 140556593907072 base_runner.py:59] task.decoder.vn.seed : NoneType\n",
      "I0709 01:07:15.409445 140556593907072 base_runner.py:59] task.dtype : float32\n",
      "I0709 01:07:15.409509 140556593907072 base_runner.py:59] task.encoder.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.409598 140556593907072 base_runner.py:59] task.encoder.bidi_rnn_type : 'func'\n",
      "I0709 01:07:15.409669 140556593907072 base_runner.py:59] task.encoder.cc_schedule : NoneType\n",
      "I0709 01:07:15.409734 140556593907072 base_runner.py:59] task.encoder.cls : type/lingvo.tasks.mt.encoder/MTEncoderBiRNN\n",
      "I0709 01:07:15.409804 140556593907072 base_runner.py:59] task.encoder.dropout_prob : 0.3\n",
      "I0709 01:07:15.409865 140556593907072 base_runner.py:59] task.encoder.dtype : float32\n",
      "I0709 01:07:15.409932 140556593907072 base_runner.py:59] task.encoder.emb.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.409998 140556593907072 base_runner.py:59] task.encoder.emb.cls : type/lingvo.core.layers/EmbeddingLayer\n",
      "I0709 01:07:15.410079 140556593907072 base_runner.py:59] task.encoder.emb.dtype : float32\n",
      "I0709 01:07:15.410142 140556593907072 base_runner.py:59] task.encoder.emb.embedding_dim : 1024\n",
      "I0709 01:07:15.410220 140556593907072 base_runner.py:59] task.encoder.emb.fprop_dtype : NoneType\n",
      "I0709 01:07:15.410287 140556593907072 base_runner.py:59] task.encoder.emb.inference_driver_name : NoneType\n",
      "I0709 01:07:15.410407 140556593907072 base_runner.py:59] task.encoder.emb.is_inference : NoneType\n",
      "I0709 01:07:15.410479 140556593907072 base_runner.py:59] task.encoder.emb.max_num_shards : 16\n",
      "I0709 01:07:15.410579 140556593907072 base_runner.py:59] task.encoder.emb.name : ''\n",
      "I0709 01:07:15.410683 140556593907072 base_runner.py:59] task.encoder.emb.on_ps : True\n",
      "I0709 01:07:15.410777 140556593907072 base_runner.py:59] task.encoder.emb.params_init.method : 'uniform'\n",
      "I0709 01:07:15.410822 140556593907072 base_runner.py:59] task.encoder.emb.params_init.scale : 0.04\n",
      "I0709 01:07:15.410878 140556593907072 base_runner.py:59] task.encoder.emb.params_init.seed : NoneType\n",
      "I0709 01:07:15.411011 140556593907072 base_runner.py:59] task.encoder.emb.random_seed : NoneType\n",
      "I0709 01:07:15.411096 140556593907072 base_runner.py:59] task.encoder.emb.scale_sqrt_depth : False\n",
      "I0709 01:07:15.411148 140556593907072 base_runner.py:59] task.encoder.emb.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.411221 140556593907072 base_runner.py:59] task.encoder.emb.vn.global_vn : False\n",
      "I0709 01:07:15.411279 140556593907072 base_runner.py:59] task.encoder.emb.vn.per_step_vn : False\n",
      "I0709 01:07:15.411322 140556593907072 base_runner.py:59] task.encoder.emb.vn.scale : 1.0\n",
      "I0709 01:07:15.411373 140556593907072 base_runner.py:59] task.encoder.emb.vn.seed : NoneType\n",
      "I0709 01:07:15.411444 140556593907072 base_runner.py:59] task.encoder.emb.vocab_size : 16000\n",
      "I0709 01:07:15.411496 140556593907072 base_runner.py:59] task.encoder.encoder_out_dim : 1024\n",
      "I0709 01:07:15.411538 140556593907072 base_runner.py:59] task.encoder.fprop_dtype : NoneType\n",
      "I0709 01:07:15.411593 140556593907072 base_runner.py:59] task.encoder.inference_driver_name : NoneType\n",
      "I0709 01:07:15.411664 140556593907072 base_runner.py:59] task.encoder.is_inference : NoneType\n",
      "I0709 01:07:15.411712 140556593907072 base_runner.py:59] task.encoder.is_transparent : False\n",
      "I0709 01:07:15.411753 140556593907072 base_runner.py:59] task.encoder.lstm_cell_size : 1024\n",
      "I0709 01:07:15.411812 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.411880 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.apply_pruning : False\n",
      "I0709 01:07:15.411924 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.apply_pruning_to_projection : False\n",
      "I0709 01:07:15.411964 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.bias_init.method : 'constant'\n",
      "I0709 01:07:15.412069 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.bias_init.scale : 0.0\n",
      "I0709 01:07:15.412153 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.bias_init.seed : 0\n",
      "I0709 01:07:15.412245 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.cell_value_cap : 10.0\n",
      "I0709 01:07:15.412316 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.cls : type/lingvo.core.rnn_cell/LayerNormalizedLSTMCellSimple\n",
      "I0709 01:07:15.412406 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.couple_input_forget_gates : False\n",
      "I0709 01:07:15.412460 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.dtype : float32\n",
      "I0709 01:07:15.412559 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.enable_lstm_bias : True\n",
      "I0709 01:07:15.412647 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.forget_gate_bias : 0.0\n",
      "I0709 01:07:15.412695 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.fprop_dtype : NoneType\n",
      "I0709 01:07:15.412736 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.gradient_pruning : False\n",
      "I0709 01:07:15.412800 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.inference_driver_name : NoneType\n",
      "I0709 01:07:15.412916 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.inputs_arity : 1\n",
      "I0709 01:07:15.412972 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.is_inference : NoneType\n",
      "I0709 01:07:15.413068 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.layer_norm_epsilon : 1e-08\n",
      "I0709 01:07:15.413148 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.name : ''\n",
      "I0709 01:07:15.413222 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.num_hidden_nodes : 0\n",
      "I0709 01:07:15.413301 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.num_input_nodes : 0\n",
      "I0709 01:07:15.413393 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.num_output_nodes : 1024\n",
      "I0709 01:07:15.413480 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.output_nonlinearity : False\n",
      "I0709 01:07:15.413563 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.params_init.method : 'uniform'\n",
      "I0709 01:07:15.413640 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.params_init.scale : 0.04\n",
      "I0709 01:07:15.413720 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.params_init.seed : NoneType\n",
      "I0709 01:07:15.413793 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.qdomain.c_state : NoneType\n",
      "I0709 01:07:15.413866 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.qdomain.default : NoneType\n",
      "I0709 01:07:15.413929 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.qdomain.fullyconnected : NoneType\n",
      "I0709 01:07:15.414010 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.qdomain.m_state : NoneType\n",
      "I0709 01:07:15.414113 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.qdomain.weight : NoneType\n",
      "I0709 01:07:15.414191 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.random_seed : NoneType\n",
      "I0709 01:07:15.414258 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.reset_cell_state : False\n",
      "I0709 01:07:15.414328 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.414394 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.vn.global_vn : False\n",
      "I0709 01:07:15.414457 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.vn.per_step_vn : False\n",
      "I0709 01:07:15.414518 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.vn.scale : NoneType\n",
      "I0709 01:07:15.414573 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.vn.seed : NoneType\n",
      "I0709 01:07:15.414638 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.zero_state_init_params.method : 'zeros'\n",
      "I0709 01:07:15.414695 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.zero_state_init_params.seed : NoneType\n",
      "I0709 01:07:15.414742 140556593907072 base_runner.py:59] task.encoder.lstm_tpl.zo_prob : 0.0\n",
      "I0709 01:07:15.414805 140556593907072 base_runner.py:59] task.encoder.name : ''\n",
      "I0709 01:07:15.414868 140556593907072 base_runner.py:59] task.encoder.num_lstm_layers : 6\n",
      "I0709 01:07:15.414936 140556593907072 base_runner.py:59] task.encoder.packed_input : False\n",
      "I0709 01:07:15.415006 140556593907072 base_runner.py:59] task.encoder.params_init.method : 'xavier'\n",
      "I0709 01:07:15.415092 140556593907072 base_runner.py:59] task.encoder.params_init.scale : 1.000001\n",
      "I0709 01:07:15.415135 140556593907072 base_runner.py:59] task.encoder.params_init.seed : NoneType\n",
      "I0709 01:07:15.415183 140556593907072 base_runner.py:59] task.encoder.proj_tpl.activation : 'RELU'\n",
      "I0709 01:07:15.415255 140556593907072 base_runner.py:59] task.encoder.proj_tpl.affine_last : False\n",
      "I0709 01:07:15.415318 140556593907072 base_runner.py:59] task.encoder.proj_tpl.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.415361 140556593907072 base_runner.py:59] task.encoder.proj_tpl.apply_pruning : False\n",
      "I0709 01:07:15.415419 140556593907072 base_runner.py:59] task.encoder.proj_tpl.batch_norm : NoneType\n",
      "I0709 01:07:15.415486 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bias_init : 0.0\n",
      "I0709 01:07:15.415550 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_fold_weights : NoneType\n",
      "I0709 01:07:15.415618 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.add_stats_to_moving_average_variables : NoneType\n",
      "I0709 01:07:15.415686 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.415740 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.cls : type/lingvo.core.bn_layers/BatchNormLayer\n",
      "I0709 01:07:15.415805 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.decay : 0.999\n",
      "I0709 01:07:15.415863 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.dim : 0\n",
      "I0709 01:07:15.415923 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.dtype : float32\n",
      "I0709 01:07:15.415986 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.enable_cross_replica_sum_on_tpu : True\n",
      "I0709 01:07:15.416078 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.fprop_dtype : NoneType\n",
      "I0709 01:07:15.416137 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.gamma_zero_init : False\n",
      "I0709 01:07:15.416203 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.inference_driver_name : NoneType\n",
      "I0709 01:07:15.416266 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.is_inference : NoneType\n",
      "I0709 01:07:15.416314 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.name : ''\n",
      "I0709 01:07:15.416378 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.params_init.method : 'xavier'\n",
      "I0709 01:07:15.416443 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.params_init.scale : 1.000001\n",
      "I0709 01:07:15.416500 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.params_init.seed : NoneType\n",
      "I0709 01:07:15.416568 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.random_seed : NoneType\n",
      "I0709 01:07:15.416625 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.set_padded_output_to_zero : True\n",
      "I0709 01:07:15.416687 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.416759 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.use_fused_batch_norm_for_eval : False\n",
      "I0709 01:07:15.416824 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.use_moving_avg_in_training : False\n",
      "I0709 01:07:15.416884 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.vn.global_vn : False\n",
      "I0709 01:07:15.416945 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.vn.per_step_vn : False\n",
      "I0709 01:07:15.417014 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.vn.scale : NoneType\n",
      "I0709 01:07:15.417080 140556593907072 base_runner.py:59] task.encoder.proj_tpl.bn_params.vn.seed : NoneType\n",
      "I0709 01:07:15.417147 140556593907072 base_runner.py:59] task.encoder.proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer\n",
      "I0709 01:07:15.417203 140556593907072 base_runner.py:59] task.encoder.proj_tpl.dtype : float32\n",
      "I0709 01:07:15.417267 140556593907072 base_runner.py:59] task.encoder.proj_tpl.fprop_dtype : NoneType\n",
      "I0709 01:07:15.417333 140556593907072 base_runner.py:59] task.encoder.proj_tpl.has_bias : False\n",
      "I0709 01:07:15.417398 140556593907072 base_runner.py:59] task.encoder.proj_tpl.inference_driver_name : NoneType\n",
      "I0709 01:07:15.417463 140556593907072 base_runner.py:59] task.encoder.proj_tpl.input_dim : 0\n",
      "I0709 01:07:15.417525 140556593907072 base_runner.py:59] task.encoder.proj_tpl.is_inference : NoneType\n",
      "I0709 01:07:15.417590 140556593907072 base_runner.py:59] task.encoder.proj_tpl.name : ''\n",
      "I0709 01:07:15.417647 140556593907072 base_runner.py:59] task.encoder.proj_tpl.output_dim : 0\n",
      "I0709 01:07:15.417717 140556593907072 base_runner.py:59] task.encoder.proj_tpl.params_init.method : 'xavier'\n",
      "I0709 01:07:15.417782 140556593907072 base_runner.py:59] task.encoder.proj_tpl.params_init.scale : 1.000001\n",
      "I0709 01:07:15.417838 140556593907072 base_runner.py:59] task.encoder.proj_tpl.params_init.seed : NoneType\n",
      "I0709 01:07:15.417896 140556593907072 base_runner.py:59] task.encoder.proj_tpl.qdomain.default : NoneType\n",
      "I0709 01:07:15.417958 140556593907072 base_runner.py:59] task.encoder.proj_tpl.random_seed : NoneType\n",
      "I0709 01:07:15.418017 140556593907072 base_runner.py:59] task.encoder.proj_tpl.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.418090 140556593907072 base_runner.py:59] task.encoder.proj_tpl.use_einsum : False\n",
      "I0709 01:07:15.418150 140556593907072 base_runner.py:59] task.encoder.proj_tpl.vn.global_vn : False\n",
      "I0709 01:07:15.418214 140556593907072 base_runner.py:59] task.encoder.proj_tpl.vn.per_step_vn : False\n",
      "I0709 01:07:15.418279 140556593907072 base_runner.py:59] task.encoder.proj_tpl.vn.scale : NoneType\n",
      "I0709 01:07:15.418344 140556593907072 base_runner.py:59] task.encoder.proj_tpl.vn.seed : NoneType\n",
      "I0709 01:07:15.418408 140556593907072 base_runner.py:59] task.encoder.proj_tpl.weight_norm : False\n",
      "I0709 01:07:15.418502 140556593907072 base_runner.py:59] task.encoder.random_seed : NoneType\n",
      "I0709 01:07:15.418554 140556593907072 base_runner.py:59] task.encoder.residual_start : 2\n",
      "I0709 01:07:15.418605 140556593907072 base_runner.py:59] task.encoder.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.418671 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.add_weight_summaries : True\n",
      "I0709 01:07:15.418739 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.418802 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.cls : type/lingvo.core.layers/WeightedSumLayer\n",
      "I0709 01:07:15.418868 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.dtype : float32\n",
      "I0709 01:07:15.418933 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.fprop_dtype : NoneType\n",
      "I0709 01:07:15.418998 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.global_weight_scale : 1.0\n",
      "I0709 01:07:15.419068 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.inference_driver_name : NoneType\n",
      "I0709 01:07:15.419133 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.is_inference : NoneType\n",
      "I0709 01:07:15.419197 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.minimal_prob : 0.0\n",
      "I0709 01:07:15.419260 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.name : ''\n",
      "I0709 01:07:15.419324 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.num_sources : 0\n",
      "I0709 01:07:15.419387 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.params_init.method : 'xavier'\n",
      "I0709 01:07:15.419453 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.params_init.scale : 1.000001\n",
      "I0709 01:07:15.419517 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.params_init.seed : NoneType\n",
      "I0709 01:07:15.419579 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.random_seed : NoneType\n",
      "I0709 01:07:15.419643 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.419708 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.vn.global_vn : False\n",
      "I0709 01:07:15.419771 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.vn.per_step_vn : False\n",
      "I0709 01:07:15.419838 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.vn.scale : NoneType\n",
      "I0709 01:07:15.419902 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.vn.seed : NoneType\n",
      "I0709 01:07:15.419966 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.weighted_merger_dropout_prob : 0.1\n",
      "I0709 01:07:15.420031 140556593907072 base_runner.py:59] task.encoder.transparent_merger_tpl.weighted_merger_softmax : True\n",
      "I0709 01:07:15.420124 140556593907072 base_runner.py:59] task.encoder.vn.global_vn : False\n",
      "I0709 01:07:15.420246 140556593907072 base_runner.py:59] task.encoder.vn.per_step_vn : False\n",
      "I0709 01:07:15.420345 140556593907072 base_runner.py:59] task.encoder.vn.scale : NoneType\n",
      "I0709 01:07:15.420414 140556593907072 base_runner.py:59] task.encoder.vn.seed : NoneType\n",
      "I0709 01:07:15.420506 140556593907072 base_runner.py:59] task.eval.decoder_samples_per_summary : 0\n",
      "I0709 01:07:15.420578 140556593907072 base_runner.py:59] task.eval.load_checkpoint_from : NoneType\n",
      "I0709 01:07:15.420643 140556593907072 base_runner.py:59] task.eval.samples_per_summary : 2466\n",
      "I0709 01:07:15.420702 140556593907072 base_runner.py:59] task.eval.start_decoder_after : 0\n",
      "I0709 01:07:15.420759 140556593907072 base_runner.py:59] task.eval.start_eval_after : 0\n",
      "I0709 01:07:15.420866 140556593907072 base_runner.py:59] task.fprop_dtype : NoneType\n",
      "I0709 01:07:15.420942 140556593907072 base_runner.py:59] task.inference_driver_name : NoneType\n",
      "I0709 01:07:15.421020 140556593907072 base_runner.py:59] task.input : NoneType\n",
      "I0709 01:07:15.421121 140556593907072 base_runner.py:59] task.is_inference : NoneType\n",
      "I0709 01:07:15.421192 140556593907072 base_runner.py:59] task.name : 'punctuator_rnmt'\n",
      "I0709 01:07:15.421259 140556593907072 base_runner.py:59] task.online_encoder : NoneType\n",
      "I0709 01:07:15.421326 140556593907072 base_runner.py:59] task.params_init.method : 'xavier'\n",
      "I0709 01:07:15.421390 140556593907072 base_runner.py:59] task.params_init.scale : 1.000001\n",
      "I0709 01:07:15.421459 140556593907072 base_runner.py:59] task.params_init.seed : NoneType\n",
      "I0709 01:07:15.421517 140556593907072 base_runner.py:59] task.random_seed : NoneType\n",
      "I0709 01:07:15.421571 140556593907072 base_runner.py:59] task.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.421638 140556593907072 base_runner.py:59] task.train.bprop_variable_exclusion : NoneType\n",
      "I0709 01:07:15.421702 140556593907072 base_runner.py:59] task.train.bprop_variable_filter : NoneType\n",
      "I0709 01:07:15.421765 140556593907072 base_runner.py:59] task.train.clip_gradient_norm_to_value : 0.0\n",
      "I0709 01:07:15.421825 140556593907072 base_runner.py:59] task.train.clip_gradient_single_norm_to_value : 0.0\n",
      "I0709 01:07:15.421891 140556593907072 base_runner.py:59] task.train.colocate_gradients_with_ops : True\n",
      "I0709 01:07:15.421954 140556593907072 base_runner.py:59] task.train.early_stop.metric_history.jobname : 'eval_dev'\n",
      "I0709 01:07:15.422025 140556593907072 base_runner.py:59] task.train.early_stop.metric_history.local_filesystem : False\n",
      "I0709 01:07:15.422100 140556593907072 base_runner.py:59] task.train.early_stop.metric_history.logdir : ''\n",
      "I0709 01:07:15.422223 140556593907072 base_runner.py:59] task.train.early_stop.metric_history.metric : 'log_pplx'\n",
      "I0709 01:07:15.422294 140556593907072 base_runner.py:59] task.train.early_stop.metric_history.minimize : True\n",
      "I0709 01:07:15.422364 140556593907072 base_runner.py:59] task.train.early_stop.metric_history.name : 'MetricHistory'\n",
      "I0709 01:07:15.422458 140556593907072 base_runner.py:59] task.train.early_stop.metric_history.tfevent_file : False\n",
      "I0709 01:07:15.422515 140556593907072 base_runner.py:59] task.train.early_stop.min_steps : 0\n",
      "I0709 01:07:15.422580 140556593907072 base_runner.py:59] task.train.early_stop.name : 'EarlyStop'\n",
      "I0709 01:07:15.422642 140556593907072 base_runner.py:59] task.train.early_stop.tolerance : 0.0\n",
      "I0709 01:07:15.422706 140556593907072 base_runner.py:59] task.train.early_stop.verbose : True\n",
      "I0709 01:07:15.422765 140556593907072 base_runner.py:59] task.train.early_stop.window : 0\n",
      "I0709 01:07:15.422861 140556593907072 base_runner.py:59] task.train.ema_decay : 0.0\n",
      "I0709 01:07:15.422932 140556593907072 base_runner.py:59] task.train.ema_decay_moving_vars : NoneType\n",
      "I0709 01:07:15.423033 140556593907072 base_runner.py:59] task.train.enqueue_max_steps : -1\n",
      "I0709 01:07:15.423132 140556593907072 base_runner.py:59] task.train.gate_gradients : False\n",
      "I0709 01:07:15.423219 140556593907072 base_runner.py:59] task.train.grad_aggregation_method : 1\n",
      "I0709 01:07:15.423273 140556593907072 base_runner.py:59] task.train.grad_norm_to_clip_to_zero : 100000.0\n",
      "I0709 01:07:15.423316 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.423379 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.clip_threshold : 4.0\n",
      "I0709 01:07:15.423445 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.cls : type/lingvo.core.layers/GradNormTracker\n",
      "I0709 01:07:15.423490 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.decay : 0.995\n",
      "I0709 01:07:15.423544 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.dtype : float32\n",
      "I0709 01:07:15.423611 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.fprop_dtype : NoneType\n",
      "I0709 01:07:15.423654 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.grad_norm_clip_cap_min : 0.0\n",
      "I0709 01:07:15.423715 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.grad_norm_lower_cap : 0.01\n",
      "I0709 01:07:15.423783 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.inference_driver_name : NoneType\n",
      "I0709 01:07:15.423839 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.is_inference : NoneType\n",
      "I0709 01:07:15.423905 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.name : 'gradient_norm_tracker'\n",
      "I0709 01:07:15.423973 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.params_init.method : 'xavier'\n",
      "I0709 01:07:15.424037 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.params_init.scale : 1.000001\n",
      "I0709 01:07:15.424126 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.params_init.seed : NoneType\n",
      "I0709 01:07:15.424196 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.random_seed : NoneType\n",
      "I0709 01:07:15.424266 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.424327 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.vn.global_vn : False\n",
      "I0709 01:07:15.424378 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.vn.per_step_vn : False\n",
      "I0709 01:07:15.424445 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.vn.scale : NoneType\n",
      "I0709 01:07:15.424511 140556593907072 base_runner.py:59] task.train.grad_norm_tracker.vn.seed : NoneType\n",
      "I0709 01:07:15.424571 140556593907072 base_runner.py:59] task.train.init_from_checkpoint_rules : {}\n",
      "I0709 01:07:15.424629 140556593907072 base_runner.py:59] task.train.l1_regularizer_weight : NoneType\n",
      "I0709 01:07:15.424678 140556593907072 base_runner.py:59] task.train.l2_regularizer_weight : 1e-05\n",
      "I0709 01:07:15.424743 140556593907072 base_runner.py:59] task.train.learner : NoneType\n",
      "I0709 01:07:15.424807 140556593907072 base_runner.py:59] task.train.learning_rate : 0.0001\n",
      "I0709 01:07:15.424870 140556593907072 base_runner.py:59] task.train.lr_schedule.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.424926 140556593907072 base_runner.py:59] task.train.lr_schedule.cls : type/lingvo.core.schedule/LinearRampupExponentialDecayScaledByNumSplitSchedule\n",
      "I0709 01:07:15.424993 140556593907072 base_runner.py:59] task.train.lr_schedule.decay_end : 1200000\n",
      "I0709 01:07:15.425072 140556593907072 base_runner.py:59] task.train.lr_schedule.decay_start : 400000\n",
      "I0709 01:07:15.425151 140556593907072 base_runner.py:59] task.train.lr_schedule.dtype : float32\n",
      "I0709 01:07:15.425204 140556593907072 base_runner.py:59] task.train.lr_schedule.fprop_dtype : NoneType\n",
      "I0709 01:07:15.425266 140556593907072 base_runner.py:59] task.train.lr_schedule.inference_driver_name : NoneType\n",
      "I0709 01:07:15.425326 140556593907072 base_runner.py:59] task.train.lr_schedule.is_inference : NoneType\n",
      "I0709 01:07:15.425389 140556593907072 base_runner.py:59] task.train.lr_schedule.max : 100000000.0\n",
      "I0709 01:07:15.425454 140556593907072 base_runner.py:59] task.train.lr_schedule.min : 0.5\n",
      "I0709 01:07:15.425518 140556593907072 base_runner.py:59] task.train.lr_schedule.name : 'LRSched'\n",
      "I0709 01:07:15.425583 140556593907072 base_runner.py:59] task.train.lr_schedule.num_splits : 0\n",
      "I0709 01:07:15.425651 140556593907072 base_runner.py:59] task.train.lr_schedule.params_init.method : 'xavier'\n",
      "I0709 01:07:15.425715 140556593907072 base_runner.py:59] task.train.lr_schedule.params_init.scale : 1.000001\n",
      "I0709 01:07:15.425761 140556593907072 base_runner.py:59] task.train.lr_schedule.params_init.seed : NoneType\n",
      "I0709 01:07:15.425825 140556593907072 base_runner.py:59] task.train.lr_schedule.random_seed : NoneType\n",
      "I0709 01:07:15.425889 140556593907072 base_runner.py:59] task.train.lr_schedule.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.425951 140556593907072 base_runner.py:59] task.train.lr_schedule.vn.global_vn : False\n",
      "I0709 01:07:15.426015 140556593907072 base_runner.py:59] task.train.lr_schedule.vn.per_step_vn : False\n",
      "I0709 01:07:15.426086 140556593907072 base_runner.py:59] task.train.lr_schedule.vn.scale : NoneType\n",
      "I0709 01:07:15.426150 140556593907072 base_runner.py:59] task.train.lr_schedule.vn.seed : NoneType\n",
      "I0709 01:07:15.426213 140556593907072 base_runner.py:59] task.train.lr_schedule.warmup : 500\n",
      "I0709 01:07:15.426269 140556593907072 base_runner.py:59] task.train.lr_schedule.warmup_init : 1.0\n",
      "I0709 01:07:15.426334 140556593907072 base_runner.py:59] task.train.max_steps : 4000000\n",
      "I0709 01:07:15.426398 140556593907072 base_runner.py:59] task.train.optimizer.allow_implicit_capture : NoneType\n",
      "I0709 01:07:15.426462 140556593907072 base_runner.py:59] task.train.optimizer.beta1 : 0.9\n",
      "I0709 01:07:15.426522 140556593907072 base_runner.py:59] task.train.optimizer.beta2 : 0.98\n",
      "I0709 01:07:15.426576 140556593907072 base_runner.py:59] task.train.optimizer.cls : type/lingvo.core.optimizer/Adam\n",
      "I0709 01:07:15.426644 140556593907072 base_runner.py:59] task.train.optimizer.dtype : float32\n",
      "I0709 01:07:15.426709 140556593907072 base_runner.py:59] task.train.optimizer.epsilon : 1e-06\n",
      "I0709 01:07:15.426769 140556593907072 base_runner.py:59] task.train.optimizer.fprop_dtype : NoneType\n",
      "I0709 01:07:15.426834 140556593907072 base_runner.py:59] task.train.optimizer.inference_driver_name : NoneType\n",
      "I0709 01:07:15.426891 140556593907072 base_runner.py:59] task.train.optimizer.is_inference : NoneType\n",
      "I0709 01:07:15.427010 140556593907072 base_runner.py:59] task.train.optimizer.name : 'Adam'\n",
      "I0709 01:07:15.427091 140556593907072 base_runner.py:59] task.train.optimizer.params_init.method : 'xavier'\n",
      "I0709 01:07:15.427188 140556593907072 base_runner.py:59] task.train.optimizer.params_init.scale : 1.000001\n",
      "I0709 01:07:15.427252 140556593907072 base_runner.py:59] task.train.optimizer.params_init.seed : NoneType\n",
      "I0709 01:07:15.427319 140556593907072 base_runner.py:59] task.train.optimizer.random_seed : NoneType\n",
      "I0709 01:07:15.427387 140556593907072 base_runner.py:59] task.train.optimizer.skip_lp_regularization : NoneType\n",
      "I0709 01:07:15.427453 140556593907072 base_runner.py:59] task.train.optimizer.use_bf16_gradients_ar : False\n",
      "I0709 01:07:15.427506 140556593907072 base_runner.py:59] task.train.optimizer.vn.global_vn : False\n",
      "I0709 01:07:15.427571 140556593907072 base_runner.py:59] task.train.optimizer.vn.per_step_vn : False\n",
      "I0709 01:07:15.427635 140556593907072 base_runner.py:59] task.train.optimizer.vn.scale : NoneType\n",
      "I0709 01:07:15.427700 140556593907072 base_runner.py:59] task.train.optimizer.vn.seed : NoneType\n",
      "I0709 01:07:15.427780 140556593907072 base_runner.py:59] task.train.pruning_hparams_dict : NoneType\n",
      "I0709 01:07:15.427839 140556593907072 base_runner.py:59] task.train.save_interval_seconds : 600\n",
      "I0709 01:07:15.427906 140556593907072 base_runner.py:59] task.train.save_keep_checkpoint_every_n_hours : 0.5\n",
      "I0709 01:07:15.427967 140556593907072 base_runner.py:59] task.train.save_max_to_keep : 100\n",
      "I0709 01:07:15.428028 140556593907072 base_runner.py:59] task.train.scale_gradients : True\n",
      "I0709 01:07:15.428117 140556593907072 base_runner.py:59] task.train.start_up_delay_steps : 200\n",
      "I0709 01:07:15.428199 140556593907072 base_runner.py:59] task.train.summary_interval_steps : 100\n",
      "I0709 01:07:15.428280 140556593907072 base_runner.py:59] task.train.tpu_steps_per_loop : 100\n",
      "I0709 01:07:15.428346 140556593907072 base_runner.py:59] task.train.vn_start_step : 200000000\n",
      "I0709 01:07:15.428412 140556593907072 base_runner.py:59] task.train.vn_std : 0.0\n",
      "I0709 01:07:15.428486 140556593907072 base_runner.py:59] task.vn.global_vn : False\n",
      "I0709 01:07:15.428546 140556593907072 base_runner.py:59] task.vn.per_step_vn : False\n",
      "I0709 01:07:15.428607 140556593907072 base_runner.py:59] task.vn.scale : NoneType\n",
      "I0709 01:07:15.428689 140556593907072 base_runner.py:59] task.vn.seed : NoneType\n",
      "I0709 01:07:15.428760 140556593907072 base_runner.py:59] train.early_stop.metric_history.jobname : 'eval_dev'\n",
      "I0709 01:07:15.428826 140556593907072 base_runner.py:59] train.early_stop.metric_history.local_filesystem : False\n",
      "I0709 01:07:15.428887 140556593907072 base_runner.py:59] train.early_stop.metric_history.logdir : ''\n",
      "I0709 01:07:15.428953 140556593907072 base_runner.py:59] train.early_stop.metric_history.metric : 'log_pplx'\n",
      "I0709 01:07:15.429018 140556593907072 base_runner.py:59] train.early_stop.metric_history.minimize : True\n",
      "I0709 01:07:15.429082 140556593907072 base_runner.py:59] train.early_stop.metric_history.name : 'MetricHistory'\n",
      "I0709 01:07:15.429145 140556593907072 base_runner.py:59] train.early_stop.metric_history.tfevent_file : False\n",
      "I0709 01:07:15.429210 140556593907072 base_runner.py:59] train.early_stop.min_steps : 0\n",
      "I0709 01:07:15.429284 140556593907072 base_runner.py:59] train.early_stop.name : 'EarlyStop'\n",
      "I0709 01:07:15.429369 140556593907072 base_runner.py:59] train.early_stop.tolerance : 0.0\n",
      "I0709 01:07:15.429427 140556593907072 base_runner.py:59] train.early_stop.verbose : True\n",
      "I0709 01:07:15.429505 140556593907072 base_runner.py:59] train.early_stop.window : 0\n",
      "I0709 01:07:15.429565 140556593907072 base_runner.py:59] train.ema_decay : 0.0\n",
      "I0709 01:07:15.429628 140556593907072 base_runner.py:59] train.ema_decay_moving_vars : NoneType\n",
      "I0709 01:07:15.429691 140556593907072 base_runner.py:59] train.enqueue_max_steps : -1\n",
      "I0709 01:07:15.429753 140556593907072 base_runner.py:59] train.init_from_checkpoint_rules : {}\n",
      "I0709 01:07:15.429814 140556593907072 base_runner.py:59] train.max_steps : 4000000\n",
      "I0709 01:07:15.429895 140556593907072 base_runner.py:59] train.save_interval_seconds : 600\n",
      "I0709 01:07:15.429967 140556593907072 base_runner.py:59] train.save_keep_checkpoint_every_n_hours : 0.5\n",
      "I0709 01:07:15.430030 140556593907072 base_runner.py:59] train.save_max_to_keep : 100\n",
      "I0709 01:07:15.430104 140556593907072 base_runner.py:59] train.start_up_delay_steps : 200\n",
      "I0709 01:07:15.430235 140556593907072 base_runner.py:59] train.summary_interval_steps : 100\n",
      "I0709 01:07:15.430322 140556593907072 base_runner.py:59] train.tpu_steps_per_loop : 100\n",
      "I0709 01:07:15.430412 140556593907072 base_runner.py:59] vn.global_vn : False\n",
      "I0709 01:07:15.430477 140556593907072 base_runner.py:59] vn.per_step_vn : False\n",
      "I0709 01:07:15.430541 140556593907072 base_runner.py:59] vn.scale : NoneType\n",
      "I0709 01:07:15.430608 140556593907072 base_runner.py:59] vn.seed : NoneType\n",
      "I0709 01:07:15.430671 140556593907072 base_runner.py:59] \n",
      "I0709 01:07:15.430762 140556593907072 base_runner.py:60] ============================================================\n",
      "I0709 01:07:15.434077 140556593907072 base_runner.py:111] Starting ...\n",
      "I0709 01:07:15.482718 140556593907072 base_model.py:1068] Training parameters for <class 'lingvo.core.base_model.SingleTaskModel'>: {\n",
      "  early_stop: {\n",
      "    metric_history: {\n",
      "      jobname: \"eval_dev\"\n",
      "      local_filesystem: False\n",
      "      logdir: \"/tmp/punctuator\"\n",
      "      metric: \"log_pplx\"\n",
      "      minimize: True\n",
      "      name: \"MetricHistory\"\n",
      "      tfevent_file: False\n",
      "    }\n",
      "    min_steps: 0\n",
      "    name: \"EarlyStop\"\n",
      "    tolerance: 0.0\n",
      "    verbose: True\n",
      "    window: 0\n",
      "  }\n",
      "  ema_decay: 0.0\n",
      "  ema_decay_moving_vars: None\n",
      "  enqueue_max_steps: -1\n",
      "  init_from_checkpoint_rules: {}\n",
      "  max_steps: 4000000\n",
      "  save_interval_seconds: 600\n",
      "  save_keep_checkpoint_every_n_hours: 0.5\n",
      "  save_max_to_keep: 100\n",
      "  start_up_delay_steps: 200\n",
      "  summary_interval_steps: 100\n",
      "  tpu_steps_per_loop: 100\n",
      "}\n",
      "I0709 01:07:15.498249 140556593907072 base_model.py:280] input_params: {\n",
      "  allow_implicit_capture: None\n",
      "  bucket_adjust_every_n: 0\n",
      "  bucket_batch_limit: [16, 16, 16, 16, 4, 4]\n",
      "  bucket_upper_bound: [10, 20, 30, 60, 120, 200]\n",
      "  cls: <class 'input_generator.PunctuatorInput'>\n",
      "  dtype: <dtype: 'float32'>\n",
      "  file_buffer_size: 1\n",
      "  file_buffer_size_in_seconds: 0\n",
      "  file_datasource: None\n",
      "  file_parallelism: 1\n",
      "  file_pattern: \"text:/tmp/punctuator_data/test.txt\"\n",
      "  file_random_seed: 27182818\n",
      "  flush_every_n: 0\n",
      "  fprop_dtype: None\n",
      "  inference_driver_name: None\n",
      "  is_inference: None\n",
      "  name: \"input\"\n",
      "  num_batcher_threads: 1\n",
      "  num_partitions: None\n",
      "  num_samples: 0\n",
      "  pad_to_max_seq_length: False\n",
      "  params_init: {\n",
      "    method: \"xavier\"\n",
      "    scale: 1.000001\n",
      "    seed: None\n",
      "  }\n",
      "  random_seed: None\n",
      "  remote: {\n",
      "    max_inflights_per_target: 32\n",
      "    shardable_batch: False\n",
      "  }\n",
      "  repeat_count: -1\n",
      "  require_sequential_order: False\n",
      "  skip_lp_regularization: None\n",
      "  source_max_length: 202\n",
      "  target_max_length: 202\n",
      "  tokenizer: {\n",
      "    allow_implicit_capture: None\n",
      "    append_eos: True\n",
      "    cls: <class 'lingvo.core.tokenizers.WpmTokenizer'>\n",
      "    dtype: <dtype: 'float32'>\n",
      "    fprop_dtype: None\n",
      "    inference_driver_name: None\n",
      "    is_inference: None\n",
      "    merge_prob: 1.0\n",
      "    name: \"tokenizer\"\n",
      "    pad_to_max_length: False\n",
      "    params_init: {\n",
      "      method: \"xavier\"\n",
      "      scale: 1.000001\n",
      "      seed: None\n",
      "    }\n",
      "    random_seed: None\n",
      "    skip_lp_regularization: None\n",
      "    target_eos_id: 2\n",
      "    target_sos_id: 1\n",
      "    target_unk_id: 0\n",
      "    target_wb_id: -1\n",
      "    vn: {\n",
      "      global_vn: False\n",
      "      per_step_vn: False\n",
      "      scale: None\n",
      "      seed: None\n",
      "    }\n",
      "    vocab_filepath: \"brown_corpus_wpm.16000.vocab\"\n",
      "    vocab_size: 16000\n",
      "  }\n",
      "  tokenizer_dict: {}\n",
      "  tpu_infeed_parallelism: 1\n",
      "  use_chaining: False\n",
      "  use_partitioned_infeed_queue: False\n",
      "  use_per_host_infeed: False\n",
      "  use_within_batch_mixing: False\n",
      "  vn: {\n",
      "    global_vn: False\n",
      "    per_step_vn: False\n",
      "    scale: None\n",
      "    seed: None\n",
      "  }\n",
      "}\n",
      "I0709 01:07:15.559511 140556593907072 base_input_generator.py:673] Building data source <lingvo.core.datasource.SimpleDataSource object at 0x7fd5544ed390> with params {\n",
      "  allow_implicit_capture: None\n",
      "  cls: <class 'lingvo.core.datasource.SimpleDataSource'>\n",
      "  dtype: <dtype: 'float32'>\n",
      "  file_pattern: \"text:/tmp/punctuator_data/test.txt\"\n",
      "  file_type: \"\"\n",
      "  fprop_dtype: None\n",
      "  inference_driver_name: None\n",
      "  is_inference: None\n",
      "  name: \"datasource\"\n",
      "  params_init: {\n",
      "    method: \"xavier\"\n",
      "    scale: 1.000001\n",
      "    seed: None\n",
      "  }\n",
      "  random_seed: None\n",
      "  skip_lp_regularization: None\n",
      "  vn: {\n",
      "    global_vn: False\n",
      "    per_step_vn: False\n",
      "    scale: None\n",
      "    seed: None\n",
      "  }\n",
      "} and file_pattern text:/tmp/punctuator_data/test.txt\n",
      "I0709 01:07:15.559844 140556593907072 base_input_generator.py:775] infeed_bucket_batch_limit=[16, 16, 16, 16, 4, 4] num_splits_per_client=1 bucket_batch_limit=[16, 16, 16, 16, 4, 4]\n",
      "I0709 01:07:15.559912 140556593907072 base_input_generator.py:798] infeed_bucket_batch_limit [16, 16, 16, 16, 4, 4]\n",
      "2020-07-09 01:07:15.892608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.893721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2020-07-09 01:07:15.893859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.894878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \n",
      "pciBusID: 0000:00:05.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2020-07-09 01:07:15.894961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.895986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 2 with properties: \n",
      "pciBusID: 0000:00:06.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2020-07-09 01:07:15.896087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.897105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 3 with properties: \n",
      "pciBusID: 0000:00:07.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2020-07-09 01:07:15.897170: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-07-09 01:07:15.897193: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-07-09 01:07:15.897212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-07-09 01:07:15.897231: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-07-09 01:07:15.897249: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-07-09 01:07:15.897267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-07-09 01:07:15.897286: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-07-09 01:07:15.897353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.898421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.899474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.900577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.901642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.902704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.903745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.904805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-09 01:07:15.905826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1, 2, 3\n",
      "I0709 01:07:17.566511 140556593907072 learner.py:383] Ignoring legacy param start_up_delay_steps=200 for optimization program\n",
      "I0709 01:07:17.566695 140556593907072 learner.py:383] Ignoring legacy param max_steps=4000000 for optimization program\n",
      "I0709 01:07:17.566756 140556593907072 learner.py:383] Ignoring legacy param tpu_steps_per_loop=100 for optimization program\n",
      "I0709 01:07:17.566825 140556593907072 learner.py:383] Ignoring legacy param vn_start_step=200000000 for optimization program\n",
      "I0709 01:07:17.566885 140556593907072 learner.py:383] Ignoring legacy param vn_std=0.0 for optimization program\n",
      "I0709 01:07:17.566931 140556593907072 learner.py:383] Ignoring legacy param early_stop={\n",
      "  metric_history: {\n",
      "    jobname: \"eval_dev\"\n",
      "    local_filesystem: False\n",
      "    logdir: \"/tmp/punctuator\"\n",
      "    metric: \"log_pplx\"\n",
      "    minimize: True\n",
      "    name: \"MetricHistory\"\n",
      "    tfevent_file: False\n",
      "  }\n",
      "  min_steps: 0\n",
      "  name: \"EarlyStop\"\n",
      "  tolerance: 0.0\n",
      "  verbose: True\n",
      "  window: 0\n",
      "} for optimization program\n",
      "I0709 01:07:17.567040 140556593907072 learner.py:383] Ignoring legacy param ema_decay=0.0 for optimization program\n",
      "I0709 01:07:17.567087 140556593907072 learner.py:383] Ignoring legacy param ema_decay_moving_vars=None for optimization program\n",
      "I0709 01:07:17.567129 140556593907072 learner.py:383] Ignoring legacy param init_from_checkpoint_rules={} for optimization program\n",
      "I0709 01:07:17.567172 140556593907072 learner.py:383] Ignoring legacy param pruning_hparams_dict=None for optimization program\n",
      "I0709 01:07:17.567213 140556593907072 learner.py:383] Ignoring legacy param enqueue_max_steps=-1 for optimization program\n",
      "I0709 01:07:17.567253 140556593907072 learner.py:383] Ignoring legacy param save_interval_seconds=600 for optimization program\n",
      "I0709 01:07:17.567293 140556593907072 learner.py:383] Ignoring legacy param save_max_to_keep=100 for optimization program\n",
      "I0709 01:07:17.567333 140556593907072 learner.py:383] Ignoring legacy param save_keep_checkpoint_every_n_hours=0.5 for optimization program\n",
      "I0709 01:07:17.567376 140556593907072 learner.py:383] Ignoring legacy param summary_interval_steps=100 for optimization program\n",
      "I0709 01:07:17.567416 140556593907072 learner.py:383] Ignoring legacy param learner=None for optimization program\n",
      "I0709 01:07:17.568015 140556593907072 learner.py:388] Learner params: allow_implicit_capture : NoneType\n",
      "I0709 01:07:17.568118 140556593907072 learner.py:388] Learner params: bprop_variable_exclusion : NoneType\n",
      "I0709 01:07:17.568170 140556593907072 learner.py:388] Learner params: bprop_variable_filter : NoneType\n",
      "I0709 01:07:17.568215 140556593907072 learner.py:388] Learner params: clip_gradient_norm_to_value : 0.0\n",
      "I0709 01:07:17.568257 140556593907072 learner.py:388] Learner params: clip_gradient_single_norm_to_value : 0.0\n",
      "I0709 01:07:17.568298 140556593907072 learner.py:388] Learner params: cls : type/lingvo.core.learner/Learner\n",
      "I0709 01:07:17.568338 140556593907072 learner.py:388] Learner params: colocate_gradients_with_ops : True\n",
      "I0709 01:07:17.568378 140556593907072 learner.py:388] Learner params: dtype : float32\n",
      "I0709 01:07:17.568418 140556593907072 learner.py:388] Learner params: fprop_dtype : NoneType\n",
      "I0709 01:07:17.568458 140556593907072 learner.py:388] Learner params: gate_gradients : False\n",
      "I0709 01:07:17.568497 140556593907072 learner.py:388] Learner params: grad_aggregation_method : 1\n",
      "I0709 01:07:17.568537 140556593907072 learner.py:388] Learner params: grad_norm_to_clip_to_zero : 100000.0\n",
      "I0709 01:07:17.568577 140556593907072 learner.py:388] Learner params: grad_norm_tracker.allow_implicit_capture : NoneType\n",
      "I0709 01:07:17.568617 140556593907072 learner.py:388] Learner params: grad_norm_tracker.clip_threshold : 4.0\n",
      "I0709 01:07:17.568657 140556593907072 learner.py:388] Learner params: grad_norm_tracker.cls : type/lingvo.core.layers/GradNormTracker\n",
      "I0709 01:07:17.568697 140556593907072 learner.py:388] Learner params: grad_norm_tracker.decay : 0.995\n",
      "I0709 01:07:17.568737 140556593907072 learner.py:388] Learner params: grad_norm_tracker.dtype : float32\n",
      "I0709 01:07:17.568777 140556593907072 learner.py:388] Learner params: grad_norm_tracker.fprop_dtype : NoneType\n",
      "I0709 01:07:17.568816 140556593907072 learner.py:388] Learner params: grad_norm_tracker.grad_norm_clip_cap_min : 0.0\n",
      "I0709 01:07:17.568856 140556593907072 learner.py:388] Learner params: grad_norm_tracker.grad_norm_lower_cap : 0.01\n",
      "I0709 01:07:17.568896 140556593907072 learner.py:388] Learner params: grad_norm_tracker.inference_driver_name : NoneType\n",
      "I0709 01:07:17.568936 140556593907072 learner.py:388] Learner params: grad_norm_tracker.is_inference : NoneType\n",
      "I0709 01:07:17.568975 140556593907072 learner.py:388] Learner params: grad_norm_tracker.name : 'gradient_norm_tracker'\n",
      "I0709 01:07:17.569022 140556593907072 learner.py:388] Learner params: grad_norm_tracker.params_init.method : 'xavier'\n",
      "I0709 01:07:17.569062 140556593907072 learner.py:388] Learner params: grad_norm_tracker.params_init.scale : 1.000001\n",
      "I0709 01:07:17.569102 140556593907072 learner.py:388] Learner params: grad_norm_tracker.params_init.seed : NoneType\n",
      "I0709 01:07:17.569141 140556593907072 learner.py:388] Learner params: grad_norm_tracker.random_seed : NoneType\n",
      "I0709 01:07:17.569181 140556593907072 learner.py:388] Learner params: grad_norm_tracker.skip_lp_regularization : NoneType\n",
      "I0709 01:07:17.569221 140556593907072 learner.py:388] Learner params: grad_norm_tracker.vn.global_vn : False\n",
      "I0709 01:07:17.569261 140556593907072 learner.py:388] Learner params: grad_norm_tracker.vn.per_step_vn : False\n",
      "I0709 01:07:17.569300 140556593907072 learner.py:388] Learner params: grad_norm_tracker.vn.scale : NoneType\n",
      "I0709 01:07:17.569340 140556593907072 learner.py:388] Learner params: grad_norm_tracker.vn.seed : NoneType\n",
      "I0709 01:07:17.569380 140556593907072 learner.py:388] Learner params: inference_driver_name : NoneType\n",
      "I0709 01:07:17.569419 140556593907072 learner.py:388] Learner params: is_inference : NoneType\n",
      "I0709 01:07:17.569459 140556593907072 learner.py:388] Learner params: l1_regularizer_weight : NoneType\n",
      "I0709 01:07:17.569499 140556593907072 learner.py:388] Learner params: l2_regularizer_weight : 1e-05\n",
      "I0709 01:07:17.569539 140556593907072 learner.py:388] Learner params: learning_rate : 0.0001\n",
      "I0709 01:07:17.569579 140556593907072 learner.py:388] Learner params: lr_schedule.allow_implicit_capture : NoneType\n",
      "I0709 01:07:17.569618 140556593907072 learner.py:388] Learner params: lr_schedule.cls : type/lingvo.core.schedule/LinearRampupExponentialDecayScaledByNumSplitSchedule\n",
      "I0709 01:07:17.569658 140556593907072 learner.py:388] Learner params: lr_schedule.decay_end : 1200000\n",
      "I0709 01:07:17.569698 140556593907072 learner.py:388] Learner params: lr_schedule.decay_start : 400000\n",
      "I0709 01:07:17.569737 140556593907072 learner.py:388] Learner params: lr_schedule.dtype : float32\n",
      "I0709 01:07:17.569777 140556593907072 learner.py:388] Learner params: lr_schedule.fprop_dtype : NoneType\n",
      "I0709 01:07:17.569836 140556593907072 learner.py:388] Learner params: lr_schedule.inference_driver_name : NoneType\n",
      "I0709 01:07:17.569899 140556593907072 learner.py:388] Learner params: lr_schedule.is_inference : NoneType\n",
      "I0709 01:07:17.569940 140556593907072 learner.py:388] Learner params: lr_schedule.max : 100000000.0\n",
      "I0709 01:07:17.569981 140556593907072 learner.py:388] Learner params: lr_schedule.min : 0.5\n",
      "I0709 01:07:17.570028 140556593907072 learner.py:388] Learner params: lr_schedule.name : 'LRSched'\n",
      "I0709 01:07:17.570068 140556593907072 learner.py:388] Learner params: lr_schedule.num_splits : 0\n",
      "I0709 01:07:17.570109 140556593907072 learner.py:388] Learner params: lr_schedule.params_init.method : 'xavier'\n",
      "I0709 01:07:17.570150 140556593907072 learner.py:388] Learner params: lr_schedule.params_init.scale : 1.000001\n",
      "I0709 01:07:17.570191 140556593907072 learner.py:388] Learner params: lr_schedule.params_init.seed : NoneType\n",
      "I0709 01:07:17.570244 140556593907072 learner.py:388] Learner params: lr_schedule.random_seed : NoneType\n",
      "I0709 01:07:17.570284 140556593907072 learner.py:388] Learner params: lr_schedule.skip_lp_regularization : NoneType\n",
      "I0709 01:07:17.570324 140556593907072 learner.py:388] Learner params: lr_schedule.vn.global_vn : False\n",
      "I0709 01:07:17.570363 140556593907072 learner.py:388] Learner params: lr_schedule.vn.per_step_vn : False\n",
      "I0709 01:07:17.570403 140556593907072 learner.py:388] Learner params: lr_schedule.vn.scale : NoneType\n",
      "I0709 01:07:17.570442 140556593907072 learner.py:388] Learner params: lr_schedule.vn.seed : NoneType\n",
      "I0709 01:07:17.570482 140556593907072 learner.py:388] Learner params: lr_schedule.warmup : 500\n",
      "I0709 01:07:17.570522 140556593907072 learner.py:388] Learner params: lr_schedule.warmup_init : 1.0\n",
      "I0709 01:07:17.570561 140556593907072 learner.py:388] Learner params: name : 'loss'\n",
      "I0709 01:07:17.570619 140556593907072 learner.py:388] Learner params: optimizer.allow_implicit_capture : NoneType\n",
      "I0709 01:07:17.570666 140556593907072 learner.py:388] Learner params: optimizer.beta1 : 0.9\n",
      "I0709 01:07:17.570720 140556593907072 learner.py:388] Learner params: optimizer.beta2 : 0.98\n",
      "I0709 01:07:17.570759 140556593907072 learner.py:388] Learner params: optimizer.cls : type/lingvo.core.optimizer/Adam\n",
      "I0709 01:07:17.570799 140556593907072 learner.py:388] Learner params: optimizer.dtype : float32\n",
      "I0709 01:07:17.570838 140556593907072 learner.py:388] Learner params: optimizer.epsilon : 1e-06\n",
      "I0709 01:07:17.570878 140556593907072 learner.py:388] Learner params: optimizer.fprop_dtype : NoneType\n",
      "I0709 01:07:17.570917 140556593907072 learner.py:388] Learner params: optimizer.inference_driver_name : NoneType\n",
      "I0709 01:07:17.570992 140556593907072 learner.py:388] Learner params: optimizer.is_inference : NoneType\n",
      "I0709 01:07:17.571036 140556593907072 learner.py:388] Learner params: optimizer.name : 'Adam'\n",
      "I0709 01:07:17.571077 140556593907072 learner.py:388] Learner params: optimizer.params_init.method : 'xavier'\n",
      "I0709 01:07:17.571118 140556593907072 learner.py:388] Learner params: optimizer.params_init.scale : 1.000001\n",
      "I0709 01:07:17.571160 140556593907072 learner.py:388] Learner params: optimizer.params_init.seed : NoneType\n",
      "I0709 01:07:17.571201 140556593907072 learner.py:388] Learner params: optimizer.random_seed : NoneType\n",
      "I0709 01:07:17.571242 140556593907072 learner.py:388] Learner params: optimizer.skip_lp_regularization : NoneType\n",
      "I0709 01:07:17.571284 140556593907072 learner.py:388] Learner params: optimizer.use_bf16_gradients_ar : False\n",
      "I0709 01:07:17.571325 140556593907072 learner.py:388] Learner params: optimizer.vn.global_vn : False\n",
      "I0709 01:07:17.571366 140556593907072 learner.py:388] Learner params: optimizer.vn.per_step_vn : False\n",
      "I0709 01:07:17.571407 140556593907072 learner.py:388] Learner params: optimizer.vn.scale : NoneType\n",
      "I0709 01:07:17.571448 140556593907072 learner.py:388] Learner params: optimizer.vn.seed : NoneType\n",
      "I0709 01:07:17.571489 140556593907072 learner.py:388] Learner params: params_init.method : 'xavier'\n",
      "I0709 01:07:17.571541 140556593907072 learner.py:388] Learner params: params_init.scale : 1.000001\n",
      "I0709 01:07:17.571580 140556593907072 learner.py:388] Learner params: params_init.seed : NoneType\n",
      "I0709 01:07:17.571639 140556593907072 learner.py:388] Learner params: random_seed : NoneType\n",
      "I0709 01:07:17.571692 140556593907072 learner.py:388] Learner params: scale_gradients : True\n",
      "I0709 01:07:17.571732 140556593907072 learner.py:388] Learner params: skip_lp_regularization : NoneType\n",
      "I0709 01:07:17.571772 140556593907072 learner.py:388] Learner params: skip_zero_gradients : NoneType\n",
      "I0709 01:07:17.571811 140556593907072 learner.py:388] Learner params: vn.global_vn : False\n",
      "I0709 01:07:17.571850 140556593907072 learner.py:388] Learner params: vn.per_step_vn : False\n",
      "I0709 01:07:17.571889 140556593907072 learner.py:388] Learner params: vn.scale : NoneType\n",
      "I0709 01:07:17.571928 140556593907072 learner.py:388] Learner params: vn.seed : NoneType\n",
      "I0709 01:07:17.571968 140556593907072 learner.py:388] Learner params: \n",
      "I0709 01:07:17.577763 140556593907072 py_utils.py:1694] Creating var punctuator_rnmt/gradient_norm_tracker/log_mean/var:0 shape=() on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0709 01:07:17.581812 140556593907072 py_utils.py:1694] Creating var punctuator_rnmt/gradient_norm_tracker/log_mean_squared/var:0 shape=() on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0709 01:07:17.585772 140556593907072 py_utils.py:1694] Creating var punctuator_rnmt/gradient_norm_tracker/total_weight/var:0 shape=() on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "I0709 01:07:17.589769 140556593907072 py_utils.py:1694] Creating var punctuator_rnmt/gradient_norm_tracker/total_rejections/var:0 shape=() on device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1862, in <module>\n",
      "    tf.app.run(main)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 299, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1853, in main\n",
      "    RunnerManager(FLAGS.model).Start()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1849, in Start\n",
      "    self.StartRunners(self.CreateRunners(FLAGS.job.split(','), FLAGS.logdir))\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1593, in CreateRunners\n",
      "    trial)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 1556, in _CreateRunner\n",
      "    return self.Evaler(dataset_name.lower(), cfg, *common_args)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/trainer.py\", line 960, in __init__\n",
      "    self._model = self.params.Instantiate()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/hyperparams.py\", line 848, in Instantiate\n",
      "    return self.cls(self)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 1225, in __init__\n",
      "    self.CreateChild('_task', p.task)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 693, in CreateChild\n",
      "    child = p.Instantiate()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/hyperparams.py\", line 848, in Instantiate\n",
      "    return self.cls(self)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/tasks/mt/model.py\", line 53, in __init__\n",
      "    super(MTBaseModel, self).__init__(params)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_model.py\", line 321, in __init__\n",
      "    self.CreateChildren('learners', [tp.learner])\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 739, in CreateChildren\n",
      "    child_scopes)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 735, in CreateChildrenHelper\n",
      "    children.append(p.Instantiate())\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/hyperparams.py\", line 848, in Instantiate\n",
      "    return self.cls(self)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/learner.py\", line 113, in __init__\n",
      "    self.CreateChild('lr_schedule', p.lr_schedule)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 693, in CreateChild\n",
      "    child = p.Instantiate()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/hyperparams.py\", line 848, in Instantiate\n",
      "    return self.cls(self)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/base_layer.py\", line 124, in wrapper\n",
      "    func(self, *args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/lingvo/core/schedule.py\", line 424, in __init__\n",
      "    assert cluster_params.mode == 'sync'\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "!python3 -m lingvo.trainer --model=codelab.RNMTModel --job=evaler_test --logdir=/tmp/punctuator --run_locally=cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wwqXVIaG2hA_"
   },
   "source": [
    "There is also a Decoder job that can be run the same way. The difference between the Evaler and Decoder varies by model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1V7f-Oxw9vp"
   },
   "source": [
    "## Model Inference\n",
    "\n",
    "After the model has been trained for around 10-20k steps (a few hours on GPU), its inference graph can be used to interact with the model using arbitrary inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "7KU0zeUTKS9Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she asked do you know the way to san jose\n",
      "Using checkpoint /tmp/punctuator/train/ckpt-00009607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_imports.py: Importing codelab\n",
      "model_imports.py: Imported codelab\n",
      "WARNING:absl:WARNING!!! var w is using the default xavier initializer. Make sure this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/punctuator/train/ckpt-00009607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/punctuator/train/ckpt-00009607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.16266: b'She asked, do you know the way to San Jose.'\n",
      "-6.52501: b'She asked do you know the way to San Jose.'\n",
      "-6.75966: b'She asked, do you know the way to San Dose.'\n",
      "-7.14988: b'She asked do you know the way to San Dose.'\n",
      "-7.19616: b'She asked, do you know the way to James Jose.'\n",
      "-7.38158: b'She asked, \"I you know the way to San Jose.'\n",
      "-7.42551: b'She asked: do you know the way to San Jose.'\n",
      "-7.77516: b'She asked, do you know the way to Johnson Jose.'\n",
      "-7.78337: b'She asked, do you know the way to Samuel Jose.'\n",
      "-7.97293: b'She asked, do you know the way to Alfred Jose.'\n",
      "-8.01028: b'She asked: do you know the way to San Dose.'\n",
      "-8.01129: b'She asked, \"I you know the way to San Dose.'\n",
      "-8.02825: b'She asked, do you know the way to James Dose.'\n",
      "-8.08241: b'She asked, do you know the way to Mike Jose.'\n",
      "-8.15518: b'She asked, do you know the way to San Dose\".'\n",
      "-8.22986: b'She asked, do you know the way to San Jose\".'\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "from lingvo import compat as tf\n",
    "from lingvo import model_imports\n",
    "from lingvo import model_registry\n",
    "from lingvo.core import inference_graph_exporter\n",
    "from lingvo.core import predictor\n",
    "from lingvo.core.ops.hyps_pb2 import Hypothesis\n",
    "\n",
    "tf.flags.FLAGS.mark_as_parsed()\n",
    "\n",
    "\n",
    "src = \"she asked do you know the way to san jose\" #@param {type:'string'}\n",
    "src = src.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "print(src)\n",
    "\n",
    "checkpoint = tf.train.latest_checkpoint('/tmp/punctuator/train')\n",
    "print('Using checkpoint %s' % checkpoint)\n",
    "\n",
    "# Run inference\n",
    "params = model_registry.GetParams('codelab.RNMTModel', 'Test')\n",
    "inference_graph = inference_graph_exporter.InferenceGraphExporter.Export(params)\n",
    "pred = predictor.Predictor(inference_graph, \n",
    "                           checkpoint=checkpoint, \n",
    "                           device_type='cpu')\n",
    "src_ids, decoded, scores, hyps = pred.Run(\n",
    "    ['src_ids', 'topk_decoded', 'topk_scores', 'topk_hyps'], src_strings=[src])\n",
    "# print(src_ids[0])\n",
    "for text, score in zip(decoded[0].tolist(), scores[0].tolist()):\n",
    "  print(\"%.5f: %s\" % (score, text))\n",
    "# for i, hyp in enumerate(hyps[0]):\n",
    "#   print(\"=======hyp %d=======\" % i)\n",
    "#   print(Hypothesis().FromString(hyp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBDiPV07mFmX"
   },
   "source": [
    "Footnote: One might wonder why our result places the question mark outside the quotation. This happens because the Brown corpus follows a 1959 US Patent Office precedure of transliterating texts into punch cards, where the closing question mark is always punched before the punctuation, so that the punctuation mark occurs at the end of the sentence. See [this link](http://clu.uni.no/icame/manuals/BROWN/INDEX.HTM) for more details. Our result is just following this pattern in our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_hW7aTI8fM4O"
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "For more advanced topics or to get a deeper understanding of Lingvo beyond this codelab, see the [paper](https://arxiv.org/abs/1902.08295)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Introduction to Lingvo",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
